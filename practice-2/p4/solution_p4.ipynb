{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2 Problem 4 Solutions\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Both forward and backward passes are a part of the backpropagation\n",
    "algorithm.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (a).\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "Which of the following is the best option that can be done to reduce a\n",
    "model’s bias?\n",
    "\n",
    "1.  Add more input features.\n",
    "\n",
    "2.  Standardize/normalize the data.\n",
    "\n",
    "3.  Add regularization.\n",
    "\n",
    "4.  Collect more data.\n",
    "\n",
    "**Solution:** The solution is (a).\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Draw the maximum margin separating boundary between the hollow and\n",
    "filled points.\n",
    "\n",
    "<img src=\"img/q3.png\" width=\"450px\">\n",
    "\n",
    "A Cartesian coordinate system is shown with an x-axis ranging from 0 to\n",
    "4 and a y-axis ranging from 0 to 4. Grid lines are present at 0.5 unit\n",
    "intervals.\n",
    "\n",
    "There are two types of points plotted:\n",
    "\n",
    "**Hollow points (circles):** - $(1, 2)$ - $(2, 3)$ - $(2, 4)$ -\n",
    "$(3, 3)$ - $(4, 3.5)$\n",
    "\n",
    "**Filled points (solid dots):** - $(0, 0)$ - $(1, 0.25)$ - $(2, 0.5)$ -\n",
    "$(2, 1)$ - $(3, 0.5)$\n",
    "\n",
    "**Explanation:** The solution for part 2 is (red = actual,\n",
    "purple=acceptable):\n",
    "\n",
    "<img src=\"img/q3_answer.png\" width=\"350px\">\n",
    "\n",
    "A second Cartesian coordinate system is shown, identical in scale and\n",
    "points to the first.\n",
    "\n",
    "All the hollow and filled points are plotted as described above.\n",
    "\n",
    "A red line, representing the “actual” maximum margin separating\n",
    "boundary, is drawn. This line appears to pass through approximately\n",
    "$(0, 0.5)$, $(1, 1)$, $(2, 1.5)$, $(3, 2)$, and $(4, 2.5)$. The equation\n",
    "of this line can be approximated as $y = 0.5x + 0.5$.\n",
    "\n",
    "A purple shaded band, representing the “acceptable” region for the\n",
    "separating boundary, surrounds the red line. The lower boundary of this\n",
    "purple band appears to pass through approximately $(0, 0.25)$,\n",
    "$(1, 0.75)$, $(2, 1.25)$, $(3, 1.75)$, and $(4, 2.25)$, which can be\n",
    "approximated as $y = 0.5x + 0.25$. The upper boundary of the purple band\n",
    "appears to pass through approximately $(0, 0.75)$, $(1, 1.25)$,\n",
    "$(2, 1.75)$, $(3, 2.25)$, and $(4, 2.75)$, which can be approximated as\n",
    "$y = 0.5x + 0.75$. The red line is centered within this purple band.\n",
    "\n",
    "The hollow points are all above the upper boundary of the purple band,\n",
    "and the filled points are all below the lower boundary of the purple\n",
    "band, indicating a clear separation.\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Fix a kernel $K$ and corresponding feature map $\\phi$. True/False: One\n",
    "can train and evaluate a kernelized SVM (with this kernel) in polynomial\n",
    "time only if $\\phi(x)$ runs in polynomial time for every $x$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Extra credit:** explain your answer.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "Consider a data matrix $X \\in \\mathbb{R}^{n \\times d}$. What is the\n",
    "smallest upper bound on $\\operatorname{rank}(X)$ which holds for every\n",
    "$X$?\n",
    "\n",
    "**Answer:** $\\operatorname{rank}(X) \\le \\min(n, d)$\n",
    "\n",
    "**Explanation:** The answer is $\\operatorname{rank}(X) \\le \\min(n, d)$.\n",
    "We won’t accept $n, d$ or really anything else which isn’t equivalent to\n",
    "$n, d$.\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "Consider a kernel matrix $P$ that is given by\n",
    "$P_{ij} = \\langle\\phi(x_i), \\phi(x_j)\\rangle$ for a kernel map $\\phi$,\n",
    "inner product $\\langle\\cdot, \\cdot\\rangle$, and data samples\n",
    "$x_i, x_j \\in \\mathbb{R}^d$. Write the closed-form solution for the\n",
    "$\\hat{\\alpha}$ that minimizes the loss function\n",
    "$L(\\alpha) = \\|y - P\\alpha\\|_2^2 + \\lambda\\alpha^T P\\alpha$.\n",
    "\n",
    "**Answer:** $\\hat{\\alpha} = (P + \\lambda I)^{-1}y$\n",
    "\n",
    "**Explanation:** The answer is $\\hat{\\alpha} = (P + \\lambda I)^{-1}y$\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "You have a batch of size $N$ $256 \\times 256$ RGB images as your input.\n",
    "The input tensor your neural network has the shape $(N, 3, 256, 256)$.\n",
    "You pass your input through a convolutional layer like below:\n",
    "\n",
    "`Conv2d(in_channels=3, out_channels=28, kernel_size=9, stride=1, padding=1)`\n",
    "\n",
    "What is the shape of your output tensor?\n",
    "\n",
    "Answer: (\\_\\_\\_\\_, \\_\\_\\_\\_, \\_\\_\\_\\_, \\_\\_\\_\\_)\n",
    "\n",
    "**Explanation:** The answer is $(N, 28, 250, 250)$\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "For ridge regression, how will the bias and variance in our estimate\n",
    "$\\hat{w}$ change as the number of training examples $N$ increases?\n",
    "Assume the regularization parameter $\\lambda$ is fixed.\n",
    "\n",
    "1.  $\\downarrow$ bias, $\\uparrow$ variance\n",
    "\n",
    "2.  same bias, $\\downarrow$ variance\n",
    "\n",
    "3.  same bias, $\\uparrow$ variance\n",
    "\n",
    "4.  $\\downarrow$ bias, $\\downarrow$ variance\n",
    "\n",
    "5.  same bias, same variance\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "Suppose you have a data matrix $X \\in \\mathbb{R}^{10,000 \\times 10,000}$\n",
    "where $x_{ij} \\sim \\text{iid } N(0, \\sigma^2)$ for each\n",
    "$i, j \\in [10,000]$ and you want to understand how many principal\n",
    "components are needed to have reconstruction error $\\le 5/10,000$. What\n",
    "would be an efficient way to answer this question?\n",
    "\n",
    "Answer: \\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "**Explanation:** Accept SVD, or anything that refers to .eig/other\n",
    "packages. Kudos (+1)? if they also mention how to use these results\n",
    "(namely, look at the reconstruction error for each $d$ and pick the min\n",
    "$d$ with reconstruction error below the quantity. If they explain why\n",
    "this is the better choice (e.g, that this is likely a full-rank matrix\n",
    "so we’ll need an overwhelming majority of our features for that level of\n",
    "reconstruction error), another +1. We don’t accept the power method.\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "What method can be described as a resampling method used to estimate\n",
    "population parameters by repeatedly sampling from a dataset?\n",
    "\n",
    "1.  Power method\n",
    "\n",
    "2.  Bootstrapping\n",
    "\n",
    "3.  k-means\n",
    "\n",
    "4.  SVD\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times m}$ and $x$ in $\\mathbb{R}^m$. What is\n",
    "$\\nabla_x x^T A x$?\n",
    "\n",
    "Answer: $\\nabla_x x^T A x = \\rule{5cm}{0.15mm}$\n",
    "\n",
    "**Explanation:** The solution is $(A + A^T)x$.\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "What is the biggest advantage of k-fold cross-validation over\n",
    "Leave-one-out (LOO) cross-validation?\n",
    "\n",
    "1.  It provides a more accurate estimation of model performance\n",
    "\n",
    "2.  Prevents overfitting\n",
    "\n",
    "3.  Easier to compute\n",
    "\n",
    "4.  Minimizes impact from sample size\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "What is the expression for logistic loss? Here $\\hat{y}$ is a\n",
    "prediction, and $y$ is the corresponding ground truth label.\n",
    "\n",
    "1.  $\\log(1+e^{-y\\hat{y}})$\n",
    "\n",
    "2.  $-\\log(1+e^{-y\\hat{y}})$\n",
    "\n",
    "3.  $1 + e^{-y\\hat{y}}$\n",
    "\n",
    "4.  $\\log(1+e^{y\\hat{y}})$\n",
    "\n",
    "**Solution:** The solution is (a).\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "Suppose that you have a convolutional neural network with the following\n",
    "components: 1. One 2D-convolutional layer with two 2x2 kernels, stride\n",
    "2, and no zero-padding 2. A max pooling layer of size 2x2 with stride 2.\n",
    "3. One 2D-convolutional layer with one 1x1 kernel, stride 1, and no\n",
    "zero-padding Suppose you propagate the input below (left) through the\n",
    "CNN with the following kernel weights. Assume there are no bias terms.\n",
    "\n",
    "<img src=\"img/q14.png\" width=\"450px\">\n",
    "\n",
    "**Input:** A $4 \\times 4$ matrix labeled “Input”:\n",
    "$$\\begin{pmatrix}\n",
    "1 & 3 & 0 & 3 \\\\\n",
    "2 & 0 & 1 & 4 \\\\\n",
    "7 & 1 & 6 & 2 \\\\\n",
    "5 & 2 & 5 & 0\n",
    "\\end{pmatrix}$$\n",
    "Below the matrix, it is labeled “$4 \\times 4$”.\n",
    "\n",
    "**Layer 1 Kernel 1:** A $2 \\times 2$ matrix labeled “Layer 1 Kernel 1”:\n",
    "$$\\begin{pmatrix}\n",
    "-1 & 1 \\\\\n",
    "-1 & 1\n",
    "\\end{pmatrix}$$\n",
    "Below the matrix, it is labeled “$2 \\times 2$”.\n",
    "\n",
    "**Layer 1 Kernel 2:** A $2 \\times 2$ matrix labeled “Layer 1 Kernel 2”:\n",
    "$$\\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "-1 & -1\n",
    "\\end{pmatrix}$$\n",
    "Below the matrix, it is labeled “$2 \\times 2$”.\n",
    "\n",
    "**Layer 2 Kernel 1:** A 3D block representing a kernel, with a ‘1’ on\n",
    "its top face and a ‘1’ on its bottom face. This visually implies a 1x1\n",
    "kernel operating on two input channels (one ‘1’ for each channel). Below\n",
    "it, it is labeled “$1 \\times 2$”.\n",
    "\n",
    "What is the output of this network given the current weights and input?\n",
    "\n",
    "1.  0\n",
    "\n",
    "2.  4.5\n",
    "\n",
    "3.  8\n",
    "\n",
    "4.  9\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "True/False: Given a set of points in a $d$-dimensional space, using PCA\n",
    "to reduce the dataset to $d' < d$ dimensions will **always** lead to\n",
    "loss of information.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "True/False: The bootstrap method can be applied to both regression and\n",
    "classification questions.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "Which of the following techniques can be helpful in reducing the\n",
    "original dimensions of input data? Select **all** that apply.\n",
    "\n",
    "1.  L1 Regularization (LASSO)\n",
    "\n",
    "2.  L2 Regularization (Ridge)\n",
    "\n",
    "3.  Principal Component Analysis (PCA)\n",
    "\n",
    "4.  $k$-means Clustering\n",
    "\n",
    "**Correct answers:** (a), (c)\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "True/False: Given a dataset $X$ in a $d$-dimensional space, using PCA to\n",
    "project $X$ onto $d_1 < d_2 < d$ dimensions leads to the $d_1$\n",
    "dimensional projection to being a subspace of the $d_2$-dimensional\n",
    "projection.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "Shade in the region where decision boundaries that lie inside it have\n",
    "equal training error.\n",
    "\n",
    "<img src=\"img/q19_problem.png\" width=\"450px\">\n",
    "\n",
    "A Cartesian coordinate system is shown with an x-axis ranging from 0 to\n",
    "4 and a y-axis ranging from 0 to 4. Grid lines are present at 0.5 unit\n",
    "intervals.\n",
    "\n",
    "There are two types of points plotted:\n",
    "\n",
    "**Hollow points (circles):** - $(1, 2)$ - $(2, 3)$ - $(2, 4)$ -\n",
    "$(3, 3)$ - $(4, 3.5)$\n",
    "\n",
    "**Filled points (solid dots):** - $(0, 0)$ - $(1, 0.25)$ - $(2, 0.5)$ -\n",
    "$(2, 1)$ - $(3, 0.5)$\n",
    "\n",
    "**Explanation:** The solution for part 1 is:\n",
    "\n",
    "<img src=\"img/q19_solution.png\" width=\"450px\">\n",
    "\n",
    "A Cartesian coordinate system is shown with an x-axis ranging from 0 to\n",
    "4 and a y-axis ranging from 0 to 4. Grid lines are present. The same\n",
    "hollow and filled points as in the problem description are plotted.\n",
    "\n",
    "A region is shaded in gray. This shaded region is bounded by two dashed\n",
    "lines: - The upper dashed line passes through the points $(1, 2)$ and\n",
    "$(4, 3.5)$. Its equation is approximately $y = 0.5x + 1.5$. - The lower\n",
    "dashed line passes through the points $(0, 0)$ and $(3, 0.5)$. Its\n",
    "equation is approximately $y = \\frac{1}{6}x$.\n",
    "\n",
    "The shaded region represents the area between these two dashed lines,\n",
    "inclusive of the lines themselves.\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "Which of the following features could allow a logistic regression model\n",
    "to perfectly classify all data points in the following figure? Select\n",
    "all that apply.\n",
    "\n",
    "A Cartesian coordinate system is shown with an x-axis labeled ‘X’\n",
    "ranging from -3 to 3 and a y-axis labeled ‘y’ ranging from -3 to 3.\n",
    "Major grid lines are present at integer values on both axes, and minor\n",
    "grid lines are present at 0.5 unit intervals.\n",
    "\n",
    "<img src=\"img/q20_problem.png\" width=\"450px\" >\n",
    "\n",
    "There are two types of data points: - **Crosses (x):** These points are\n",
    "distributed widely across the entire plot area, forming an outer region.\n",
    "They are present in all four quadrants. - **Solid Circles (•):** These\n",
    "points are clustered tightly around the origin, primarily within the\n",
    "region where X is approximately between -0.5 and 0.5, and Y is\n",
    "approximately between -0.5 and 0.5. This cluster of solid circles forms\n",
    "an inner region, completely surrounded by the crosses.\n",
    "\n",
    "1.  $|x_i|, |y_i|$\n",
    "\n",
    "2.  $x_i + y_i, x_i - y_i$\n",
    "\n",
    "3.  $x_i^2, y_i^2$\n",
    "\n",
    "4.  $x_i^3, y_i^3$\n",
    "\n",
    "**Correct answers:** (a), (c)\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "**Extra credit:** Suppose that we have $x_1, x_2, \\dots, x_{2n}$ are\n",
    "independent and identically distributed realizations from the Laplacian\n",
    "distribution, the density of which is described by\n",
    "\n",
    "$$f(x | \\theta) = \\frac{1}{2}e^{-|x-\\theta|}$$\n",
    "\n",
    "Find the M.L.E of $\\theta$. Note that for this problem you may find the\n",
    "sign function useful, the definition of which is as follows\n",
    "\n",
    "$$\\operatorname{sign}(x) = \\begin{cases} +1 & x \\ge 0 \\\\ -1 & x < 0 \\end{cases}$$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** The solution is $\\hat{\\theta} \\in [x_n, x_{n+1}]$\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "SVM models that use slack variables have higher bias compared to SVM\n",
    "models that do not use slack variables.\n",
    "\n",
    "1.  equal\n",
    "\n",
    "2.  lower\n",
    "\n",
    "3.  higher\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "The following expression for $\\hat{\\Theta}_{2}$ will appear twice in\n",
    "this exam. Consider a distribution X with unknown mean and variance\n",
    "$\\sigma^{2}$. We define the population variance to be as follows\n",
    "\n",
    "$\\hat{\\Theta}_{2}=\\frac{1}{n}(\\sum_{i=1}^{n}(x_{i}-\\hat{\\Theta}_{1})^{2})$\n",
    "for $\\hat{\\Theta}_{1}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$\n",
    "\n",
    "What is the expected value of $\\Theta_{2}$?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** The solution is\n",
    "$\\hat{\\Theta}_{2}=(1-\\frac{1}{n})\\sigma^{2}$\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "Which of the following statements about kernels is/are true? Select all\n",
    "that apply.\n",
    "\n",
    "1.  A kernel feature map\n",
    "    $\\phi(x):\\mathbb{R}^{d}\\longrightarrow\\mathbb{R}^{k}$ always maps to\n",
    "    higher dimensional space (i.e., $k>d$.\n",
    "\n",
    "2.  Kernel matrices depend on the size of the dataset.\n",
    "\n",
    "3.  Kernel matrices are square.\n",
    "\n",
    "4.  Kernel matrices are used for data dimensionality reduction.\n",
    "\n",
    "**Correct answers:** (b), (c)\n",
    "\n",
    "## Problem 25\n",
    "\n",
    "Both LASSO and PCA can be used for feature selection. Which of the\n",
    "following statements are true? Select all that apply.\n",
    "\n",
    "1.  LASSO selects a subset (not necessarily a strict subset) of the\n",
    "    original features\n",
    "\n",
    "2.  If you use the kernel trick, principal component analysis and LASSO\n",
    "    are equivalent learning “techniques”\n",
    "\n",
    "3.  PCA produces features that are linear combinations of the original\n",
    "    features\n",
    "\n",
    "4.  PCA is a supervised learning algorithm\n",
    "\n",
    "**Correct answers:** (a), (c)\n",
    "\n",
    "## Problem 26\n",
    "\n",
    "Consider a dataset X where row $X_{i}$ corresponds to a complete medical\n",
    "record of an individual $i\\in[n].$ Suppose the first column of X\n",
    "contains each patient’s name, and no other column contains their name.\n",
    "\n",
    "True/False: Removing the first column from X gives a dataset $X_{.,2:d}$\n",
    "where no individual (row) is unique.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 27\n",
    "\n",
    "True/False: The number of clusters k is a hyperparameter for Lloyd’s\n",
    "Algorithm for k-means clustering.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "## Problem 28\n",
    "\n",
    "You are using Lloyd’s algorithm (the algorithm described in class) to\n",
    "perform k-means clustering on a small dataset.\n",
    "\n",
    "The following figure depicts the data and cluster centers for an\n",
    "iteration of the algorithm.\n",
    "\n",
    "Dataset samples are denoted by markers and cluster centers are denoted\n",
    "by markers x.\n",
    "\n",
    "<img src=\"img/q28_problem_1.png\" width=\"350px\">\n",
    "\n",
    "Which of the following depicts the best estimate of the cluster center\n",
    "positions after the next single iteration of Lloyd’s algorithm?\n",
    "\n",
    "<img src=\"img/q28_problem_2.png\" width=\"650px\">\n",
    "\n",
    "Hint: a single iteration refers to both update steps.\n",
    "\n",
    "1.  Plot A\n",
    "\n",
    "2.  Plot B\n",
    "\n",
    "3.  Plot C\n",
    "\n",
    "4.  Plot D\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "\\[Image 2\\]\n",
    "\n",
    "\\[Image 3\\]\n",
    "\n",
    "\\[Image 4\\]\n",
    "\n",
    "\\[Image 5\\]\n",
    "\n",
    "## Problem 29\n",
    "\n",
    "Which of the following loss functions are convex? Select all that apply.\n",
    "\n",
    "1.  1-0 loss.\n",
    "\n",
    "2.  Squared loss (MSE).\n",
    "\n",
    "3.  Sigmoid loss.\n",
    "\n",
    "4.  Logistic loss.\n",
    "\n",
    "5.  Hinge loss.\n",
    "\n",
    "**Correct answers:** (b), (d), (e)\n",
    "\n",
    "## Problem 30\n",
    "\n",
    "In neural networks, the activation functions sigmoid, ReLU, and tanh all\n",
    "\n",
    "1.  always output values between 0 and 1.\n",
    "\n",
    "2.  are applied only to the output units.\n",
    "\n",
    "3.  are essential for learning non-linear decision boundaries.\n",
    "\n",
    "4.  are needed to speed up the gradient computation during\n",
    "    backpropagation (compared to not using activation functions at all).\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 31\n",
    "\n",
    "Consider a neural network with 8 layers trained on a dataset of 800\n",
    "samples with a batch size of 10. How many forward passes through the\n",
    "entire network are needed to train this model for 5 epochs?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** 400\n",
    "\n",
    "## Problem 32\n",
    "\n",
    "k-means refers to optimizing which of the following objectives? Here\n",
    "$\\mu_{C(j)}$ is the mean of the cluster that $x_{j}$ belongs to. m is\n",
    "the number of points.\n",
    "\n",
    "1.  $F(\\mu,C)=\\sum_{j=1}^{m}||\\mu_{C(j)}-x_{j}||_{2}^{2}$\n",
    "\n",
    "2.  $F(\\mu,C)=min_{j=1}^{m}||\\mu_{C(j)}-x_{j}||_{2}^{2}$\n",
    "\n",
    "3.  $F(\\mu,C)=\\sum_{j=1}^{m}||\\mu_{C(j)}-x_{j}||_{2}$\n",
    "\n",
    "4.  $F(\\mu,C)=max_{j=1}^{m}||\\mu_{C(j)}-x_{j}||_{2}^{2}$\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "## Problem 33\n",
    "\n",
    "Which of the following statements about choosing L1 regularization\n",
    "(LASSO) over L2 regularization (Ridge) are true? Select all that apply.\n",
    "\n",
    "1.  LASSO (L1) learns model weights faster than Ridge regression (L2).\n",
    "\n",
    "2.  L1 regularization can help us identify which features are important\n",
    "    for a certain task.\n",
    "\n",
    "3.  L1 regularization usually achieves lower generalization error.\n",
    "\n",
    "4.  If the feature space is large, evaluating models trained with L1\n",
    "    regularization is more computationally efficient.\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "## Problem 34\n",
    "\n",
    "**Extra Credit:** Consider one of the “semi-fresh” datasets $\\hat{X}$\n",
    "generated using the bootstrap method for a dataset X, where n is large\n",
    "and $X_{i}\\sim_{iid}\\mathcal{D}$ Let $f_{X}$ be the model trained on X.\n",
    "$err(f_{X},\\hat{X})$ is a/an\n",
    "\n",
    "1.  unbiased estimate\n",
    "\n",
    "2.  slightly biased upwards\n",
    "\n",
    "3.  slightly biased downwards\n",
    "\n",
    "of $err_{\\mathcal{D}}(f_{X}).$\n",
    "\n",
    "1.  very biased estimate (either upwards or downwards), to the point\n",
    "    where this value by itself is not useful.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 35\n",
    "\n",
    "Consider a nearest neighbor classifier that chooses the label for a test\n",
    "point to be the label of its nearest neighboring training example. What\n",
    "is its leave-one-out cross-validated error for the data in the following\n",
    "figure?\n",
    "\n",
    "()“+” and “-” indicate labels of the points).\n",
    "\n",
    "<img src=\"img/q35_problem.png\" width=\"350px\">\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** The solution is 2/5\n",
    "\n",
    "## Problem 36\n",
    "\n",
    "Consider the following scatter plots of a data matrix X with four data\n",
    "points in $\\mathbb{R}^{2}.$ Choose the plot whose line represents the\n",
    "direction of the first principal component of $X-\\mu,$ where\n",
    "$X\\in\\mathbb{R}^{n\\times d}$ the vector $\\mu\\in\\mathbb{R}^{d}$ is the\n",
    "featurewise mean of X.\n",
    "\n",
    "<img src=\"img/q36_problem.png\" width=\"550px\">\n",
    "\n",
    "1.  Plot 1\n",
    "\n",
    "2.  Plot 2\n",
    "\n",
    "3.  Plot 3\n",
    "\n",
    "4.  Plot 4\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 37\n",
    "\n",
    "Suppose that a model finds that towns with more children tend to have\n",
    "higher rates of poverty compared to towns with fewer children. Upon\n",
    "seeing this, a local mayor suggests that children be banished from the\n",
    "town in order to reduce poverty. What is the flaw of this reasoning?\n",
    "\n",
    "1.  The reasoning is correct.\n",
    "\n",
    "2.  We cannot make policy decisions based on a machine learning model.\n",
    "\n",
    "3.  Correlation does not imply equal causation.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 38\n",
    "\n",
    "Consider the following neural network with weights shown in the image\n",
    "below. Every hidden neuron uses the ReLU activation function, and there\n",
    "is no activation function on the output neuron. Assume there are no bias\n",
    "terms. What is the output of this network with the input $x=(1,2)?$ Give\n",
    "a numerical answer.\n",
    "\n",
    "<img src=\"img/q38_problem.png\" width=\"450px\">\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** The answer is -3.\n",
    "\n",
    "## Problem 39\n",
    "\n",
    "Suppose you have a data matrix $X\\in\\mathbb{R}^{n\\times10,000}$ and you\n",
    "want the 3 principal components of X. What is an efficient algorithm to\n",
    "compute these?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** Accept “the power method”, or skinny SVD, (I’ll also\n",
    "accept anything that refers to eig/other packages). We won’t accept SVD.\n",
    "\n",
    "## Problem 40\n",
    "\n",
    "In PCA, the following words go together (draw lines to match the words\n",
    "on the left with the words on the right)\n",
    "\n",
    "<img src=\"img/q40_problem.png\" width=\"350px\">\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "<img src=\"img/q40_solution.png\" width=\"350px\">\n",
    "\n",
    "## Problem 41\n",
    "\n",
    "The following expression for $\\hat{\\Theta}_{2}$ will appear twice in\n",
    "this exam. Consider a distribution X with unknown mean and variance\n",
    "$\\sigma^{2}$ We define the population variance to be as follows\n",
    "\n",
    "Is $\\hat{\\Theta}_{2}$ unbiased?\n",
    "\n",
    "1.  Yes\n",
    "\n",
    "2.  No\n",
    "\n",
    "$\\hat{\\Theta}_{2}=\\frac{1}{n}(\\sum_{i=1}^{n}(x_{i}-\\hat{\\Theta}_{1})^{2})$\n",
    "for $\\hat{\\Theta}_{1}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 42\n",
    "\n",
    "Which of the following shapes are convex? Select all that apply.\n",
    "\n",
    "<img src=\"img/q42_problem.png\" width=\"450px\">\n",
    "\n",
    "1.  Shape A.\n",
    "\n",
    "2.  Shape B.\n",
    "\n",
    "3.  Shape C.\n",
    "\n",
    "4.  Shape D.\n",
    "\n",
    "5.  Shape E.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "## Problem 43\n",
    "\n",
    "Given a dataset X in a d-dimensional space, using PCA to project X onto\n",
    "$d1 < d2 < d$ dimensions leads to the d1 dimensional projection to have\n",
    "higher compared to the d2-dimensional projection.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:** Reconstruction error, or average distance from the\n",
    "original points to their projections. Also accept mathematical notation\n",
    "for these.\n",
    "\n",
    "## Problem 44\n",
    "\n",
    "What are support vectors in an SVM without slack?\n",
    "\n",
    "1.  The data points that don’t fall into a specific classification.\n",
    "\n",
    "2.  The most important features in the dataset.\n",
    "\n",
    "3.  The data points on the margin of the SVM.\n",
    "\n",
    "4.  All points within the dataset are considered support vectors.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 45\n",
    "\n",
    "While training a neural network for a classification task, you realize\n",
    "that there isn’t a significant change to the weights of the first few\n",
    "layers between iterations. What could NOT be a reason for this?\n",
    "\n",
    "1.  The model is stuck in a local minimum.\n",
    "\n",
    "2.  The network is very wide.\n",
    "\n",
    "3.  The weights of the network are all zero.\n",
    "\n",
    "4.  The learning rate is very small.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 46\n",
    "\n",
    "Let $\\eta(X)$ be an unknown function relating random variables X and Y ,\n",
    "D be a dataset consisting of sample pairs (xi, yi) drawn iid from the\n",
    "probability distribution PXY , and $\\hat{f}_D$ an estimator of $\\eta$.\n",
    "Draw lines to match the expressions on the left with the words on the\n",
    "right.\n",
    "\n",
    "<img src=\"img/q46_problem.png\" width=\"450px\">\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "<img src=\"img/q46_solution.png\" width=\"450px\">\n",
    "\n",
    "## Problem 47\n",
    "\n",
    "Given differentiable functions $f(x) : \\mathbb{R} \\to \\mathbb{R}$ and\n",
    "$g(x) : \\mathbb{R} \\to \\mathbb{R}$, which of the following statements is\n",
    "false?\n",
    "\n",
    "1.  if f(x) is concave, then -f(x) is convex.\n",
    "\n",
    "2.  if f(x) and g(x) are convex, then $h(x) := \\max(f(x), g(x))$ is also\n",
    "    convex.\n",
    "\n",
    "3.  if f(x) and g(x) are convex, then $h(x) := \\min(f(x), g(x))$ is also\n",
    "    convex.\n",
    "\n",
    "4.  f(x) can be both convex and concave on the same domain.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 48\n",
    "\n",
    "Let A be an $n \\times n$ matrix. Which of the following statements is\n",
    "true?\n",
    "\n",
    "1.  If A is invertible, then $A^T$ is invertible\n",
    "\n",
    "2.  If A is PSD, then A is invertible\n",
    "\n",
    "3.  If A is symmetric, then A is invertible\n",
    "\n",
    "4.  None of these answers.\n",
    "\n",
    "**Correct answers:** (a)"
   ],
   "id": "72b50439-a087-45f6-ab7b-08cee1ed5579"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
