{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08085b6-2e7e-4e9e-b76e-4271e57b2f71",
   "metadata": {},
   "source": [
    "# Practice Problems 5 Solutions\n",
    "\n",
    "## 1. One Answer\n",
    "\n",
    "Imagine you are building a machine learning model to predict the stopping distance of cars based on their speed.\n",
    "\n",
    "You obtain a large dataset where each data point is a pair of observed speeds and stopping distances, and you decide to use a simple linear regression model to predict stopping distances from speed.\n",
    "\n",
    "However, in reality, the stopping distance increases quadratically with speed.\n",
    "As a result, your model consistently underestimates the stopping distance at higher speeds.\n",
    "\n",
    "Compared to using a model that can model a quadratic relationship between stopping distance and speed, would your model have high or low bias?\n",
    "\n",
    "a. High bias\n",
    "\n",
    "b. Low bias\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(a) - High bias**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Problem:**\n",
    "\n",
    "**True Relationship:**\n",
    "The stopping distance increases **quadratically** with speed:\n",
    "$$\\text{Stopping Distance} = k \\cdot \\text{Speed}^2$$\n",
    "\n",
    "**Model Assumption:**\n",
    "Your linear regression model assumes a **linear** relationship:\n",
    "$$\\text{Stopping Distance} = w \\cdot \\text{Speed} + b$$\n",
    "\n",
    "**Why This Creates High Bias:**\n",
    "\n",
    "**1. Model Mismatch:**\n",
    "- **True relationship**: Quadratic (curved)\n",
    "- **Model assumption**: Linear (straight line)\n",
    "- The model is fundamentally incapable of capturing the true relationship\n",
    "\n",
    "**2. Systematic Underestimation:**\n",
    "- At higher speeds, the quadratic relationship grows much faster than linear\n",
    "- Your linear model will consistently underestimate stopping distances\n",
    "- This creates **systematic error** (bias)\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**True Function:**\n",
    "$$f_{\\text{true}}(x) = kx^2$$\n",
    "\n",
    "**Linear Model:**\n",
    "$$f_{\\text{model}}(x) = wx + b$$\n",
    "\n",
    "**Bias at Point $x$:**\n",
    "$$\\text{Bias}(x) = \\mathbb{E}[f_{\\text{model}}(x)] - f_{\\text{true}}(x) = wx + b - kx^2$$\n",
    "\n",
    "**As $x$ increases:**\n",
    "- $f_{\\text{true}}(x) = kx^2$ grows quadratically\n",
    "- $f_{\\text{model}}(x) = wx + b$ grows linearly\n",
    "- The bias becomes increasingly negative (underestimation)\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "Speed:    10    20    30    40    50\n",
    "True:     100   400   900   1600  2500\n",
    "Linear:   50    100   150   200   250\n",
    "Bias:     -50   -300  -750  -1400 -2250\n",
    "```\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (b) - Low bias:**\n",
    "- Low bias would mean the model can capture the true relationship well\n",
    "- A linear model cannot capture a quadratic relationship\n",
    "- The systematic underestimation creates high bias\n",
    "\n",
    "**Bias-Variance Tradeoff Context:**\n",
    "\n",
    "**High Bias (Current Situation):**\n",
    "- Model is too simple for the data\n",
    "- Cannot capture the true underlying relationship\n",
    "- Results in systematic prediction errors\n",
    "\n",
    "**Low Variance:**\n",
    "- Linear models are stable and consistent\n",
    "- Predictions don't vary much with small changes in training data\n",
    "- But this stability comes at the cost of high bias\n",
    "\n",
    "**Solutions to Reduce Bias:**\n",
    "\n",
    "1. **Use a quadratic model**: $f(x) = w_1x^2 + w_2x + b$\n",
    "2. **Polynomial regression**: Higher degree polynomials\n",
    "3. **Non-linear basis expansion**: Transform features to capture curvature\n",
    "4. **Neural networks**: Can learn non-linear relationships\n",
    "\n",
    "**Conclusion:**\n",
    "The linear model has **high bias** because it cannot capture the quadratic relationship between speed and stopping distance, leading to systematic underestimation at higher speeds.\n",
    "\n",
    "## 2. One Answer\n",
    "\n",
    "Follow the same car scenario as the above question. Compared to using a model that can model a quadratic relationship between stopping distance and speed, would your model have high or low variance?\n",
    "\n",
    "a. High variance\n",
    "\n",
    "b. Low variance\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b) - Low variance**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Problem:**\n",
    "\n",
    "Continuing from the previous question, we have:\n",
    "- **True relationship**: Quadratic (stopping distance = $k \\cdot \\text{speed}^2$)\n",
    "- **Model assumption**: Linear (stopping distance = $w \\cdot \\text{speed} + b$)\n",
    "- **Result**: High bias due to model mismatch\n",
    "\n",
    "**Why This Creates Low Variance:**\n",
    "\n",
    "**1. Model Simplicity:**\n",
    "- Linear models are **simple and stable**\n",
    "- They have few parameters to learn\n",
    "- Predictions are consistent across different training sets\n",
    "\n",
    "**2. Insensitive to Training Data Changes:**\n",
    "- Small changes in training data don't dramatically affect the learned line\n",
    "- The model always learns a straight line relationship\n",
    "- Predictions remain relatively stable\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Linear Model:**\n",
    "$$f(x) = wx + b$$\n",
    "\n",
    "**Parameter Learning:**\n",
    "- $w$ and $b$ are learned from training data\n",
    "- Small changes in training data cause small changes in $w$ and $b$\n",
    "- The overall linear structure remains the same\n",
    "\n",
    "**Variance Definition:**\n",
    "$$\\text{Variance} = \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2]$$\n",
    "\n",
    "**Why Variance is Low:**\n",
    "- Linear models are **deterministic** given the parameters\n",
    "- Small parameter changes lead to small prediction changes\n",
    "- The model cannot \"memorize\" complex patterns in the data\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "Training Set 1: w=2, b=10\n",
    "Training Set 2: w=2.1, b=9.8\n",
    "Training Set 3: w=1.9, b=10.2\n",
    "\n",
    "Predictions at speed=20:\n",
    "Set 1: 2(20) + 10 = 50\n",
    "Set 2: 2.1(20) + 9.8 = 51.8\n",
    "Set 3: 1.9(20) + 10.2 = 48.2\n",
    "\n",
    "Variance is low because predictions are similar across different training sets.\n",
    "```\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - High variance:**\n",
    "- High variance would mean the model is very sensitive to training data\n",
    "- Linear models are inherently stable and consistent\n",
    "- They don't overfit to noise in the training data\n",
    "\n",
    "**Bias-Variance Tradeoff Context:**\n",
    "\n",
    "**Current Situation:**\n",
    "- **High Bias**: Model cannot capture true quadratic relationship\n",
    "- **Low Variance**: Model is stable and consistent\n",
    "\n",
    "**This is a classic underfitting scenario:**\n",
    "- Model is too simple for the data\n",
    "- Cannot capture the true relationship (high bias)\n",
    "- But is very stable across different training sets (low variance)\n",
    "\n",
    "**Comparison with More Complex Models:**\n",
    "\n",
    "**Linear Model (Current):**\n",
    "- **Bias**: High (cannot capture quadratic relationship)\n",
    "- **Variance**: Low (stable predictions)\n",
    "\n",
    "**Quadratic Model (Ideal):**\n",
    "- **Bias**: Low (can capture true relationship)\n",
    "- **Variance**: Low (still relatively simple)\n",
    "\n",
    "**Very Complex Model (e.g., high-degree polynomial):**\n",
    "- **Bias**: Low (can capture complex relationships)\n",
    "- **Variance**: High (sensitive to training data)\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**Advantages of Low Variance:**\n",
    "- Predictions are consistent and reliable\n",
    "- Model generalizes well to similar data\n",
    "- Less prone to overfitting\n",
    "\n",
    "**Disadvantages in This Case:**\n",
    "- The low variance comes at the cost of high bias\n",
    "- Model consistently makes the wrong type of prediction\n",
    "- Cannot capture the true underlying relationship\n",
    "\n",
    "**Conclusion:**\n",
    "The linear model has **low variance** because it is simple and stable, making consistent predictions across different training sets, even though these predictions are systematically wrong due to the high bias.\n",
    "\n",
    "## 3. One Answer\n",
    "\n",
    "Follow the same car scenario as the above question. In reality, stopping distance is also affected by weather conditions, which your our model does not capture.\n",
    "Which of these components of overall model error captures the error from not including weather conditions as a feature?\n",
    "\n",
    "a. Bias\n",
    "\n",
    "b. Variance\n",
    "\n",
    "c. Irreducible error\n",
    "\n",
    "Correct answers: (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(c) - Irreducible error**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Problem:**\n",
    "\n",
    "We have a model that predicts stopping distance based on speed, but in reality, stopping distance is also affected by weather conditions. The model doesn't include weather as a feature.\n",
    "\n",
    "**Components of Model Error:**\n",
    "\n",
    "The total prediction error can be decomposed into three components:\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**1. Bias:**\n",
    "- Error due to model assumptions (e.g., linear vs quadratic relationship)\n",
    "- In our case: using linear model when true relationship is quadratic\n",
    "\n",
    "**2. Variance:**\n",
    "- Error due to model sensitivity to training data\n",
    "- In our case: how much predictions vary with different training sets\n",
    "\n",
    "**3. Irreducible Error:**\n",
    "- Error due to inherent randomness or missing information\n",
    "- Cannot be reduced by any model, no matter how complex\n",
    "\n",
    "**Why Weather Conditions Create Irreducible Error:**\n",
    "\n",
    "**1. Missing Information:**\n",
    "- Weather conditions affect stopping distance but are not in our model\n",
    "- This creates **inherent unpredictability** in the data\n",
    "- No model can perfectly predict stopping distance without weather information\n",
    "\n",
    "**2. Inherent Randomness:**\n",
    "- Even with perfect information, there's natural variation in stopping distances\n",
    "- Weather introduces additional sources of variation\n",
    "- This randomness cannot be eliminated by any model\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**True Model (with weather):**\n",
    "$$\\text{Stopping Distance} = f(\\text{Speed}, \\text{Weather}) + \\epsilon$$\n",
    "\n",
    "**Our Model (without weather):**\n",
    "$$\\text{Stopping Distance} = g(\\text{Speed}) + \\epsilon'$$\n",
    "\n",
    "**Error Decomposition:**\n",
    "$$\\text{Error} = \\underbrace{(g(\\text{Speed}) - f(\\text{Speed}, \\text{Weather}))}_{\\text{Irreducible}} + \\underbrace{\\epsilon}_{\\text{Noise}}$$\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - Bias:**\n",
    "- Bias comes from model assumptions (linear vs quadratic)\n",
    "- Weather conditions don't affect the model's functional form\n",
    "- This is about missing features, not wrong assumptions\n",
    "\n",
    "**Option (b) - Variance:**\n",
    "- Variance comes from model sensitivity to training data\n",
    "- Weather conditions don't make the model more or less sensitive\n",
    "- This is about inherent unpredictability, not model instability\n",
    "\n",
    "**Examples of Irreducible Error:**\n",
    "\n",
    "**1. Weather Effects:**\n",
    "- Rain: Increases stopping distance by 20-30%\n",
    "- Snow: Increases stopping distance by 50-100%\n",
    "- Ice: Increases stopping distance by 200-300%\n",
    "\n",
    "**2. Other Missing Factors:**\n",
    "- Driver reaction time\n",
    "- Tire condition\n",
    "- Road surface quality\n",
    "- Vehicle weight\n",
    "\n",
    "**3. Natural Variation:**\n",
    "- Even with identical conditions, stopping distances vary\n",
    "- Human factors (driver skill, attention)\n",
    "- Mechanical factors (brake wear, tire pressure)\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Model Limitations:**\n",
    "- No model can achieve perfect predictions without all relevant features\n",
    "- Irreducible error sets a lower bound on achievable performance\n",
    "\n",
    "**2. Feature Engineering:**\n",
    "- Adding weather features would reduce irreducible error\n",
    "- But some randomness will always remain\n",
    "\n",
    "**3. Realistic Expectations:**\n",
    "- Understanding irreducible error helps set realistic performance goals\n",
    "- Don't expect perfect predictions when important factors are missing\n",
    "\n",
    "**Conclusion:**\n",
    "The error from not including weather conditions is **irreducible error** because it represents inherent unpredictability that cannot be eliminated by any model that doesn't have access to weather information.\n",
    "\n",
    "## 4. Select All That Apply\n",
    "\n",
    "Which of the following will generally help to reduce model variance?\n",
    "\n",
    "a. Increasing the size of the training data.\n",
    "\n",
    "b. Increasing the size of the validation data.\n",
    "\n",
    "c. Increasing the number of model parameters.\n",
    "\n",
    "d. Increasing the amount of regularization.\n",
    "\n",
    "Correct answers: (a), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(a) and (d)**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Model Variance:**\n",
    "\n",
    "**Definition of Variance:**\n",
    "Variance measures how much the model's predictions change when trained on different datasets drawn from the same underlying distribution:\n",
    "$$\\text{Variance} = \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2]$$\n",
    "\n",
    "**High Variance = Overfitting:**\n",
    "- Model is too sensitive to training data\n",
    "- Learns noise and idiosyncrasies in the training set\n",
    "- Poor generalization to new data\n",
    "\n",
    "**Low Variance = Stability:**\n",
    "- Model makes consistent predictions across different training sets\n",
    "- Generalizes well to new data\n",
    "- Less prone to overfitting\n",
    "\n",
    "**Analysis of Each Option:**\n",
    "\n",
    "**Option (a) - Increasing the size of the training data: ✓ CORRECT**\n",
    "\n",
    "**Why This Reduces Variance:**\n",
    "1. **More Information**: Larger training sets provide more representative samples\n",
    "2. **Less Noise Sensitivity**: Model is less likely to overfit to noise in smaller datasets\n",
    "3. **Better Generalization**: More data helps the model learn the true underlying pattern\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "- With more data points, the model can better estimate the true relationship\n",
    "- Law of large numbers: estimates become more stable with more samples\n",
    "- Reduces the impact of individual noisy data points\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Small dataset (10 points): Model might fit noise perfectly\n",
    "Large dataset (1000 points): Model learns the true trend, ignores noise\n",
    "```\n",
    "\n",
    "**Option (b) - Increasing the size of the validation data: ✗ INCORRECT**\n",
    "\n",
    "**Why This Doesn't Reduce Variance:**\n",
    "1. **Validation data is not used for training**: It doesn't affect what the model learns\n",
    "2. **Only affects evaluation**: Helps estimate model performance more accurately\n",
    "3. **No impact on model behavior**: The model itself doesn't change\n",
    "\n",
    "**What validation data does:**\n",
    "- Provides unbiased estimate of model performance\n",
    "- Helps in model selection and hyperparameter tuning\n",
    "- Does not prevent overfitting during training\n",
    "\n",
    "**Option (c) - Increasing the number of model parameters: ✗ INCORRECT**\n",
    "\n",
    "**Why This Increases Variance:**\n",
    "1. **More Flexibility**: More parameters allow the model to fit training data more closely\n",
    "2. **Higher Risk of Overfitting**: Model can memorize training data instead of learning patterns\n",
    "3. **Reduces Bias**: More parameters typically reduce bias but increase variance\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- More parameters → Lower bias, Higher variance\n",
    "- Fewer parameters → Higher bias, Lower variance\n",
    "\n",
    "**Option (d) - Increasing the amount of regularization: ✓ CORRECT**\n",
    "\n",
    "**Why This Reduces Variance:**\n",
    "1. **Constrains Model Complexity**: Prevents the model from fitting noise in training data\n",
    "2. **Promotes Simplicity**: Encourages the model to learn simpler patterns\n",
    "3. **Improves Generalization**: Model becomes less sensitive to training data variations\n",
    "\n",
    "**Types of Regularization:**\n",
    "- **L1 (Lasso)**: $||w||_1$ penalty, promotes sparsity\n",
    "- **L2 (Ridge)**: $||w||_2^2$ penalty, promotes small weights\n",
    "- **Dropout**: Randomly deactivates neurons during training\n",
    "- **Early Stopping**: Stops training before overfitting\n",
    "\n",
    "**Mathematical Effect:**\n",
    "$$\\text{Loss} = \\text{Training Loss} + \\lambda \\cdot \\text{Regularization Term}$$\n",
    "\n",
    "As $\\lambda$ increases:\n",
    "- Model becomes simpler\n",
    "- Less sensitive to training data\n",
    "- Lower variance, but potentially higher bias\n",
    "\n",
    "**Practical Examples:**\n",
    "\n",
    "**Training Data Size:**\n",
    "```\n",
    "Small dataset: Model variance high (overfits to noise)\n",
    "Large dataset: Model variance low (learns true pattern)\n",
    "```\n",
    "\n",
    "**Regularization:**\n",
    "```\n",
    "No regularization: Model can fit training data perfectly (high variance)\n",
    "With regularization: Model constrained, more stable (lower variance)\n",
    "```\n",
    "\n",
    "**Conclusion:**\n",
    "Options **(a)** and **(d)** reduce model variance by providing more information (larger training set) and constraining model complexity (regularization), respectively.\n",
    "\n",
    "## 5. One Answer\n",
    "\n",
    "For machine learning models and datasets in general, as the number of training data points grows, the prediction error of the model on unseen data (data not found in the training set) approaches 0.\n",
    "\n",
    "a. True\n",
    "\n",
    "b. False\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b) - False**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Problem:**\n",
    "\n",
    "The statement claims that as the number of training data points grows, the prediction error on unseen data approaches 0. This is false because of the fundamental limitations in machine learning.\n",
    "\n",
    "**Components of Prediction Error:**\n",
    "\n",
    "The total prediction error can be decomposed into:\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**1. Bias:**\n",
    "- Error due to model assumptions\n",
    "- Can be reduced with more data and better models\n",
    "- Approaches zero as model complexity increases\n",
    "\n",
    "**2. Variance:**\n",
    "- Error due to model sensitivity to training data\n",
    "- Can be reduced with more data\n",
    "- Approaches zero as data size increases\n",
    "\n",
    "**3. Irreducible Error:**\n",
    "- Error due to inherent randomness or missing information\n",
    "- **Cannot be reduced by any amount of data**\n",
    "- Sets a fundamental lower bound on achievable performance\n",
    "\n",
    "**Why Prediction Error Cannot Approach Zero:**\n",
    "\n",
    "**1. Irreducible Error:**\n",
    "- **Noise in the data**: Measurement errors, random fluctuations\n",
    "- **Missing information**: Important features not captured\n",
    "- **Inherent randomness**: Natural variation in the phenomenon\n",
    "\n",
    "**Mathematical Example:**\n",
    "\n",
    "**True Model with Noise:**\n",
    "$$y = f(x) + \\epsilon$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is irreducible noise.\n",
    "\n",
    "**Expected Prediction Error:**\n",
    "$$\\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\sigma^2$$\n",
    "\n",
    "**As $n \\to \\infty$:**\n",
    "- $\\mathbb{E}[(f(x) - \\hat{f}(x))^2] \\to 0$ (bias and variance approach zero)\n",
    "- But $\\sigma^2$ remains (irreducible error)\n",
    "\n",
    "**Result**: Prediction error approaches $\\sigma^2$, not zero.\n",
    "\n",
    "**Practical Examples:**\n",
    "\n",
    "**1. Medical Diagnosis:**\n",
    "- Even with perfect models and infinite data\n",
    "- Human biology has inherent variability\n",
    "- Some diseases have random onset patterns\n",
    "- Prediction error cannot be zero\n",
    "\n",
    "**2. Stock Price Prediction:**\n",
    "- Even with all available information\n",
    "- Market movements have random components\n",
    "- Unpredictable events affect prices\n",
    "- Perfect prediction is impossible\n",
    "\n",
    "**3. Weather Forecasting:**\n",
    "- Even with perfect models and infinite historical data\n",
    "- Weather has chaotic, unpredictable elements\n",
    "- Small changes can lead to large differences\n",
    "- Perfect prediction is impossible\n",
    "\n",
    "**Mathematical Verification:**\n",
    "\n",
    "**Cramér-Rao Lower Bound:**\n",
    "For any unbiased estimator $\\hat{\\theta}$:\n",
    "$$\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)}$$\n",
    "where $I(\\theta)$ is the Fisher information.\n",
    "\n",
    "**This sets a fundamental lower bound** on estimation error, regardless of data size.\n",
    "\n",
    "**Bayesian Perspective:**\n",
    "Even with infinite data, there's always uncertainty in predictions due to:\n",
    "- Model uncertainty\n",
    "- Parameter uncertainty\n",
    "- Inherent randomness\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - True:**\n",
    "- **Problem**: Ignores irreducible error\n",
    "- **Issue**: Assumes perfect predictability\n",
    "- **Result**: Unrealistic expectation\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "**1. Data Limitations:**\n",
    "- More data reduces bias and variance\n",
    "- But cannot eliminate irreducible error\n",
    "- There's always a fundamental limit\n",
    "\n",
    "**2. Model Limitations:**\n",
    "- Perfect models don't exist\n",
    "- All models make assumptions\n",
    "- These assumptions create bias\n",
    "\n",
    "**3. Reality Limitations:**\n",
    "- Many phenomena are inherently random\n",
    "- Perfect prediction is often impossible\n",
    "- Understanding irreducible error is crucial\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Realistic Expectations:**\n",
    "- Don't expect perfect predictions\n",
    "- Focus on reducing bias and variance\n",
    "- Accept that some error is unavoidable\n",
    "\n",
    "**2. Model Selection:**\n",
    "- Choose models appropriate for the problem\n",
    "- Consider the irreducible error when evaluating performance\n",
    "- Don't overfit trying to achieve impossible accuracy\n",
    "\n",
    "**3. Business Decisions:**\n",
    "- Set realistic performance targets\n",
    "- Consider the cost of reducing error further\n",
    "- Focus on actionable improvements\n",
    "\n",
    "**Conclusion:**\n",
    "The statement is **False** because prediction error cannot approach zero due to irreducible error, which represents inherent randomness and missing information that cannot be eliminated by any amount of data or any model.\n",
    "\n",
    "## 6. Select All That Apply\n",
    "\n",
    "Which of the following statements about (binary) logistic regression is true?\n",
    "Recall that the sigmoid function is defined as $\\sigma(x)=\\frac{1}{1+e^{-x}}$ for $x\\in\\mathbb{R}.$\n",
    "\n",
    "a. For any finite input $x\\in\\mathbb{R}$, $\\sigma(x)$ is strictly greater than 0 and strictly less than 1. Thus, a binary logistic regression model with finite input and weights can never output a probability of exactly 0 or 1, and can never achieve a training loss of exactly 0.\n",
    "\n",
    "b. The first derivative of o is monotonically increasing.\n",
    "\n",
    "c. There exists a constant value $c\\in\\mathbb{R}$ such that o is convex when restricted to $x<c$ C and concave when restricted to $x\\ge c$\n",
    "\n",
    "d. For binary logistic regression, if the probability of the positive class is $\\sigma(x)$ then the probability of the negative class is $\\sigma(-x)$\n",
    "\n",
    "Correct answers: (a), (c), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(a)**, **(c)**, and **(d)**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Sigmoid Function:**\n",
    "\n",
    "**Sigmoid Function Definition:**\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Key Properties:**\n",
    "- **Range**: $(0, 1)$ (strictly bounded)\n",
    "- **Symmetry**: $\\sigma(-x) = 1 - \\sigma(x)$\n",
    "- **Monotonic**: Strictly increasing function\n",
    "- **Smooth**: Continuous and differentiable everywhere\n",
    "\n",
    "**Analysis of Each Statement:**\n",
    "\n",
    "**Option (a) - For any finite input $x \\in \\mathbb{R}$, $\\sigma(x)$ is strictly greater than 0 and strictly less than 1: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Lower Bound:**\n",
    "For any finite $x \\in \\mathbb{R}$:\n",
    "- $e^{-x} > 0$ (exponential function is always positive)\n",
    "- $1 + e^{-x} > 1$\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}} < 1$\n",
    "\n",
    "**Upper Bound:**\n",
    "For any finite $x \\in \\mathbb{R}$:\n",
    "- $e^{-x} < \\infty$ (finite exponential)\n",
    "- $1 + e^{-x} < \\infty$\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}} > 0$\n",
    "\n",
    "**Implications for Logistic Regression:**\n",
    "- **Output probabilities**: Always in $(0, 1)$, never exactly 0 or 1\n",
    "- **Training loss**: Cannot achieve exactly 0 (perfect classification)\n",
    "- **Asymptotic behavior**: Can get arbitrarily close to 0 or 1\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "x = 10: σ(10) ≈ 0.99995 (very close to 1, but not exactly 1)\n",
    "x = -10: σ(-10) ≈ 0.00005 (very close to 0, but not exactly 0)\n",
    "```\n",
    "\n",
    "**Option (b) - The first derivative of σ is monotonically increasing: FALSE** ❌\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**First Derivative:**\n",
    "$$\\sigma'(x) = \\frac{d}{dx}\\sigma(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "**Second Derivative:**\n",
    "$$\\sigma''(x) = \\frac{d^2}{dx^2}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x))$$\n",
    "\n",
    "**Behavior:**\n",
    "- **At $x = 0$**: $\\sigma(0) = 0.5$, $\\sigma'(0) = 0.25$ (maximum)\n",
    "- **As $x \\to \\infty$**: $\\sigma'(x) \\to 0$ (decreasing)\n",
    "- **As $x \\to -\\infty$**: $\\sigma'(x) \\to 0$ (decreasing)\n",
    "\n",
    "**Result**: The derivative is **not monotonically increasing**. It peaks at $x = 0$ and decreases towards the asymptotes.\n",
    "\n",
    "**Option (c) - There exists a constant value $c \\in \\mathbb{R}$ such that σ is convex when restricted to $x < c$ and concave when restricted to $x \\geq c$: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Second Derivative Analysis:**\n",
    "$$\\sigma''(x) = \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x))$$\n",
    "\n",
    "**Convexity Conditions:**\n",
    "- **Convex**: $\\sigma''(x) > 0$\n",
    "- **Concave**: $\\sigma''(x) < 0$\n",
    "\n",
    "**Critical Point:**\n",
    "When $\\sigma(x) = 0.5$:\n",
    "- $\\sigma''(x) = 0.5 \\cdot 0.5 \\cdot 0 = 0$\n",
    "\n",
    "**Behavior:**\n",
    "- **For $x < 0$**: $\\sigma(x) < 0.5$, so $1 - 2\\sigma(x) > 0$ → $\\sigma''(x) > 0$ (convex)\n",
    "- **For $x > 0$**: $\\sigma(x) > 0.5$, so $1 - 2\\sigma(x) < 0$ → $\\sigma''(x) < 0$ (concave)\n",
    "\n",
    "**Result**: $c = 0$ satisfies the condition.\n",
    "\n",
    "**Option (d) - For binary logistic regression, if the probability of the positive class is $\\sigma(x)$, then the probability of the negative class is $\\sigma(-x)$: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Sigmoid Symmetry Property:**\n",
    "$$\\sigma(-x) = \\frac{1}{1 + e^{x}} = \\frac{e^{-x}}{e^{-x} + 1} = 1 - \\frac{1}{1 + e^{-x}} = 1 - \\sigma(x)$$\n",
    "\n",
    "**For Binary Classification:**\n",
    "- **Positive class probability**: $P(Y = 1|X) = \\sigma(x)$\n",
    "- **Negative class probability**: $P(Y = 0|X) = 1 - \\sigma(x) = \\sigma(-x)$\n",
    "\n",
    "**Verification:**\n",
    "$$\\sigma(x) + \\sigma(-x) = \\sigma(x) + (1 - \\sigma(x)) = 1$$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "x = 2: σ(2) ≈ 0.88, σ(-2) ≈ 0.12\n",
    "Sum: 0.88 + 0.12 = 1 ✓\n",
    "```\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Training Loss:**\n",
    "- Cannot achieve exactly zero loss\n",
    "- Can get arbitrarily close to perfect classification\n",
    "- Important for setting realistic expectations\n",
    "\n",
    "**2. Model Interpretability:**\n",
    "- Outputs are always probabilities\n",
    "- Symmetry property useful for binary classification\n",
    "- Convexity/concavity affects optimization\n",
    "\n",
    "**3. Numerical Stability:**\n",
    "- Bounded outputs prevent numerical issues\n",
    "- Smooth function enables gradient-based optimization\n",
    "- Symmetry simplifies implementation\n",
    "\n",
    "**Conclusion:**\n",
    "Statements **(a)**, **(c)**, and **(d)** are correct. The sigmoid function has bounded outputs, changes from convex to concave at $x = 0$, and satisfies the symmetry property $\\sigma(-x) = 1 - \\sigma(x)$.\n",
    "\n",
    "## 7. Select All That Apply\n",
    "\n",
    "Consider performing Lasso regression by finding parameters $w\\in\\mathbb{R}^{d}$ that minimize\n",
    "$$f(w)=\\sum_{i=1}^{n}(y^{(i)}-x^{(i)\\top}w)^{2}+\\lambda||w||_{1}.$$ \n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. Increasing $\\lambda$ will generally reduce the $L_{1}$ norm of the parameters $w$.\n",
    "\n",
    "b. Consider two models $w_{1}$, $w_{2}\\in\\mathbb{R}^{d}.$ Assume $w_{1}$ is more sparse, i.e., $w_{1}$\n",
    "has strictly more zero coefficients than $w_{2}$. Then $||w_{1}||_{1}<||w_{2}||_{1}$\n",
    "\n",
    "c. Increasing $\\lambda$ generally increases model bias.\n",
    "\n",
    "d. Increasing $\\lambda$ generally increases model variance.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(a)** and **(c)**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding LASSO Regression:**\n",
    "\n",
    "**LASSO Objective Function:**\n",
    "$$f(w) = \\sum_{i=1}^{n}(y^{(i)} - x^{(i)\\top}w)^2 + \\lambda||w||_1$$\n",
    "\n",
    "where:\n",
    "- First term: Mean squared error (data fitting term)\n",
    "- Second term: L1 regularization (sparsity-inducing penalty)\n",
    "- $\\lambda$: Regularization strength (hyperparameter)\n",
    "\n",
    "**Analysis of Each Statement:**\n",
    "\n",
    "**Option (a) - Increasing $\\lambda$ will generally reduce the L1 norm of the parameters $w$: TRUE** ✅\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "**L1 Norm Definition:**\n",
    "$$||w||_1 = \\sum_{j=1}^d |w_j|$$\n",
    "\n",
    "**Effect of Increasing $\\lambda$:**\n",
    "- **Stronger penalty**: The L1 term becomes more important\n",
    "- **Coefficient shrinkage**: Weights are pushed towards zero\n",
    "- **Sparsity promotion**: Some coefficients become exactly zero\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "$$\\min_w \\sum_{i=1}^{n}(y^{(i)} - x^{(i)\\top}w)^2 + \\lambda||w||_1$$\n",
    "\n",
    "As $\\lambda$ increases:\n",
    "- The regularization term dominates\n",
    "- The optimal solution has smaller $||w||_1$\n",
    "- More coefficients become zero\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "λ = 0: w = [2.1, -1.8, 0.9, -0.5], ||w||₁ = 5.3\n",
    "λ = 1: w = [1.5, -1.2, 0, -0.1], ||w||₁ = 2.8\n",
    "λ = 5: w = [0.8, 0, 0, 0], ||w||₁ = 0.8\n",
    "```\n",
    "\n",
    "**Option (b) - Consider two models $w_1$, $w_2 \\in \\mathbb{R}^d$. Assume $w_1$ is more sparse, i.e., $w_1$ has strictly more zero coefficients than $w_2$. Then $||w_1||_1 < ||w_2||_1$: FALSE** ❌\n",
    "\n",
    "**Counterexample:**\n",
    "\n",
    "**Model 1 (more sparse):**\n",
    "$w_1 = [0, 0, 0, 10]$\n",
    "- Number of zeros: 3\n",
    "- L1 norm: $||w_1||_1 = 10$\n",
    "\n",
    "**Model 2 (less sparse):**\n",
    "$w_2 = [1, 1, 1, 1]$\n",
    "- Number of zeros: 0\n",
    "- L1 norm: $||w_2||_1 = 4$\n",
    "\n",
    "**Result**: $w_1$ is more sparse but has larger L1 norm.\n",
    "\n",
    "**Why This Happens:**\n",
    "- **Sparsity**: Counts number of non-zero coefficients\n",
    "- **L1 norm**: Sum of absolute values of coefficients\n",
    "- **Magnitude matters**: Large non-zero coefficients can dominate\n",
    "\n",
    "**Option (c) - Increasing $\\lambda$ generally increases model bias: TRUE** ✅\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**Effect of Increasing $\\lambda$:**\n",
    "\n",
    "**1. Model Simplification:**\n",
    "- Larger $\\lambda$ forces coefficients to be smaller\n",
    "- Model becomes less flexible\n",
    "- Cannot fit complex patterns in data\n",
    "\n",
    "**2. Bias Increase:**\n",
    "- Model assumptions become more restrictive\n",
    "- Systematic error increases\n",
    "- Underfitting occurs\n",
    "\n",
    "**Mathematical Verification:**\n",
    "As $\\lambda \\to \\infty$:\n",
    "- $w \\to 0$ (all coefficients approach zero)\n",
    "- Model becomes constant predictor\n",
    "- High bias, low variance\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "λ = 0: Complex model, low bias, high variance\n",
    "λ = 1: Moderate model, moderate bias, moderate variance  \n",
    "λ = 10: Simple model, high bias, low variance\n",
    "```\n",
    "\n",
    "**Option (d) - Increasing $\\lambda$ generally increases model variance: FALSE** ❌\n",
    "\n",
    "**Why This is Incorrect:**\n",
    "\n",
    "**Effect of Increasing $\\lambda$ on Variance:**\n",
    "- **Reduced flexibility**: Model becomes less sensitive to training data\n",
    "- **Stability**: Predictions become more consistent\n",
    "- **Lower variance**: Model is more robust\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "- **Small $\\lambda$**: Model can fit training data closely (high variance)\n",
    "- **Large $\\lambda$**: Model is constrained, less sensitive to data (low variance)\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Model Selection:**\n",
    "- **Small $\\lambda$**: Good for complex relationships, risk of overfitting\n",
    "- **Large $\\lambda$**: Good for simple relationships, risk of underfitting\n",
    "- **Optimal $\\lambda$**: Balances bias and variance\n",
    "\n",
    "**2. Feature Selection:**\n",
    "- **Sparsity**: LASSO automatically performs feature selection\n",
    "- **Interpretability**: Fewer features make model easier to understand\n",
    "- **Computational efficiency**: Fewer features mean faster prediction\n",
    "\n",
    "**3. Regularization Effect:**\n",
    "- **Prevents overfitting**: Constrains model complexity\n",
    "- **Improves generalization**: Better performance on unseen data\n",
    "- **Robustness**: Less sensitive to noise in training data\n",
    "\n",
    "**Conclusion:**\n",
    "Options **(a)** and **(c)** are correct. Increasing $\\lambda$ in LASSO reduces the L1 norm of parameters (promoting sparsity) and increases model bias (simplifying the model), while decreasing variance (making the model more stable).\n",
    "\n",
    "## 8. Select All That Apply\n",
    "\n",
    "Which of the following statements about ridge regression are true?\n",
    "\n",
    "a. When there are correlated features, ridge regression typically sets the weights of all but one of the correlated features to 0.\n",
    "\n",
    "b. Compared to unregularized linear regression, the additional computational cost of ridge regression scales with respect to the number of data points in the dataset.\n",
    "\n",
    "c. Ridge regression reduces variance at the expense of increasing bias.\n",
    "\n",
    "d. Using ridge and lasso regularization together (e.g., minimizing a training objective of the form $$f(w)=\\sum_{i=1}^{n}(y^{(i)}-x^{(i)\\top}w)^{2}+\\lambda_{1}||w||_{1}+\\lambda_{2}||w||_{2}^{2})$$ makes the training loss no longer convex.\n",
    "\n",
    "Correct answers: (c)\n",
    "\n",
    "Explanation:\n",
    "a) False. This statement is more akin to Lasso regression.\n",
    "Ridge regression is more likely to somewhat equally decrease the weights of correlated features to each be smaller (as opposed to only keeping one large).\n",
    "See lecture 5 slide 37-38.\n",
    "b) False. Ridge regression additional computational cost consists of calculating the L2-norm of all weights.\n",
    "This scales with respect to the number of features, not number of data points.\n",
    "c) True. Ridge regression biases the model to have smaller weights and with the hope of being less likely to overfit-adding bias to reduce variance.\n",
    "d) False. The sum of convex functions is also convex.\n",
    "\n",
    "## 9. Select All That Apply\n",
    "\n",
    "Consider minimizing a function $f(x):\\mathbb{R}\\rightarrow\\mathbb{R}$.\n",
    "Recall the following definitions:\n",
    "- $x\\in\\mathbb{R}$ is a global minimum for $f$ if $f(x^{\\prime})\\ge f(x)$ for all $x^{\\prime}\\in\\mathbb{R}$\n",
    "- $x\\in\\mathbb{R}$ is a local minimum for $f$ if there exists $\\epsilon>0$ such that $f(x^{\\prime})\\ge f(x)$ for all $x^{\\prime}\\in\\mathbb{R}$ within $\\epsilon$ distance of $x$, that is, $|x^{\\prime}-x|<\\epsilon.$\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. All linear functions $f(x)=ax+b$ for some $a, b\\in\\mathbb{R}$ are both convex and concave.\n",
    "\n",
    "b. If $f$ is convex, then it can have at most one global minimum. (That is, if $u, v\\in\\mathbb{R}$\n",
    "are both global minima for $f$, then that implies $u=v$.)\n",
    "\n",
    "c. If $f$ is convex, then all local minima are global minima.\n",
    "\n",
    "d. If $f$ is convex and bounded below (i.e., there exists $c\\in\\mathbb{R}$ such that $f(x)\\ge c$ for all\n",
    "$x\\in\\mathbb{R}$) then it must have at least one global minimum.\n",
    "\n",
    "e. If $f$ is concave, then it must have no global minima.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "Explanation:\n",
    "a) True. Linear functions are convex. Any of the tests we discussed in class apply, e.g., their second derivative (which is 0) is always greater than or equal to 0. If f is linear, then f is also linear and therefore convex, so f is also concave.\n",
    "b) False. Consider the constant function $f(x)=0$. Every\n",
    "$x\\in\\mathbb{R}$ is a global minimum.\n",
    "c) True. See class notes from lecture 7.\n",
    "d) False. For example, $f(x)$ could be monotonically decreasing and asymptotically approaching $0$ as $x$ increases, so it is bounded below by 0 but has no global minimum.\n",
    "e) False. Consider the same constant function $f(x)=0$.\n",
    "\n",
    "## 10. One Answer\n",
    "\n",
    "Let's say we want to standardize our data (i.e., normalizing the data to have zero mean and unit variance in each dimension) for the purposes of training and evaluating a ML model.\n",
    "Which of the following would be most appropriate?\n",
    "\n",
    "a. Split the dataset into the train/val/test splits, standardize the data separately for each split using the mean and variance statistics of that split.\n",
    "\n",
    "b. Split the dataset into the train/val/test splits, standardize the data for the training set, and use the mean and variance statistics of the training data to standardize the validation and test sets.\n",
    "\n",
    "c. Split the dataset into the train/val/test splits, standardize the training and validation sets separately using the mean and variance statistics of each split, then use the mean and variance statistics of the validation split to normalize the test set.\n",
    "\n",
    "d. Standardize the entire dataset (i.e., all splits combined) using the combined mean and variance statistics.\n",
    "Then, split the standardized data into train/val/test sets.\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "Explanation: We should do (b) to avoid leaking test set information to the training process.\n",
    "Other options may lead to overfitting to the validation or test data when picking hyperparameters.\n",
    "\n",
    "## 11. Select All That Apply\n",
    "\n",
    "Which of the following statements about gradient descent are true?\n",
    "Recall that the gradient descent algorithm updates the weight parameter $w$ at iteration $t$ as follows: $$w_{t+1}=w_{t}-\\eta\\nabla_{w}l(w)|_{w=w_{t}}$$ (with $\\eta$ being the step size).\n",
    "For this question, we say that gradient descent has converged by iteration $T$ if there is some iteration $t<T$ such that $||\\nabla_{w}l(w_{t})||_{2}^{2}\\le\\epsilon$ for some fixed $\\epsilon>0$.\n",
    "\n",
    "a. The gradient $\\nabla_{w}l(w)$ points in the direction that maximizes the training loss.\n",
    "\n",
    "b. Assume $l(w)$ is convex. Then if gradient descent converges by iteration $T$ for some\n",
    "fixed $\\epsilon>0$ and some step size $\\eta$, it will converge in at most $T$ iterations if we increase the step size $\\eta$.\n",
    "\n",
    "c. Assume $l(w)$ is convex. Then if gradient descent converges by iteration $T$ for some fixed $\\epsilon>0$ and some step size $\\eta$, it will also eventually converge for all smaller step sizes $0<\\eta^{\\prime}<\\eta$ given enough iterations.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "Explanation:\n",
    "a) True. $\\nabla_{w}.l(w)$ points in the direction that maximizes the loss.\n",
    "Don't confuse this with the gradient descent update which steps in the \"negative-gradient\" direction.\n",
    "b) False. large step size may cause the model to overshoot the optimum point, thus taking longer to converge.\n",
    "c) True. With smaller step size, the model is likely to gradually approach the optimal point with less overshooting even if it takes more iterations.\n",
    "\n",
    "## 12.\n",
    "\n",
    "Describe one advantage of mini-batch stochastic gradient descent over full-batch gradient descent.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: One advantage is that mini-batch SGD is faster to compute over full-batch GD, while still offering an unbiased estimate of the gradient full-batch GD would compute.\n",
    "Another advantage is the variance of mini-batch SGD can lead to randomness that might help avoid local minima where full-batch GD might get stuck.\n",
    "\n",
    "## 13.\n",
    "\n",
    "Describe one advantage of mini-batch stochastic gradient descent $(1<B<n)$ over stochastic gradient descent with batch size $B=1$ (e.g., updating the parameters at each iteration based only on one randomly sampled training point).\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: Possible answer: the update steps of mini-batch SGD will have less variance and might converge in fewer update steps.\n",
    "More possible answers:\n",
    "Noise Reduction: Mini-batches average the gradient over multiple samples, reducing the variance and leading to more stable updates.\n",
    "Faster Convergence: By reducing noise, the algorithm can converge faster to a minimum.\n",
    "Computational Efficiency: Mini-batches enable efficient use of parallelization on hardware like GPUs.\n",
    "Better Generalization: Smoother updates can help the model generalize better.\n",
    "Reduced Frequency of Parameter Updates: Fewer updates per epoch, which can improve training dynamics and efficiency.\n",
    "parallelizability\n",
    "\n",
    "## 14. One Answer\n",
    "\n",
    "In a machine learning course, the distribution of final exam scores is approximately normal.\n",
    "However, an administrative error provided some students with prior access to practice materials closely resembling the exam, resulting in significant score increases for these students.\n",
    "Considering only the scores and without labeled information about who had access to the materials, what type of model would be most appropriate to estimate the likelihood that a given student had access to the practice materials?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: This is an unsupervised learning problem because there are no labels indicating which students had access to the materials.\n",
    "The overall score distribution is a mixture of two Gaussian distributions:\n",
    "1. Students without access: Their scores follow the original normal distribution.\n",
    "2. Students with access: Their scores are higher on average, forming a second Gaussian with a higher mean.\n",
    "A Gaussian Mixture Model (GMM) is the most suitable choice, as it models this bimodal distribution by combining multiple Gaussians.\n",
    "k-means clustering could also be used but is less effective, as it assumes.\n",
    "spherical clusters and does not explicitly account for Gaussian distributions.\n",
    "\n",
    "## 15. Select All That Apply\n",
    "\n",
    "Assume we are given a fixed dataset $D=\\{x^{(1)},x^{(2)},...,x^{(n)}\\}$ drawn i.i.d. (independently and identically distributed) from an underlying distribution $P(x)$.\n",
    "We use the bootstrap to draw bootstrap samples $\\tilde{D}=\\{\\tilde{x}^{(1)},\\tilde{x}^{(2)},...\\}$ from a bootstrap distribution $Q(x)$.\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. The bootstrap samples in $\\tilde{D}$ are drawn by sampling with replacement from D.\n",
    "\n",
    "b. The bootstrap samples in $\\tilde{D}$ are drawn by sampling without replacement from D.\n",
    "\n",
    "c. The distribution of bootstrap samples in $\\tilde{D}$ is always identical to the underlying data distribution P.\n",
    "\n",
    "d. The bootstrap samples in $\\tilde{D}$ are independently and identically distributed.\n",
    "\n",
    "Correct answers: (a), (d)\n",
    "\n",
    "Explanation:\n",
    "a) True. The bootstrap distribution is created by sampling with replacement from the fixed dataset.\n",
    "b) False. Inverse of option (a)\n",
    "c) False. Bootstrap samples are not guaranteed to be identical to population distribution.\n",
    "d) True.\n",
    "By construction of the bootstrap method.\n",
    "\n",
    "## 16.\n",
    "\n",
    "You are given a dataset with four data points $x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\in\\mathbb{R}$. The coordinates of these data points are:\n",
    "- $x^{(1)}=0$\n",
    "- $x^{(2)}=1$\n",
    "- $x^{(3)}=5$\n",
    "- $x^{(4)}=9$.\n",
    "You run k-means on this dataset with $k=3$ centroids, initialized at the first 3 data points: 0, 1, and 5. After k-means converges, what will be the new coordinates of these centroids?\n",
    "Give your answer as a sequence of 3 numbers in ascending order (e.g., \"0, 1, 5\").\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: 0.1,7. In the first iteration, $x^{(1)}$ will be assigned to the first centroid, $x^{(2)}$ to the second centroid, and $x^{(3)}$ and $x^{(4)}$ to the third centroid.\n",
    "Thus the centroids will be updated to 0, 1, 7 respectively.\n",
    "The centroid assignments will not change in subsequent assignments, so k-means will converge after one iteration.\n",
    "Note that this clustering is not optimal (in the sense of $L_{2}$ distance from centroids);\n",
    "this is an example of how k-means can fail to find the globally optimal clustering.\n",
    "\n",
    "## 17. Select All That Apply\n",
    "\n",
    "Which of the following statements are true about k-means?\n",
    "\n",
    "a. The output of k-means can change depending on the initial centroid positions.\n",
    "\n",
    "b. Assuming that the number of data points is divisible by k, k-means with k clusters always outputs clusters of equal sizes.\n",
    "\n",
    "c. If run for long enough, k-means will always find the globally optimal solution (as measured by the average L2 distance between each point and its assigned cluster centroid).\n",
    "\n",
    "d. K-means will not converge unless all clusters in the underlying data distribution have equal, spherical variance.\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation:\n",
    "a) True.\n",
    "b) False. k-means is not guaranteed to produce clusters of equal sizes, it depends on where the distance between the points\n",
    "c) False.\n",
    "k-means will converge when the cluster arrangement no longer changes.\n",
    "This may only be a local optimum, running longer would not help.\n",
    "d) False.\n",
    "k-means will converge when the cluster arrangement no longer changes.\n",
    "This may only be a local optimum, running longer would not help.\n",
    "\n",
    "## 18.\n",
    "\n",
    "Should we initialize all the weights of a neural network to be the same small constant value (e.g., 0.001)?\n",
    "Why or why not?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: No. It is important to break symmetry so that all neurons do not get the same gradient updates.\n",
    "\n",
    "## 19. Select All That Apply\n",
    "\n",
    "In a neural network, the number of layers is an important hyperparameter.\n",
    "Which of these statements are true about adding layers to a neural network (keeping all other aspects of the model and training process the same)?\n",
    "\n",
    "a. Hyperparameters are independent, i.e., adding more layers will not affect the optimal choice of step size for gradient descent or the amount of regularization needed.\n",
    "\n",
    "b. We cannot use cross-validation to select hyperparameters that directly affect model architecture, such as the number of layers.\n",
    "\n",
    "c. Adding more layers generally decreases the training loss.\n",
    "\n",
    "d. Adding more layers generally increases the ability of the model to overfit the data.\n",
    "\n",
    "Correct answers: (c), (d)\n",
    "\n",
    "Explanation:\n",
    "a) False. Adding layers can affect optimal learning rates and regularization needs.\n",
    "b) False.\n",
    "Cross-validation can be used to select architecture-related hyperparameters like the number of layers.\n",
    "c) True.\n",
    "More layers improve representational capacity, reducing training loss.\n",
    "d) True. Deeper networks can overfit without proper regularization.\n",
    "\n",
    "## 20. Select All That Apply\n",
    "\n",
    "Which of the following are advantages of Gaussian Mixture Models (GMMs) over K-means for a clustering application?\n",
    "\n",
    "a GMMs are better suited if clusters have varying sizes and/or shapes.\n",
    "\n",
    "b GMMs are better equipped to model overlapping clusters.\n",
    "\n",
    "c GMMs are better suited to reason probabilistically about the data and the clusters.\n",
    "\n",
    "d On a given dataset, a single iteration of the EM algorithm for fitting a GMM requires less computation than a single iteration of Lloyd’s Algorithm for fitting K-means.\n",
    "\n",
    "Correct answers: (a), (b), (c)\n",
    "\n",
    "Explanation:\n",
    " a) True. GMMs can model clusters with different sizes and shapes because they use a combination of Gaussian distributions, each with its own mean and covariance matrix.\n",
    "b) True. GMMs can handle overlapping clusters by assigning probabilities to each data point for belonging to each cluster.\n",
    "c) True. GMMs provide a probabilistic framework, giving the likelihood of each data point belonging to each cluster, which is useful for probabilistic reasoning.\n",
    "\n",
    "\n",
    "d) False. The Expectation-Maximization (EM) algorithm used for fitting GMMs is generally more com putationally intensive per iteration compared to Lloyd’s Algorithm for K-means, due to the additional steps of calculating probabilities and updating the covariance matrices.\n",
    "\n",
    "## 21. One Answer\n",
    "\n",
    "Kernel methods calculate the inner products of features in a transformed feature space, without explicitly computing the transformed features.\n",
    "\n",
    "a. True\n",
    "\n",
    "b. False\n",
    "\n",
    "Correct answers: (a)\n",
    "Explanation: A function K : Rd × Rd → R is a kernel for a map φ if K(x, x0) = φ(x) · φ(x0) = hφ(x), φ(x0)i for all x, x0.\n",
    "φ(x) doesn’t need to be explicitly computed.\n",
    "\n",
    "## 22. One Answer\n",
    "\n",
    "Consider a fully connected neural network (MLP) with an input layer, a hidden layer, and an output layer.\n",
    "The input layer has n units, the hidden layer has h units, and the output layer has m units.\n",
    "Assume there are no bias units/terms. Which of the following statements about the number of trainable parameters is true?\n",
    "\n",
    "a. The total number of trainable parameters is $n \\cdot h \\cdot m$.\n",
    "\n",
    "b. The total number of trainable parameters is $n \\cdot h + h \\cdot m$.\n",
    "\n",
    "c. The total number of trainable parameters is $(n + 1) \\cdot h + (h + 1) \\cdot m$.\n",
    "\n",
    "d. The total number of trainable parameters is $n + h + m$.\n",
    "\n",
    "Correct answers: (b)\n",
    " Explanation: Connections between the input and the hidden layer: $n \\cdot h$;\n",
    "connections between the hidden and the output layer: $h \\cdot m$.\n",
    "\n",
    "## 23. Select All That Apply\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition $A = USV^\\top$, where $S$ is an $r \\times r$ diagonal matrix and $r = \\operatorname{rank}(A) \\leq \\min(m, n)$.\n",
    "\n",
    "Which of the following statements are correct?\n",
    "\n",
    "a. The columns of $U$ are the eigenvectors of $A^\\top A$.\n",
    "\n",
    "b. The columns of $U$ are the eigenvectors of $A A^\\top$.\n",
    "\n",
    "c. The columns of $V$ are the eigenvectors of $A^\\top A$.\n",
    "\n",
    "d. The columns of $V$ are the eigenvectors of $A A^\\top$.\n",
    "\n",
    "e. The singular values in $S$ are the square roots of the nonzero eigenvalues of $A A^\\top$.\n",
    "\n",
    "f. The singular values in $S$ are the square roots of the nonzero eigenvalues of $A^\\top A$.\n",
    "\n",
    "Correct answers: (b), (c), (e), (f)\n",
    "\n",
    "Explanation: $A A^\\top = U S^2 U^\\top$, implying that the columns of $U$ are the eigenvectors of $A A^\\top$ with correspond ing eigenvalues along the diagonal of $S^2$.\n",
    "\n",
    "Similarly, $A^\\top A = V S^2 V^\\top$, implying that the columns of $V$ are the eigenvectors of $A^\\top A$ with corresponding eigenvalues along the diagonal of $S^2$.\n",
    "\n",
    "## 24.\n",
    "\n",
    "Consider a dataset $X \\in \\mathbb{R}^{n \\times p}$ with $n$ observations and $p$ features, and with corresponding covariance matrix $\\Sigma$.\n",
    "\n",
    "Let $\\lambda_{1} \\geq \\lambda_{2} \\geq ... \\geq \\lambda_{p}$ be the eigenvalues of $\\Sigma$ in descending order.\n",
    "\n",
    "Express the total variance explained by the first $k$ principal components (obtained by performing Principal Component Analysis (PCA) on $X$) as a fraction of the total variance in the original data.\n",
    "\n",
    "Answer: Fraction of total variance explained $= \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}$.\n",
    "\n",
    "The fraction of total variance explained by the first $k$ principal components in PCA can be expressed as the ratio of the sum of the first $k$ eigenvalues to the sum of all eigenvalues of the covariance matrix $\\Sigma$.\n",
    "\n",
    "## 25.\n",
    "\n",
    "Consider a dataset $X \\in \\mathbb{R}^{n \\times 2}$ with $n$ observations and 2 features.\n",
    "Suppose $\\Sigma$ is the covariance matrix of the dataset:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} 3 & \\sqrt{3} \\\\ \\sqrt{3} & 5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This covariance matrix has the following unit-norm eigenvectors $u$ and $v$:\n",
    "\n",
    "$u = \\frac{1}{2}\\begin{pmatrix} -\\sqrt{3} \\\\ 1 \\end{pmatrix}$, $v = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ \\sqrt{3} \\end{pmatrix}$\n",
    "\n",
    "Write the second principal component as a unit-length vector in vector form (i.e., $[a, b]$).\n",
    "Second principal component:\n",
    "\n",
    "Explanation: $u = \\frac{1}{2}\\begin{pmatrix} -\\sqrt{3} \\\\ 1 \\end{pmatrix}$\n",
    "A vector $x$ and value $\\lambda$ are defined to be an eigenvector-eigenvalue pair of $A$ if $A x = \\lambda x$.\n",
    "$\\Sigma u = 2u$, so $\\lambda_u = 2$.\n",
    "$\\Sigma v = 6v$, so $\\lambda_v = 6$.\n",
    "Eigenvector-eigenvalue pairs of a covariance matrix represent pairs of principal components and the variance explained by that principal component.\n",
    "$u$'s eigenvalue is less than $v$'s, so it is the second principal component.\n",
    "$u$ is already unit-length, so it is the final answer.\n",
    "\n",
    "## 26. Select All That Apply\n",
    "\n",
    "You are applying PCA to a training dataset of $n = 1024$ grayscale images that are each $16 \\times 16$ pixels ($256$ pixels per image).\n",
    "Consider reshaping each image into a vector $x_i \\in \\mathbb{R}^{256}$ and then composing a data matrix $X \\in \\mathbb{R}^{1024 \\times 256}$, where the $i$th row is $x_i^\\top$.\n",
    "Let $\\hat{x}_{i,k} \\in \\mathbb{R}^{256}$ be the PCA reconstruction of image $x_i$ using the top $k$ principal component directions in the data.\n",
    "Let $R(k)$ be the average reconstruction error on the training data using $k$ principal components, $R(k) = \\frac{1}{n} \\sum_{i=1}^{n} ||x_i - \\hat{x}_{i,k}||^2_2$.\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. $R(k)$ is monotonically decreasing as $k$ increases, up to $k = 1024$. That is, if $0 < k_1 < k_2 \\leq 1024$, then $R(k_1) > R(k_2)$.\n",
    "\n",
    "b. If $k < \\operatorname{rank}(X)$, then $R(k) > 0$.\n",
    "\n",
    "c. If $k \\geq \\operatorname{rank}(X)$, then $R(k) = 0$.\n",
    "\n",
    "d. For $k \\geq 1$, let $\\delta(k) = R(k-1) - R(k)$ be the decrease in reconstruction error by going from $k-1$ to $k$ principal components.\n",
    "(When $k = 0$, define the reconstruction of $x_i$ to simply be the mean image $\\bar{x}$.) Then, $\\delta(k)$ is monotonically non-increasing as $k$ increases.\n",
    "\n",
    "Correct answers: (b), (c), (d)\n",
    "\n",
    "Explanation:\n",
    "a) False. The number of principal components cannot exceed the rank of $X$, which is $\\min(n, 256)$.\n",
    "Since $X$ is $1024 \\times 256$, its rank is at most $256$. Thus, $R(k)$ is only guaranteed to monotonically decrease for $k \\leq 256$, not $k \\leq 1024$.\n",
    "b) True.\n",
    "The reconstruction error is non-zero when the number of principal components $k$ is less than the rank of $X$, as there are remaining variations in $X$ not captured by the top $k$ components.\n",
    "c) True. When $k$ is greater than or equal to the rank of $X$, the PCA reconstruction captures all the variation in $X$, resulting in zero reconstruction error.\n",
    "d) True. Each additional principal component explains the maximum remaining variance, so the decrease in reconstruction error ($R(k)$) diminishes as $k$ increases, making $R(k)$ monotonically non-increasing.\n",
    "\n",
    "## 27. Select All That Apply\n",
    "\n",
    "Which of the following is/are true about the $k$-Nearest Neighbors (k-NN) algorithm?\n",
    "\n",
    "a. Testing time (i.e., the amount of time it takes to produce an output for a new test point) increases with the number of training samples.\n",
    "\n",
    "b. The number of hyperparameters increases with the number of training samples.\n",
    "\n",
    "c. $k$-NN can learn non-linear decision boundaries.\n",
    "\n",
    "d. $k$-NN clusters unlabeled samples in a $k$-dimensional space based on their similarity.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "## 28. Select All That Apply\n",
    "\n",
    "Which of the following statements about random forests and decision trees are true?\n",
    "\n",
    "a Random forests are generally easier for humans to interpret than individual decision trees.\n",
    "\n",
    "b Random forests reduce variance (compared to individual decision trees) by aggre gating predictions over multiple decision trees.\n",
    "\n",
    "c When constructing the individual trees in the random forest, we want their predic tions to be as correlated with each other as possible.\n",
    "\n",
    "d Random forests can give a notion of confidence estimates by examining the distri bution of outputs that each individual tree in the random forest produces.\n",
    "\n",
    "Correct answers: (b), (d)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "a) False. Procedure is similar except random forest utilizes multiple decision trees\n",
    "\n",
    "b) True.\n",
    "Aggregating predictions from multiple trees reduces sensitivity compared to a single tree.\n",
    "\n",
    "c) False.\n",
    "Having as correlated trees as possible degenerates to a single tree, losing the benefits of a more complex forest.\n",
    "\n",
    "d) True. Spread of decisions across different trees gives a confidence estimate. \n",
    "\n",
    "## 29. Select All That Apply\n",
    "\n",
    "Which of the following is a correct statement about (mini-batch) Stochastic Gradient Descent (SGD)?\n",
    "\n",
    "a The variance of the gradient estimates in SGD decreases as the batch size increases.\n",
    "\n",
    "b Running SGD with batch size 1 for n iterations is generally slower than running full batch gradient descent with batch size n for 1 iteration, because the gradients for each training point in SGD have to be computed sequentially, whereas the gradients in full-batch gradient descent can be computed in parallel.\n",
    "\n",
    "c SGD is faster than full-batch gradient descent because it only updates a subset of model parameters with each step.\n",
    "\n",
    "d SGD provides an unbiased estimate of the true (full-batch) gradient of the training loss.\n",
    "\n",
    "Correct answers: (a), (b), (d)\n",
    "Explanation:\n",
    "\n",
    "a) True. In SGD, the gradient is estimated using a subset of the data.\n",
    "A sampled batch might not represent the entire dataset well.\n",
    "As the batch size increases, it becomes more representative of the entire dataset, reducing the variance in the gradient estimates.\n",
    "\n",
    "b) True. In batch gradient descent, all gradients for the entire dataset are computed in one forward backward pass, which can leverage parallel processing (e.g., on GPUs).\n",
    "\n",
    "c) False. SGD does not update a subset of model parameters.\n",
    "\n",
    "It updates all parameters based on the gradient computed from a subset of the data.\n",
    "The faster convergence of SGD compared to full-batch gradient descent is due to the more frequent updates.\n",
    "\n",
    "d) True.\n",
    "\n",
    "The gradient computed on a mini-batch is an unbiased estimate of the full gradient because the mini-batch is a random sample of the dataset.\n",
    "This randomness ensures that, on average, the mini-batch gradient equals the true gradient over the entire dataset.\n",
    "\n",
    "## 30. One Answer\n",
    "\n",
    "The probability density function for a gamma distribution with parameters $\\theta > 0$, $k > 0$ is\n",
    " $f(x; \\theta, k) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{- \\frac{x}{\\theta}}$,\n",
    " where\n",
    " $\\Gamma(x) = (x − 1)!$\n",
    "\n",
    "Say we have a dataset D of n data points, $\\{x^{(1)}, x^{(2)}, . . .$\n",
    "$, x^{(n)}\\}$, where each $x \\in R$.\n",
    " \n",
    "Assume that k is given to us and fixed.\n",
    "\n",
    "We would like to use D to find the maximum likelihood estimator for $\\theta$.\n",
    "What is the maximum likelihood estimator for $\\theta$ in terms of k, n, and $x^{(1)}, x^{(2)}, . . .$\n",
    "$, x^{(n)}$?\n",
    "\n",
    "Hint: The argmax of the logarithm of a function is the same as the argmax of the function.\n",
    "\n",
    "a. $\\frac{1}{kn}\\sum_{i=1}^{n} x^{(i)}$\n",
    "\n",
    "b. $\\frac{n(k-1)!}{\\sum_{i=1}^{n} x^{(i)}e^{- \\frac{x^{(i)}}{k}}}$\n",
    "\n",
    "c. $\\ln(\\frac{1}{n}\\sum_{i=1}^{n} x^{(i)}) − n(k − 1)!$\n",
    "\n",
    "d. $\\ln(k)−\\frac{(k-1)!}{k}$\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation: To find the maximum likelihood estimator (MLE) for $\\theta$, we start with the likelihood function for a dataset D = $\\{x^{(1)}, x^{(2)}, .$.\n",
    "$. , x^{(n)}\\}$:\n",
    " $L(\\theta) = \\prod_{i=1}^{n} f(x^{(i)}; \\theta, k) = \\prod_{i=1}^{n} \\frac{1}{\\Gamma(k)\\theta^k}(x^{(i)})^{k-1}e^{- \\frac{x^{(i)}}{\\theta}}$.\n",
    "\n",
    "The log-likelihood function is:\n",
    " $\\ell(\\theta) = \\sum_{i=1}^{n} \\ln f(x^{(i)}; \\theta, k) = -n \\ln \\Gamma(k) - kn \\ln \\theta + (k - 1) \\sum_{i=1}^{n} \\ln x^{(i)} - \\frac{1}{\\theta} \\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "To maximize $\\ell(\\theta)$, we differentiate with respect to $\\theta$ and set the derivative to zero:\n",
    " $\\frac{\\partial\\ell}{\\partial\\theta} = -\\frac{kn}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^{n} x^{(i)} = 0$.\n",
    "\n",
    "Multiply through by $\\theta^2$to simplify:\n",
    " $-kn\\theta + \\sum_{i=1}^{n} x^{(i)} = 0 \\Rightarrow \\theta = \\frac{1}{kn} \\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "Thus, the maximum likelihood estimator for $\\theta$ is:\n",
    " $\\hat{\\theta}=\\frac{1}{kn}\\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "The correct answer is (a).\n",
    "\n",
    "## 31. One Answer\n",
    "\n",
    "Many ML algorithms, like the k-nearest neighbors (k-NN) algorithm, relies on distances between points.\n",
    "\n",
    "In high-dimensional spaces, distances can behave counterintuitively. This question illustrates one such example.\n",
    "\n",
    "Consider two d-dimensional hypercubes S and T centered around the origin.\n",
    "S has side length 2, while T is contained within S and has side length 1:\n",
    " $S = \\{x \\in R^d: ||x||_\\infty \\le 1\\}$\n",
    " $T = \\{x \\in R^d: ||x||_\\infty \\le \\frac{1}{2}\\}$.\n",
    "\n",
    "Alternatively, we can write $S = [-1, 1]^d$, and $T = [-\\frac{1}{2},\\frac{1}{2}]^d$.\n",
    "\n",
    "Let P be the uniform distribution of points in S. What is the probability of drawing a point x ∼ P such that x ∈ T, that is, x is contained within T?\n",
    "\n",
    "Give your answer in terms of d.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: The volume of S is $2^d$, while the volume of T is $1^d$. Since x is uniformly distributed in S, the probability of x $\\in$ T is the relative ratio of their volumes, which is $\\frac{1}{2^d}$.\n",
    "\n",
    "## 32. Select All That Apply\n",
    "\n",
    "Consider the following dataset of four points in R2:\n",
    "\n",
    " $x^{(1)} = (0, 0)$ $y^{(1)} = −1$\n",
    "\n",
    " $x^{(2)} = (0, 1)$ $y^{(2)} = +1$\n",
    "\n",
    " $x^{(3)} = (1, 0)$ $y^{(3)} = +1$\n",
    "\n",
    " $x^{(4)} = (1, 1)$ $y^{(4)} = −1$.\n",
    "\n",
    "This is also known as a XOR problem because the labels y are the result of applying the XOR operation to the two components of x.\n",
    "\n",
    "For a given data point $x \\in R^2$, denote its first dimension as $x_1$ and its second dimension as $x_2$.\n",
    "\n",
    "For example, $x^{(2)}_1 = 0$ and $x^{(2)}_2 = 1$. Which of the following statements are true?\n",
    "\n",
    "a. There exists a linear model $w \\in R^3$, which predicts +1 if\n",
    " $w^T \\begin{bmatrix} x_1 \\\\ x_2 \\\\ 1 \\end{bmatrix} \\ge 0$\n",
    " and −1 otherwise, that achieves 100% accuracy on this dataset.\n",
    "\n",
    "b. There exists a linear model $w \\in R^6$, which predicts +1 if\n",
    " $w^T \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1x_2 \\\\ 1 \\end{bmatrix} \\ge 0$\n",
    " and −1 otherwise, that achieves 100% accuracy on this dataset.\n",
    "\n",
    "c. Define a polynomial feature expansion $\\phi(x)$ as any function $\\phi(x) : R^2 \\to R^d$ that can be written as\n",
    " $\\begin{bmatrix} x_1^{a_1} x_2^{b_1} \\\\ x_1^{a_2} x_2^{b_2} \\\\ . \\\\ . \\\\ . \\\\ x_1^{a_d} x_2^{b_d} \\end{bmatrix}$\n",
    " for some integer d > 0 and integer vectors a, b $\\in Z^d$.\n",
    "\n",
    "Then there does not exist any polynomial feature expansion $\\phi(x)$ such that a linear model w which predicts +1 if $w^T\\phi(x) \\ge 0$, and −1 otherwise, achieves 100% accuracy on this dataset.\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "Explanation:\n",
    "a) There is no way to separate with linear features.\n",
    "\n",
    "For option b) and example weight vector is\n",
    " $\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ -4 \\\\ -1 \\end{bmatrix}$\n",
    "\n",
    "For option c) a counter example is feature expansion\n",
    " $\\begin{bmatrix} x_1^0 x_2^0 \\\\ x_1^0 x_2^1 \\\\ x_1^1 x_2^0 \\\\ x_1^1 x_2^1 \\end{bmatrix}$\n",
    " with weight vector\n",
    " $\\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$\n",
    "\n",
    "## 33. One Answer\n",
    "\n",
    "Consider the following transfer learning setting.\n",
    "\n",
    "We have a large neural network $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ pretrained on ImageNet, and we would like to use this to learn a classifier for our own binary classification task for medical images.\n",
    "\n",
    "We decide to freeze the neural network $\\phi$ and train a logistic regression classifier on top.\n",
    "\n",
    "Formally, we are given $n$ data points from our own medical imaging task $\\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$, where $x^{(i)} \\in \\mathbb{R}^d$, $y^{(i)} \\in \\{-1, +1\\}$.\n",
    "\n",
    "We train a classifier $\\hat{w} \\in \\mathbb{R}^p$:\n",
    "\n",
    "$$\\hat{w}= \\operatorname{argmin}_{w \\in \\mathbb{R}^p} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp \\left( -y^{(i)}w^\\top\\phi(x^{(i)}) \\right) \\right).$$\n",
    "\n",
    "Which of the following statements is true?\n",
    "\n",
    "a. Learning $\\hat{w}$ in this way is a convex optimization problem regardless of how complex $\\phi$ is.\n",
    "\n",
    "b. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is a convex function in each dimension.\n",
    "(Let $\\phi = [\\phi_1; \\phi_2; \\ldots ; \\phi_p]$; then we say $\\phi$ is convex in each dimension if each of $\\phi_1, \\phi_2, \\ldots, \\phi_p$ is a convex function).\n",
    "\n",
    "c. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is a linear function.\n",
    "\n",
    "d. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is the identity function and $p = d$.\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation: Since we freeze $\\phi$ and do not update it, this is equivalent to logistic regression with a fixed basis expansion.\n",
    "Thus, it is a convex optimization problem regardless of how complex $\\phi$ is.\n",
    "\n",
    "## 34.\n",
    "\n",
    "Recall that influence functions are used to approximate the effect of leaving out one training point, without actually retraining the model.\n",
    "\n",
    "Assume that we have a twice-differentiable, strongly convex loss function $\\ell(x, y; w)$, and as usual, we train a model $\\hat{w}$ to minimize the average training loss:\n",
    "\n",
    "$$\\hat{w}= \\operatorname{argmin}_{w} \\frac{1}{n} \\sum_{i=1}^{n} \\ell_i(w),$$\n",
    "\n",
    "where $\\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$ is our training set, and for notational convenience we define $\\ell_i(w) = \\ell(x^{(i)}, y^{(i)}; w)$.\n",
    "\n",
    "Let $\\Delta_{-i}$ be the change in the parameters $w$ after we remove training point $(x^{(i)}, y^{(i)})$ and retrain the model.\n",
    "\n",
    "The influence function approximation tells us that\n",
    "\n",
    "$$\\Delta_{-i} = \\frac{1}{n} H(\\hat{w})^{-1} \\nabla_w \\ell_i(w) \\Big|_{w=\\hat{w}}$$\n",
    "\n",
    "where the Hessian matrix $H(\\hat{w})$ is defined as\n",
    "\n",
    "$$H(\\hat{w}) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_w^2 \\ell_i(w) \\Big|_{w=\\hat{w}}$$\n",
    "\n",
    "Consider the following linear regression model $f_w(x) = w^\\top x$, where $x, w \\in \\mathbb{R}^d$.\n",
    "We train with unregularized least squares regression to obtain $\\hat{w}$.\n",
    "\n",
    "What is $\\Delta_{-i}$ for this model, in terms of $\\hat{w}$ and the training data points?\n",
    "\n",
    "Note: The symbols $\\ell$ and $H$ should not appear in your answer. Replace them by working out the appropriate loss.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: For least squares regression, we have that $\\ell_i(w) = \\frac{1}{2}(y^{(i)} - w^\\top x^{(i)})^2$. (The $\\frac{1}{2}$ is for convenience;\n",
    "we can leave it out without changing the final answer.) Thus, $\\nabla_w\\ell_i(w) = -(y^{(i)} - w^\\top x^{(i)})x^{(i)}$, and $H(w) = \\frac{1}{n}\\sum_{i=1}^{n} x^{(i)}x^{(i)\\top}$.\n",
    "\n",
    "Putting this together,\n",
    "\n",
    "$$\\Delta_{-i} = -\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x^{(i)}x^{(i)\\top}\\right)^{-1}(y^{(i)} - \\hat{w}^\\top x^{(i)}) x^{(i)}$$\n",
    "\n",
    "$$= -\\left(\\sum_{i=1}^{n}x^{(i)}x^{(i)\\top}\\right)^{-1}(y^{(i)} - \\hat{w}^\\top x^{(i)}) x^{(i)}.$$\n",
    "\n",
    "We accept both the simplified version (canceling $\\frac{1}{n}$) and the unsimplified version.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
