{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08085b6-2e7e-4e9e-b76e-4271e57b2f71",
   "metadata": {},
   "source": [
    "# Practice Problems 5 Solutions\n",
    "\n",
    "## 1. One Answer\n",
    "\n",
    "Imagine you are building a machine learning model to predict the stopping distance of cars based on their speed.\n",
    "\n",
    "You obtain a large dataset where each data point is a pair of observed speeds and stopping distances, and you decide to use a simple linear regression model to predict stopping distances from speed.\n",
    "\n",
    "However, in reality, the stopping distance increases quadratically with speed.\n",
    "As a result, your model consistently underestimates the stopping distance at higher speeds.\n",
    "\n",
    "Compared to using a model that can model a quadratic relationship between stopping distance and speed, would your model have high or low bias?\n",
    "\n",
    "a. High bias\n",
    "\n",
    "b. Low bias\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "## 2. One Answer\n",
    "\n",
    "Follow the same car scenario as the above question. Compared to using a model that can model a quadratic relationship between stopping distance and speed, would your model have high or low variance?\n",
    "\n",
    "a. High variance\n",
    "\n",
    "b. Low variance\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "## 3. One Answer\n",
    "\n",
    "Follow the same car scenario as the above question. In reality, stopping distance is also affected by weather conditions, which your our model does not capture.\n",
    "Which of these components of overall model error captures the error from not including weather conditions as a feature?\n",
    "\n",
    "a. Bias\n",
    "\n",
    "b. Variance\n",
    "\n",
    "c. Irreducible error\n",
    "\n",
    "Correct answers: (c)\n",
    "\n",
    "## 4. Select All That Apply\n",
    "\n",
    "Which of the following will generally help to reduce model variance?\n",
    "\n",
    "a. Increasing the size of the training data.\n",
    "\n",
    "b. Increasing the size of the validation data.\n",
    "\n",
    "c. Increasing the number of model parameters.\n",
    "\n",
    "d. Increasing the amount of regularization.\n",
    "\n",
    "Correct answers: (a), (d)\n",
    "\n",
    "Explanation:\n",
    "a) The model has access to more information and thus is less likely to overfit to noise.\n",
    "b) Increasing the size of the validation data does not help prevent the model from picking up noise in the training set.\n",
    "c) This helps reduce bias not variance\n",
    "d) Regularization helps prevent the model from overfitting to the training data.\n",
    "\n",
    "## 5. One Answer\n",
    "\n",
    "For machine learning models and datasets in general, as the number of training data points grows, the prediction error of the model on unseen data (data not found in the training set) approaches 0.\n",
    "\n",
    "a. True\n",
    "\n",
    "b. False\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "Explanation: Even with infinite data, there may be noise in the data or inherent unpredictability in the relationship between input and output, which limits how low the prediction error can go.\n",
    "\n",
    "## 6. Select All That Apply\n",
    "\n",
    "Which of the following statements about (binary) logistic regression is true?\n",
    "Recall that the sigmoid function is defined as $\\sigma(x)=\\frac{1}{1+e^{-x}}$ for $x\\in\\mathbb{R}.$\n",
    "\n",
    "a. For any finite input $x\\in\\mathbb{R}$, $\\sigma(x)$ is strictly greater than 0 and strictly less than 1. Thus, a binary logistic regression model with finite input and weights can never output a probability of exactly 0 or 1, and can never achieve a training loss of exactly 0.\n",
    "\n",
    "b. The first derivative of o is monotonically increasing.\n",
    "\n",
    "c. There exists a constant value $c\\in\\mathbb{R}$ such that o is convex when restricted to $x<c$ C and concave when restricted to $x\\ge c$\n",
    "\n",
    "d. For binary logistic regression, if the probability of the positive class is $\\sigma(x)$ then the probability of the negative class is $\\sigma(-x)$\n",
    "\n",
    "Correct answers: (a), (c), (d)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "a) True. $\\sigma(x)$ has horizontal asymptotes at 0 and 1 and therefore is strictly bounded between those values.\n",
    "Because the output probability is the output of o, this implies that the output probability is also strictly contained in (0,1).\n",
    "As it cannot output positive or negative labels with probability 1, it is therefore unable to reduce the training loss to exactly 0, though it can get arbitrarily close.\n",
    "\n",
    "b) False. The gradient is highest around $x=0$ and lowest at its asymptotes.\n",
    "\n",
    "c) True.\n",
    "True for $c=0$ and is apparent from visual inspection.\n",
    "\n",
    "d) True. $\\sigma(x)=\\frac{1}{1+e^{-x}}=\\frac{e^{x}}{1+e^{x}}=1-\\sigma(-x)$\n",
    "\n",
    "## 7. Select All That Apply\n",
    "\n",
    "Consider performing Lasso regression by finding parameters $w\\in\\mathbb{R}^{d}$ that minimize\n",
    "$$f(w)=\\sum_{i=1}^{n}(y^{(i)}-x^{(i)\\top}w)^{2}+\\lambda||w||_{1}.$$ \n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. Increasing $\\lambda$ will generally reduce the $L_{1}$ norm of the parameters $w$.\n",
    "\n",
    "b. Consider two models $w_{1}$, $w_{2}\\in\\mathbb{R}^{d}.$ Assume $w_{1}$ is more sparse, i.e., $w_{1}$\n",
    "has strictly more zero coefficients than $w_{2}$. Then $||w_{1}||_{1}<||w_{2}||_{1}$\n",
    "\n",
    "c. Increasing $\\lambda$ generally increases model bias.\n",
    "\n",
    "d. Increasing $\\lambda$ generally increases model variance.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "Explanation:\n",
    "a) True. Higher A shrinks coefficients, encouraging sparsity.\n",
    "\n",
    "b) False. Sparsity doesn't guarantee a smaller norm; non-zero coefficients' magnitudes matter.\n",
    "\n",
    "c) True. Larger A simplifies the model, leading to underfitting and higher bias.\n",
    "\n",
    "d) False. Larger A reduces flexibility, lowering variance.\n",
    "\n",
    "## 8. Select All That Apply\n",
    "\n",
    "Which of the following statements about ridge regression are true?\n",
    "\n",
    "a. When there are correlated features, ridge regression typically sets the weights of all but one of the correlated features to 0.\n",
    "\n",
    "b. Compared to unregularized linear regression, the additional computational cost of ridge regression scales with respect to the number of data points in the dataset.\n",
    "\n",
    "c. Ridge regression reduces variance at the expense of increasing bias.\n",
    "\n",
    "d. Using ridge and lasso regularization together (e.g., minimizing a training objective of the form $$f(w)=\\sum_{i=1}^{n}(y^{(i)}-x^{(i)\\top}w)^{2}+\\lambda_{1}||w||_{1}+\\lambda_{2}||w||_{2}^{2})$$ makes the training loss no longer convex.\n",
    "\n",
    "Correct answers: (c)\n",
    "\n",
    "Explanation:\n",
    "a) False. This statement is more akin to Lasso regression.\n",
    "Ridge regression is more likely to somewhat equally decrease the weights of correlated features to each be smaller (as opposed to only keeping one large).\n",
    "See lecture 5 slide 37-38.\n",
    "b) False. Ridge regression additional computational cost consists of calculating the L2-norm of all weights.\n",
    "This scales with respect to the number of features, not number of data points.\n",
    "c) True. Ridge regression biases the model to have smaller weights and with the hope of being less likely to overfit-adding bias to reduce variance.\n",
    "d) False. The sum of convex functions is also convex.\n",
    "\n",
    "## 9. Select All That Apply\n",
    "\n",
    "Consider minimizing a function $f(x):\\mathbb{R}\\rightarrow\\mathbb{R}$.\n",
    "Recall the following definitions:\n",
    "- $x\\in\\mathbb{R}$ is a global minimum for $f$ if $f(x^{\\prime})\\ge f(x)$ for all $x^{\\prime}\\in\\mathbb{R}$\n",
    "- $x\\in\\mathbb{R}$ is a local minimum for $f$ if there exists $\\epsilon>0$ such that $f(x^{\\prime})\\ge f(x)$ for all $x^{\\prime}\\in\\mathbb{R}$ within $\\epsilon$ distance of $x$, that is, $|x^{\\prime}-x|<\\epsilon.$\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. All linear functions $f(x)=ax+b$ for some $a, b\\in\\mathbb{R}$ are both convex and concave.\n",
    "\n",
    "b. If $f$ is convex, then it can have at most one global minimum. (That is, if $u, v\\in\\mathbb{R}$\n",
    "are both global minima for $f$, then that implies $u=v$.)\n",
    "\n",
    "c. If $f$ is convex, then all local minima are global minima.\n",
    "\n",
    "d. If $f$ is convex and bounded below (i.e., there exists $c\\in\\mathbb{R}$ such that $f(x)\\ge c$ for all\n",
    "$x\\in\\mathbb{R}$) then it must have at least one global minimum.\n",
    "\n",
    "e. If $f$ is concave, then it must have no global minima.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "Explanation:\n",
    "a) True. Linear functions are convex. Any of the tests we discussed in class apply, e.g., their second derivative (which is 0) is always greater than or equal to 0. If f is linear, then f is also linear and therefore convex, so f is also concave.\n",
    "b) False. Consider the constant function $f(x)=0$. Every\n",
    "$x\\in\\mathbb{R}$ is a global minimum.\n",
    "c) True. See class notes from lecture 7.\n",
    "d) False. For example, $f(x)$ could be monotonically decreasing and asymptotically approaching $0$ as $x$ increases, so it is bounded below by 0 but has no global minimum.\n",
    "e) False. Consider the same constant function $f(x)=0$.\n",
    "\n",
    "## 10. One Answer\n",
    "\n",
    "Let's say we want to standardize our data (i.e., normalizing the data to have zero mean and unit variance in each dimension) for the purposes of training and evaluating a ML model.\n",
    "Which of the following would be most appropriate?\n",
    "\n",
    "a. Split the dataset into the train/val/test splits, standardize the data separately for each split using the mean and variance statistics of that split.\n",
    "\n",
    "b. Split the dataset into the train/val/test splits, standardize the data for the training set, and use the mean and variance statistics of the training data to standardize the validation and test sets.\n",
    "\n",
    "c. Split the dataset into the train/val/test splits, standardize the training and validation sets separately using the mean and variance statistics of each split, then use the mean and variance statistics of the validation split to normalize the test set.\n",
    "\n",
    "d. Standardize the entire dataset (i.e., all splits combined) using the combined mean and variance statistics.\n",
    "Then, split the standardized data into train/val/test sets.\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "Explanation: We should do (b) to avoid leaking test set information to the training process.\n",
    "Other options may lead to overfitting to the validation or test data when picking hyperparameters.\n",
    "\n",
    "## 11. Select All That Apply\n",
    "\n",
    "Which of the following statements about gradient descent are true?\n",
    "Recall that the gradient descent algorithm updates the weight parameter $w$ at iteration $t$ as follows: $$w_{t+1}=w_{t}-\\eta\\nabla_{w}l(w)|_{w=w_{t}}$$ (with $\\eta$ being the step size).\n",
    "For this question, we say that gradient descent has converged by iteration $T$ if there is some iteration $t<T$ such that $||\\nabla_{w}l(w_{t})||_{2}^{2}\\le\\epsilon$ for some fixed $\\epsilon>0$.\n",
    "\n",
    "a. The gradient $\\nabla_{w}l(w)$ points in the direction that maximizes the training loss.\n",
    "\n",
    "b. Assume $l(w)$ is convex. Then if gradient descent converges by iteration $T$ for some\n",
    "fixed $\\epsilon>0$ and some step size $\\eta$, it will converge in at most $T$ iterations if we increase the step size $\\eta$.\n",
    "\n",
    "c. Assume $l(w)$ is convex. Then if gradient descent converges by iteration $T$ for some fixed $\\epsilon>0$ and some step size $\\eta$, it will also eventually converge for all smaller step sizes $0<\\eta^{\\prime}<\\eta$ given enough iterations.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "Explanation:\n",
    "a) True. $\\nabla_{w}.l(w)$ points in the direction that maximizes the loss.\n",
    "Don't confuse this with the gradient descent update which steps in the \"negative-gradient\" direction.\n",
    "b) False. large step size may cause the model to overshoot the optimum point, thus taking longer to converge.\n",
    "c) True. With smaller step size, the model is likely to gradually approach the optimal point with less overshooting even if it takes more iterations.\n",
    "\n",
    "## 12.\n",
    "\n",
    "Describe one advantage of mini-batch stochastic gradient descent over full-batch gradient descent.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: One advantage is that mini-batch SGD is faster to compute over full-batch GD, while still offering an unbiased estimate of the gradient full-batch GD would compute.\n",
    "Another advantage is the variance of mini-batch SGD can lead to randomness that might help avoid local minima where full-batch GD might get stuck.\n",
    "\n",
    "## 13.\n",
    "\n",
    "Describe one advantage of mini-batch stochastic gradient descent $(1<B<n)$ over stochastic gradient descent with batch size $B=1$ (e.g., updating the parameters at each iteration based only on one randomly sampled training point).\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: Possible answer: the update steps of mini-batch SGD will have less variance and might converge in fewer update steps.\n",
    "More possible answers:\n",
    "Noise Reduction: Mini-batches average the gradient over multiple samples, reducing the variance and leading to more stable updates.\n",
    "Faster Convergence: By reducing noise, the algorithm can converge faster to a minimum.\n",
    "Computational Efficiency: Mini-batches enable efficient use of parallelization on hardware like GPUs.\n",
    "Better Generalization: Smoother updates can help the model generalize better.\n",
    "Reduced Frequency of Parameter Updates: Fewer updates per epoch, which can improve training dynamics and efficiency.\n",
    "parallelizability\n",
    "\n",
    "## 14. One Answer\n",
    "\n",
    "In a machine learning course, the distribution of final exam scores is approximately normal.\n",
    "However, an administrative error provided some students with prior access to practice materials closely resembling the exam, resulting in significant score increases for these students.\n",
    "Considering only the scores and without labeled information about who had access to the materials, what type of model would be most appropriate to estimate the likelihood that a given student had access to the practice materials?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: This is an unsupervised learning problem because there are no labels indicating which students had access to the materials.\n",
    "The overall score distribution is a mixture of two Gaussian distributions:\n",
    "1. Students without access: Their scores follow the original normal distribution.\n",
    "2. Students with access: Their scores are higher on average, forming a second Gaussian with a higher mean.\n",
    "A Gaussian Mixture Model (GMM) is the most suitable choice, as it models this bimodal distribution by combining multiple Gaussians.\n",
    "k-means clustering could also be used but is less effective, as it assumes.\n",
    "spherical clusters and does not explicitly account for Gaussian distributions.\n",
    "\n",
    "## 15. Select All That Apply\n",
    "\n",
    "Assume we are given a fixed dataset $D=\\{x^{(1)},x^{(2)},...,x^{(n)}\\}$ drawn i.i.d. (independently and identically distributed) from an underlying distribution $P(x)$.\n",
    "We use the bootstrap to draw bootstrap samples $\\tilde{D}=\\{\\tilde{x}^{(1)},\\tilde{x}^{(2)},...\\}$ from a bootstrap distribution $Q(x)$.\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. The bootstrap samples in $\\tilde{D}$ are drawn by sampling with replacement from D.\n",
    "\n",
    "b. The bootstrap samples in $\\tilde{D}$ are drawn by sampling without replacement from D.\n",
    "\n",
    "c. The distribution of bootstrap samples in $\\tilde{D}$ is always identical to the underlying data distribution P.\n",
    "\n",
    "d. The bootstrap samples in $\\tilde{D}$ are independently and identically distributed.\n",
    "\n",
    "Correct answers: (a), (d)\n",
    "\n",
    "Explanation:\n",
    "a) True. The bootstrap distribution is created by sampling with replacement from the fixed dataset.\n",
    "b) False. Inverse of option (a)\n",
    "c) False. Bootstrap samples are not guaranteed to be identical to population distribution.\n",
    "d) True.\n",
    "By construction of the bootstrap method.\n",
    "\n",
    "## 16.\n",
    "\n",
    "You are given a dataset with four data points $x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\in\\mathbb{R}$. The coordinates of these data points are:\n",
    "- $x^{(1)}=0$\n",
    "- $x^{(2)}=1$\n",
    "- $x^{(3)}=5$\n",
    "- $x^{(4)}=9$.\n",
    "You run k-means on this dataset with $k=3$ centroids, initialized at the first 3 data points: 0, 1, and 5. After k-means converges, what will be the new coordinates of these centroids?\n",
    "Give your answer as a sequence of 3 numbers in ascending order (e.g., \"0, 1, 5\").\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: 0.1,7. In the first iteration, $x^{(1)}$ will be assigned to the first centroid, $x^{(2)}$ to the second centroid, and $x^{(3)}$ and $x^{(4)}$ to the third centroid.\n",
    "Thus the centroids will be updated to 0, 1, 7 respectively.\n",
    "The centroid assignments will not change in subsequent assignments, so k-means will converge after one iteration.\n",
    "Note that this clustering is not optimal (in the sense of $L_{2}$ distance from centroids);\n",
    "this is an example of how k-means can fail to find the globally optimal clustering.\n",
    "\n",
    "## 17. Select All That Apply\n",
    "\n",
    "Which of the following statements are true about k-means?\n",
    "\n",
    "a. The output of k-means can change depending on the initial centroid positions.\n",
    "\n",
    "b. Assuming that the number of data points is divisible by k, k-means with k clusters always outputs clusters of equal sizes.\n",
    "\n",
    "c. If run for long enough, k-means will always find the globally optimal solution (as measured by the average L2 distance between each point and its assigned cluster centroid).\n",
    "\n",
    "d. K-means will not converge unless all clusters in the underlying data distribution have equal, spherical variance.\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation:\n",
    "a) True.\n",
    "b) False. k-means is not guaranteed to produce clusters of equal sizes, it depends on where the distance between the points\n",
    "c) False.\n",
    "k-means will converge when the cluster arrangement no longer changes.\n",
    "This may only be a local optimum, running longer would not help.\n",
    "d) False.\n",
    "k-means will converge when the cluster arrangement no longer changes.\n",
    "This may only be a local optimum, running longer would not help.\n",
    "\n",
    "## 18.\n",
    "\n",
    "Should we initialize all the weights of a neural network to be the same small constant value (e.g., 0.001)?\n",
    "Why or why not?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: No. It is important to break symmetry so that all neurons do not get the same gradient updates.\n",
    "\n",
    "## 19. Select All That Apply\n",
    "\n",
    "In a neural network, the number of layers is an important hyperparameter.\n",
    "Which of these statements are true about adding layers to a neural network (keeping all other aspects of the model and training process the same)?\n",
    "\n",
    "a. Hyperparameters are independent, i.e., adding more layers will not affect the optimal choice of step size for gradient descent or the amount of regularization needed.\n",
    "\n",
    "b. We cannot use cross-validation to select hyperparameters that directly affect model architecture, such as the number of layers.\n",
    "\n",
    "c. Adding more layers generally decreases the training loss.\n",
    "\n",
    "d. Adding more layers generally increases the ability of the model to overfit the data.\n",
    "\n",
    "Correct answers: (c), (d)\n",
    "\n",
    "Explanation:\n",
    "a) False. Adding layers can affect optimal learning rates and regularization needs.\n",
    "b) False.\n",
    "Cross-validation can be used to select architecture-related hyperparameters like the number of layers.\n",
    "c) True.\n",
    "More layers improve representational capacity, reducing training loss.\n",
    "d) True. Deeper networks can overfit without proper regularization.\n",
    "\n",
    "## 20. Select All That Apply\n",
    "\n",
    "Which of the following are advantages of Gaussian Mixture Models (GMMs) over K-means for a clustering application?\n",
    "\n",
    "a GMMs are better suited if clusters have varying sizes and/or shapes.\n",
    "\n",
    "b GMMs are better equipped to model overlapping clusters.\n",
    "\n",
    "c GMMs are better suited to reason probabilistically about the data and the clusters.\n",
    "\n",
    "d On a given dataset, a single iteration of the EM algorithm for fitting a GMM requires less computation than a single iteration of Lloyd’s Algorithm for fitting K-means.\n",
    "\n",
    "Correct answers: (a), (b), (c)\n",
    "\n",
    "Explanation:\n",
    " a) True. GMMs can model clusters with different sizes and shapes because they use a combination of Gaussian distributions, each with its own mean and covariance matrix.\n",
    "b) True. GMMs can handle overlapping clusters by assigning probabilities to each data point for belonging to each cluster.\n",
    "c) True. GMMs provide a probabilistic framework, giving the likelihood of each data point belonging to each cluster, which is useful for probabilistic reasoning.\n",
    "\n",
    "\n",
    "d) False. The Expectation-Maximization (EM) algorithm used for fitting GMMs is generally more com putationally intensive per iteration compared to Lloyd’s Algorithm for K-means, due to the additional steps of calculating probabilities and updating the covariance matrices.\n",
    "\n",
    "## 21. One Answer\n",
    "\n",
    "Kernel methods calculate the inner products of features in a transformed feature space, without explicitly computing the transformed features.\n",
    "\n",
    "a. True\n",
    "\n",
    "b. False\n",
    "\n",
    "Correct answers: (a)\n",
    "Explanation: A function K : Rd × Rd → R is a kernel for a map φ if K(x, x0) = φ(x) · φ(x0) = hφ(x), φ(x0)i for all x, x0.\n",
    "φ(x) doesn’t need to be explicitly computed.\n",
    "\n",
    "## 22. One Answer\n",
    "\n",
    "Consider a fully connected neural network (MLP) with an input layer, a hidden layer, and an output layer.\n",
    "The input layer has n units, the hidden layer has h units, and the output layer has m units.\n",
    "Assume there are no bias units/terms. Which of the following statements about the number of trainable parameters is true?\n",
    "\n",
    "a. The total number of trainable parameters is $n \\cdot h \\cdot m$.\n",
    "\n",
    "b. The total number of trainable parameters is $n \\cdot h + h \\cdot m$.\n",
    "\n",
    "c. The total number of trainable parameters is $(n + 1) \\cdot h + (h + 1) \\cdot m$.\n",
    "\n",
    "d. The total number of trainable parameters is $n + h + m$.\n",
    "\n",
    "Correct answers: (b)\n",
    " Explanation: Connections between the input and the hidden layer: $n \\cdot h$;\n",
    "connections between the hidden and the output layer: $h \\cdot m$.\n",
    "\n",
    "## 23. Select All That Apply\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition $A = USV^\\top$, where $S$ is an $r \\times r$ diagonal matrix and $r = \\operatorname{rank}(A) \\leq \\min(m, n)$.\n",
    "\n",
    "Which of the following statements are correct?\n",
    "\n",
    "a. The columns of $U$ are the eigenvectors of $A^\\top A$.\n",
    "\n",
    "b. The columns of $U$ are the eigenvectors of $A A^\\top$.\n",
    "\n",
    "c. The columns of $V$ are the eigenvectors of $A^\\top A$.\n",
    "\n",
    "d. The columns of $V$ are the eigenvectors of $A A^\\top$.\n",
    "\n",
    "e. The singular values in $S$ are the square roots of the nonzero eigenvalues of $A A^\\top$.\n",
    "\n",
    "f. The singular values in $S$ are the square roots of the nonzero eigenvalues of $A^\\top A$.\n",
    "\n",
    "Correct answers: (b), (c), (e), (f)\n",
    "\n",
    "Explanation: $A A^\\top = U S^2 U^\\top$, implying that the columns of $U$ are the eigenvectors of $A A^\\top$ with correspond ing eigenvalues along the diagonal of $S^2$.\n",
    "\n",
    "Similarly, $A^\\top A = V S^2 V^\\top$, implying that the columns of $V$ are the eigenvectors of $A^\\top A$ with corresponding eigenvalues along the diagonal of $S^2$.\n",
    "\n",
    "## 24.\n",
    "\n",
    "Consider a dataset $X \\in \\mathbb{R}^{n \\times p}$ with $n$ observations and $p$ features, and with corresponding covariance matrix $\\Sigma$.\n",
    "\n",
    "Let $\\lambda_{1} \\geq \\lambda_{2} \\geq ... \\geq \\lambda_{p}$ be the eigenvalues of $\\Sigma$ in descending order.\n",
    "\n",
    "Express the total variance explained by the first $k$ principal components (obtained by performing Principal Component Analysis (PCA) on $X$) as a fraction of the total variance in the original data.\n",
    "\n",
    "Answer: Fraction of total variance explained $= \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}$.\n",
    "\n",
    "The fraction of total variance explained by the first $k$ principal components in PCA can be expressed as the ratio of the sum of the first $k$ eigenvalues to the sum of all eigenvalues of the covariance matrix $\\Sigma$.\n",
    "\n",
    "## 25.\n",
    "\n",
    "Consider a dataset $X \\in \\mathbb{R}^{n \\times 2}$ with $n$ observations and 2 features.\n",
    "Suppose $\\Sigma$ is the covariance matrix of the dataset:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} 3 & \\sqrt{3} \\\\ \\sqrt{3} & 5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This covariance matrix has the following unit-norm eigenvectors $u$ and $v$:\n",
    "\n",
    "$u = \\frac{1}{2}\\begin{pmatrix} -\\sqrt{3} \\\\ 1 \\end{pmatrix}$, $v = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ \\sqrt{3} \\end{pmatrix}$\n",
    "\n",
    "Write the second principal component as a unit-length vector in vector form (i.e., $[a, b]$).\n",
    "Second principal component:\n",
    "\n",
    "Explanation: $u = \\frac{1}{2}\\begin{pmatrix} -\\sqrt{3} \\\\ 1 \\end{pmatrix}$\n",
    "A vector $x$ and value $\\lambda$ are defined to be an eigenvector-eigenvalue pair of $A$ if $A x = \\lambda x$.\n",
    "$\\Sigma u = 2u$, so $\\lambda_u = 2$.\n",
    "$\\Sigma v = 6v$, so $\\lambda_v = 6$.\n",
    "Eigenvector-eigenvalue pairs of a covariance matrix represent pairs of principal components and the variance explained by that principal component.\n",
    "$u$'s eigenvalue is less than $v$'s, so it is the second principal component.\n",
    "$u$ is already unit-length, so it is the final answer.\n",
    "\n",
    "## 26. Select All That Apply\n",
    "\n",
    "You are applying PCA to a training dataset of $n = 1024$ grayscale images that are each $16 \\times 16$ pixels ($256$ pixels per image).\n",
    "Consider reshaping each image into a vector $x_i \\in \\mathbb{R}^{256}$ and then composing a data matrix $X \\in \\mathbb{R}^{1024 \\times 256}$, where the $i$th row is $x_i^\\top$.\n",
    "Let $\\hat{x}_{i,k} \\in \\mathbb{R}^{256}$ be the PCA reconstruction of image $x_i$ using the top $k$ principal component directions in the data.\n",
    "Let $R(k)$ be the average reconstruction error on the training data using $k$ principal components, $R(k) = \\frac{1}{n} \\sum_{i=1}^{n} ||x_i - \\hat{x}_{i,k}||^2_2$.\n",
    "Which of the following statements are true?\n",
    "\n",
    "a. $R(k)$ is monotonically decreasing as $k$ increases, up to $k = 1024$. That is, if $0 < k_1 < k_2 \\leq 1024$, then $R(k_1) > R(k_2)$.\n",
    "\n",
    "b. If $k < \\operatorname{rank}(X)$, then $R(k) > 0$.\n",
    "\n",
    "c. If $k \\geq \\operatorname{rank}(X)$, then $R(k) = 0$.\n",
    "\n",
    "d. For $k \\geq 1$, let $\\delta(k) = R(k-1) - R(k)$ be the decrease in reconstruction error by going from $k-1$ to $k$ principal components.\n",
    "(When $k = 0$, define the reconstruction of $x_i$ to simply be the mean image $\\bar{x}$.) Then, $\\delta(k)$ is monotonically non-increasing as $k$ increases.\n",
    "\n",
    "Correct answers: (b), (c), (d)\n",
    "\n",
    "Explanation:\n",
    "a) False. The number of principal components cannot exceed the rank of $X$, which is $\\min(n, 256)$.\n",
    "Since $X$ is $1024 \\times 256$, its rank is at most $256$. Thus, $R(k)$ is only guaranteed to monotonically decrease for $k \\leq 256$, not $k \\leq 1024$.\n",
    "b) True.\n",
    "The reconstruction error is non-zero when the number of principal components $k$ is less than the rank of $X$, as there are remaining variations in $X$ not captured by the top $k$ components.\n",
    "c) True. When $k$ is greater than or equal to the rank of $X$, the PCA reconstruction captures all the variation in $X$, resulting in zero reconstruction error.\n",
    "d) True. Each additional principal component explains the maximum remaining variance, so the decrease in reconstruction error ($R(k)$) diminishes as $k$ increases, making $R(k)$ monotonically non-increasing.\n",
    "\n",
    "## 27. Select All That Apply\n",
    "\n",
    "Which of the following is/are true about the $k$-Nearest Neighbors (k-NN) algorithm?\n",
    "\n",
    "a. Testing time (i.e., the amount of time it takes to produce an output for a new test point) increases with the number of training samples.\n",
    "\n",
    "b. The number of hyperparameters increases with the number of training samples.\n",
    "\n",
    "c. $k$-NN can learn non-linear decision boundaries.\n",
    "\n",
    "d. $k$-NN clusters unlabeled samples in a $k$-dimensional space based on their similarity.\n",
    "\n",
    "Correct answers: (a), (c)\n",
    "\n",
    "## 28. Select All That Apply\n",
    "\n",
    "Which of the following statements about random forests and decision trees are true?\n",
    "\n",
    "a Random forests are generally easier for humans to interpret than individual decision trees.\n",
    "\n",
    "b Random forests reduce variance (compared to individual decision trees) by aggre gating predictions over multiple decision trees.\n",
    "\n",
    "c When constructing the individual trees in the random forest, we want their predic tions to be as correlated with each other as possible.\n",
    "\n",
    "d Random forests can give a notion of confidence estimates by examining the distri bution of outputs that each individual tree in the random forest produces.\n",
    "\n",
    "Correct answers: (b), (d)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "a) False. Procedure is similar except random forest utilizes multiple decision trees\n",
    "\n",
    "b) True.\n",
    "Aggregating predictions from multiple trees reduces sensitivity compared to a single tree.\n",
    "\n",
    "c) False.\n",
    "Having as correlated trees as possible degenerates to a single tree, losing the benefits of a more complex forest.\n",
    "\n",
    "d) True. Spread of decisions across different trees gives a confidence estimate. \n",
    "\n",
    "## 29. Select All That Apply\n",
    "\n",
    "Which of the following is a correct statement about (mini-batch) Stochastic Gradient Descent (SGD)?\n",
    "\n",
    "a The variance of the gradient estimates in SGD decreases as the batch size increases.\n",
    "\n",
    "b Running SGD with batch size 1 for n iterations is generally slower than running full batch gradient descent with batch size n for 1 iteration, because the gradients for each training point in SGD have to be computed sequentially, whereas the gradients in full-batch gradient descent can be computed in parallel.\n",
    "\n",
    "c SGD is faster than full-batch gradient descent because it only updates a subset of model parameters with each step.\n",
    "\n",
    "d SGD provides an unbiased estimate of the true (full-batch) gradient of the training loss.\n",
    "\n",
    "Correct answers: (a), (b), (d)\n",
    "Explanation:\n",
    "\n",
    "a) True. In SGD, the gradient is estimated using a subset of the data.\n",
    "A sampled batch might not represent the entire dataset well.\n",
    "As the batch size increases, it becomes more representative of the entire dataset, reducing the variance in the gradient estimates.\n",
    "\n",
    "b) True. In batch gradient descent, all gradients for the entire dataset are computed in one forward backward pass, which can leverage parallel processing (e.g., on GPUs).\n",
    "\n",
    "c) False. SGD does not update a subset of model parameters.\n",
    "\n",
    "It updates all parameters based on the gradient computed from a subset of the data.\n",
    "The faster convergence of SGD compared to full-batch gradient descent is due to the more frequent updates.\n",
    "\n",
    "d) True.\n",
    "\n",
    "The gradient computed on a mini-batch is an unbiased estimate of the full gradient because the mini-batch is a random sample of the dataset.\n",
    "This randomness ensures that, on average, the mini-batch gradient equals the true gradient over the entire dataset.\n",
    "\n",
    "## 30. One Answer\n",
    "\n",
    "The probability density function for a gamma distribution with parameters $\\theta > 0$, $k > 0$ is\n",
    " $f(x; \\theta, k) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{- \\frac{x}{\\theta}}$,\n",
    " where\n",
    " $\\Gamma(x) = (x − 1)!$\n",
    "\n",
    "Say we have a dataset D of n data points, $\\{x^{(1)}, x^{(2)}, . . .$\n",
    "$, x^{(n)}\\}$, where each $x \\in R$.\n",
    " \n",
    "Assume that k is given to us and fixed.\n",
    "\n",
    "We would like to use D to find the maximum likelihood estimator for $\\theta$.\n",
    "What is the maximum likelihood estimator for $\\theta$ in terms of k, n, and $x^{(1)}, x^{(2)}, . . .$\n",
    "$, x^{(n)}$?\n",
    "\n",
    "Hint: The argmax of the logarithm of a function is the same as the argmax of the function.\n",
    "\n",
    "a. $\\frac{1}{kn}\\sum_{i=1}^{n} x^{(i)}$\n",
    "\n",
    "b. $\\frac{n(k-1)!}{\\sum_{i=1}^{n} x^{(i)}e^{- \\frac{x^{(i)}}{k}}}$\n",
    "\n",
    "c. $\\ln(\\frac{1}{n}\\sum_{i=1}^{n} x^{(i)}) − n(k − 1)!$\n",
    "\n",
    "d. $\\ln(k)−\\frac{(k-1)!}{k}$\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation: To find the maximum likelihood estimator (MLE) for $\\theta$, we start with the likelihood function for a dataset D = $\\{x^{(1)}, x^{(2)}, .$.\n",
    "$. , x^{(n)}\\}$:\n",
    " $L(\\theta) = \\prod_{i=1}^{n} f(x^{(i)}; \\theta, k) = \\prod_{i=1}^{n} \\frac{1}{\\Gamma(k)\\theta^k}(x^{(i)})^{k-1}e^{- \\frac{x^{(i)}}{\\theta}}$.\n",
    "\n",
    "The log-likelihood function is:\n",
    " $\\ell(\\theta) = \\sum_{i=1}^{n} \\ln f(x^{(i)}; \\theta, k) = -n \\ln \\Gamma(k) - kn \\ln \\theta + (k - 1) \\sum_{i=1}^{n} \\ln x^{(i)} - \\frac{1}{\\theta} \\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "To maximize $\\ell(\\theta)$, we differentiate with respect to $\\theta$ and set the derivative to zero:\n",
    " $\\frac{\\partial\\ell}{\\partial\\theta} = -\\frac{kn}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^{n} x^{(i)} = 0$.\n",
    "\n",
    "Multiply through by $\\theta^2$to simplify:\n",
    " $-kn\\theta + \\sum_{i=1}^{n} x^{(i)} = 0 \\Rightarrow \\theta = \\frac{1}{kn} \\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "Thus, the maximum likelihood estimator for $\\theta$ is:\n",
    " $\\hat{\\theta}=\\frac{1}{kn}\\sum_{i=1}^{n} x^{(i)}$.\n",
    "\n",
    "The correct answer is (a).\n",
    "\n",
    "## 31. One Answer\n",
    "\n",
    "Many ML algorithms, like the k-nearest neighbors (k-NN) algorithm, relies on distances between points.\n",
    "\n",
    "In high-dimensional spaces, distances can behave counterintuitively. This question illustrates one such example.\n",
    "\n",
    "Consider two d-dimensional hypercubes S and T centered around the origin.\n",
    "S has side length 2, while T is contained within S and has side length 1:\n",
    " $S = \\{x \\in R^d: ||x||_\\infty \\le 1\\}$\n",
    " $T = \\{x \\in R^d: ||x||_\\infty \\le \\frac{1}{2}\\}$.\n",
    "\n",
    "Alternatively, we can write $S = [-1, 1]^d$, and $T = [-\\frac{1}{2},\\frac{1}{2}]^d$.\n",
    "\n",
    "Let P be the uniform distribution of points in S. What is the probability of drawing a point x ∼ P such that x ∈ T, that is, x is contained within T?\n",
    "\n",
    "Give your answer in terms of d.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: The volume of S is $2^d$, while the volume of T is $1^d$. Since x is uniformly distributed in S, the probability of x $\\in$ T is the relative ratio of their volumes, which is $\\frac{1}{2^d}$.\n",
    "\n",
    "## 32. Select All That Apply\n",
    "\n",
    "Consider the following dataset of four points in R2:\n",
    "\n",
    " $x^{(1)} = (0, 0)$ $y^{(1)} = −1$\n",
    "\n",
    " $x^{(2)} = (0, 1)$ $y^{(2)} = +1$\n",
    "\n",
    " $x^{(3)} = (1, 0)$ $y^{(3)} = +1$\n",
    "\n",
    " $x^{(4)} = (1, 1)$ $y^{(4)} = −1$.\n",
    "\n",
    "This is also known as a XOR problem because the labels y are the result of applying the XOR operation to the two components of x.\n",
    "\n",
    "For a given data point $x \\in R^2$, denote its first dimension as $x_1$ and its second dimension as $x_2$.\n",
    "\n",
    "For example, $x^{(2)}_1 = 0$ and $x^{(2)}_2 = 1$. Which of the following statements are true?\n",
    "\n",
    "a. There exists a linear model $w \\in R^3$, which predicts +1 if\n",
    " $w^T \\begin{bmatrix} x_1 \\\\ x_2 \\\\ 1 \\end{bmatrix} \\ge 0$\n",
    " and −1 otherwise, that achieves 100% accuracy on this dataset.\n",
    "\n",
    "b. There exists a linear model $w \\in R^6$, which predicts +1 if\n",
    " $w^T \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1x_2 \\\\ 1 \\end{bmatrix} \\ge 0$\n",
    " and −1 otherwise, that achieves 100% accuracy on this dataset.\n",
    "\n",
    "c. Define a polynomial feature expansion $\\phi(x)$ as any function $\\phi(x) : R^2 \\to R^d$ that can be written as\n",
    " $\\begin{bmatrix} x_1^{a_1} x_2^{b_1} \\\\ x_1^{a_2} x_2^{b_2} \\\\ . \\\\ . \\\\ . \\\\ x_1^{a_d} x_2^{b_d} \\end{bmatrix}$\n",
    " for some integer d > 0 and integer vectors a, b $\\in Z^d$.\n",
    "\n",
    "Then there does not exist any polynomial feature expansion $\\phi(x)$ such that a linear model w which predicts +1 if $w^T\\phi(x) \\ge 0$, and −1 otherwise, achieves 100% accuracy on this dataset.\n",
    "\n",
    "Correct answers: (b)\n",
    "\n",
    "Explanation:\n",
    "a) There is no way to separate with linear features.\n",
    "\n",
    "For option b) and example weight vector is\n",
    " $\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ -4 \\\\ -1 \\end{bmatrix}$\n",
    "\n",
    "For option c) a counter example is feature expansion\n",
    " $\\begin{bmatrix} x_1^0 x_2^0 \\\\ x_1^0 x_2^1 \\\\ x_1^1 x_2^0 \\\\ x_1^1 x_2^1 \\end{bmatrix}$\n",
    " with weight vector\n",
    " $\\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$\n",
    "\n",
    "## 33. One Answer\n",
    "\n",
    "Consider the following transfer learning setting.\n",
    "\n",
    "We have a large neural network $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ pretrained on ImageNet, and we would like to use this to learn a classifier for our own binary classification task for medical images.\n",
    "\n",
    "We decide to freeze the neural network $\\phi$ and train a logistic regression classifier on top.\n",
    "\n",
    "Formally, we are given $n$ data points from our own medical imaging task $\\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$, where $x^{(i)} \\in \\mathbb{R}^d$, $y^{(i)} \\in \\{-1, +1\\}$.\n",
    "\n",
    "We train a classifier $\\hat{w} \\in \\mathbb{R}^p$:\n",
    "\n",
    "$$\\hat{w}= \\operatorname{argmin}_{w \\in \\mathbb{R}^p} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp \\left( -y^{(i)}w^\\top\\phi(x^{(i)}) \\right) \\right).$$\n",
    "\n",
    "Which of the following statements is true?\n",
    "\n",
    "a. Learning $\\hat{w}$ in this way is a convex optimization problem regardless of how complex $\\phi$ is.\n",
    "\n",
    "b. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is a convex function in each dimension.\n",
    "(Let $\\phi = [\\phi_1; \\phi_2; \\ldots ; \\phi_p]$; then we say $\\phi$ is convex in each dimension if each of $\\phi_1, \\phi_2, \\ldots, \\phi_p$ is a convex function).\n",
    "\n",
    "c. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is a linear function.\n",
    "\n",
    "d. Learning $\\hat{w}$ in this way is a convex optimization problem if and only if $\\phi$ is the identity function and $p = d$.\n",
    "\n",
    "Correct answers: (a)\n",
    "\n",
    "Explanation: Since we freeze $\\phi$ and do not update it, this is equivalent to logistic regression with a fixed basis expansion.\n",
    "Thus, it is a convex optimization problem regardless of how complex $\\phi$ is.\n",
    " \n",
    "\n",
    "## 34.\n",
    "\n",
    "Recall that influence functions are used to approximate the effect of leaving out one training point, without actually retraining the model.\n",
    "\n",
    "Assume that we have a twice-differentiable, strongly convex loss function $\\ell(x, y; w)$, and as usual, we train a model $\\hat{w}$ to minimize the average training loss:\n",
    "\n",
    "$$\\hat{w}= \\operatorname{argmin}_{w} \\frac{1}{n} \\sum_{i=1}^{n} \\ell_i(w),$$\n",
    "\n",
    "where $\\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$ is our training set, and for notational convenience we define $\\ell_i(w) = \\ell(x^{(i)}, y^{(i)}; w)$.\n",
    "\n",
    "Let $\\Delta_{-i}$ be the change in the parameters $w$ after we remove training point $(x^{(i)}, y^{(i)})$ and retrain the model.\n",
    "\n",
    "The influence function approximation tells us that\n",
    "\n",
    "$$\\Delta_{-i} = \\frac{1}{n} H(\\hat{w})^{-1} \\nabla_w \\ell_i(w) \\Big|_{w=\\hat{w}}$$\n",
    "\n",
    "where the Hessian matrix $H(\\hat{w})$ is defined as\n",
    "\n",
    "$$H(\\hat{w}) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_w^2 \\ell_i(w) \\Big|_{w=\\hat{w}}$$\n",
    "\n",
    "Consider the following linear regression model $f_w(x) = w^\\top x$, where $x, w \\in \\mathbb{R}^d$.\n",
    "We train with unregularized least squares regression to obtain $\\hat{w}$.\n",
    "\n",
    "What is $\\Delta_{-i}$ for this model, in terms of $\\hat{w}$ and the training data points?\n",
    "\n",
    "Note: The symbols $\\ell$ and $H$ should not appear in your answer. Replace them by working out the appropriate loss.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Explanation: For least squares regression, we have that $\\ell_i(w) = \\frac{1}{2}(y^{(i)} - w^\\top x^{(i)})^2$. (The $\\frac{1}{2}$ is for convenience;\n",
    "we can leave it out without changing the final answer.) Thus, $\\nabla_w\\ell_i(w) = -(y^{(i)} - w^\\top x^{(i)})x^{(i)}$, and $H(w) = \\frac{1}{n}\\sum_{i=1}^{n} x^{(i)}x^{(i)\\top}$.\n",
    "\n",
    "Putting this together,\n",
    "\n",
    "$$\\Delta_{-i} = -\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x^{(i)}x^{(i)\\top}\\right)^{-1}(y^{(i)} - \\hat{w}^\\top x^{(i)}) x^{(i)}$$\n",
    "\n",
    "$$= -\\left(\\sum_{i=1}^{n}x^{(i)}x^{(i)\\top}\\right)^{-1}(y^{(i)} - \\hat{w}^\\top x^{(i)}) x^{(i)}.$$\n",
    "\n",
    "We accept both the simplified version (canceling $\\frac{1}{n}$) and the unsimplified version."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
