{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51255722-9f8f-4ad4-9a1b-b582f60b9176",
   "metadata": {},
   "source": [
    "# Problem Set 6\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "**One Answer** Let $L_i(w)$ be the loss of parameter $w$ corresponding to a sample point $X_i$ with label $y_i$. The update rule for stochastic gradient descent with step size $\\eta$ is\n",
    "\n",
    "(a) $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_{X_i} L_i(w)$\n",
    "\n",
    "(b) $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_{X_i} L_i(w)$\n",
    "\n",
    "(c) $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_w L_i(w)$\n",
    "\n",
    "(d) $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_w L_i(w)$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "**One Answer** Suppose data $x_1, \\dots, x_n$ is drawn from an exponential distribution $\\text{exp}(\\lambda)$ with PDF $p(x|\\lambda) = \\lambda \\text{exp}(-\\lambda x)$. Find the maximum likelihood for $\\lambda$?\n",
    "\n",
    "(a) $\\lambda = \\frac{n}{\\sum_{i=1}^n x_i}$\n",
    "\n",
    "(b) $\\lambda = \\sum_{i=1}^n x_i$\n",
    "\n",
    "(c) $\\lambda = \\frac{\\sum_{i=1}^n x_i}{n}$\n",
    "\n",
    "(d) $\\lambda = \\log(\\sum_{i=1}^n x_i)$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "**One Answer** Aman and Ed built a model on their data with two regularization hyperparameters $\\lambda$ and $\\gamma$. They have 4 good candidate values for $\\lambda$ and 3 possible values for $\\gamma$, and they are wondering which $\\lambda, \\gamma$ pair will be the best choice. If they were to perform five-fold cross-validation, how many validation errors would they need to calculate?\n",
    "\n",
    "(a) 12\n",
    "\n",
    "(b) 17\n",
    "\n",
    "(c) 24\n",
    "\n",
    "(d) 60\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "**One Answer** Which of the following is most indicative of a model overfitting?\n",
    "\n",
    "(a) Low bias, low variance.\n",
    "\n",
    "(b) Low bias, high variance.\n",
    "\n",
    "(c) High bias, low variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "**One Answer** In k-fold cross-validation, what is the primary advantage of setting k to a higher value (e.g., k=10) compared to a lower value (e.g., k=2)?\n",
    "\n",
    "(a) It increases the accuracy of the model on unseen data.\n",
    "\n",
    "(b) It provides a more reliable estimate of model performance.\n",
    "\n",
    "(c) It reduces computational time.\n",
    "\n",
    "(d) It eliminates the need for a separate test set.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "**One Answer** Two realtors are creating machine learning models to predict house costs based on house traits (i.e. house size, neighborhood, school district, etc.) trained on the same set of houses, using the same model hyperparameters. Realtor A includes 30 different housing traits in their model. Realtor B includes 5 traits in their model. Which of the following outcomes is most likely?\n",
    "\n",
    "(a) Realtor B's model has higher variance and lower bias than Realtor A's model.\n",
    "\n",
    "(b) Realtor A's model has higher variance than Realtor B's model and without additional information, we cannot know which model has a higher bias.\n",
    "\n",
    "(c) Realtor A's model has higher variance and lower bias than Realtor B's model.\n",
    "\n",
    "(d) Realtor A's model has higher variance and higher bias than Realtor B's model.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "**Select All** Suppose we have $N$ data points $x_1, x_2, \\dots, x_N$ that $x_i \\in \\mathbb{R}^d$. Define $X \\in \\mathbb{R}^{N \\times d}$ such that $X_{i,j} = (x_i)_j$, $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i$, and $\\mathbf{1}_N = (1,1,\\dots,1)^T \\in \\mathbb{R}^N$. Which of the following are true about principal components analysis (PCA)?\n",
    "\n",
    "(a) The principal components are eigenvectors of the centered data matrix $X - \\mathbf{1}_N \\bar{x}^T$.\n",
    "\n",
    "(b) The principal components are right singular vectors of the centered data matrix.\n",
    "\n",
    "(c) The principal components are eigenvectors of the sample covariance matrix $\\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})^T$.\n",
    "\n",
    "(d) Applying a rigid rotation matrix $Q$ (i.e., $QQ^T = Q^T Q = I$) to $X$ will not change the principal components' directions.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "**Select All** In the context of singular value decomposition (SVD) $A = U \\Sigma V^T$, which of the following statements are correct?\n",
    "\n",
    "(a) The columns of $U$ are called left singular vectors and form an orthonormal basis for the range of $A$, while the columns of $V$ are called right singular vectors and form an orthonormal basis for the range of $A^T$.\n",
    "\n",
    "(b) For any $A$ that is real and symmetric, we have $U = V$.\n",
    "\n",
    "(c) For a square matrix $A$, the singular values of $A$ are the absolute values of the eigenvalues of $A$.\n",
    "\n",
    "(d) Singular values are always non-negative real numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "**Select All** Which of the following statements about matrix completion are correct?\n",
    "\n",
    "(a) It may not perform well when the real-world data is not inherently low-rank or when the pattern of missing observations is not random.\n",
    "\n",
    "(b) The purpose of matrix completion is to estimate missing entries in a partially observed matrix.\n",
    "\n",
    "(c) Matrix completion is only applicable for square matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "**One Answer** Consider a feature map $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}^4$ defined as:\n",
    "\n",
    "$\\phi \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_1 x_2 \\\\ x_2 x_1 \\end{bmatrix}$\n",
    "\n",
    "What is the corresponding kernel function $K$ for $\\phi$?\n",
    "\n",
    "(a) $K: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ and $K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^\\top \\mathbf{x}')^2$.\n",
    "\n",
    "(b) $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ and $K(x, x') = x^4 + x'^4 + 2x^2x'^2$.\n",
    "\n",
    "(c) $K: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ and $K(\\mathbf{x}, \\mathbf{x}') = \\mathbf{x}^\\top \\mathbf{x}'$.\n",
    "\n",
    "(d) $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ and $K(x,x') = x^2 + x'^2$.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "**One Answer** In the context of kernel methods, what does the \"kernel trick\" refer to?\n",
    "\n",
    "(a) Adding an extra kernel layer to the end of a neural network.\n",
    "\n",
    "(b) A technique for explicitly computing the coordinates in a high-dimensional space.\n",
    "\n",
    "(c) A method for computing the inner products in a high-dimensional feature space without explicitly mapping data to that space.\n",
    "\n",
    "(d) A technique for speeding up the convergence of gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "**One Answer** When using a kernel method to solve a regression problem with training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ and $\\mathbf{x}_i \\in \\mathbb{R}^d$, we first prove that there exists an $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$ such that the weight vector $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, where $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ is a feature map transforming $\\mathbf{x}_i$ into a very high dimensional space $\\mathbb{R}^p$ with $p \\gg d$. Then, solving the problem is equivalent to finding $\\hat{\\boldsymbol{\\alpha}} = \\text{argmin}_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n(y_i - \\sum_{j=1}^n \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j))^2 + \\lambda \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$. After we computed the value of $\\hat{\\boldsymbol{\\alpha}}$, given an input $\\mathbf{x}' \\in \\mathbb{R}^d$ in the test set, how can we make the prediction?\n",
    "\n",
    "(a) Because $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, we can compute the value of $\\hat{\\mathbf{w}}$, and then applying $\\hat{y} = \\hat{\\mathbf{w}}^T \\mathbf{x}'$.\n",
    "\n",
    "(b) Because $\\hat{y} = \\hat{\\mathbf{w}}^T \\mathbf{x}' = \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i^T \\mathbf{x}'$, we can compute the values of $\\mathbf{x}_i^T \\mathbf{x}'$ and then get the value of $\\hat{y}$.\n",
    "\n",
    "(c) Because $\\hat{y} = \\hat{\\mathbf{w}}^T \\phi(\\mathbf{x}') = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}') = \\sum_{i=1}^n \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}')$, we can compute the values of $K(\\mathbf{x}_i, \\mathbf{x}')$ and then get the value of $\\hat{y}$.\n",
    "\n",
    "(d) Because $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, we can compute the value of $\\hat{\\mathbf{w}}$, and then applying $\\hat{y} = \\hat{\\mathbf{w}}^T \\phi(\\mathbf{x}')$.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "**One Answer** Consider the following neural network with weights shown in the image below. Every hidden neuron uses the ReLU activation function, and there is no activation function on the output neuron. Assume there are no bias terms. What is the output of this network with the input $x = (3,2)$?\n",
    "\n",
    "<img src=\"./q13_problem.png\" width=\"450px\">\n",
    "\n",
    "The image displays a feedforward neural network with two input neurons, three hidden neurons, and one output neuron.\n",
    "\n",
    "**Input Layer:**\n",
    "*   The first input neuron is labeled $x_1 = 3$.\n",
    "*   The second input neuron is labeled $x_2 = 2$.\n",
    "\n",
    "**Connections from Input Layer to Hidden Layer (with weights):**\n",
    "*   From $x_1$:\n",
    "    *   To the top hidden neuron: weight $-1$\n",
    "    *   To the middle hidden neuron: weight $-2$\n",
    "    *   To the bottom hidden neuron: weight $3$\n",
    "*   From $x_2$:\n",
    "    *   To the top hidden neuron: weight $-2$\n",
    "    *   To the middle hidden neuron: weight $2$\n",
    "    *   To the bottom hidden neuron: weight $-1$\n",
    "\n",
    "**Connections from Hidden Layer to Output Layer (with weights):**\n",
    "*   From the top hidden neuron to the output neuron: weight $4$\n",
    "*   From the middle hidden neuron to the output neuron: weight $-2$\n",
    "*   From the bottom hidden neuron to the output neuron: weight $1$\n",
    "\n",
    "**Activation Functions:**\n",
    "*   Every hidden neuron uses the ReLU (Rectified Linear Unit) activation function, defined as $f(z) = \\max(0, z)$.\n",
    "*   There is no activation function on the output neuron (linear activation).\n",
    "*   There are no bias terms in the network.\n",
    "\n",
    "The output of the network is denoted as $\\hat{y}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "**One Answer** How many parameters does the neural network shown in the previous problem have?\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "**One Answer** Which of the following defines the correct ordering of steps needed to perform backpropagation in PyTorch?\n",
    "\n",
    "(a) (1) compute loss, (2) compute gradients, (3) take step, (4) zero the gradient buffers\n",
    "\n",
    "(b) (1) compute loss, (2) zero the gradient buffers, (3) compute gradients, (4) take step\n",
    "\n",
    "(c) (1) compute loss, (2) take step, (3) zero the gradient buffers, (4) compute gradients\n",
    "\n",
    "(d) (1) zero the gradient buffers, (2) compute gradients, (3) take step, (4) compute loss\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "**One Answer** Consider a convolutional neural network (CNN) layer with the following parameters:\n",
    "\n",
    "*   Input image size: 3 x 32 x 32 (channels, height, width)\n",
    "*   Number of filters: 16\n",
    "*   filter size: 3x3\n",
    "*   Stride: 1\n",
    "*   Padding: 1\n",
    "\n",
    "What will be the shape of the output after applying this convolutional layer (in the order of channels, height, width)?\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "**One Answer** Which of the following best describes the purpose of pooling layers in a convolutional neural network (CNN)?\n",
    "\n",
    "(a) To increase the resolution of the feature maps.\n",
    "\n",
    "(b) To reduce the spatial dimensions of the feature maps, thereby reducing the computational load and the number of parameters.\n",
    "\n",
    "(c) To convert the feature maps into a fully connected layer.\n",
    "\n",
    "(d) To normalize the feature maps by scaling them to a fixed range.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "**One Answer** Given the following setup in a simple recurrent neural network (RNN):\n",
    "\n",
    "Input at time t: $x_t$\n",
    "\n",
    "The RNN has one hidden layer with the following parameters:\n",
    "* Input to hidden state weights: $W_{xh}$\n",
    "* Hidden state to hidden state weights: $W_{hh}$\n",
    "* Hidden state to output weights: $W_{hy}$\n",
    "* Bias for the hidden state: $b_h$\n",
    "* Bias for the output: $b_y$\n",
    "\n",
    "The activation function for the hidden state is ReLU.\n",
    "\n",
    "Given the following parameter values:\n",
    "$W_{xh} = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.3 & 0.2 \\end{bmatrix}$, $W_{hh} = \\begin{bmatrix} 0.6 & 0.4 \\\\ 0.2 & 0.5 \\end{bmatrix}$, $W_{hy} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b_h = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$, $b_y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "Calculate the hidden state $h_1$ after processing the first input $x_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. Assume the initial hidden state $h_0$ is a zero vector.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "**One Answer** Which of the following statements about the k-means clustering algorithm is true?\n",
    "\n",
    "(a) It guarantees convergence to the global optimum.\n",
    "\n",
    "(b) It is robust against the initialization of cluster means.\n",
    "\n",
    "(c) It may converge to a local optimum depending on the initial placement of cluster means.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "**One Answer** Why might a Gaussian Mixture Model (GMM) be preferred over K-means in cases where the data contains mixed or overlapping clusters?\n",
    "\n",
    "(a) GMM exclusively uses hard assignments which are better for mixed clusters.\n",
    "\n",
    "(b) GMM utilizes soft assignments, allowing points to belong to multiple clusters with varying probabilities.\n",
    "\n",
    "(c) GMM always converges faster than k-means.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "**One Answer** In k-means clustering, the algorithm is executed several times on the same dataset, each time with a fresh random initialization of cluster centers and the same number of clusters. If these multiple runs yield widely varying cluster outcomes, what might this suggest about the algorithm's sensitivity to initial conditions?\n",
    "\n",
    "(a) The choice of k is optimal.\n",
    "\n",
    "(b) The dataset is perfectly clustered.\n",
    "\n",
    "(c) The initialization of centers might be influencing the results.\n",
    "\n",
    "(d) The algorithm is not suitable for clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "**One Answer** Consider an LSTM (Long Short-Term Memory) network with the following characteristics: a forget gate, an input gate, a memory cell, and an output gate. Which of the following statements correctly describes the function of the forget gate in an LSTM?\n",
    "\n",
    "<img src=\"./q22_problem.png\" width=\"450px\">\n",
    "\n",
    "The image displays a diagram of a Long Short-Term Memory (LSTM) network, illustrating its repeating module structure and internal gates.\n",
    "\n",
    "The diagram shows three identical blocks, labeled 'A', representing the repeating module of the LSTM. The central block 'A' is expanded to show its internal components and data flow.\n",
    "\n",
    "**Inputs to the central LSTM module:**\n",
    "- $h_{t-1}$: Previous hidden state, represented by an arrow pointing down into the module.\n",
    "- $X_t$: Current input, represented by an arrow pointing down into the module.\n",
    "- $C_{t-1}$: Previous cell state, implicitly flowing into the module from the left.\n",
    "\n",
    "**Outputs from the central LSTM module:**\n",
    "- $h_t$: Current hidden state, represented by an arrow pointing up from the module.\n",
    "- $C_t$: Current cell state, implicitly flowing out of the module to the right.\n",
    "\n",
    "**Internal structure of the LSTM module (from left to right, top to bottom):**\n",
    "\n",
    "1.  **Forget Gate:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated (double arrow pointing right).\n",
    "    -   The concatenated vector goes into a \"Neural Network Layer\" (yellow rectangle).\n",
    "    -   The output of the neural network layer goes into a \"Pointwise Operation\" (circle), which is a sigmoid function ($\\sigma$).\n",
    "    -   The output of the sigmoid is multiplied (Pointwise Operation, $\\times$) with the previous cell state $C_{t-1}$.\n",
    "\n",
    "2.  **Input Gate and Candidate Cell State:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated.\n",
    "    -   This concatenated vector splits into two paths:\n",
    "        -   **Path 1 (Input Gate):** Goes into a \"Neural Network Layer\", then a \"Pointwise Operation\" (sigmoid $\\sigma$).\n",
    "        -   **Path 2 (Candidate Cell State):** Goes into a \"Neural Network Layer\" with a 'tanh' activation function indicated.\n",
    "    -   The outputs of Path 1 (Input Gate) and Path 2 (Candidate Cell State) are multiplied (Pointwise Operation, $\\times$).\n",
    "\n",
    "3.  **Cell State Update:**\n",
    "    -   The output of the Forget Gate (multiplication with $C_{t-1}$) and the output of the Input Gate/Candidate Cell State multiplication are added (Pointwise Operation, $+$). This sum forms the new cell state $C_t$.\n",
    "\n",
    "4.  **Output Gate:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated.\n",
    "    -   The concatenated vector goes into a \"Neural Network Layer\", then a \"Pointwise Operation\" (sigmoid $\\sigma$).\n",
    "\n",
    "5.  **Hidden State Calculation:**\n",
    "    -   The new cell state $C_t$ goes into a \"Pointwise Operation\" (tanh activation).\n",
    "    -   The output of the Output Gate (sigmoid) and the tanh-activated $C_t$ are multiplied (Pointwise Operation, $\\times$). This result is the new hidden state $h_t$.\n",
    "\n",
    "**Legend for symbols used in the diagram:**\n",
    "-   Yellow rectangle: Neural Network Layer\n",
    "-   Circle: Pointwise Operation (e.g., sigmoid, tanh, addition, multiplication)\n",
    "-   Single arrow: Vector Transfer\n",
    "-   Double arrow pointing right: Concatenate\n",
    "-   Double arrow splitting: Copy\n",
    "\n",
    "(a) The forget gate decides which information should be discarded from the cell state.\n",
    "\n",
    "(b) The forget gate calculates the output of the current position at each time step.\n",
    "\n",
    "(c) The forget gate extracts useful information from the input to update memory.\n",
    "\n",
    "(d) The forget gate calculates the next hidden state based on the current input.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "**One Answer** What is the key benefit of the attention mechanism over standard RNN models in the context of neural machine translation?\n",
    "\n",
    "(a) It significantly reduces computational complexity.\n",
    "\n",
    "(b) It uses convolutional layers for long-term dependencies.\n",
    "\n",
    "(c) It relies entirely on recurrent layers for processing sequences.\n",
    "\n",
    "(d) It solves the bottleneck problem and long-term dependency issues by focusing on specific parts of the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "**One Answer** What is the main purpose of using positional encoding in the Transformer architecture?\n",
    "\n",
    "(a) It introduces non-linearity in the model.\n",
    "\n",
    "(b) It helps in maintaining long-term dependencies.\n",
    "\n",
    "(c) It provides information about the order of the input sequence.\n",
    "\n",
    "(d) It reduces the computational complexity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
