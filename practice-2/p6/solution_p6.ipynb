{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8622dc-f2b5-41b0-9f97-0f5577545df3",
   "metadata": {},
   "source": [
    "# Problem Set 6 Solutions\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "**One Answer** Let $L_i(w)$ be the loss of parameter $w$ corresponding to a sample point $X_i$ with label $y_i$. The update rule for stochastic gradient descent with step size $\\eta$ is\n",
    "\n",
    "(a) $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_{X_i} L_i(w)$\n",
    "\n",
    "(b) $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_{X_i} L_i(w)$\n",
    "\n",
    "(c) $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_w L_i(w)$\n",
    "\n",
    "(d) $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_w L_i(w)$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(c) - $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_w L_i(w)$**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Stochastic Gradient Descent (SGD):**\n",
    "\n",
    "**Definition:**\n",
    "SGD is an optimization algorithm that updates parameters using the gradient of the loss function computed on a single training example (or a small batch) rather than the entire dataset.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "**1. Loss Function:**\n",
    "- $L_i(w)$ is the loss for a single training example $(X_i, y_i)$\n",
    "- This is a function of the parameters $w$, not the input $X_i$\n",
    "\n",
    "**2. Gradient:**\n",
    "- $\\nabla_w L_i(w)$ is the gradient of the loss with respect to the parameters $w$\n",
    "- This tells us how to change $w$ to reduce the loss for this specific example\n",
    "\n",
    "**3. Step Size:**\n",
    "- $\\eta$ (eta) is the learning rate\n",
    "- Controls how big a step we take in the direction of the gradient\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Correct Update Rule:**\n",
    "$$w_{\\text{new}} \\leftarrow w - \\eta \\nabla_w L_i(w)$$\n",
    "\n",
    "**Why This is Correct:**\n",
    "1. **Gradient Direction**: $\\nabla_w L_i(w)$ points in the direction of steepest increase of the loss\n",
    "2. **Negative Sign**: We subtract because we want to minimize the loss (move in opposite direction)\n",
    "3. **Parameter Update**: We update $w$, not $X_i$\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - $w_{\\text{new}} \\leftarrow w - \\eta \\nabla_{X_i} L_i(w)$:**\n",
    "- **Problem**: Gradient with respect to input $X_i$, not parameters $w$\n",
    "- **Issue**: We want to update model parameters, not input data\n",
    "- **Result**: This doesn't make sense for optimization\n",
    "\n",
    "**Option (b) - $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_{X_i} L_i(w)$:**\n",
    "- **Problem**: Sum over all examples AND gradient with respect to inputs\n",
    "- **Issue**: This is neither SGD nor correct gradient computation\n",
    "- **Result**: Computationally expensive and conceptually wrong\n",
    "\n",
    "**Option (d) - $w_{\\text{new}} \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_w L_i(w)$:**\n",
    "- **Problem**: Sum over all training examples\n",
    "- **Issue**: This is **batch gradient descent**, not stochastic gradient descent\n",
    "- **Result**: Computationally expensive for large datasets\n",
    "\n",
    "**Comparison of Methods:**\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "- Uses single example: $w \\leftarrow w - \\eta \\nabla_w L_i(w)$\n",
    "- Fast updates, noisy gradients\n",
    "- Good for large datasets\n",
    "\n",
    "**Batch Gradient Descent:**\n",
    "- Uses all examples: $w \\leftarrow w - \\eta \\sum_{i=1}^n \\nabla_w L_i(w)$\n",
    "- Slow updates, accurate gradients\n",
    "- Computationally expensive\n",
    "\n",
    "**Mini-batch Gradient Descent:**\n",
    "- Uses subset of examples: $w \\leftarrow w - \\eta \\sum_{i \\in B} \\nabla_w L_i(w)$\n",
    "- Balance between speed and accuracy\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "**Consider linear regression:**\n",
    "- Loss: $L_i(w) = \\frac{1}{2}(y_i - w^T x_i)^2$\n",
    "- Gradient: $\\nabla_w L_i(w) = -(y_i - w^T x_i)x_i$\n",
    "- Update: $w_{\\text{new}} = w + \\eta(y_i - w^T x_i)x_i$\n",
    "\n",
    "**Algorithm Flow:**\n",
    "```\n",
    "1. Initialize w randomly\n",
    "2. For each training example (x_i, y_i):\n",
    "   a. Compute gradient: ∇_w L_i(w)\n",
    "   b. Update parameters: w ← w - η ∇_w L_i(w)\n",
    "3. Repeat until convergence\n",
    "```\n",
    "\n",
    "**Advantages of SGD:**\n",
    "- **Memory efficient**: Only needs one example at a time\n",
    "- **Fast updates**: Can start learning immediately\n",
    "- **Escape local minima**: Noise can help escape poor local optima\n",
    "- **Online learning**: Can learn from streaming data\n",
    "\n",
    "**Disadvantages of SGD:**\n",
    "- **Noisy gradients**: Single examples may not represent true gradient\n",
    "- **Slower convergence**: May take more iterations to converge\n",
    "- **Sensitive to learning rate**: Need careful tuning\n",
    "\n",
    "**Conclusion:**\n",
    "The correct SGD update rule is **$w_{\\text{new}} \\leftarrow w - \\eta \\nabla_w L_i(w)$**, which updates the parameters using the gradient of the loss with respect to the parameters for a single training example.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "**One Answer** Suppose data $x_1, \\dots, x_n$ is drawn from an exponential distribution $\\text{exp}(\\lambda)$ with PDF $p(x|\\lambda) = \\lambda \\text{exp}(-\\lambda x)$. Find the maximum likelihood for $\\lambda$?\n",
    "\n",
    "(a) $\\lambda = \\frac{n}{\\sum_{i=1}^n x_i}$\n",
    "\n",
    "(b) $\\lambda = \\sum_{i=1}^n x_i$\n",
    "\n",
    "(c) $\\lambda = \\frac{\\sum_{i=1}^n x_i}{n}$\n",
    "\n",
    "(d) $\\lambda = \\log(\\sum_{i=1}^n x_i)$\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(a) - $\\lambda = \\frac{n}{\\sum_{i=1}^n x_i}$**. Here's the detailed derivation:\n",
    "\n",
    "**Understanding Maximum Likelihood Estimation (MLE):**\n",
    "\n",
    "**Goal:**\n",
    "Find the parameter value $\\lambda$ that maximizes the likelihood of observing the given data $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "**Exponential Distribution:**\n",
    "The probability density function (PDF) is:\n",
    "$$p(x|\\lambda) = \\lambda e^{-\\lambda x}$$\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "**Step 1: Write the Likelihood Function**\n",
    "The likelihood of observing all $n$ data points is:\n",
    "$$L(\\lambda) = \\prod_{i=1}^n p(x_i|\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}$$\n",
    "\n",
    "**Step 2: Simplify the Likelihood**\n",
    "$$L(\\lambda) = \\lambda^n \\prod_{i=1}^n e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$$\n",
    "\n",
    "**Step 3: Take the Log-Likelihood**\n",
    "$$\\log L(\\lambda) = \\log(\\lambda^n) + \\log(e^{-\\lambda \\sum_{i=1}^n x_i})$$\n",
    "$$\\log L(\\lambda) = n\\log(\\lambda) - \\lambda \\sum_{i=1}^n x_i$$\n",
    "\n",
    "**Step 4: Find the Maximum**\n",
    "To find the maximum, take the derivative and set it to zero:\n",
    "$$\\frac{d}{d\\lambda} \\log L(\\lambda) = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0$$\n",
    "\n",
    "**Step 5: Solve for $\\lambda$**\n",
    "$$\\frac{n}{\\lambda} = \\sum_{i=1}^n x_i$$\n",
    "$$\\lambda = \\frac{n}{\\sum_{i=1}^n x_i}$$\n",
    "\n",
    "**Step 6: Verify it's a Maximum**\n",
    "Take the second derivative:\n",
    "$$\\frac{d^2}{d\\lambda^2} \\log L(\\lambda) = -\\frac{n}{\\lambda^2} < 0$$\n",
    "Since the second derivative is negative, this is indeed a maximum.\n",
    "\n",
    "**Mathematical Verification:**\n",
    "\n",
    "**Why This Makes Sense:**\n",
    "1. **Reciprocal Relationship**: $\\lambda$ is inversely related to the mean of the data\n",
    "2. **Intuition**: For exponential distribution, $E[X] = \\frac{1}{\\lambda}$\n",
    "3. **Consistency**: The MLE estimate $\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}$ where $\\bar{x}$ is the sample mean\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (b) - $\\lambda = \\sum_{i=1}^n x_i$:**\n",
    "- **Problem**: This is the sum of all observations, not the MLE\n",
    "- **Issue**: Doesn't account for the number of observations\n",
    "- **Result**: Incorrect scaling\n",
    "\n",
    "**Option (c) - $\\lambda = \\frac{\\sum_{i=1}^n x_i}{n}$:**\n",
    "- **Problem**: This is the sample mean, which is $\\frac{1}{\\lambda}$ for exponential distribution\n",
    "- **Issue**: Confuses the parameter with its reciprocal\n",
    "- **Result**: Wrong relationship\n",
    "\n",
    "**Option (d) - $\\lambda = \\log(\\sum_{i=1}^n x_i)$:**\n",
    "- **Problem**: Takes logarithm of the sum\n",
    "- **Issue**: No mathematical justification for this transformation\n",
    "- **Result**: Arbitrary and incorrect\n",
    "\n",
    "**Properties of the MLE:**\n",
    "\n",
    "**1. Consistency:**\n",
    "- As $n \\to \\infty$, $\\hat{\\lambda} \\to \\lambda_{\\text{true}}$\n",
    "- The estimator converges to the true parameter value\n",
    "\n",
    "**2. Efficiency:**\n",
    "- MLE achieves the Cramér-Rao lower bound\n",
    "- It's the most efficient unbiased estimator\n",
    "\n",
    "**3. Invariance:**\n",
    "- If $\\hat{\\lambda}$ is the MLE of $\\lambda$, then $f(\\hat{\\lambda})$ is the MLE of $f(\\lambda)$\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "**Given data:** $x_1 = 2, x_2 = 3, x_3 = 1, x_4 = 4$\n",
    "\n",
    "**Step 1:** Calculate sum: $\\sum_{i=1}^4 x_i = 2 + 3 + 1 + 4 = 10$\n",
    "\n",
    "**Step 2:** Calculate MLE: $\\hat{\\lambda} = \\frac{4}{10} = 0.4$\n",
    "\n",
    "**Step 3:** Verify: The sample mean is $\\bar{x} = \\frac{10}{4} = 2.5$, and $\\frac{1}{\\hat{\\lambda}} = \\frac{1}{0.4} = 2.5$\n",
    "\n",
    "**Conclusion:**\n",
    "The maximum likelihood estimator for the exponential distribution parameter $\\lambda$ is **$\\lambda = \\frac{n}{\\sum_{i=1}^n x_i}$**.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "**One Answer** Aman and Ed built a model on their data with two regularization hyperparameters $\\lambda$ and $\\gamma$. They have 4 good candidate values for $\\lambda$ and 3 possible values for $\\gamma$, and they are wondering which $\\lambda, \\gamma$ pair will be the best choice. If they were to perform five-fold cross-validation, how many validation errors would they need to calculate?\n",
    "\n",
    "(a) 12\n",
    "\n",
    "(b) 17\n",
    "\n",
    "(c) 24\n",
    "\n",
    "(d) 60\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(d) - 60**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Cross-Validation:**\n",
    "\n",
    "**Definition:**\n",
    "Cross-validation is a technique for assessing how well a model will generalize to new data by dividing the dataset into training and validation sets multiple times.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "- Data is divided into $k$ equal parts (folds)\n",
    "- Model is trained on $k-1$ folds and validated on the remaining fold\n",
    "- This process is repeated $k$ times, with each fold serving as validation once\n",
    "- Results are averaged to get the final performance estimate\n",
    "\n",
    "**Problem Setup:**\n",
    "- **Hyperparameters**: $\\lambda$ and $\\gamma$\n",
    "- **Values for $\\lambda$**: 4 candidate values\n",
    "- **Values for $\\gamma$**: 3 candidate values\n",
    "- **Cross-validation**: 5-fold\n",
    "\n",
    "**Step-by-Step Calculation:**\n",
    "\n",
    "**Step 1: Count Hyperparameter Combinations**\n",
    "- Number of $\\lambda$ values: 4\n",
    "- Number of $\\gamma$ values: 3\n",
    "- Total combinations: $4 \\times 3 = 12$\n",
    "\n",
    "**Step 2: Count Validation Errors per Combination**\n",
    "- For each hyperparameter combination, we need to evaluate the model\n",
    "- In 5-fold cross-validation, each combination requires 5 validation errors\n",
    "- One validation error per fold\n",
    "\n",
    "**Step 3: Total Validation Errors**\n",
    "- Total combinations: 12\n",
    "- Validation errors per combination: 5\n",
    "- Total validation errors: $12 \\times 5 = 60$\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$\\text{Total Validation Errors} = (\\text{Number of } \\lambda \\text{ values}) \\times (\\text{Number of } \\gamma \\text{ values}) \\times (\\text{Number of folds})$$\n",
    "$$\\text{Total Validation Errors} = 4 \\times 3 \\times 5 = 60$$\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - 12:**\n",
    "- **Problem**: Only counts the number of hyperparameter combinations\n",
    "- **Issue**: Ignores the fact that each combination needs 5-fold cross-validation\n",
    "- **Result**: Underestimates the computational cost\n",
    "\n",
    "**Option (b) - 17:**\n",
    "- **Problem**: No clear mathematical relationship to the problem\n",
    "- **Issue**: Doesn't follow from the given parameters\n",
    "- **Result**: Arbitrary number\n",
    "\n",
    "**Option (c) - 24:**\n",
    "- **Problem**: Might be counting something else (e.g., $4 \\times 3 \\times 2$)\n",
    "- **Issue**: Incorrect interpretation of cross-validation process\n",
    "- **Result**: Wrong calculation\n",
    "\n",
    "**Detailed Cross-Validation Process:**\n",
    "\n",
    "**For each hyperparameter combination $(\\lambda_i, \\gamma_j)$:**\n",
    "\n",
    "**Fold 1:**\n",
    "- Train on folds 2, 3, 4, 5\n",
    "- Validate on fold 1\n",
    "- Record validation error\n",
    "\n",
    "**Fold 2:**\n",
    "- Train on folds 1, 3, 4, 5\n",
    "- Validate on fold 2\n",
    "- Record validation error\n",
    "\n",
    "**Fold 3:**\n",
    "- Train on folds 1, 2, 4, 5\n",
    "- Validate on fold 3\n",
    "- Record validation error\n",
    "\n",
    "**Fold 4:**\n",
    "- Train on folds 1, 2, 3, 5\n",
    "- Validate on fold 4\n",
    "- Record validation error\n",
    "\n",
    "**Fold 5:**\n",
    "- Train on folds 1, 2, 3, 4\n",
    "- Validate on fold 5\n",
    "- Record validation error\n",
    "\n",
    "**Average the 5 validation errors for this combination.**\n",
    "\n",
    "**Repeat for all 12 combinations.**\n",
    "\n",
    "**Computational Complexity:**\n",
    "\n",
    "**Time Complexity:**\n",
    "- Training time per fold: $O(\\text{training time})$\n",
    "- Total training time: $O(12 \\times 5 \\times \\text{training time}) = O(60 \\times \\text{training time})$\n",
    "\n",
    "**Space Complexity:**\n",
    "- Need to store 60 validation error values\n",
    "- Additional memory for model training\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "**Advantages of K-Fold Cross-Validation:**\n",
    "- More reliable performance estimate than single train/validation split\n",
    "- Uses all data for both training and validation\n",
    "- Reduces variance in performance estimates\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive (60 model trainings in this case)\n",
    "- May not be feasible for large datasets or complex models\n",
    "\n",
    "**Optimization Strategies:**\n",
    "- **Grid Search**: Try all combinations (what we're doing here)\n",
    "- **Random Search**: Sample random combinations\n",
    "- **Bayesian Optimization**: Use previous results to guide search\n",
    "\n",
    "**Conclusion:**\n",
    "The total number of validation errors needed is **60**, calculated as $4 \\times 3 \\times 5 = 60$ (number of $\\lambda$ values $\\times$ number of $\\gamma$ values $\\times$ number of folds).\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "**One Answer** Which of the following is most indicative of a model overfitting?\n",
    "\n",
    "(a) Low bias, low variance.\n",
    "\n",
    "(b) Low bias, high variance.\n",
    "\n",
    "(c) High bias, low variance.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b) - Low bias, high variance**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Overfitting:**\n",
    "\n",
    "**Definition of Overfitting:**\n",
    "Overfitting occurs when a model learns the training data too well, including noise and idiosyncrasies, leading to poor generalization on new, unseen data.\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "The total error of a model can be decomposed into three components:\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**Characteristics of Overfitting:**\n",
    "\n",
    "**1. Low Bias:**\n",
    "- Model can capture complex patterns in the training data\n",
    "- Training error is very low (often close to zero)\n",
    "- Model has high capacity/flexibility\n",
    "\n",
    "**2. High Variance:**\n",
    "- Model is very sensitive to changes in training data\n",
    "- Small changes in training set lead to large changes in predictions\n",
    "- Model has memorized the training data instead of learning generalizable patterns\n",
    "\n",
    "**Why Overfitting Shows Low Bias, High Variance:**\n",
    "\n",
    "**Low Bias (Good for Training):**\n",
    "- **Definition**: Bias measures how well the model can approximate the true underlying function\n",
    "- **In Overfitting**: Model can fit the training data very well, including noise\n",
    "- **Result**: Training error is low, indicating low bias\n",
    "\n",
    "**High Variance (Bad for Generalization):**\n",
    "- **Definition**: Variance measures how much predictions change with different training sets\n",
    "- **In Overfitting**: Model is very sensitive to training data variations\n",
    "- **Result**: Poor generalization to new data\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "\n",
    "**Training Performance:**\n",
    "$$\\text{Training Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}$$\n",
    "- In overfitting: Training error is low\n",
    "- This suggests low bias (model can fit training data well)\n",
    "\n",
    "**Test Performance:**\n",
    "$$\\text{Test Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}$$\n",
    "- In overfitting: Test error is high\n",
    "- This suggests high variance (model doesn't generalize well)\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "**Underfitting (High Bias, Low Variance):**\n",
    "```\n",
    "Training: Simple model, high error\n",
    "Test: Simple model, high error\n",
    "Result: Consistent but poor performance\n",
    "```\n",
    "\n",
    "**Good Fit (Low Bias, Low Variance):**\n",
    "```\n",
    "Training: Complex model, low error\n",
    "Test: Complex model, low error\n",
    "Result: Good performance on both\n",
    "```\n",
    "\n",
    "**Overfitting (Low Bias, High Variance):**\n",
    "```\n",
    "Training: Very complex model, very low error\n",
    "Test: Very complex model, high error\n",
    "Result: Great on training, poor on test\n",
    "```\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - Low bias, low variance:**\n",
    "- **Problem**: This describes a well-fitted model\n",
    "- **Issue**: Overfitting models don't have low variance\n",
    "- **Result**: This is the ideal scenario, not overfitting\n",
    "\n",
    "**Option (c) - High bias, low variance:**\n",
    "- **Problem**: This describes underfitting\n",
    "- **Issue**: Overfitting models have low bias, not high bias\n",
    "- **Result**: This is the opposite of overfitting\n",
    "\n",
    "**Signs of Overfitting:**\n",
    "\n",
    "**1. Training vs Test Performance:**\n",
    "- Training accuracy: Very high (e.g., 99%)\n",
    "- Test accuracy: Much lower (e.g., 70%)\n",
    "- Large gap between training and test performance\n",
    "\n",
    "**2. Model Complexity:**\n",
    "- Too many parameters relative to data size\n",
    "- Very deep neural networks on small datasets\n",
    "- High-degree polynomials on few data points\n",
    "\n",
    "**3. Learning Curves:**\n",
    "- Training error continues to decrease\n",
    "- Validation error starts increasing after a point\n",
    "- Gap between training and validation error grows\n",
    "\n",
    "**Examples of Overfitting:**\n",
    "\n",
    "**1. Polynomial Regression:**\n",
    "- Degree 10 polynomial on 5 data points\n",
    "- Fits training data perfectly\n",
    "- Poor generalization\n",
    "\n",
    "**2. Neural Networks:**\n",
    "- Very wide/deep network on small dataset\n",
    "- Memorizes training examples\n",
    "- Fails on new data\n",
    "\n",
    "**3. Decision Trees:**\n",
    "- Tree grown to maximum depth\n",
    "- Each leaf contains one training example\n",
    "- No generalization\n",
    "\n",
    "**Solutions to Overfitting:**\n",
    "\n",
    "**1. Regularization:**\n",
    "- L1/L2 regularization\n",
    "- Dropout in neural networks\n",
    "- Early stopping\n",
    "\n",
    "**2. More Data:**\n",
    "- Collect more training examples\n",
    "- Data augmentation\n",
    "- Transfer learning\n",
    "\n",
    "**3. Reduce Model Complexity:**\n",
    "- Fewer parameters\n",
    "- Simpler architectures\n",
    "- Feature selection\n",
    "\n",
    "**4. Cross-Validation:**\n",
    "- Monitor validation performance\n",
    "- Stop training when validation error increases\n",
    "- Choose appropriate model complexity\n",
    "\n",
    "**Conclusion:**\n",
    "Overfitting is characterized by **low bias and high variance** - the model can fit the training data very well (low bias) but is too sensitive to training data variations (high variance), leading to poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "**One Answer** In k-fold cross-validation, what is the primary advantage of setting k to a higher value (e.g., k=10) compared to a lower value (e.g., k=2)?\n",
    "\n",
    "(a) It increases the accuracy of the model on unseen data.\n",
    "\n",
    "(b) It provides a more reliable estimate of model performance.\n",
    "\n",
    "(c) It reduces computational time.\n",
    "\n",
    "(d) It eliminates the need for a separate test set.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b) - It provides a more reliable estimate of model performance**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding K-Fold Cross-Validation:**\n",
    "\n",
    "**Definition:**\n",
    "K-fold cross-validation is a resampling technique that divides the dataset into k equal parts (folds), trains the model on k-1 folds, and validates on the remaining fold. This process is repeated k times, with each fold serving as validation once.\n",
    "\n",
    "**Effect of K Value:**\n",
    "\n",
    "**Low K (e.g., k=2):**\n",
    "- **Split**: 50% training, 50% validation\n",
    "- **Issues**: \n",
    "  - Large validation set reduces training data\n",
    "  - High variance in performance estimates\n",
    "  - Less reliable performance assessment\n",
    "\n",
    "**High K (e.g., k=10):**\n",
    "- **Split**: 90% training, 10% validation\n",
    "- **Benefits**:\n",
    "  - More training data available\n",
    "  - Lower variance in performance estimates\n",
    "  - More reliable performance assessment\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Variance of Performance Estimate:**\n",
    "$$\\text{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{k}$$\n",
    "where:\n",
    "- $\\hat{\\mu}$ is the estimated performance\n",
    "- $\\sigma^2$ is the variance of individual fold performances\n",
    "- $k$ is the number of folds\n",
    "\n",
    "**As k increases:**\n",
    "- Variance decreases\n",
    "- More reliable estimate\n",
    "- Better confidence in model performance\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - It increases the accuracy of the model on unseen data:**\n",
    "- **Problem**: K-fold CV is for evaluation, not model improvement\n",
    "- **Issue**: The model itself doesn't change based on k\n",
    "- **Result**: K value affects estimation reliability, not model accuracy\n",
    "\n",
    "**Option (c) - It reduces computational time:**\n",
    "- **Problem**: Higher k means more model trainings\n",
    "- **Issue**: k=10 requires 10 model trainings vs k=2 requires 2\n",
    "- **Result**: Higher k increases computational cost\n",
    "\n",
    "**Option (d) - It eliminates the need for a separate test set:**\n",
    "- **Problem**: Cross-validation is for model selection, not final evaluation\n",
    "- **Issue**: Test set is still needed for unbiased final evaluation\n",
    "- **Result**: K-fold CV doesn't replace test set\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "**High K (k=10):**\n",
    "- **Pros**: More reliable estimates, more training data per fold\n",
    "- **Cons**: Higher computational cost, longer training time\n",
    "\n",
    "**Low K (k=2):**\n",
    "- **Pros**: Faster computation, simpler implementation\n",
    "- **Cons**: Less reliable estimates, less training data per fold\n",
    "\n",
    "**Optimal K Selection:**\n",
    "- **k=5 or k=10**: Good balance for most datasets\n",
    "- **k=n (Leave-One-Out)**: Maximum reliability but high computational cost\n",
    "- **k=2**: Only when computational resources are severely limited\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Dataset with 1000 samples:**\n",
    "\n",
    "**k=2 (2-fold CV):**\n",
    "- Training: 500 samples\n",
    "- Validation: 500 samples\n",
    "- Variance: High (large validation set)\n",
    "\n",
    "**k=10 (10-fold CV):**\n",
    "- Training: 900 samples\n",
    "- Validation: 100 samples\n",
    "- Variance: Low (small validation set)\n",
    "\n",
    "**Conclusion:**\n",
    "Higher k values provide **more reliable estimates of model performance** by reducing the variance of the performance estimate, though at the cost of increased computational time.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "**One Answer** Two realtors are creating machine learning models to predict house costs based on house traits (i.e. house size, neighborhood, school district, etc.) trained on the same set of houses, using the same model hyperparameters. Realtor A includes 30 different housing traits in their model. Realtor B includes 5 traits in their model. Which of the following outcomes is most likely?\n",
    "\n",
    "(a) Realtor B's model has higher variance and lower bias than Realtor A's model.\n",
    "\n",
    "(b) Realtor A's model has higher variance than Realtor B's model and without additional information, we cannot know which model has a higher bias.\n",
    "\n",
    "(c) Realtor A's model has higher variance and lower bias than Realtor B's model.\n",
    "\n",
    "(d) Realtor A's model has higher variance and higher bias than Realtor B's model.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b) - Realtor A's model has higher variance than Realtor B's model and without additional information, we cannot know which model has a higher bias**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Bias-Variance Tradeoff:**\n",
    "\n",
    "**Definition:**\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between model complexity and generalization error.\n",
    "\n",
    "**Components of Error:**\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**Analysis of the Two Models:**\n",
    "\n",
    "**Realtor A's Model (30 features):**\n",
    "- **High complexity**: Many parameters to learn\n",
    "- **High flexibility**: Can capture complex relationships\n",
    "- **High variance**: Sensitive to training data variations\n",
    "- **Bias**: Depends on whether the true relationship is captured\n",
    "\n",
    "**Realtor B's Model (5 features):**\n",
    "- **Low complexity**: Few parameters to learn\n",
    "- **Low flexibility**: Limited in capturing complex relationships\n",
    "- **Low variance**: Stable predictions across different training sets\n",
    "- **Bias**: Depends on whether the true relationship is captured\n",
    "\n",
    "**Why Realtor A Has Higher Variance:**\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "- **More parameters**: More degrees of freedom\n",
    "- **More sensitive**: Small changes in training data affect more parameters\n",
    "- **Less stable**: Predictions vary more across different training sets\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Realtor A (30 features):\n",
    "- Training Set 1: Predicts $500,000\n",
    "- Training Set 2: Predicts $450,000\n",
    "- Training Set 3: Predicts $550,000\n",
    "- High variance (unstable predictions)\n",
    "\n",
    "Realtor B (5 features):\n",
    "- Training Set 1: Predicts $480,000\n",
    "- Training Set 2: Predicts $485,000\n",
    "- Training Set 3: Predicts $475,000\n",
    "- Low variance (stable predictions)\n",
    "```\n",
    "\n",
    "**Why We Cannot Determine Bias:**\n",
    "\n",
    "**Bias Depends on True Relationship:**\n",
    "- **If true relationship is simple**: Realtor B has lower bias\n",
    "- **If true relationship is complex**: Realtor A has lower bias\n",
    "- **Without knowing the true relationship**: Cannot determine bias\n",
    "\n",
    "**Mathematical Example:**\n",
    "\n",
    "**Scenario 1: Simple True Relationship**\n",
    "True price = $100,000 + $50 × size + $10,000 × bedrooms\n",
    "\n",
    "- **Realtor A**: Tries to fit 30 features to a 3-feature relationship\n",
    "- **Result**: High bias (overfitting to noise)\n",
    "- **Realtor B**: Uses 5 features including the important ones\n",
    "- **Result**: Lower bias (better fit to true relationship)\n",
    "\n",
    "**Scenario 2: Complex True Relationship**\n",
    "True price depends on 25 different features in complex ways\n",
    "\n",
    "- **Realtor A**: Can capture the complex relationship\n",
    "- **Result**: Lower bias (good fit to true relationship)\n",
    "- **Realtor B**: Cannot capture the complexity\n",
    "- **Result**: High bias (underfitting)\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - Realtor B's model has higher variance and lower bias than Realtor A's model:**\n",
    "- **Problem**: Contradicts the principle that simpler models have lower variance\n",
    "- **Issue**: 5 features vs 30 features clearly indicates different complexity\n",
    "- **Result**: Incorrect understanding of bias-variance tradeoff\n",
    "\n",
    "**Option (c) - Realtor A's model has higher variance and lower bias than Realtor B's model:**\n",
    "- **Problem**: Assumes we know the bias relationship\n",
    "- **Issue**: Without knowing the true relationship, we cannot determine bias\n",
    "- **Result**: Makes an assumption not supported by the problem\n",
    "\n",
    "**Option (d) - Realtor A's model has higher variance and higher bias than Realtor B's model:**\n",
    "- **Problem**: Assumes we know the bias relationship\n",
    "- **Issue**: More complex models don't necessarily have higher bias\n",
    "- **Result**: Incorrect assumption about bias\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**Model Selection:**\n",
    "- **High variance**: Model is unstable, may overfit\n",
    "- **Unknown bias**: Need to evaluate on validation data\n",
    "- **Trade-off**: Balance between bias and variance\n",
    "\n",
    "**Feature Engineering:**\n",
    "- **More features**: Higher variance, potentially lower bias\n",
    "- **Fewer features**: Lower variance, potentially higher bias\n",
    "- **Optimal number**: Depends on data and true relationship\n",
    "\n",
    "**Cross-Validation:**\n",
    "- **High variance models**: Need more folds for reliable estimates\n",
    "- **Low variance models**: More stable performance estimates\n",
    "- **Model comparison**: Must consider both bias and variance\n",
    "\n",
    "**Conclusion:**\n",
    "Realtor A's model has **higher variance** due to its greater complexity (30 features vs 5 features), but we **cannot determine the bias** without knowing the true underlying relationship between house features and price.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "**Select All** Suppose we have $N$ data points $x_1, x_2, \\dots, x_N$ that $x_i \\in \\mathbb{R}^d$. Define $X \\in \\mathbb{R}^{N \\times d}$ such that $X_{i,j} = (x_i)_j$, $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i$, and $\\mathbf{1}_N = (1,1,\\dots,1)^T \\in \\mathbb{R}^N$. Which of the following are true about principal components analysis (PCA)?\n",
    "\n",
    "(a) The principal components are eigenvectors of the centered data matrix $X - \\mathbf{1}_N \\bar{x}^T$.\n",
    "\n",
    "(b) The principal components are right singular vectors of the centered data matrix.\n",
    "\n",
    "(c) The principal components are eigenvectors of the sample covariance matrix $\\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})^T$.\n",
    "\n",
    "(d) Applying a rigid rotation matrix $Q$ (i.e., $QQ^T = Q^T Q = I$) to $X$ will not change the principal components' directions.\n",
    "\n",
    "**Correct answers:** (b), (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(b)** and **(c)**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Principal Component Analysis (PCA):**\n",
    "\n",
    "**Definition:**\n",
    "PCA is a dimensionality reduction technique that finds the directions of maximum variance in the data and projects the data onto these directions (principal components).\n",
    "\n",
    "**Mathematical Setup:**\n",
    "- **Data matrix**: $X \\in \\mathbb{R}^{N \\times d}$ where $X_{i,j} = (x_i)_j$\n",
    "- **Centered data matrix**: $X_c = X - \\mathbf{1}_N \\bar{x}^T$\n",
    "- **Sample covariance matrix**: $S = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})^T$\n",
    "\n",
    "**Analysis of Each Statement:**\n",
    "\n",
    "**Option (a) - The principal components are eigenvectors of the centered data matrix $X - \\mathbf{1}_N \\bar{x}^T$: FALSE** ❌\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Centered Data Matrix:**\n",
    "$X_c = X - \\mathbf{1}_N \\bar{x}^T$ is an $N \\times d$ matrix\n",
    "\n",
    "**Eigenvectors of $X_c$:**\n",
    "- $X_c$ is not square (unless $N = d$)\n",
    "- Eigenvectors are only defined for square matrices\n",
    "- The statement is mathematically incorrect\n",
    "\n",
    "**Correct Relationship:**\n",
    "The principal components are eigenvectors of $X_c^T X_c$, which is the $d \\times d$ matrix:\n",
    "$$X_c^T X_c = (X - \\mathbf{1}_N \\bar{x}^T)^T (X - \\mathbf{1}_N \\bar{x}^T)$$\n",
    "\n",
    "**Option (b) - The principal components are right singular vectors of the centered data matrix: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Singular Value Decomposition (SVD):**\n",
    "For the centered data matrix $X_c$:\n",
    "$$X_c = U \\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{N \\times r}$: Left singular vectors\n",
    "- $\\Sigma \\in \\mathbb{R}^{r \\times r}$: Singular values\n",
    "- $V \\in \\mathbb{R}^{d \\times r}$: Right singular vectors\n",
    "\n",
    "**Connection to PCA:**\n",
    "- **Right singular vectors** $V$ are the principal components\n",
    "- **Singular values** $\\Sigma$ are related to the variance explained\n",
    "- **Left singular vectors** $U$ are the projections of data onto principal components\n",
    "\n",
    "**Mathematical Verification:**\n",
    "$$X_c^T X_c = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T$$\n",
    "\n",
    "This shows that the right singular vectors $V$ are eigenvectors of $X_c^T X_c$.\n",
    "\n",
    "**Option (c) - The principal components are eigenvectors of the sample covariance matrix: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Sample Covariance Matrix:**\n",
    "$$S = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})^T$$\n",
    "\n",
    "**Relationship to Centered Data:**\n",
    "$$S = \\frac{1}{N-1} X_c^T X_c$$\n",
    "\n",
    "**Eigenvalue Decomposition:**\n",
    "$$S = Q \\Lambda Q^T$$\n",
    "\n",
    "where:\n",
    "- $Q$ contains the eigenvectors (principal components)\n",
    "- $\\Lambda$ contains the eigenvalues (variances)\n",
    "\n",
    "**Connection to PCA:**\n",
    "- **Eigenvectors** of $S$ are the principal components\n",
    "- **Eigenvalues** of $S$ are the variances along each principal component\n",
    "- **Ordering**: Eigenvalues in descending order correspond to principal components in order of importance\n",
    "\n",
    "**Option (d) - Applying a rigid rotation matrix $Q$ to $X$ will not change the principal components' directions: FALSE** ❌\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Rigid Rotation:**\n",
    "$Q$ is orthogonal: $QQ^T = Q^T Q = I$\n",
    "\n",
    "**Effect on Data:**\n",
    "$X' = XQ$ (rotation of data)\n",
    "\n",
    "**Effect on Principal Components:**\n",
    "- **Original PCA**: Eigenvectors of $X_c^T X_c$\n",
    "- **Rotated PCA**: Eigenvectors of $(X_c Q)^T (X_c Q) = Q^T X_c^T X_c Q$\n",
    "\n",
    "**Result:**\n",
    "The principal components change by the rotation $Q$:\n",
    "- **Original**: $v$ (eigenvector of $X_c^T X_c$)\n",
    "- **Rotated**: $Q^T v$ (eigenvector of $Q^T X_c^T X_c Q$)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Original Data:**\n",
    "```\n",
    "X = [1, 2; 3, 4; 5, 6]\n",
    "Principal components: [0.707, 0.707] and [-0.707, 0.707]\n",
    "```\n",
    "\n",
    "**After 90° Rotation:**\n",
    "```\n",
    "Q = [0, -1; 1, 0]\n",
    "X' = XQ\n",
    "Principal components: [0, 1] and [1, 0] (rotated by Q)\n",
    "```\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Computational Methods:**\n",
    "- **SVD method**: More numerically stable\n",
    "- **Eigenvalue method**: Direct computation of covariance matrix\n",
    "- **Both equivalent**: Choose based on computational efficiency\n",
    "\n",
    "**2. Data Preprocessing:**\n",
    "- **Centering**: Essential for PCA\n",
    "- **Scaling**: Often recommended for equal feature importance\n",
    "- **Rotation**: Changes principal components\n",
    "\n",
    "**3. Interpretation:**\n",
    "- **Principal components**: Directions of maximum variance\n",
    "- **Eigenvalues**: Amount of variance explained\n",
    "- **Cumulative variance**: $\\sum_{i=1}^k \\lambda_i / \\sum_{i=1}^d \\lambda_i$\n",
    "\n",
    "**Conclusion:**\n",
    "Options **(b)** and **(c)** are correct. The principal components are the right singular vectors of the centered data matrix and the eigenvectors of the sample covariance matrix. Option (a) is incorrect because eigenvectors are only defined for square matrices, and option (d) is incorrect because rotations change the principal component directions.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "**Select All** In the context of singular value decomposition (SVD) $A = U \\Sigma V^T$, which of the following statements are correct?\n",
    "\n",
    "(a) The columns of $U$ are called left singular vectors and form an orthonormal basis for the range of $A$, while the columns of $V$ are called right singular vectors and form an orthonormal basis for the range of $A^T$.\n",
    "\n",
    "(b) For any $A$ that is real and symmetric, we have $U = V$.\n",
    "\n",
    "(c) For a square matrix $A$, the singular values of $A$ are the absolute values of the eigenvalues of $A$.\n",
    "\n",
    "(d) Singular values are always non-negative real numbers.\n",
    "\n",
    "**Correct answers:** (a), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(a)** and **(d)**. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Singular Value Decomposition (SVD):**\n",
    "\n",
    "**Definition:**\n",
    "SVD decomposes a matrix $A \\in \\mathbb{R}^{m \\times n}$ into:\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$: Left singular vectors (orthonormal)\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$: Diagonal matrix of singular values\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$: Right singular vectors (orthonormal)\n",
    "\n",
    "**Analysis of Each Statement:**\n",
    "\n",
    "**Option (a) - The columns of $U$ are called left singular vectors and form an orthonormal basis for the range of $A$, while the columns of $V$ are called right singular vectors and form an orthonormal basis for the range of $A^T$: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Left Singular Vectors ($U$):**\n",
    "- **Definition**: Columns of $U$ are left singular vectors\n",
    "- **Orthonormality**: $U^T U = I$ (orthonormal)\n",
    "- **Range of $A$**: $\\text{range}(A) = \\text{span}\\{u_1, u_2, \\ldots, u_r\\}$\n",
    "  where $r = \\text{rank}(A)$\n",
    "\n",
    "**Right Singular Vectors ($V$):**\n",
    "- **Definition**: Columns of $V$ are right singular vectors\n",
    "- **Orthonormality**: $V^T V = I$ (orthonormal)\n",
    "- **Range of $A^T$**: $\\text{range}(A^T) = \\text{span}\\{v_1, v_2, \\ldots, v_r\\}$\n",
    "\n",
    "**Mathematical Verification:**\n",
    "$$A = U \\Sigma V^T \\Rightarrow A^T = V \\Sigma^T U^T$$\n",
    "\n",
    "**Option (b) - For any $A$ that is real and symmetric, we have $U = V$: FALSE** ❌\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Symmetric Matrix:**\n",
    "If $A$ is real and symmetric, then $A = A^T$\n",
    "\n",
    "**SVD for Symmetric Matrix:**\n",
    "$$A = U \\Sigma V^T = A^T = V \\Sigma^T U^T$$\n",
    "\n",
    "**Result:**\n",
    "- $U \\Sigma V^T = V \\Sigma U^T$ (since $\\Sigma^T = \\Sigma$ for diagonal matrix)\n",
    "- This does **not** imply $U = V$\n",
    "- The relationship is more complex\n",
    "\n",
    "**Counterexample:**\n",
    "Consider $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$ (symmetric)\n",
    "- Eigenvalues: $\\lambda_1 = 1, \\lambda_2 = -1$\n",
    "- SVD: $A = U \\Sigma V^T$ where $U \\neq V$\n",
    "\n",
    "**Option (c) - For a square matrix $A$, the singular values of $A$ are the absolute values of the eigenvalues of $A$: FALSE** ❌\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Relationship Between Eigenvalues and Singular Values:**\n",
    "For a square matrix $A$:\n",
    "- **Eigenvalues**: Solutions to $|A - \\lambda I| = 0$\n",
    "- **Singular values**: Square roots of eigenvalues of $A^T A$\n",
    "\n",
    "**Mathematical Relationship:**\n",
    "$$\\sigma_i = \\sqrt{\\lambda_i(A^T A)}$$\n",
    "\n",
    "**Counterexample:**\n",
    "Consider $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "**Eigenvalues of $A$:**\n",
    "$$|A - \\lambda I| = \\begin{vmatrix} 1-\\lambda & 1 \\\\ 0 & 1-\\lambda \\end{vmatrix} = (1-\\lambda)^2 = 0$$\n",
    "$$\\lambda_1 = \\lambda_2 = 1$$\n",
    "\n",
    "**Eigenvalues of $A^T A$:**\n",
    "$$A^T A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$$\n",
    "\n",
    "$$|A^T A - \\lambda I| = \\begin{vmatrix} 1-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{vmatrix} = (1-\\lambda)(2-\\lambda) - 1 = \\lambda^2 - 3\\lambda + 1 = 0$$\n",
    "\n",
    "$$\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}$$\n",
    "\n",
    "**Singular Values:**\n",
    "$$\\sigma_1 = \\sqrt{\\frac{3 + \\sqrt{5}}{2}} \\approx 1.618$$\n",
    "$$\\sigma_2 = \\sqrt{\\frac{3 - \\sqrt{5}}{2}} \\approx 0.618$$\n",
    "\n",
    "**Result**: Singular values $\\{1.618, 0.618\\}$ ≠ absolute values of eigenvalues $\\{1, 1\\}$\n",
    "\n",
    "**Option (d) - Singular values are always non-negative real numbers: TRUE** ✅\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "**Definition of Singular Values:**\n",
    "Singular values are the square roots of the eigenvalues of $A^T A$:\n",
    "$$\\sigma_i = \\sqrt{\\lambda_i(A^T A)}$$\n",
    "\n",
    "**Properties of $A^T A$:**\n",
    "- **Symmetric**: $(A^T A)^T = A^T A$\n",
    "- **Positive semi-definite**: $x^T A^T A x = ||Ax||^2 \\geq 0$ for all $x$\n",
    "\n",
    "**Eigenvalues of Positive Semi-definite Matrix:**\n",
    "- All eigenvalues are **non-negative real numbers**\n",
    "- $\\lambda_i(A^T A) \\geq 0$ for all $i$\n",
    "\n",
    "**Result:**\n",
    "$$\\sigma_i = \\sqrt{\\lambda_i(A^T A)} \\geq 0$$\n",
    "since $\\lambda_i(A^T A) \\geq 0$\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**1. Numerical Stability:**\n",
    "- Singular values are always non-negative\n",
    "- Useful for determining matrix rank\n",
    "- Helps identify numerical issues\n",
    "\n",
    "**2. Matrix Analysis:**\n",
    "- **Condition number**: $\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$\n",
    "- **Rank**: Number of non-zero singular values\n",
    "- **Frobenius norm**: $||A||_F = \\sqrt{\\sum_i \\sigma_i^2}$\n",
    "\n",
    "**3. Applications:**\n",
    "- **PCA**: Singular values indicate variance explained\n",
    "- **Image compression**: Keep largest singular values\n",
    "- **Noise reduction**: Filter small singular values\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Matrix $A = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$:**\n",
    "\n",
    "**Eigenvalues:**\n",
    "$$\\lambda_1 = 3, \\lambda_2 = 2$$\n",
    "\n",
    "**SVD:**\n",
    "$$A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}^T$$\n",
    "\n",
    "**Singular Values:**\n",
    "$$\\sigma_1 = 3, \\sigma_2 = 2$$\n",
    "\n",
    "**Note**: In this case, singular values equal eigenvalues because $A$ is diagonal and positive.\n",
    "\n",
    "**Conclusion:**\n",
    "Options **(a)** and **(d)** are correct. The left and right singular vectors form orthonormal bases for the appropriate spaces, and singular values are always non-negative real numbers. Option (b) is incorrect because symmetric matrices don't necessarily have $U = V$, and option (c) is incorrect because singular values are not always the absolute values of eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "**Select All** Which of the following statements about matrix completion are correct?\n",
    "\n",
    "(a) It may not perform well when the real-world data is not inherently low-rank or when the pattern of missing observations is not random.\n",
    "\n",
    "(b) The purpose of matrix completion is to estimate missing entries in a partially observed matrix.\n",
    "\n",
    "(c) Matrix completion is only applicable for square matrices.\n",
    "\n",
    "**Correct answers:** (a), (b)\n",
    "\n",
    "**Explanation:** (c) No such restriction.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "**One Answer** Consider a feature map $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}^4$ defined as:\n",
    "\n",
    "$\\phi \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_1 x_2 \\\\ x_2 x_1 \\end{bmatrix}$\n",
    "\n",
    "What is the corresponding kernel function $K$ for $\\phi$?\n",
    "\n",
    "(a) $K: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ and $K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^\\top \\mathbf{x}')^2$.\n",
    "\n",
    "(b) $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ and $K(x, x') = x^4 + x'^4 + 2x^2x'^2$.\n",
    "\n",
    "(c) $K: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ and $K(\\mathbf{x}, \\mathbf{x}') = \\mathbf{x}^\\top \\mathbf{x}'$.\n",
    "\n",
    "(d) $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ and $K(x,x') = x^2 + x'^2$.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "**One Answer** In the context of kernel methods, what does the \"kernel trick\" refer to?\n",
    "\n",
    "(a) Adding an extra kernel layer to the end of a neural network.\n",
    "\n",
    "(b) A technique for explicitly computing the coordinates in a high-dimensional space.\n",
    "\n",
    "(c) A method for computing the inner products in a high-dimensional feature space without explicitly mapping data to that space.\n",
    "\n",
    "(d) A technique for speeding up the convergence of gradient descent.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "**One Answer** When using a kernel method to solve a regression problem with training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ and $\\mathbf{x}_i \\in \\mathbb{R}^d$, we first prove that there exists an $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$ such that the weight vector $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, where $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ is a feature map transforming $\\mathbf{x}_i$ into a very high dimensional space $\\mathbb{R}^p$ with $p \\gg d$. Then, solving the problem is equivalent to finding $\\hat{\\boldsymbol{\\alpha}} = \\text{argmin}_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n(y_i - \\sum_{j=1}^n \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j))^2 + \\lambda \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$. After we computed the value of $\\hat{\\boldsymbol{\\alpha}}$, given an input $\\mathbf{x}' \\in \\mathbb{R}^d$ in the test set, how can we make the prediction?\n",
    "\n",
    "(a) Because $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, we can compute the value of $\\hat{\\mathbf{w}}$, and then applying $\\hat{y} = \\hat{\\mathbf{w}}^T \\mathbf{x}'$.\n",
    "\n",
    "(b) Because $\\hat{y} = \\hat{\\mathbf{w}}^T \\mathbf{x}' = \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i^T \\mathbf{x}'$, we can compute the values of $\\mathbf{x}_i^T \\mathbf{x}'$ and then get the value of $\\hat{y}$.\n",
    "\n",
    "(c) Because $\\hat{y} = \\hat{\\mathbf{w}}^T \\phi(\\mathbf{x}') = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}') = \\sum_{i=1}^n \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}')$, we can compute the values of $K(\\mathbf{x}_i, \\mathbf{x}')$ and then get the value of $\\hat{y}$.\n",
    "\n",
    "(d) Because $\\hat{\\mathbf{w}} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$, we can compute the value of $\\hat{\\mathbf{w}}$, and then applying $\\hat{y} = \\hat{\\mathbf{w}}^T \\phi(\\mathbf{x}')$.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "**One Answer** Consider the following neural network with weights shown in the image below. Every hidden neuron uses the ReLU activation function, and there is no activation function on the output neuron. Assume there are no bias terms. What is the output of this network with the input $x = (3,2)$?\n",
    "\n",
    "<img src=\"./q13_problem.png\" width=\"450px\">\n",
    "\n",
    "The image displays a feedforward neural network with two input neurons, three hidden neurons, and one output neuron.\n",
    "\n",
    "**Input Layer:**\n",
    "*   The first input neuron is labeled $x_1 = 3$.\n",
    "*   The second input neuron is labeled $x_2 = 2$.\n",
    "\n",
    "**Connections from Input Layer to Hidden Layer (with weights):**\n",
    "*   From $x_1$:\n",
    "    *   To the top hidden neuron: weight $-1$\n",
    "    *   To the middle hidden neuron: weight $-2$\n",
    "    *   To the bottom hidden neuron: weight $3$\n",
    "*   From $x_2$:\n",
    "    *   To the top hidden neuron: weight $-2$\n",
    "    *   To the middle hidden neuron: weight $2$\n",
    "    *   To the bottom hidden neuron: weight $-1$\n",
    "\n",
    "**Connections from Hidden Layer to Output Layer (with weights):**\n",
    "*   From the top hidden neuron to the output neuron: weight $4$\n",
    "*   From the middle hidden neuron to the output neuron: weight $-2$\n",
    "*   From the bottom hidden neuron to the output neuron: weight $1$\n",
    "\n",
    "**Activation Functions:**\n",
    "*   Every hidden neuron uses the ReLU (Rectified Linear Unit) activation function, defined as $f(z) = \\max(0, z)$.\n",
    "*   There is no activation function on the output neuron (linear activation).\n",
    "*   There are no bias terms in the network.\n",
    "\n",
    "The output of the network is denoted as $\\hat{y}$.\n",
    "\n",
    "**Answer:** $7$\n",
    "\n",
    "**Explanation:** Answer is $7$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "**One Answer** How many parameters does the neural network shown in the previous problem have?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**Explanation:** Answer is 9\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "**One Answer** Which of the following defines the correct ordering of steps needed to perform backpropagation in PyTorch?\n",
    "\n",
    "(a) (1) compute loss, (2) compute gradients, (3) take step, (4) zero the gradient buffers\n",
    "\n",
    "(b) (1) compute loss, (2) zero the gradient buffers, (3) compute gradients, (4) take step\n",
    "\n",
    "(c) (1) compute loss, (2) take step, (3) zero the gradient buffers, (4) compute gradients\n",
    "\n",
    "(d) (1) zero the gradient buffers, (2) compute gradients, (3) take step, (4) compute loss\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "**One Answer** Consider a convolutional neural network (CNN) layer with the following parameters:\n",
    "\n",
    "*   Input image size: 3 x 32 x 32 (channels, height, width)\n",
    "*   Number of filters: 16\n",
    "*   filter size: 3x3\n",
    "*   Stride: 1\n",
    "*   Padding: 1\n",
    "\n",
    "What will be the shape of the output after applying this convolutional layer (in the order of channels, height, width)?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**Explanation:** 16 x 32 x 32. $\\frac{32-3+2\\times1}{1}+1=32$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "**One Answer** Which of the following best describes the purpose of pooling layers in a convolutional neural network (CNN)?\n",
    "\n",
    "(a) To increase the resolution of the feature maps.\n",
    "\n",
    "(b) To reduce the spatial dimensions of the feature maps, thereby reducing the computational load and the number of parameters.\n",
    "\n",
    "(c) To convert the feature maps into a fully connected layer.\n",
    "\n",
    "(d) To normalize the feature maps by scaling them to a fixed range.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "**One Answer** Given the following setup in a simple recurrent neural network (RNN):\n",
    "\n",
    "Input at time t: $x_t$\n",
    "\n",
    "The RNN has one hidden layer with the following parameters:\n",
    "* Input to hidden state weights: $W_{xh}$\n",
    "* Hidden state to hidden state weights: $W_{hh}$\n",
    "* Hidden state to output weights: $W_{hy}$\n",
    "* Bias for the hidden state: $b_h$\n",
    "* Bias for the output: $b_y$\n",
    "\n",
    "The activation function for the hidden state is ReLU.\n",
    "\n",
    "Given the following parameter values:\n",
    "$W_{xh} = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.3 & 0.2 \\end{bmatrix}$, $W_{hh} = \\begin{bmatrix} 0.6 & 0.4 \\\\ 0.2 & 0.5 \\end{bmatrix}$, $W_{hy} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b_h = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$, $b_y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "Calculate the hidden state $h_1$ after processing the first input $x_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. Assume the initial hidden state $h_0$ is a zero vector.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**Explanation:** $\\text{relu}(W_{xh}x_1 + W_{hh}h_0 + b_h) = \\begin{bmatrix} 0.6 \\\\ 0.5 \\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "**One Answer** Which of the following statements about the k-means clustering algorithm is true?\n",
    "\n",
    "(a) It guarantees convergence to the global optimum.\n",
    "\n",
    "(b) It is robust against the initialization of cluster means.\n",
    "\n",
    "(c) It may converge to a local optimum depending on the initial placement of cluster means.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "**One Answer** Why might a Gaussian Mixture Model (GMM) be preferred over K-means in cases where the data contains mixed or overlapping clusters?\n",
    "\n",
    "(a) GMM exclusively uses hard assignments which are better for mixed clusters.\n",
    "\n",
    "(b) GMM utilizes soft assignments, allowing points to belong to multiple clusters with varying probabilities.\n",
    "\n",
    "(c) GMM always converges faster than k-means.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "**One Answer** In k-means clustering, the algorithm is executed several times on the same dataset, each time with a fresh random initialization of cluster centers and the same number of clusters. If these multiple runs yield widely varying cluster outcomes, what might this suggest about the algorithm's sensitivity to initial conditions?\n",
    "\n",
    "(a) The choice of k is optimal.\n",
    "\n",
    "(b) The dataset is perfectly clustered.\n",
    "\n",
    "(c) The initialization of centers might be influencing the results.\n",
    "\n",
    "(d) The algorithm is not suitable for clustering.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "**One Answer** Consider an LSTM (Long Short-Term Memory) network with the following characteristics: a forget gate, an input gate, a memory cell, and an output gate. Which of the following statements correctly describes the function of the forget gate in an LSTM?\n",
    "\n",
    "<img src=\"./q22_problem.png\" width=\"450px\">\n",
    "\n",
    "The image displays a diagram of a Long Short-Term Memory (LSTM) network, illustrating its repeating module structure and internal gates.\n",
    "\n",
    "The diagram shows three identical blocks, labeled 'A', representing the repeating module of the LSTM. The central block 'A' is expanded to show its internal components and data flow.\n",
    "\n",
    "**Inputs to the central LSTM module:**\n",
    "- $h_{t-1}$: Previous hidden state, represented by an arrow pointing down into the module.\n",
    "- $X_t$: Current input, represented by an arrow pointing down into the module.\n",
    "- $C_{t-1}$: Previous cell state, implicitly flowing into the module from the left.\n",
    "\n",
    "**Outputs from the central LSTM module:**\n",
    "- $h_t$: Current hidden state, represented by an arrow pointing up from the module.\n",
    "- $C_t$: Current cell state, implicitly flowing out of the module to the right.\n",
    "\n",
    "**Internal structure of the LSTM module (from left to right, top to bottom):**\n",
    "\n",
    "1.  **Forget Gate:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated (double arrow pointing right).\n",
    "    -   The concatenated vector goes into a \"Neural Network Layer\" (yellow rectangle).\n",
    "    -   The output of the neural network layer goes into a \"Pointwise Operation\" (circle), which is a sigmoid function ($\\sigma$).\n",
    "    -   The output of the sigmoid is multiplied (Pointwise Operation, $\\times$) with the previous cell state $C_{t-1}$.\n",
    "\n",
    "2.  **Input Gate and Candidate Cell State:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated.\n",
    "    -   This concatenated vector splits into two paths:\n",
    "        -   **Path 1 (Input Gate):** Goes into a \"Neural Network Layer\", then a \"Pointwise Operation\" (sigmoid $\\sigma$).\n",
    "        -   **Path 2 (Candidate Cell State):** Goes into a \"Neural Network Layer\" with a 'tanh' activation function indicated.\n",
    "    -   The outputs of Path 1 (Input Gate) and Path 2 (Candidate Cell State) are multiplied (Pointwise Operation, $\\times$).\n",
    "\n",
    "3.  **Cell State Update:**\n",
    "    -   The output of the Forget Gate (multiplication with $C_{t-1}$) and the output of the Input Gate/Candidate Cell State multiplication are added (Pointwise Operation, $+$). This sum forms the new cell state $C_t$.\n",
    "\n",
    "4.  **Output Gate:**\n",
    "    -   Inputs $h_{t-1}$ and $X_t$ are concatenated.\n",
    "    -   The concatenated vector goes into a \"Neural Network Layer\", then a \"Pointwise Operation\" (sigmoid $\\sigma$).\n",
    "\n",
    "5.  **Hidden State Calculation:**\n",
    "    -   The new cell state $C_t$ goes into a \"Pointwise Operation\" (tanh activation).\n",
    "    -   The output of the Output Gate (sigmoid) and the tanh-activated $C_t$ are multiplied (Pointwise Operation, $\\times$). This result is the new hidden state $h_t$.\n",
    "\n",
    "**Legend for symbols used in the diagram:**\n",
    "-   Yellow rectangle: Neural Network Layer\n",
    "-   Circle: Pointwise Operation (e.g., sigmoid, tanh, addition, multiplication)\n",
    "-   Single arrow: Vector Transfer\n",
    "-   Double arrow pointing right: Concatenate\n",
    "-   Double arrow splitting: Copy\n",
    "\n",
    "(a) The forget gate decides which information should be discarded from the cell state.\n",
    "\n",
    "(b) The forget gate calculates the output of the current position at each time step.\n",
    "\n",
    "(c) The forget gate extracts useful information from the input to update memory.\n",
    "\n",
    "(d) The forget gate calculates the next hidden state based on the current input.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** The forget gate in an LSTM outputs a value between 0 and 1 for each number in the cell state $C_{t-1}$, where 1 represents \"completely keep this\" and 0 represents \"completely forget this\". It is used to remove information that is no longer needed from the cell state.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "**One Answer** What is the key benefit of the attention mechanism over standard RNN models in the context of neural machine translation?\n",
    "\n",
    "(a) It significantly reduces computational complexity.\n",
    "\n",
    "(b) It uses convolutional layers for long-term dependencies.\n",
    "\n",
    "(c) It relies entirely on recurrent layers for processing sequences.\n",
    "\n",
    "(d) It solves the bottleneck problem and long-term dependency issues by focusing on specific parts of the input sequence.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** The attention mechanism addresses the bottleneck problem and long-term dependency issues in standard Seq2Seq models by allowing the model to focus on specific parts of the input sequence during decoding.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "**One Answer** What is the main purpose of using positional encoding in the Transformer architecture?\n",
    "\n",
    "(a) It introduces non-linearity in the model.\n",
    "\n",
    "(b) It helps in maintaining long-term dependencies.\n",
    "\n",
    "(c) It provides information about the order of the input sequence.\n",
    "\n",
    "(d) It reduces the computational complexity.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Positional encoding provides information about the position of each element in the input sequence, which is crucial because the self-attention mechanism in Transformers is order-invariant and does not inherently capture sequence order.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
