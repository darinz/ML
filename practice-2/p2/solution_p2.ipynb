{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d28ca06-25ed-438c-9388-f2c1b28d1fec",
   "metadata": {},
   "source": [
    "# Practice Problem Set 2 Solutions\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "A fair six-sided die is rolled twice. What is the conditional probability that the first roll showed a 2, given that the sum of the two rolls is 6?\n",
    "\n",
    "(a) $\\frac{1}{6}$\n",
    "\n",
    "(b) $\\frac{1}{5}$\n",
    "\n",
    "(c) $\\frac{3}{11}$\n",
    "\n",
    "(d) $\\frac{2}{5}$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **conditional probability** problem. We need to find $P(\\text{First roll} = 2 \\mid \\text{Sum} = 6)$.\n",
    "\n",
    "**Step-by-Step Solution:**\n",
    "\n",
    "1. **Define the Events:**\n",
    "   - Event $A$: First roll shows a 2\n",
    "   - Event $B$: Sum of two rolls equals 6\n",
    "\n",
    "2. **Find $P(B)$ - Total ways to get sum of 6:**\n",
    "   The possible combinations that sum to 6 are:\n",
    "   - $(1,5)$ - First roll: 1, Second roll: 5\n",
    "   - $(2,4)$ - First roll: 2, Second roll: 4  \n",
    "   - $(3,3)$ - First roll: 3, Second roll: 3\n",
    "   - $(4,2)$ - First roll: 4, Second roll: 2\n",
    "   - $(5,1)$ - First roll: 5, Second roll: 1\n",
    "   \n",
    "   Total: 5 equally likely outcomes\n",
    "\n",
    "3. **Find $P(A \\cap B)$ - Ways to get sum of 6 AND first roll is 2:**\n",
    "   Only one combination satisfies both conditions: $(2,4)$\n",
    "   \n",
    "   So $P(A \\cap B) = \\frac{1}{36}$ (since there are 36 total possible outcomes for two dice)\n",
    "\n",
    "4. **Apply Conditional Probability Formula:**\n",
    "   $$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{\\frac{1}{36}}{\\frac{5}{36}} = \\frac{1}{5}$$\n",
    "\n",
    "**Alternative Intuitive Approach:**\n",
    "Since we know the sum is 6, we're only considering the 5 possible combinations above. Among these 5 equally likely outcomes, only one has the first roll as 2. Therefore, the probability is $\\frac{1}{5}$.\n",
    "\n",
    "**Mathematical Verification:**\n",
    "- Total outcomes for two dice: $6 \\times 6 = 36$\n",
    "- Outcomes with sum 6: 5\n",
    "- Outcomes with sum 6 AND first roll 2: 1\n",
    "- Conditional probability: $\\frac{1}{5}$\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "If matrix A has distinct eigenvalues, what can be said about its eigenvectors?\n",
    "\n",
    "(a) The eigenvectors form a linearly independent set\n",
    "\n",
    "(b) The eigenvalues are orthogonal to each other\n",
    "\n",
    "(c) A must be positive semi-definite\n",
    "\n",
    "(d) None of the above\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(a)** - the eigenvectors form a linearly independent set. Here's the detailed explanation:\n",
    "\n",
    "**Key Theorem:**\n",
    "If a matrix $A$ has **distinct eigenvalues**, then the corresponding eigenvectors are **linearly independent**.\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "1. **Eigenvalue-Eigenvector Relationship:**\n",
    "   For distinct eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ and corresponding eigenvectors $v_1, v_2, \\ldots, v_n$:\n",
    "   $$Av_i = \\lambda_i v_i \\quad \\text{for } i = 1, 2, \\ldots, n$$\n",
    "\n",
    "2. **Linear Independence Proof by Contradiction:**\n",
    "   Assume the eigenvectors are linearly dependent. Then there exist scalars $c_1, c_2, \\ldots, c_n$ (not all zero) such that:\n",
    "   $$c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n = 0$$\n",
    "\n",
    "3. **Applying Matrix A:**\n",
    "   $$A(c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n) = A \\cdot 0 = 0$$\n",
    "   $$c_1 Av_1 + c_2 Av_2 + \\cdots + c_n Av_n = 0$$\n",
    "   $$c_1 \\lambda_1 v_1 + c_2 \\lambda_2 v_2 + \\cdots + c_n \\lambda_n v_n = 0$$\n",
    "\n",
    "4. **Subtracting Scaled Original Equation:**\n",
    "   Multiply the original equation by $\\lambda_1$:\n",
    "   $$c_1 \\lambda_1 v_1 + c_2 \\lambda_1 v_2 + \\cdots + c_n \\lambda_1 v_n = 0$$\n",
    "   \n",
    "   Subtract from the previous equation:\n",
    "   $$c_2(\\lambda_2 - \\lambda_1)v_2 + c_3(\\lambda_3 - \\lambda_1)v_3 + \\cdots + c_n(\\lambda_n - \\lambda_1)v_n = 0$$\n",
    "\n",
    "5. **Contradiction:**\n",
    "   Since $\\lambda_i \\neq \\lambda_1$ for $i > 1$, and $v_2, v_3, \\ldots, v_n$ are linearly independent (by induction), we must have $c_2 = c_3 = \\cdots = c_n = 0$. This contradicts our assumption.\n",
    "\n",
    "**Why Other Options Are Incorrect:**\n",
    "\n",
    "- **(b) Eigenvalues are orthogonal**: Eigenvalues are scalars, not vectors, so they can't be orthogonal\n",
    "- **(c) A must be positive semi-definite**: Not true - matrices with distinct eigenvalues can be indefinite\n",
    "- **(d) None of the above**: Incorrect since (a) is true\n",
    "\n",
    "**Important Note:**\n",
    "The question could have been more precise by specifying \"eigenvectors corresponding to distinct eigenvalues form a linearly independent set,\" but the intent is clear from the context.\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "In the context of multi-class logistic regression, which statement most accurately describes the decision boundaries?\n",
    "\n",
    "(a) They are linear and distinctly separate distinct classes.\n",
    "\n",
    "(b) They are non-linear and may overlap.\n",
    "\n",
    "(c) They remain unchanged, regardless of any transformations of the data.\n",
    "\n",
    "(d) They may be linear or non-linear, depending on the distribution of the data.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(a)** - the decision boundaries are linear and distinctly separate distinct classes. Here's the detailed explanation:\n",
    "\n",
    "**Multi-Class Logistic Regression Model:**\n",
    "\n",
    "1. **Model Formulation:**\n",
    "   For $k$ classes, the model outputs probabilities:\n",
    "   $$P(y = i \\mid x) = \\frac{e^{w_i^T x + b_i}}{\\sum_{j=1}^k e^{w_j^T x + b_j}}$$\n",
    "   \n",
    "   where $w_i$ and $b_i$ are the weights and bias for class $i$.\n",
    "\n",
    "2. **Decision Rule:**\n",
    "   The predicted class is:\n",
    "   $$\\hat{y} = \\arg\\max_{i} P(y = i \\mid x) = \\arg\\max_{i} (w_i^T x + b_i)$$\n",
    "\n",
    "**Why Decision Boundaries Are Linear:**\n",
    "\n",
    "1. **Linear Decision Functions:**\n",
    "   The decision function for class $i$ vs class $j$ is:\n",
    "   $$f_{ij}(x) = (w_i^T x + b_i) - (w_j^T x + b_j) = (w_i - w_j)^T x + (b_i - b_j)$$\n",
    "   \n",
    "   This is a **linear function** of $x$.\n",
    "\n",
    "2. **Decision Boundary:**\n",
    "   The decision boundary between classes $i$ and $j$ occurs when:\n",
    "   $$f_{ij}(x) = 0$$\n",
    "   $$(w_i - w_j)^T x + (b_i - b_j) = 0$$\n",
    "   \n",
    "   This is a **linear equation** in $x$, representing a **hyperplane**.\n",
    "\n",
    "**Visual Example in 2D:**\n",
    "For 3 classes, the decision boundaries are three lines that divide the plane into three regions:\n",
    "- Class 1: $w_1^T x + b_1 > w_2^T x + b_2$ AND $w_1^T x + b_1 > w_3^T x + b_3$\n",
    "- Class 2: $w_2^T x + b_2 > w_1^T x + b_1$ AND $w_2^T x + b_2 > w_3^T x + b_3$\n",
    "- Class 3: $w_3^T x + b_3 > w_1^T x + b_1$ AND $w_3^T x + b_3 > w_2^T x + b_2$\n",
    "\n",
    "**Why Boundaries Don't Overlap:**\n",
    "- Each point belongs to exactly one class (mutually exclusive)\n",
    "- The decision boundaries are the boundaries between these regions\n",
    "- At any point on a decision boundary, the probabilities for the two adjacent classes are equal\n",
    "\n",
    "**Mathematical Verification:**\n",
    "The decision boundary between classes $i$ and $j$ is:\n",
    "$$(w_i - w_j)^T x + (b_i - b_j) = 0$$\n",
    "\n",
    "This is a linear hyperplane in the feature space, confirming that multi-class logistic regression produces linear decision boundaries.\n",
    "\n",
    "**Contrast with Non-Linear Methods:**\n",
    "- **Kernel SVM**: Can produce non-linear decision boundaries\n",
    "- **Neural Networks**: Can learn complex, non-linear decision boundaries\n",
    "- **Decision Trees**: Produce piecewise linear boundaries\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Which of the following is true about linear and logistic regression?\n",
    "\n",
    "(a) Both models output a probability distribution.\n",
    "\n",
    "(b) Both models are good choices for regression classes of problems.\n",
    "\n",
    "(c) Both models are good choices for classification.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(c)** - both models are good choices for classification. Here's the detailed explanation:\n",
    "\n",
    "**Linear Regression for Classification:**\n",
    "\n",
    "1. **Mathematical Foundation:**\n",
    "   Linear regression can be used for binary classification by:\n",
    "   - Training: $\\min_w \\sum_{i=1}^n (y_i - w^T x_i)^2$\n",
    "   - Prediction: $\\hat{y} = \\text{sign}(w^T x)$\n",
    "   \n",
    "   where $y_i \\in \\{-1, +1\\}$ for binary classification.\n",
    "\n",
    "2. **Why It Works:**\n",
    "   - The squared error loss encourages $w^T x_i$ to be close to $y_i$\n",
    "   - For $y_i = +1$, we want $w^T x_i > 0$\n",
    "   - For $y_i = -1$, we want $w^T x_i < 0$\n",
    "   - This naturally creates a linear decision boundary\n",
    "\n",
    "3. **Advantages:**\n",
    "   - **Simple and fast**: Closed-form solution available\n",
    "   - **Interpretable**: Weights have clear meaning\n",
    "   - **Stable**: Unique solution when $X^T X$ is invertible\n",
    "   - **Good baseline**: Often performs surprisingly well\n",
    "\n",
    "**Logistic Regression for Classification:**\n",
    "\n",
    "1. **Mathematical Foundation:**\n",
    "   $$P(y = 1 \\mid x) = \\frac{1}{1 + e^{-w^T x}}$$\n",
    "   \n",
    "   The model maximizes the likelihood of the observed data.\n",
    "\n",
    "2. **Advantages:**\n",
    "   - **Probabilistic output**: Direct probability estimates\n",
    "   - **Well-calibrated**: Probabilities are meaningful\n",
    "   - **Convex optimization**: Guaranteed global optimum\n",
    "   - **Regularization friendly**: Easy to add L1/L2 penalties\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Aspect | Linear Regression | Logistic Regression |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Output** | Real-valued | Probability $[0,1]$ |\n",
    "| **Loss Function** | Squared error | Log-likelihood |\n",
    "| **Optimization** | Closed-form | Iterative (gradient descent) |\n",
    "| **Interpretability** | Direct | Through log-odds |\n",
    "| **Calibration** | Poor | Good |\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "**Linear Regression for Classification:**\n",
    "- Quick prototyping\n",
    "- When you need fast training\n",
    "- When interpretability is crucial\n",
    "- When you have linearly separable data\n",
    "\n",
    "**Logistic Regression for Classification:**\n",
    "- When you need probability estimates\n",
    "- When you want well-calibrated predictions\n",
    "- When you plan to use the model in a larger system\n",
    "- When you need to handle class imbalance\n",
    "\n",
    "**Historical Context:**\n",
    "Linear regression was actually used for classification before logistic regression became popular. The famous Fisher's Iris dataset was originally classified using linear discriminant analysis, which is closely related to linear regression.\n",
    "\n",
    "**Note:** The question was revised because option (b) was ambiguous about what \"regression classes of problems\" means.\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "Suppose you train a binary classifier in which the final two layers of your model are a ReLU activation followed by a sigmoid activation. How will this affect the domain of your final predictions?\n",
    "\n",
    "(a) This will cause all predictions to be positive.\n",
    "\n",
    "(b) This will have no effect on the distribution of predictions.\n",
    "\n",
    "(c) This will cause cause all predictions to be negative.\n",
    "\n",
    "(d) None of the above.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(a)** - this will cause all predictions to be positive. Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Architecture:**\n",
    "The model has two final layers:\n",
    "1. **ReLU activation**: $f(x) = \\max(0, x)$\n",
    "2. **Sigmoid activation**: $g(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "1. **ReLU Function Properties:**\n",
    "   - $f(x) = \\max(0, x)$\n",
    "   - Output range: $[0, \\infty)$\n",
    "   - All outputs are **non-negative**\n",
    "\n",
    "2. **Sigmoid Function Properties:**\n",
    "   - $g(x) = \\frac{1}{1 + e^{-x}}$\n",
    "   - Output range: $(0, 1)$\n",
    "   - Monotonic increasing function\n",
    "\n",
    "3. **Composition Effect:**\n",
    "   Let $z$ be the input to the ReLU layer, then:\n",
    "   - ReLU output: $a = \\max(0, z) \\geq 0$\n",
    "   - Sigmoid input: $a \\geq 0$\n",
    "   - Final output: $g(a) = \\frac{1}{1 + e^{-a}}$\n",
    "\n",
    "4. **Key Insight:**\n",
    "   Since $a \\geq 0$ (due to ReLU), we have:\n",
    "   - When $a = 0$: $g(0) = \\frac{1}{1 + e^0} = \\frac{1}{2} = 0.5$\n",
    "   - When $a > 0$: $g(a) > 0.5$ (because sigmoid is increasing)\n",
    "   - When $a \\to \\infty$: $g(a) \\to 1$\n",
    "\n",
    "**Why All Predictions Are Positive:**\n",
    "\n",
    "1. **ReLU Constraint**: The ReLU layer ensures all inputs to the sigmoid are non-negative\n",
    "2. **Sigmoid Behavior**: For non-negative inputs, sigmoid outputs values in $(0.5, 1]$\n",
    "3. **No Negative Predictions**: The model can never output values below 0.5\n",
    "\n",
    "**Mathematical Verification:**\n",
    "For any input $z$ to the ReLU layer:\n",
    "$$\\text{Output} = \\frac{1}{1 + e^{-\\max(0, z)}} \\geq \\frac{1}{1 + e^0} = 0.5$$\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "1. **Binary Classification**: If using 0.5 as threshold, all predictions would be class 1\n",
    "2. **Probability Interpretation**: All probabilities would be $\\geq 0.5$\n",
    "3. **Model Limitation**: The model cannot represent the full range of probabilities\n",
    "\n",
    "**Better Alternatives:**\n",
    "\n",
    "1. **Remove ReLU**: Use only sigmoid for probability output\n",
    "2. **Use Different Activation**: Replace ReLU with tanh or linear activation\n",
    "3. **Adjust Architecture**: Use softmax for multi-class or different activation pattern\n",
    "\n",
    "**Example:**\n",
    "- Input to ReLU: $z = -2$\n",
    "- ReLU output: $a = \\max(0, -2) = 0$\n",
    "- Sigmoid output: $g(0) = 0.5$\n",
    "- Even negative inputs result in 0.5 probability!\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "You are tasked with building a regression model to predict whether an email is spam [label=1] or not spam [label=0] based on various features. You are debating using linear or logistic regression. What type of regression is most suitable and why?\n",
    "\n",
    "(a) Linear regression, because it is optimized for learning the influence of multiple features.\n",
    "\n",
    "(b) Linear regression, because logistic regression cannot predict the comparative magnitude of the likelihood that an email is spam.\n",
    "\n",
    "(c) Logistic regression, because it models the probability of an instance belonging to a particular class.\n",
    "\n",
    "(d) Logistic regression, because it allows for complex non-linear interactions between features and thus will be more accurate.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(c)** - logistic regression is most suitable because it models the probability of an instance belonging to a particular class. Here's the detailed explanation:\n",
    "\n",
    "**Why Logistic Regression is Ideal for Binary Classification:**\n",
    "\n",
    "1. **Probabilistic Output:**\n",
    "   Logistic regression outputs a probability between 0 and 1:\n",
    "   $$P(\\text{spam} \\mid x) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n",
    "   \n",
    "   This directly represents the probability that an email is spam.\n",
    "\n",
    "2. **Mathematical Properties:**\n",
    "   - **Range**: Output is always in $[0, 1]$, perfect for probabilities\n",
    "   - **Monotonic**: As $w^T x + b$ increases, probability increases\n",
    "   - **Interpretable**: Log-odds are linear in the features\n",
    "\n",
    "3. **Decision Rule:**\n",
    "   - If $P(\\text{spam} \\mid x) > 0.5$, classify as spam\n",
    "   - If $P(\\text{spam} \\mid x) \\leq 0.5$, classify as not spam\n",
    "\n",
    "**Why Linear Regression is Inappropriate:**\n",
    "\n",
    "1. **Unbounded Output:**\n",
    "   Linear regression can output any real number: $\\hat{y} = w^T x + b \\in (-\\infty, \\infty)$\n",
    "   \n",
    "   This doesn't make sense for probabilities, which must be in $[0, 1]$.\n",
    "\n",
    "2. **No Probability Interpretation:**\n",
    "   - Output of 2.5 doesn't mean 250% probability\n",
    "   - Output of -1 doesn't mean -100% probability\n",
    "\n",
    "3. **Poor Calibration:**\n",
    "   Linear regression doesn't produce well-calibrated probability estimates\n",
    "\n",
    "**Mathematical Comparison:**\n",
    "\n",
    "| Aspect | Linear Regression | Logistic Regression |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Output Range** | $(-\\infty, \\infty)$ | $(0, 1)$ |\n",
    "| **Interpretation** | Arbitrary real value | Probability |\n",
    "| **Loss Function** | Mean squared error | Log-likelihood |\n",
    "| **Decision Boundary** | Linear | Linear (in log-odds) |\n",
    "| **Calibration** | Poor | Good |\n",
    "\n",
    "**Example:**\n",
    "For an email with features $x$:\n",
    "- **Linear regression**: $\\hat{y} = 2.3$ (meaningless for classification)\n",
    "- **Logistic regression**: $P(\\text{spam}) = 0.91$ (91% chance of being spam)\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "- **(a) Linear regression for multiple features**: Linear regression can handle multiple features, but that's not the main issue\n",
    "- **(b) Linear regression for comparative magnitude**: This is incorrect - logistic regression can provide probability estimates\n",
    "- **(d) Logistic regression for non-linear interactions**: While logistic regression can be extended with kernels, this isn't the primary reason for choosing it\n",
    "\n",
    "**Practical Considerations:**\n",
    "- **Threshold tuning**: Can adjust decision threshold based on cost of false positives vs false negatives\n",
    "- **Feature engineering**: Can add polynomial features for non-linear relationships\n",
    "- **Regularization**: Can add L1/L2 penalties to prevent overfitting\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "Which of the following matrices represents some kernel function $K: X \\times X \\to \\mathbb{R}$ evaluated on two points?\n",
    "\n",
    "(a) $\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$\n",
    "\n",
    "(b) $\\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix}$\n",
    "\n",
    "(c) $\\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "(d) $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(d)** - $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$ is the only matrix that represents a valid kernel function. Here's the detailed explanation:\n",
    "\n",
    "**Kernel Matrix Properties:**\n",
    "A matrix $K$ represents a valid kernel function if and only if it is **positive semi-definite (PSD)**.\n",
    "\n",
    "**Definition of PSD Matrix:**\n",
    "A matrix $K$ is PSD if for any vector $x \\neq 0$:\n",
    "$$x^T K x \\geq 0$$\n",
    "\n",
    "**Testing Each Matrix:**\n",
    "\n",
    "1. **Matrix (a)**: $\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$\n",
    "   - Let $x = [1, 1]^T$\n",
    "   - $x^T K x = [1, 1] \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1 - 1 = 0$\n",
    "   - Let $x = [1, 2]^T$\n",
    "   - $x^T K x = [1, 2] \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = 1 - 4 = -3 < 0$\n",
    "   - **Not PSD** ❌\n",
    "\n",
    "2. **Matrix (b)**: $\\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix}$\n",
    "   - Let $x = [1, -1]^T$\n",
    "   - $x^T K x = [1, -1] \\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 1 - 3 - 3 + 1 = -4 < 0$\n",
    "   - **Not PSD** ❌\n",
    "\n",
    "3. **Matrix (c)**: $\\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "   - Let $x = [1, 1]^T$\n",
    "   - $x^T K x = [1, 1] \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1 - 1 + 1 + 1 = 2 > 0$\n",
    "   - Let $x = [1, -1]^T$\n",
    "   - $x^T K x = [1, -1] \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 1 + 1 + 1 - 1 = 2 > 0$\n",
    "   - Let $x = [1, 0]^T$\n",
    "   - $x^T K x = [1, 0] \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 1 + 0 = 1 > 0$\n",
    "   - Let $x = [0, 1]^T$\n",
    "   - $x^T K x = [0, 1] \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0 + 1 = 1 > 0$\n",
    "   - **PSD** ✅\n",
    "\n",
    "4. **Matrix (d)**: $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$\n",
    "   - Let $x = [1, 1]^T$\n",
    "   - $x^T K x = [1, 1] \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 2 - 1 - 1 + 2 = 2 > 0$\n",
    "   - Let $x = [1, -1]^T$\n",
    "   - $x^T K x = [1, -1] \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 2 + 1 + 1 + 2 = 6 > 0$\n",
    "   - **PSD** ✅\n",
    "\n",
    "**Wait! Both (c) and (d) appear to be PSD. Let me check more carefully...**\n",
    "\n",
    "Actually, let me use the eigenvalue test for PSD matrices:\n",
    "\n",
    "**Eigenvalue Test for PSD:**\n",
    "A matrix is PSD if and only if all eigenvalues are non-negative.\n",
    "\n",
    "1. **Matrix (c)**: $\\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "   - Characteristic equation: $\\det(K - \\lambda I) = \\det\\begin{bmatrix} 1-\\lambda & -1 \\\\ 1 & 1-\\lambda \\end{bmatrix} = (1-\\lambda)^2 + 1 = \\lambda^2 - 2\\lambda + 2 = 0$\n",
    "   - Eigenvalues: $\\lambda = 1 \\pm i$ (complex eigenvalues!)\n",
    "   - **Not PSD** (PSD matrices must have real, non-negative eigenvalues)\n",
    "\n",
    "2. **Matrix (d)**: $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$\n",
    "   - Characteristic equation: $\\det(K - \\lambda I) = \\det\\begin{bmatrix} 2-\\lambda & -1 \\\\ -1 & 2-\\lambda \\end{bmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0$\n",
    "   - Eigenvalues: $\\lambda = 1, 3$ (both positive)\n",
    "   - **PSD** ✅\n",
    "\n",
    "**Conclusion:**\n",
    "Only matrix (d) is positive semi-definite and therefore represents a valid kernel function.\n",
    "\n",
    "**Why PSD is Required:**\n",
    "- Kernel matrices represent inner products in a feature space\n",
    "- Inner products must satisfy the Cauchy-Schwarz inequality\n",
    "- This leads to the PSD property: $x^T K x = \\langle \\phi(x), \\phi(x) \\rangle \\geq 0$\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "Consider kernel ridge regression\n",
    "$$\\hat{w} = \\operatorname{argmin}_w \\frac{1}{n} \\sum_{i=1}^n (y_i - w^T \\phi(x_i))^2 + \\lambda ||w||^2$$\n",
    "where $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^q$ denotes the feature mapping and $d \\neq q$, and $K_{i,j} := \\langle \\phi(x_i), \\phi(x_j) \\rangle$ denotes the entry $(i,j)$ in the kernel matrix $K$. Which of the following statements are true? Select all that apply.\n",
    "\n",
    "(a) The optimal $\\hat{w}$ is always a linear combination of $x_i$'s for $i = 1, 2, ..., n$.\n",
    "\n",
    "(b) The optimal $\\hat{\\alpha}$ is $\\hat{\\alpha} = (KK^T + \\lambda I)^{-1}Y$.\n",
    "\n",
    "(c) The kernel method will still work even if the feature mapping is not one-to-one.\n",
    "\n",
    "(d) If $K$ is positive semi-definite, then we can find a solution even when $\\lambda = 0$.\n",
    "\n",
    "**Correct answers:** (c), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answers are **(c)** and **(d)**. Here's the detailed analysis of each statement:\n",
    "\n",
    "**Kernel Ridge Regression Setup:**\n",
    "$$\\hat{w} = \\operatorname{argmin}_w \\frac{1}{n} \\sum_{i=1}^n (y_i - w^T \\phi(x_i))^2 + \\lambda ||w||^2$$\n",
    "\n",
    "where $K_{i,j} = \\langle \\phi(x_i), \\phi(x_j) \\rangle$ is the kernel matrix.\n",
    "\n",
    "**Analyzing Each Statement:**\n",
    "\n",
    "**(a) The optimal $\\hat{w}$ is always a linear combination of $x_i$'s for $i = 1, 2, ..., n$.**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Explanation:**\n",
    "- The optimal $\\hat{w}$ is a linear combination of **$\\phi(x_i)$'s**, not $x_i$'s\n",
    "- This is the **Representer Theorem**: $\\hat{w} = \\sum_{i=1}^n \\alpha_i \\phi(x_i)$\n",
    "- The feature mapping $\\phi$ transforms $x_i$ to a higher-dimensional space\n",
    "- Since $d \\neq q$, the original $x_i$'s and $\\phi(x_i)$'s are in different spaces\n",
    "\n",
    "**(b) The optimal $\\hat{\\alpha}$ is $\\hat{\\alpha} = (KK^T + \\lambda I)^{-1}Y$.**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Explanation:**\n",
    "- The correct formula is: $\\hat{\\alpha} = (K + \\lambda I)^{-1}Y$\n",
    "- The kernel matrix $K$ is already $n \\times n$, so we don't need $KK^T$\n",
    "- $KK^T$ would be $n \\times n \\times n \\times n$, which doesn't make sense dimensionally\n",
    "\n",
    "**Derivation of Correct Formula:**\n",
    "1. Using the Representer Theorem: $\\hat{w} = \\sum_{i=1}^n \\alpha_i \\phi(x_i)$\n",
    "2. The objective becomes: $\\min_{\\alpha} \\frac{1}{n} ||Y - K\\alpha||^2 + \\lambda \\alpha^T K \\alpha$\n",
    "3. Setting derivative to zero: $-2K^T(Y - K\\alpha) + 2\\lambda K\\alpha = 0$\n",
    "4. Since $K$ is symmetric: $K(Y - K\\alpha) = \\lambda K\\alpha$\n",
    "5. Assuming $K$ is invertible: $Y - K\\alpha = \\lambda\\alpha$\n",
    "6. Therefore: $\\alpha = (K + \\lambda I)^{-1}Y$\n",
    "\n",
    "**(c) The kernel method will still work even if the feature mapping is not one-to-one.**\n",
    "\n",
    "**TRUE** ✅\n",
    "\n",
    "**Explanation:**\n",
    "- Kernel methods only require the kernel function $K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle$\n",
    "- The feature mapping $\\phi$ doesn't need to be injective (one-to-one)\n",
    "- Multiple different inputs could map to the same feature representation\n",
    "- The kernel trick allows us to work with $K$ directly without computing $\\phi$\n",
    "\n",
    "**Example:**\n",
    "- Consider $\\phi(x) = [x, x^2]$ for $x \\in \\mathbb{R}$\n",
    "- This is not one-to-one (e.g., $\\phi(2) = \\phi(-2) = [2, 4]$)\n",
    "- But the kernel $K(x_i, x_j) = x_i x_j + x_i^2 x_j^2$ is still valid\n",
    "\n",
    "**(d) If $K$ is positive semi-definite, then we can find a solution even when $\\lambda = 0$.**\n",
    "\n",
    "**TRUE** ✅\n",
    "\n",
    "**Explanation:**\n",
    "- When $\\lambda = 0$, the problem becomes: $\\min_w \\frac{1}{n} \\sum_{i=1}^n (y_i - w^T \\phi(x_i))^2$\n",
    "- This is equivalent to: $\\min_w ||Y - X_{\\phi} w||^2$ where $X_{\\phi} = [\\phi(x_1), \\ldots, \\phi(x_n)]^T$\n",
    "- The solution is: $\\hat{w} = (X_{\\phi}^T X_{\\phi})^+ X_{\\phi}^T Y$ where $^+$ denotes the pseudo-inverse\n",
    "- Since $K = X_{\\phi} X_{\\phi}^T$ is PSD, the pseudo-inverse exists\n",
    "- Therefore, a solution always exists, even if it's not unique\n",
    "\n",
    "**Key Insights:**\n",
    "- Kernel methods are robust to the choice of feature mapping\n",
    "- The kernel matrix $K$ contains all necessary information\n",
    "- PSD property ensures mathematical well-posedness\n",
    "- Regularization ($\\lambda > 0$) helps with numerical stability and generalization\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "The bootstrap method cannot be used to estimate the distribution of which of the following statistics?\n",
    "\n",
    "(a) Mean\n",
    "\n",
    "(b) Median\n",
    "\n",
    "(c) Variance\n",
    "\n",
    "(d) The bootstrap method can be applied to all of the above statistics.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(d)** - the bootstrap method can be applied to all of the above statistics (mean, median, and variance). Here's the detailed explanation:\n",
    "\n",
    "**Understanding the Bootstrap Method:**\n",
    "\n",
    "**Bootstrap Definition:**\n",
    "The bootstrap is a resampling technique that involves drawing samples with replacement from the original dataset to estimate the sampling distribution of any statistic.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "- **Original dataset**: $X = \\{x_1, x_2, \\ldots, x_n\\}$\n",
    "- **Bootstrap sample**: $X^* = \\{x_1^*, x_2^*, \\ldots, x_n^*\\}$ (drawn with replacement)\n",
    "- **Bootstrap statistic**: $\\hat{\\theta}^* = f(X^*)$ where $f$ is any statistic\n",
    "\n",
    "**Why Bootstrap Works for Any Statistic:**\n",
    "\n",
    "**1. General Applicability:**\n",
    "The bootstrap is a **general method** that can estimate the sampling distribution of any statistic, not just specific ones. The key requirement is that the statistic can be computed from the data.\n",
    "\n",
    "**2. Mathematical Justification:**\n",
    "For any statistic $\\theta = f(X)$, the bootstrap estimates:\n",
    "$$\\text{SE}(\\hat{\\theta}) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B (\\hat{\\theta}_b^* - \\bar{\\theta}^*)^2}$$\n",
    "\n",
    "where $B$ is the number of bootstrap samples.\n",
    "\n",
    "**Application to Each Statistic:**\n",
    "\n",
    "**Mean:**\n",
    "- **Statistic**: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$\n",
    "- **Bootstrap estimate**: $\\bar{x}^* = \\frac{1}{n}\\sum_{i=1}^n x_i^*$\n",
    "- **Result**: Bootstrap distribution of sample means\n",
    "\n",
    "**Median:**\n",
    "- **Statistic**: $\\text{median}(X) = x_{(\\frac{n+1}{2})}$ (for odd $n$)\n",
    "- **Bootstrap estimate**: $\\text{median}(X^*)$\n",
    "- **Result**: Bootstrap distribution of sample medians\n",
    "\n",
    "**Variance:**\n",
    "- **Statistic**: $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$\n",
    "- **Bootstrap estimate**: $s^{*2} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i^* - \\bar{x}^*)^2$\n",
    "- **Result**: Bootstrap distribution of sample variances\n",
    "\n",
    "**Mathematical Example:**\n",
    "\n",
    "**Original Data**: $X = [1, 2, 3, 4, 5]$\n",
    "\n",
    "**Bootstrap Sample 1**: $X_1^* = [1, 1, 3, 5, 5]$\n",
    "- Mean: $\\bar{x}_1^* = 3$\n",
    "- Median: $\\text{median}_1^* = 3$\n",
    "- Variance: $s_1^{*2} = 3.2$\n",
    "\n",
    "**Bootstrap Sample 2**: $X_2^* = [2, 2, 4, 4, 4]$\n",
    "- Mean: $\\bar{x}_2^* = 3.2$\n",
    "- Median: $\\text{median}_2^* = 4$\n",
    "- Variance: $s_2^{*2} = 1.2$\n",
    "\n",
    "**Bootstrap Distribution**: $\\{\\bar{x}_1^*, \\bar{x}_2^*, \\ldots\\}$, $\\{\\text{median}_1^*, \\text{median}_2^*, \\ldots\\}$, $\\{s_1^{*2}, s_2^{*2}, \\ldots\\}$\n",
    "\n",
    "**Why Bootstrap is Universal:**\n",
    "\n",
    "**1. No Distributional Assumptions:**\n",
    "- Works with any underlying data distribution\n",
    "- Doesn't require normal distributions\n",
    "- Non-parametric approach\n",
    "\n",
    "**2. Flexibility:**\n",
    "- Can estimate any statistic: mean, median, variance, correlation, etc.\n",
    "- Works with any sample size\n",
    "- Applicable to any type of data\n",
    "\n",
    "**3. Robustness:**\n",
    "- Handles outliers well\n",
    "- Works with complex statistics\n",
    "- Provides confidence intervals for any statistic\n",
    "\n",
    "**Practical Applications:**\n",
    "\n",
    "**Mean Estimation:**\n",
    "- **Confidence intervals**: $[\\bar{x}_{0.025}, \\bar{x}_{0.975}]$\n",
    "- **Standard error**: $\\text{SE}(\\bar{x})$\n",
    "- **Bias estimation**: $\\text{Bias} = \\bar{x}^* - \\bar{x}$\n",
    "\n",
    "**Median Estimation:**\n",
    "- **Confidence intervals**: $[\\text{median}_{0.025}, \\text{median}_{0.975}]$\n",
    "- **Robust to outliers**: Median is less sensitive than mean\n",
    "- **Non-parametric**: No assumptions about data distribution\n",
    "\n",
    "**Variance Estimation:**\n",
    "- **Confidence intervals**: $[s^2_{0.025}, s^2_{0.975}]$\n",
    "- **Precision estimation**: How reliable is our variance estimate?\n",
    "- **Model validation**: Estimate uncertainty in model parameters\n",
    "\n",
    "**Computational Considerations:**\n",
    "\n",
    "**1. Bootstrap Sample Size:**\n",
    "- Typically same size as original dataset\n",
    "- With replacement allows for repeated observations\n",
    "- Some observations may not appear in a bootstrap sample\n",
    "\n",
    "**2. Number of Bootstrap Samples:**\n",
    "- Usually 1000-10000 bootstrap samples\n",
    "- More samples = more accurate estimates\n",
    "- Trade-off between accuracy and computational cost\n",
    "\n",
    "**3. Memory Requirements:**\n",
    "- Need to store all bootstrap samples\n",
    "- Can be memory-intensive for large datasets\n",
    "- May need to use approximation methods\n",
    "\n",
    "**Conclusion:**\n",
    "The bootstrap method is a **universal technique** that can be applied to estimate the sampling distribution of **any statistic**, including mean, median, variance, and many others. Its generality makes it one of the most powerful tools in statistical inference.\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "True/False: Bootstrapping is a resampling technique that involves generating multiple datasets of size $d$ by randomly sampling observations without replacement from the original dataset of size $n$ (where $d \\ll n$). True/False: Bootstrapping can be computationally prohibitive for large datasets.\n",
    "\n",
    "(a) True, False\n",
    "\n",
    "(b) True, True\n",
    "\n",
    "(c) False, True\n",
    "\n",
    "(d) False, False\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(c) - False, True**. Here's the detailed explanation:\n",
    "\n",
    "**Analyzing Each Statement:**\n",
    "\n",
    "**Statement 1: \"Bootstrapping is a resampling technique that involves generating multiple datasets of size $d$ by randomly sampling observations without replacement from the original dataset of size $n$ (where $d \\ll n$).\"**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Why This is Incorrect:**\n",
    "\n",
    "1. **Bootstrap Sampling is WITH Replacement:**\n",
    "   - **Correct definition**: Bootstrap samples are drawn **with replacement** from the original dataset\n",
    "   - **Incorrect statement**: Claims sampling is **without replacement**\n",
    "   - **Key difference**: With replacement allows the same observation to appear multiple times in a bootstrap sample\n",
    "\n",
    "2. **Bootstrap Sample Size:**\n",
    "   - **Correct practice**: Bootstrap samples are typically the **same size** as the original dataset ($d = n$)\n",
    "   - **Incorrect statement**: Claims $d \\ll n$ (much smaller than original)\n",
    "   - **Reasoning**: We want bootstrap samples to represent the original data distribution\n",
    "\n",
    "**Mathematical Example:**\n",
    "\n",
    "**Original Dataset**: $X = [1, 2, 3, 4, 5]$ (size $n = 5$)\n",
    "\n",
    "**Correct Bootstrap Sample**: $X^* = [1, 1, 3, 5, 5]$ (size $d = 5$, with replacement)\n",
    "- Same size as original\n",
    "- Observations can repeat\n",
    "- Represents original distribution\n",
    "\n",
    "**Incorrect Sampling (as described)**: $X^* = [1, 3]$ (size $d = 2 \\ll 5$, without replacement)\n",
    "- Much smaller than original\n",
    "- No repeated observations\n",
    "- Doesn't represent original distribution\n",
    "\n",
    "**Statement 2: \"Bootstrapping can be computationally prohibitive for large datasets.\"**\n",
    "\n",
    "**TRUE** ✅\n",
    "\n",
    "**Why This is Correct:**\n",
    "\n",
    "1. **Computational Complexity:**\n",
    "   - **Bootstrap samples**: Typically 1000-10000 bootstrap samples\n",
    "   - **Total computations**: $B \\times n$ where $B$ is number of bootstrap samples\n",
    "   - **Large datasets**: Can become computationally expensive\n",
    "\n",
    "2. **Memory Requirements:**\n",
    "   - **Storage**: Need to store $B$ bootstrap samples\n",
    "   - **Memory usage**: $O(B \\times n)$ memory required\n",
    "   - **Large datasets**: Can exceed available memory\n",
    "\n",
    "3. **Time Complexity:**\n",
    "   - **Sample generation**: $O(B \\times n)$ time to generate bootstrap samples\n",
    "   - **Statistic computation**: $O(B \\times f(n))$ where $f(n)$ is complexity of computing the statistic\n",
    "   - **Total time**: Can be prohibitive for large $n$ and large $B$\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Computational Cost:**\n",
    "$$\\text{Total Cost} = B \\times (\\text{Sample Generation} + \\text{Statistic Computation})$$\n",
    "\n",
    "For large datasets:\n",
    "- **Sample Generation**: $O(B \\times n)$\n",
    "- **Statistic Computation**: $O(B \\times n)$ (for simple statistics like mean)\n",
    "- **Total**: $O(B \\times n)$\n",
    "\n",
    "**Example with Large Dataset:**\n",
    "- **Dataset size**: $n = 1,000,000$\n",
    "- **Bootstrap samples**: $B = 10,000$\n",
    "- **Total operations**: $10,000 \\times 1,000,000 = 10^{10}$ operations\n",
    "- **Time estimate**: Several hours to days depending on hardware\n",
    "\n",
    "**Solutions for Large Datasets:**\n",
    "\n",
    "1. **Reduce Bootstrap Samples:**\n",
    "   - Use fewer bootstrap samples (e.g., 100-1000 instead of 10000)\n",
    "   - Trade-off between accuracy and computational cost\n",
    "\n",
    "2. **Approximation Methods:**\n",
    "   - **Bag of Little Bootstraps (BLB)**: Divide data into subsets\n",
    "   - **Subsampling**: Use smaller bootstrap samples\n",
    "   - **Parallel Processing**: Distribute computation across multiple cores\n",
    "\n",
    "3. **Online Bootstrap:**\n",
    "   - Generate bootstrap samples on-the-fly\n",
    "   - Reduce memory requirements\n",
    "   - Trade-off between memory and computation time\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "**When Bootstrap Becomes Prohibitive:**\n",
    "- **Dataset size**: $n > 100,000$\n",
    "- **Bootstrap samples**: $B > 1000$\n",
    "- **Statistic complexity**: Complex statistics that require $O(n^2)$ or higher time\n",
    "- **Memory constraints**: Limited RAM for storing bootstrap samples\n",
    "\n",
    "**When Bootstrap is Feasible:**\n",
    "- **Small to medium datasets**: $n < 10,000$\n",
    "- **Simple statistics**: Mean, median, variance\n",
    "- **Adequate computational resources**: Sufficient RAM and processing power\n",
    "\n",
    "**Conclusion:**\n",
    "- **Statement 1**: FALSE - Bootstrap uses sampling with replacement and typically same sample size\n",
    "- **Statement 2**: TRUE - Bootstrap can be computationally prohibitive for large datasets due to the need for many resamples and the associated computational and memory costs\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "Which of the following statements best describes the differences between Random Forests and Boosting in the context of decision tree-based ensemble methods?\n",
    "\n",
    "(a) Random Forests and Boosting both reduce variance by averaging multiple deep decision trees, with no significant differences in their approach.\n",
    "\n",
    "(b) In Random Forests, trees are built independently using bagging, while Boosting builds trees sequentially, with each tree learning from the errors of the previous ones.\n",
    "\n",
    "(c) Boosting reduces bias by building shallow trees, whereas Random Forests use deep trees to address variance and do not focus on reducing bias.\n",
    "\n",
    "(d) Both Random Forests and Boosting are identical in their handling of bias and variance, differing only in computational efficiency.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(b)** - In Random Forests, trees are built independently using bagging, while Boosting builds trees sequentially, with each tree learning from the errors of the previous ones. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Ensemble Methods:**\n",
    "\n",
    "**Ensemble Learning:**\n",
    "Ensemble methods combine multiple base learners to improve overall performance. The key insight is that combining several weak learners can create a strong learner.\n",
    "\n",
    "**Random Forests vs Boosting:**\n",
    "\n",
    "**Random Forests (Bagging Approach):**\n",
    "\n",
    "1. **Independent Tree Construction:**\n",
    "   - **Parallel training**: All trees are built independently\n",
    "   - **No communication**: Each tree doesn't know about other trees\n",
    "   - **Independent samples**: Each tree sees different bootstrap samples\n",
    "\n",
    "2. **Bagging (Bootstrap Aggregating):**\n",
    "   - **Bootstrap sampling**: Each tree is trained on a random subset of data\n",
    "   - **With replacement**: Same observation can appear multiple times\n",
    "   - **Out-of-bag samples**: Some observations are left out for validation\n",
    "\n",
    "3. **Mathematical Formulation:**\n",
    "   For $B$ trees, the prediction is:\n",
    "   $$\\hat{y} = \\frac{1}{B}\\sum_{b=1}^B f_b(x)$$\n",
    "   \n",
    "   where $f_b(x)$ is the prediction of tree $b$.\n",
    "\n",
    "4. **Training Process:**\n",
    "   ```\n",
    "   For b = 1 to B:\n",
    "       1. Sample bootstrap dataset D_b from original data\n",
    "       2. Train decision tree f_b on D_b\n",
    "       3. Trees are independent - no information sharing\n",
    "   ```\n",
    "\n",
    "**Boosting (Sequential Approach):**\n",
    "\n",
    "1. **Sequential Tree Construction:**\n",
    "   - **Sequential training**: Trees are built one after another\n",
    "   - **Error correction**: Each tree focuses on errors of previous trees\n",
    "   - **Weighted samples**: Misclassified samples get higher weights\n",
    "\n",
    "2. **Error Learning:**\n",
    "   - **Residual learning**: Each tree learns the residuals from previous trees\n",
    "   - **Weighted loss**: Misclassified samples are weighted more heavily\n",
    "   - **Adaptive focus**: Model adapts to difficult samples\n",
    "\n",
    "3. **Mathematical Formulation:**\n",
    "   For $B$ trees, the prediction is:\n",
    "   $$\\hat{y} = \\sum_{b=1}^B \\alpha_b f_b(x)$$\n",
    "   \n",
    "   where $\\alpha_b$ is the weight of tree $b$.\n",
    "\n",
    "4. **Training Process:**\n",
    "   ```\n",
    "   Initialize: Equal weights for all samples\n",
    "   For b = 1 to B:\n",
    "       1. Train tree f_b on weighted dataset\n",
    "       2. Calculate error and tree weight α_b\n",
    "       3. Update sample weights (increase for misclassified)\n",
    "       4. Next tree focuses on previously misclassified samples\n",
    "   ```\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Random Forests | Boosting |\n",
    "|--------|----------------|----------|\n",
    "| **Training Order** | Parallel (independent) | Sequential (dependent) |\n",
    "| **Sample Selection** | Bootstrap sampling | Weighted sampling |\n",
    "| **Error Handling** | Reduces variance | Reduces bias |\n",
    "| **Overfitting** | Less prone | More prone |\n",
    "| **Computational** | Parallelizable | Must be sequential |\n",
    "| **Robustness** | More robust to noise | Sensitive to noise |\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Random Forests Variance Reduction:**\n",
    "$$\\text{Var}(\\hat{y}_{\\text{RF}}) = \\frac{\\text{Var}(f_1)}{B} + \\left(1 - \\frac{1}{B}\\right)\\text{Cov}(f_1, f_2)$$\n",
    "\n",
    "- **First term**: Variance reduction through averaging\n",
    "- **Second term**: Correlation between trees (reduced by feature randomization)\n",
    "\n",
    "**Boosting Bias Reduction:**\n",
    "$$\\text{Bias}(\\hat{y}_{\\text{Boost}}) = \\text{Bias}(f_1) - \\sum_{b=2}^B \\alpha_b \\text{Improvement}_b$$\n",
    "\n",
    "- **Sequential improvement**: Each tree reduces bias\n",
    "- **Weighted combination**: More weight to better trees\n",
    "\n",
    "**Why Other Options Are Wrong:**\n",
    "\n",
    "**Option (a) - \"Both reduce variance by averaging multiple deep decision trees\":**\n",
    "- **Problem**: Boosting doesn't use averaging, it uses weighted combination\n",
    "- **Issue**: Boosting focuses on bias reduction, not variance reduction\n",
    "- **Result**: Incorrect characterization of both methods\n",
    "\n",
    "**Option (c) - \"Boosting reduces bias by building shallow trees, whereas Random Forests use deep trees\":**\n",
    "- **Problem**: Both methods can use trees of varying depths\n",
    "- **Issue**: Tree depth is not the fundamental difference\n",
    "- **Result**: Focuses on wrong aspect of the methods\n",
    "\n",
    "**Option (d) - \"Both are identical in their handling of bias and variance\":**\n",
    "- **Problem**: They handle bias and variance very differently\n",
    "- **Issue**: Random Forests reduce variance, Boosting reduces bias\n",
    "- **Result**: Completely incorrect understanding\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**When to Use Random Forests:**\n",
    "- **High-dimensional data**: Feature randomization helps\n",
    "- **Noisy data**: Robust to outliers and noise\n",
    "- **Parallel computing**: Can train trees in parallel\n",
    "- **Quick prototyping**: Easy to implement and tune\n",
    "\n",
    "**When to Use Boosting:**\n",
    "- **Low-dimensional data**: Can focus on complex patterns\n",
    "- **Clean data**: Sensitive to noise and outliers\n",
    "- **High accuracy needed**: Often achieves better performance\n",
    "- **Sequential training acceptable**: Can wait for sequential training\n",
    "\n",
    "**Performance Characteristics:**\n",
    "\n",
    "**Random Forests:**\n",
    "- **Bias**: Moderate (depends on tree depth)\n",
    "- **Variance**: Low (reduced through averaging)\n",
    "- **Overfitting**: Less prone\n",
    "- **Interpretability**: Can extract feature importance\n",
    "\n",
    "**Boosting:**\n",
    "- **Bias**: Low (reduced through sequential learning)\n",
    "- **Variance**: Higher (can overfit with too many trees)\n",
    "- **Overfitting**: More prone (need careful tuning)\n",
    "- **Interpretability**: More complex to interpret\n",
    "\n",
    "**Conclusion:**\n",
    "The fundamental difference between Random Forests and Boosting is their **training approach**: Random Forests use **independent, parallel training** with bagging, while Boosting uses **sequential, dependent training** with error correction. This leads to different bias-variance trade-offs and different practical applications.\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "Which of the following statements is true about a single Decision Tree and Random Forest?\n",
    "\n",
    "(a) Random Forest has lower training error because it aggregates multiple trees\n",
    "\n",
    "(b) A good Random Forest is composed of decision trees that are highly correlated\n",
    "\n",
    "(c) Random Forest is useful because it's easy to explain how a decision is made\n",
    "\n",
    "(d) A single Decision Tree can result in comparably low training error in classification task compared to Random Forest\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The correct answer is **(d)** - A single Decision Tree can result in comparably low training error in classification task compared to Random Forest. Here's the detailed explanation:\n",
    "\n",
    "**Understanding Decision Trees vs Random Forests:**\n",
    "\n",
    "**Single Decision Tree:**\n",
    "A decision tree can achieve very low (even zero) training error by growing deep enough to classify each training point correctly.\n",
    "\n",
    "**Random Forest:**\n",
    "An ensemble of decision trees that averages predictions from multiple trees to improve generalization.\n",
    "\n",
    "**Analysis of Each Statement:**\n",
    "\n",
    "**Option (a) - \"Random Forest has lower training error because it aggregates multiple trees\":**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Why This is Incorrect:**\n",
    "- **Single tree capacity**: A single decision tree can achieve **zero training error** by growing deep enough\n",
    "- **Ensemble averaging**: Random Forest averages predictions, which typically **increases training error**\n",
    "- **Mathematical reasoning**: For classification, averaging multiple predictions can only maintain or increase error compared to the best individual tree\n",
    "\n",
    "**Mathematical Example:**\n",
    "Consider binary classification with 3 trees:\n",
    "- **Tree 1**: Predicts class 1 (correct)\n",
    "- **Tree 2**: Predicts class 0 (incorrect)  \n",
    "- **Tree 3**: Predicts class 1 (correct)\n",
    "- **Random Forest**: Average = 0.67 → predicts class 1 (correct)\n",
    "- **Best single tree**: Tree 1 or Tree 3 (both correct)\n",
    "\n",
    "**Option (b) - \"A good Random Forest is composed of decision trees that are highly correlated\":**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Why This is Incorrect:**\n",
    "- **Diversity principle**: Random Forests work best when trees are **uncorrelated**\n",
    "- **Variance reduction**: Uncorrelated trees reduce ensemble variance\n",
    "- **Feature randomization**: Purpose is to create diverse, uncorrelated trees\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "$$\\text{Var}(\\hat{y}_{\\text{RF}}) = \\frac{\\text{Var}(f_1)}{B} + \\left(1 - \\frac{1}{B}\\right)\\text{Cov}(f_1, f_2)$$\n",
    "\n",
    "- **Lower correlation** → **Lower covariance** → **Lower variance**\n",
    "- **Higher correlation** → **Higher covariance** → **Higher variance**\n",
    "\n",
    "**Option (c) - \"Random Forest is useful because it's easy to explain how a decision is made\":**\n",
    "\n",
    "**FALSE** ❌\n",
    "\n",
    "**Why This is Incorrect:**\n",
    "- **Interpretability**: Single decision trees are much more interpretable\n",
    "- **Random Forest complexity**: Multiple trees make interpretation difficult\n",
    "- **Black box nature**: Ensemble methods are generally less interpretable\n",
    "\n",
    "**Single Decision Tree Interpretability:**\n",
    "- **Clear decision path**: Can trace exact path from root to leaf\n",
    "- **Feature importance**: Direct interpretation of splits\n",
    "- **Visual representation**: Easy to visualize and understand\n",
    "\n",
    "**Random Forest Interpretability:**\n",
    "- **Multiple paths**: Need to consider all trees\n",
    "- **Aggregated decisions**: Final prediction is combination of many trees\n",
    "- **Complex visualization**: Hard to represent multiple trees clearly\n",
    "\n",
    "**Option (d) - \"A single Decision Tree can result in comparably low training error in classification task compared to Random Forest\":**\n",
    "\n",
    "**TRUE** ✅\n",
    "\n",
    "**Why This is Correct:**\n",
    "\n",
    "1. **Single Tree Capacity:**\n",
    "   - **Perfect memorization**: Can achieve zero training error\n",
    "   - **Overfitting ability**: Can grow deep enough to classify each point correctly\n",
    "   - **No averaging**: Doesn't suffer from ensemble averaging effects\n",
    "\n",
    "2. **Mathematical Verification:**\n",
    "   For a dataset with $n$ points, a decision tree can have up to $n$ leaves, each containing exactly one training point.\n",
    "\n",
    "3. **Training Error Comparison:**\n",
    "   - **Single tree**: Can achieve 0% training error\n",
    "   - **Random Forest**: Typically has higher training error due to averaging\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "**Dataset**: 100 training points, binary classification\n",
    "\n",
    "**Single Decision Tree:**\n",
    "- **Training error**: 0% (perfect fit)\n",
    "- **Test error**: 15% (overfitting)\n",
    "\n",
    "**Random Forest (100 trees):**\n",
    "- **Training error**: 2% (slight averaging effect)\n",
    "- **Test error**: 8% (better generalization)\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "**Training Error vs Test Error:**\n",
    "- **Single tree**: Low training error, high test error (overfitting)\n",
    "- **Random Forest**: Higher training error, lower test error (better generalization)\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- **Single tree**: Low bias, high variance\n",
    "- **Random Forest**: Moderate bias, low variance\n",
    "\n",
    "**When Single Tree Achieves Low Training Error:**\n",
    "- **Small datasets**: Easy to memorize all points\n",
    "- **Deep trees**: Can grow to fit training data perfectly\n",
    "- **No regularization**: No constraints on tree depth\n",
    "- **Clean data**: No noise to confuse the tree\n",
    "\n",
    "**Why Random Forest Has Higher Training Error:**\n",
    "- **Averaging effect**: Multiple predictions are averaged\n",
    "- **Bootstrap sampling**: Each tree sees different data\n",
    "- **Feature randomization**: Trees may miss important features\n",
    "- **Regularization**: Implicit regularization through ensemble\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "**When to Use Single Decision Tree:**\n",
    "- **Interpretability is crucial**: Need to understand decision process\n",
    "- **Small datasets**: Can achieve good performance without overfitting\n",
    "- **Quick prototyping**: Fast to train and evaluate\n",
    "- **Feature importance analysis**: Direct interpretation of splits\n",
    "\n",
    "**When to Use Random Forest:**\n",
    "- **Large datasets**: Better generalization performance\n",
    "- **Noisy data**: More robust to outliers and noise\n",
    "- **High-dimensional data**: Feature randomization helps\n",
    "- **Production systems**: Better reliability and stability\n",
    "\n",
    "**Conclusion:**\n",
    "A single decision tree can indeed achieve comparably low (or even lower) training error compared to Random Forest. The advantage of Random Forest lies not in lower training error, but in better generalization performance and robustness. The single tree's ability to achieve low training error comes at the cost of overfitting and poor generalization.\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "How is the performance of a distance-based machine learning model typically impacted when the data dimensionality is very high?\n",
    "\n",
    "(a) The performance significantly improves because there are more distinguishing features.\n",
    "\n",
    "(b) The performance decreases because the data points tend to appear equidistant in high-dimensional space.\n",
    "\n",
    "(c) The computational complexity of the distance calculations is reduced.\n",
    "\n",
    "(d) The performance remains unaffected as high-dimensionality uniformly impacts the positional relationships among the data points.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** B. As the number of dimensions increases, the contrast between the nearest and farthest point from a given reference point tends to decrease, making it challenging for a distance-based model to discern between meaningful and uninformative patterns in the data.\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "Which of the following is true about selecting $k=1$ for a $k$-nearest neighbors model of high dimensional data?\n",
    "\n",
    "(a) $k=1$ will make the model more sensitive to noise in the data.\n",
    "\n",
    "(b) $k=1$ will more accurately represent the real world distribution because it is more specific.\n",
    "\n",
    "(c) $k=1$ is a good option because it will lead to the highest number of different groupings, to match the high dimensionality of the data.\n",
    "\n",
    "(d) $k=1$ means that there will only be one grouping.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** $k = 1$ means that each data point receives its own classification rule. Thus, the model will learn to predict noise in the data, and have a very high variance, because the rules it learns will be highly dependent on randomness in the training data.\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "Which of the following is true about `k-means` clustering?\n",
    "\n",
    "(a) `k-means` diverges and is non-convex.\n",
    "\n",
    "(b) `k-means` diverges and is convex.\n",
    "\n",
    "(c) `k-means` converges and is non-convex.\n",
    "\n",
    "(d) `k-means` converges and is convex.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** The `k-means` algorithm converges but is not convex; `k-means` can get stuck at a local minima given an unlucky initialization.\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "In which of the following plots are the points clustered by `k-means` clustering?\n",
    "\n",
    "<img src='./k-means.png' width=\"350px\">\n",
    "\n",
    "(a) Plot (a)\n",
    "\n",
    "(b) Plot (b)\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** Plot (b) shows the clustering that would result from k-means, where the algorithm divides the data into two clusters based on the geometric center of the data points, resulting in a horizontal division rather than following the natural crescent shapes of the data.\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "17. What is the main purpose of the softmax activation function in the output layer of a neural network?\n",
    "\n",
    "(a) To introduce non-linearity.\n",
    "\n",
    "(b) To normalize the output to represent probabilities.\n",
    "\n",
    "(c) To speed up convergence during training.\n",
    "\n",
    "(d) To prevent overfitting.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "18. Consider a fully-connected neural network with 3 input neurons, 4 hidden neurons, and 2 output neurons. What is the total number of parameters, including bias units for non-input layers?\n",
    "\n",
    "(a) 9\n",
    "\n",
    "(b) 11\n",
    "\n",
    "(c) 24\n",
    "\n",
    "(d) 26\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** From the input layer to the hidden layer, you have 3 neurons fully connected to 4 neurons, which gives us $3 \\cdot 4 = 12$ weights. Plus, there are 4 neurons in the hidden layer, which means there are 4 biases. So, for the first connection, there are $12+4 = 16$ parameters. From the hidden layer to the output layer, you have 4 neurons fully connected to 2 neurons, which gives us $4 \\cdot 2 = 8$ weights. Plus, there are 2 neurons in the output layer, which means there are 2 biases. So, for the second connection, there are $8+2 = 10$ parameters. In total, we have $16+10 = 26$ parameters.\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "19. Which of the following will guarantee that a neural network does not overfit to the training data during training?\n",
    "\n",
    "(a) Normalize the data before training.\n",
    "\n",
    "(b) Increase the number of layers in our network until the final training loss stops decreasing.\n",
    "\n",
    "(c) Neither of the above.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "20. Given a simple two-layer neural network:\n",
    "\n",
    "*   Weights from input to hidden layer: $W^{(1)} = \\begin{bmatrix} w_{11}^{(1)} & w_{12}^{(1)} \\\\ w_{21}^{(1)} & w_{22}^{(1)} \\end{bmatrix}$, Bias for hidden layer: $[b_1^{(1)}, b_2^{(1)}]$, Activation function: $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "*   Weights from hidden to output layer: $W^{(2)} = [w_1^{(2)}, w_2^{(2)}]$, Bias for output layer: $b^{(2)}$, Activation function: $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "*   Target output: $y$; predicted output: $\\hat{y}$\n",
    "*   Loss function: $\\frac{1}{2}(y-\\hat{y})^2$\n",
    "\n",
    "After performing a forward pass with input $[x_1, x_2]$ and computing the loss, you execute a backward pass to calculate the gradients of the loss with respect to the weights and biases. What are the correct gradients for the weight, $w_{11}^{(1)}$, after one round of backpropagation?\n",
    "\n",
    "Hint: Use chain rule to compute the gradients for $W^{(2)}$ and $W^{(1)}$. $\\sigma'(z)$ is $\\sigma(z) \\cdot (1-\\sigma(z))$.\n",
    "\n",
    "(a) $\\frac{\\partial Loss}{\\partial w_{11}^{(1)}} = (y - \\hat{y})^2 \\cdot w_1^{(2)} \\cdot \\sigma'(z_1^{(1)}) \\cdot x_1$\n",
    "\n",
    "(b) $\\frac{\\partial Loss}{\\partial w_{11}^{(1)}} = (y - \\hat{y}) \\cdot \\hat{y} \\cdot w_1^{(2)} \\cdot \\sigma'(z_1^{(1)}) \\cdot x_1$\n",
    "\n",
    "(c) $\\frac{\\partial Loss}{\\partial w_{11}^{(1)}} = (y - \\hat{y}) \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\cdot w_1^{(2)} \\cdot \\sigma'(z_1^{(1)}) \\cdot x_1$\n",
    "\n",
    "(d) $\\frac{\\partial Loss}{\\partial w_{11}^{(1)}} = (y - \\hat{y}) \\cdot x_1$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Gradient of Loss w.r.t. Output Layer Weights $W^{(2)}$: $\\frac{\\partial Loss}{\\partial W^{(2)}}$\n",
    "\n",
    "Using chain rule, $\\frac{\\partial Loss}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} = (y - \\hat{y}) \\cdot (-1) \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\cdot a^{(1)}$\n",
    "\n",
    "Gradient of Loss w.r.t. Hidden Layer Weights $W^{(1)}$:\n",
    "\n",
    "For each weight $w_{ij}^{(1)}$, $\\frac{\\partial Loss}{\\partial w_{ij}^{(1)}}$ Using chain rule, $\\frac{\\partial Loss}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial w_{ij}^{(1)}}$\n",
    "\n",
    "For $w_{11}^{(1)}$: $(y - \\hat{y}) \\cdot (-1) \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\cdot w_1^{(2)} \\cdot \\sigma'(z_1^{(1)}) \\cdot x_1$\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "21. Which of the following statement is true about the following code snippet?\n",
    "\n",
    "```python\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(images) # (a)\n",
    "        batch_loss = F.cross_entropy(y_hat, labels) # (b)\n",
    "        batch_loss.backward() # (c)\n",
    "        optimizer.step() # (d)\n",
    "```\n",
    "\n",
    "(a) Step (a) completes the forward pass in backward propagation.\n",
    "\n",
    "(b) Step (b) calculates the batch loss using a loss function that consists of its own trainable parameters, and weighs each sample differently based on those parameters.\n",
    "\n",
    "(c) Step (c) never changes the weight parameters of any previous layer.\n",
    "\n",
    "(d) Step (d) by itself performs the stochastic gradient descent by calculating the gradients and updating parameterized weights (you may assume we are using torch.optim.SGD for optimizer).\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "22. Which of the follow is true about using backpropagation to train a neural network using a package such as PyTorch or TensorFlow?\n",
    "\n",
    "(a) You need to create a method that computes the gradient of each node of your neural network to call in the backpropagation step.\n",
    "\n",
    "(b) Automatic differentiation executed by these packages takes advantage of the fact that the gradients of most functions can be pre-computed.\n",
    "\n",
    "(c) The back-propagation executed by these packages is the process of computing the derivative of the nodes of a neural network starting with the first node at the beginning of the network and then proceeding to the next node(s).\n",
    "\n",
    "(d) These packages fail on models with ReLU layers because the ReLU function is not differentiable everywhere, and thus the packages cannot execute backpropagation.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "23. How is Singular Value Decomposition (SVD) typically utilized in image compression?\n",
    "\n",
    "(a) Selecting important pixels\n",
    "\n",
    "(b) Discarding low-rank components\n",
    "\n",
    "(c) Enhancing color information\n",
    "\n",
    "(d) Increasing image resolution\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "24. In the context of image processing, which of the following will directly impact the total number of trainable weights in a convolutional layer of a convolutional neural network (CNN)?\n",
    "\n",
    "(a) The resolution of the input image\n",
    "\n",
    "(b) The kernel size of the layer\n",
    "\n",
    "(c) The stride of the layer\n",
    "\n",
    "(d) The amount of padding used\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 25\n",
    "\n",
    "25. What is the key advantage of using Gaussian Mixture Models (GMMs) over $k$-means clustering for data clustering tasks?\n",
    "\n",
    "(a) GMMs are computationally more efficient than $k$-means and are better suited for large datasets due to their simpler calculations.\n",
    "\n",
    "(b) GMMs, unlike $k$-means, can automatically determine the optimal number of clusters in a dataset without requiring this as an input parameter.\n",
    "\n",
    "(c) GMMs can model complex cluster shapes and densities, accommodating elliptical shapes, as they do not assume clusters to be spherical like $k$-means.\n",
    "\n",
    "(d) GMMs inherently handle missing data and noise better than $k$-means due to their probabilistic approach, which accounts for uncertainty in the data.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 26\n",
    "\n",
    "26. Which of the following statements are true? Select all that apply.\n",
    "\n",
    "(a) The sum of two convex functions is always convex.\n",
    "\n",
    "(b) The sum of two concave functions is always concave.\n",
    "\n",
    "(c) The sum of a convex and concave function is always concave.\n",
    "\n",
    "**Correct answers:** (a), (b)\n",
    "\n",
    "## Problem 27\n",
    "\n",
    "27. Which of the following is **not** true about an arbitrary convex function $f: \\mathbb{R} \\to \\mathbb{R}$ without any other assumptions? Select all that apply.\n",
    "\n",
    "(a) For all $x \\in \\mathbb{R}$, $f''(x) \\ge 0$\n",
    "\n",
    "(b) The set\n",
    "$$ \\{(x, y) \\in \\mathbb{R}^2 \\mid y \\ge f(x)\\} $$\n",
    "is convex\n",
    "\n",
    "(c) If $c$ is a subgradient of $f$ at $x$, then for all $y \\in \\mathbb{R}$:\n",
    "$$ f(y) \\ge f(x)+c(y - x) $$\n",
    "\n",
    "(d) $f$ cannot be concave\n",
    "\n",
    "**Correct answers:** (a), (d)\n",
    "\n",
    "**Explanation:** a is not true in general since we don't know that the second derivative exists; d is not true (e.g. $f(x) = x$)\n",
    "\n",
    "## Problem 28\n",
    "\n",
    "28. Suppose $f(x) = ax^2 + bx + c$, where $a, b, c \\in \\mathbb{R}$. Which of the following statements are true about the convexity of $f$?\n",
    "\n",
    "(a) $f$ is always convex since it is a polynomial.\n",
    "\n",
    "(b) $f$ is convex only when $a > 0, b > 0$, and $c > 0$.\n",
    "\n",
    "(c) If $a > 0$ then $f$ is convex.\n",
    "\n",
    "(d) If $a = 0$ then $f$ is never convex.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "## Problem 29\n",
    "\n",
    "29. Given this 3-D scatter plot, which of the following basis functions would you use for linear regression?\n",
    "\n",
    "<img src=\"./scatter_plot.png\" width=\"350px\">\n",
    "\n",
    "(a) $\\phi(x, y) = \\begin{bmatrix} 1 \\\\ x \\\\ y \\\\ xy \\\\ x^2 \\\\ y^2 \\end{bmatrix}$\n",
    "\n",
    "(b) $\\phi(x, y) = \\begin{bmatrix} e^{-x^2} \\\\ e^{-y^2} \\\\ e^{-(x^2+y^2)} \\end{bmatrix}$\n",
    "\n",
    "(c) $\\phi(x, y) = \\begin{bmatrix} \\cos(x) \\\\ \\cos(y) \\end{bmatrix}$\n",
    "\n",
    "(d) $\\phi(x, y) = \\begin{bmatrix} \\sin(x) \\\\ \\sin(y) \\end{bmatrix}$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 30\n",
    "\n",
    "30. Suppose that we want to train a predictor $\\hat{f}(x) = \\hat{w}^T x$ and we assume that $y = w^T x + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma^2)$. Which of the following statements about bias-variance tradeoff is true?\n",
    "\n",
    "(a) (bias$^2$ + variance) is equal to the expected error between our trained predictor $\\hat{f}(x)$ and the true data points ($y$'s).\n",
    "\n",
    "(b) Regularization is usually used to increase the variance of our trained predictor $\\hat{f}(x)$.\n",
    "\n",
    "(c) Irreducible error comes from the variance of the data points $y$'s.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Scatter plot is a Gaussian centered at 0 so b is correct\n",
    "\n",
    "## Problem 31\n",
    "\n",
    "31. Consider a dataset $x_1, x_2, ..., x_n$ drawn from a normal distribution $N(\\mu, \\sigma^2)$, with the density function given by $f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. Which of the following is true about the maximum likelihood estimation of $\\mu$ and $\\sigma^2$?\n",
    "\n",
    "(a) The MLE for both $\\mu$ and $\\sigma^2$ cannot be determined without additional information.\n",
    "\n",
    "(b) The MLE of $\\mu$ is the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$, but the MLE of $\\sigma^2$ cannot be determined without additional information.\n",
    "\n",
    "(c) The MLE of $\\mu$ is the sample mean, $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$, and the MLE of $\\sigma^2$ is the sample variance, $s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2$.\n",
    "\n",
    "(d) The MLE of $\\mu$ is the sample mean, $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$, and the MLE of $\\sigma^2$ is given by $s^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2$.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** The maximum likelihood estimator for $\\mu$ in a normal distribution is the sample mean, $\\bar{x}$. However, the MLE for $\\sigma^2$ is a biased estimator and is given by $\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2$. This is because MLE does not divide by $n-1$ (as in the unbiased sample variance formula) but by $n$, which makes it biased.\n",
    "\n",
    "## Problem 32\n",
    "\n",
    "32. In k-fold cross-validation, what is the primary advantage of setting k to a higher value (e.g., k=10) compared to a lower value (e.g., k=2)?\n",
    "\n",
    "(a) It increases the accuracy of the model on unseen data.\n",
    "\n",
    "(b) It provides a more reliable estimate of model performance.\n",
    "\n",
    "(c) It reduces computational time.\n",
    "\n",
    "(d) It eliminates the need for a separate test set.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "## Problem 33\n",
    "\n",
    "33. Which of the following statements is true for ridge regression if the regularization parameter is too large?\n",
    "\n",
    "(a) The loss function will be the same as the ordinary least squares loss function.\n",
    "\n",
    "(b) The loss function will be the same as the Lasso regression loss function.\n",
    "\n",
    "(c) Large coefficients will not be penalized.\n",
    "\n",
    "(d) The model will overfit the data.\n",
    "\n",
    "(e) The model will underfit the data.\n",
    "\n",
    "**Correct answers:** (e)\n",
    "\n",
    "## Problem 34\n",
    "\n",
    "34. Consider a binary classification task, where $\\hat{y}$ denotes the prediction and $y = +1$ or $y=-1$. Briefly describe the strength of minimizing logistic loss as opposed to 0-1 loss and sigmoid loss. The losses are formally defined as\n",
    "\n",
    "$$0-1 \\text{ loss}(\\hat{y}, y) = \\begin{cases} 0 & \\text{if sign}(y) = \\text{sign}(\\hat{y}) \\\\ 1 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "$$\\text{logistic loss}(\\hat{y}, y) = \\log(1+\\exp(-y\\hat{y}))$$\n",
    "\n",
    "$$\\text{sigmoid loss}(\\hat{y}, y) = \\frac{1}{1 + \\exp(y\\hat{y})}$$\n",
    "\n",
    "The followings are example plots of each loss when $y = +1$.\n",
    "\n",
    "<img src=\"./loss.png\" width=\"650px\">\n",
    "\n",
    "(Image of three plots: \"0-1 loss\" (step function), \"logistic loss\" (decreasing curve), \"sigmoid loss\" (decreasing curve))\n",
    "\n",
    "Strength of logistic loss compared to 0-1 loss:\n",
    "\n",
    "Strength of logistic loss compared to sigmoid loss:\n",
    "\n",
    "**Explanation:**\n",
    "Possible strength compared to 0-1 loss: Differentiable everywhere\n",
    "Possible strength compared to sigmoid loss: Convexity\n",
    "\n",
    "## Problem 35\n",
    "\n",
    "35. Name one advantage and one disadvantage of applying $k$-nearest neighbors for classification.\n",
    "\n",
    "Advantage:\n",
    "\n",
    "Disadvantage:\n",
    "\n",
    "**Explanation:**\n",
    "Possible advantages: no training, simple non-parametric. Possible disadvantages: need to store training data for inference, curse of dimensionality.\n",
    "\n",
    "## Problem 36\n",
    "\n",
    "36. Consider the following scatter plot of a multivariate Gaussian distribution. Draw the approximate first and second principal components on the graph and say which is which.\n",
    "\n",
    "<img src=\"./gaussian.png\" width=\"350px\">\n",
    "\n",
    "First principal component: ________\n",
    "\n",
    "Second principal component: ________\n",
    "\n",
    "**Explanation:**\n",
    "Data was generated to have principal components of (4, 1) and (-1,4).\n",
    "\n",
    "## Problem 37\n",
    "\n",
    "37. You are training a regression model to predict house prices. You decide to use zip-code as a feature in your model. Describe one possible problem with using zip-code as a feature in your model. [Note: zip-codes are numbers assigned to geographic regions. For example, going west from UW the zip-codes are 98105, 98103, then 98107.]\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Answers should include something about how zip code is a categorical variable, and the relative value of zip-code numbers has no meaning, and thus performing regression with them makes no sense, because a $\\Delta_1$ change in zip code cannot logically result in some $\\Delta_2$ change in house price.\n",
    "\n",
    "## Problem 38\n",
    "\n",
    "38. The real matrix $A$ has the following singular value decomposition, $A = USV^T$. Describe the structure of $S$ and what $S$ contains. (Note: for your answer you can consider this to be the compact or regular formulation of the SVD.)\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "1) $S$ has values on its diagonal only / $S$ is a diagonal matrix\n",
    "2) The diagonal contains the singular values of $A$ ordered from largest to smallest starting at the top left.\n",
    "\n",
    "## Problem 39\n",
    "\n",
    "39. Describe two advantages of using non-linear pooling layers, such as max-pooling, in a convolutional neural network (CNN).\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Two example reasons are adding non-linearity to the model and reducing the size of the input which speeds up evaluation and training\n",
    "\n",
    "## Problem 40\n",
    "\n",
    "40. What is the purpose of using multiple filters for a single convolutional layer in a neural network?\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Capturing Different Features: To enable the detection of various features or patterns in the input data, each filter specializes in recognizing specific characteristics.\n",
    "Enhancing Model Robustness: Multiple filters help the model generalize better by learning diverse representations, making it more robust to variations and nuances in the input.\n",
    "\n",
    "## Problem 41\n",
    "\n",
    "41. You are tasked with designing a convolutional neural network (CNN) to classify images as one of three classes; cat, dog, and other. You have a dataset of 10,000 images each to train and test your model, each with an image resolution of 1024 x 1024 pixels. You also have a dataset with the same 10,000 images, but with a downsampled image resolution of 16 x 16 pixels. What is one advantage and one disadvantage of using the dataset with lower resolution images?\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Advantage: smaller input size will be less computationally expensive; Disadvantage: images may not be as clear and accuracy may suffer\n",
    "\n",
    "## Problem 42\n",
    "\n",
    "42. You design a deep learning model. Describe one thing that could happen if you start training your model with too high of a learning rate:\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Some possible explanations are overshooting the minimum, failure to converge, unstable training, exploding gradients.\n",
    "\n",
    "## Problem 43\n",
    "\n",
    "43. You are creating a deep learning model. Why is it important to perform hyper-parameter tuning on a different set of data than the data that you used to test your model?\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Correct answers should include some mention of avoiding overfitting to the test data and or making the model better at generalizing to unseen data.\n",
    "\n",
    "## Problem 44\n",
    "\n",
    "44. You design a machine learning model. In your own words, conceptually describe what the bias of your model means. [Note: this is asking about the theoretical bias of the model, NOT the social biases that may be influencing training dataset.]\n",
    "\n",
    "Answer: ________\n",
    "\n",
    "**Explanation:**\n",
    "Bias is the difference between the optimal predictor and the expectation of the best possible trained version of your model (with respect to all possible training sets).\n",
    "The key for full points is describing a difference between optimal predictor (or the \"real world distribution\") and the best trained version of your model. The expectation part is not essential. Another acceptable response could describe bias as the inherent limitation of a specific model architecture that prevent it from being a better predictor."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
