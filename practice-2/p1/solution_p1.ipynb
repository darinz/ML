{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1788e585-7900-4c35-83f1-5967a93383ae",
   "metadata": {},
   "source": [
    "# Practice Problem Set 1 - Solutions\n",
    "\n",
    "Consider the following matrix $X$ with four data points in $\\mathbb{R}^2$. We would like to use PCA to find a rank-1 linear representation of these data.\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "4 & 1 \\\\ \n",
    "5 & 4 \\\\ \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![pca](./pca.png)\n",
    "\n",
    "**Figure 2:** These plots depict data points from the four-sample dataset $X$.\n",
    "\n",
    "## Question 1\n",
    "Which line in Figure 2 represents the direction of the first principal component of $X - \\mu$, where $X \\in \\mathbb{R}^{n \\times d}$ the vector $\\mu \\in \\mathbb{R}^d$ is the featurewise mean of $X$?\n",
    "\n",
    "(A) Plot 1  \n",
    "(B) Plot 2  \n",
    "(C) Plot 3  \n",
    "(D) Plot 4  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "The first principal component is the direction of maximum variance in the data. Here's the detailed reasoning:\n",
    "\n",
    "1. **Data Centering**: First, we subtract the mean $\\mu$ from each data point to center the data around the origin. This gives us $X - \\mu$.\n",
    "\n",
    "2. **Covariance Matrix**: The covariance matrix is computed as $C = \\frac{1}{n}(X - \\mu)^T(X - \\mu)$, where $n$ is the number of samples.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: PCA finds the eigenvectors and eigenvalues of the covariance matrix. The first principal component is the eigenvector corresponding to the largest eigenvalue.\n",
    "\n",
    "4. **Maximum Variance Direction**: The first principal component represents the direction along which the projected data has maximum variance. This is because the variance of the projected data is given by $\\text{Var}(Xv) = v^T C v$, which is maximized when $v$ is the eigenvector of $C$ with the largest eigenvalue.\n",
    "\n",
    "5. **Visual Interpretation**: In the figure, Plot 1 shows the line that best captures the spread of the centered data points. This line represents the direction where the data varies the most, which is exactly what the first principal component represents.\n",
    "\n",
    "Mathematically, if $v_1$ is the first principal component, then:\n",
    "$$\\text{Var}(Xv_1) = \\max_{||v||=1} \\text{Var}(Xv) = \\lambda_1$$\n",
    "where $\\lambda_1$ is the largest eigenvalue of the covariance matrix.\n",
    "\n",
    "## Question 2\n",
    "Which of the following statements about kernels is false?\n",
    "\n",
    "(A) Kernel feature vectors $\\phi(x_i)$ can be infinite-dimensional  \n",
    "(B) Kernels methods scale well to large datasets because the size of the kernel matrix does not depend on the size of the dataset  \n",
    "(C) Kernel matrices store the results of inner products of the data's features computed in a higher-dimensional space  \n",
    "(D) Kernels allow otherwise linear models to find non-linear decision boundaries  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Statement (B) is false. Here's the detailed analysis:\n",
    "\n",
    "**Why (B) is False:**\n",
    "The kernel matrix $K$ has dimensions $n \\times n$ where $n$ is the number of data points. This means:\n",
    "- **Memory Complexity**: $O(n^2)$ storage required\n",
    "- **Computational Complexity**: $O(n^3)$ for matrix operations like inversion\n",
    "- **Scaling Issue**: As dataset size grows, the kernel matrix grows quadratically, making it computationally prohibitive for large datasets\n",
    "\n",
    "**Why the Other Statements are True:**\n",
    "\n",
    "(A) **Infinite-dimensional feature vectors**: The RBF kernel $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$ corresponds to an infinite-dimensional feature space. This is because the Taylor expansion of the exponential function has infinitely many terms.\n",
    "\n",
    "(C) **Kernel matrix stores inner products**: The kernel matrix is defined as $K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle$, where $\\phi$ is the feature mapping. This allows us to compute inner products in the high-dimensional feature space without explicitly computing $\\phi(x_i)$.\n",
    "\n",
    "(D) **Non-linear decision boundaries**: The kernel trick allows linear models (like SVM) to operate in high-dimensional feature spaces, effectively creating non-linear decision boundaries in the original input space.\n",
    "\n",
    "**Mathematical Example:**\n",
    "For a polynomial kernel of degree 2: $K(x_i, x_j) = (1 + x_i^T x_j)^2$, the feature mapping $\\phi$ maps to a space of dimension $O(d^2)$, where $d$ is the input dimension.\n",
    "\n",
    "## Question 3\n",
    "Suppose you have a logistic regression model for spam detection, using a dataset with a binary outcome that indicates whether an email is spam (1) or not spam (0). The predictor variables $x_1$, $x_2$, and $x_3$ are boolean values (0 or 1) that indicate whether the email contains the words \"free\", \"order\", and \"homework\", respectively. The model has four parameters: weights $w_1$, $w_2$, $w_3$, and offset $b$. You find that emails containing the words \"free\" and \"order\" have a higher probability of being spam, while emails containing the word \"homework\" have a lower probability of being spam. Given this information, which of the following signs is most likely for the weights $w_1$, $w_2$, and $w_3$?\n",
    "\n",
    "(A) All positive  \n",
    "(B) All negative  \n",
    "(C) $w_1$ and $w_2$ are positive, $w_3$ is negative  \n",
    "(D) $w_1$ and $w_2$ are negative, $w_3$ is positive  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Let's analyze this step by step using the logistic regression model:\n",
    "\n",
    "**Logistic Regression Model:**\n",
    "The probability of an email being spam is given by:\n",
    "$$P(\\text{spam}) = \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2 + w_3 x_3 + b)}}$$\n",
    "\n",
    "where:\n",
    "- $x_1 = 1$ if the email contains \"free\", $0$ otherwise\n",
    "- $x_2 = 1$ if the email contains \"order\", $0$ otherwise  \n",
    "- $x_3 = 1$ if the email contains \"homework\", $0$ otherwise\n",
    "- $w_1, w_2, w_3$ are the weights for each word\n",
    "- $b$ is the bias term\n",
    "\n",
    "**Analyzing Each Word's Effect:**\n",
    "\n",
    "1. **\"free\" increases spam probability**: When $x_1 = 1$, we want $P(\\text{spam})$ to increase. This means the exponent $-(w_1 \\cdot 1 + w_2 x_2 + w_3 x_3 + b)$ should become less negative (closer to zero), so $w_1$ must be **positive**.\n",
    "\n",
    "2. **\"order\" increases spam probability**: When $x_2 = 1$, we want $P(\\text{spam})$ to increase. Similarly, $w_2$ must be **positive**.\n",
    "\n",
    "3. **\"homework\" decreases spam probability**: When $x_3 = 1$, we want $P(\\text{spam})$ to decrease. This means the exponent should become more negative, so $w_3$ must be **negative**.\n",
    "\n",
    "**Mathematical Verification:**\n",
    "- For an email with \"free\" and \"order\": $P(\\text{spam}) = \\frac{1}{1 + e^{-(w_1 + w_2 + b)}}$\n",
    "- For an email with \"homework\": $P(\\text{spam}) = \\frac{1}{1 + e^{-(w_3 + b)}}$\n",
    "\n",
    "Since $w_1, w_2 > 0$ and $w_3 < 0$, the first probability is higher than the second.\n",
    "\n",
    "**Conclusion:** $w_1 > 0$, $w_2 > 0$, and $w_3 < 0$, which corresponds to option (C).\n",
    "\n",
    "## Question 4\n",
    "**True/False:** Solving the k-means objective is an unsupervised learning problem.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is **True**. K-means is a classic example of unsupervised learning. Here's the detailed explanation:\n",
    "\n",
    "**What is Unsupervised Learning?**\n",
    "Unsupervised learning involves finding patterns in data without any labeled examples or target variables. The algorithm learns the structure of the data on its own.\n",
    "\n",
    "**Why K-means is Unsupervised:**\n",
    "\n",
    "1. **No Labels Required**: K-means doesn't need any information about the \"correct\" cluster assignments. It works purely on the input features.\n",
    "\n",
    "2. **Self-Discovery**: The algorithm discovers clusters by finding groups of similar data points based on their distances to centroids.\n",
    "\n",
    "3. **Objective Function**: K-means minimizes the within-cluster sum of squares:\n",
    "   $$\\min_{\\{C_1, \\ldots, C_k\\}} \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "   where:\n",
    "   - $C_i$ is the $i$-th cluster\n",
    "   - $\\mu_i$ is the centroid of cluster $C_i$\n",
    "   - $||x - \\mu_i||^2$ is the squared Euclidean distance\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Initialize $k$ centroids randomly\n",
    "2. Assign each data point to the nearest centroid\n",
    "3. Update centroids as the mean of assigned points\n",
    "4. Repeat steps 2-3 until convergence\n",
    "\n",
    "**Contrast with Supervised Learning:**\n",
    "- **Supervised**: Uses labeled data $(x_i, y_i)$ to learn a mapping $f: X \\rightarrow Y$\n",
    "- **Unsupervised**: Uses only $x_i$ to discover structure in the data\n",
    "\n",
    "**Examples of Unsupervised Learning:**\n",
    "- Clustering (K-means, hierarchical clustering)\n",
    "- Dimensionality reduction (PCA, t-SNE)\n",
    "- Association rule learning\n",
    "- Anomaly detection\n",
    "\n",
    "## Question 5\n",
    "Which of the following is typical for decision trees trained to have 0 training error?\n",
    "\n",
    "(A) High bias, low variance  \n",
    "(B) High bias, high variance  \n",
    "(C) Low bias, high variance  \n",
    "(D) Low bias, low variance  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Decision trees with 0 training error exhibit **low bias, high variance**. Here's the detailed analysis:\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "The expected prediction error can be decomposed as:\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**Why Low Bias:**\n",
    "1. **Perfect Training Fit**: A decision tree can achieve 0 training error by growing deep enough to classify each training point correctly\n",
    "2. **High Model Capacity**: Decision trees can represent any boolean function given enough depth\n",
    "3. **No Model Limitations**: The tree can perfectly capture the training data patterns\n",
    "\n",
    "**Why High Variance:**\n",
    "1. **Overfitting**: The tree memorizes the training data, including noise\n",
    "2. **Instability**: Small changes in training data can lead to completely different tree structures\n",
    "3. **Poor Generalization**: The model fails to generalize to unseen data\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "- **Bias**: Measures how well the model can approximate the true underlying function\n",
    "- **Variance**: Measures how much the model's predictions vary across different training sets\n",
    "\n",
    "**Example:**\n",
    "Consider a decision tree that grows until each leaf contains exactly one training point:\n",
    "- **Training Error**: 0% (perfect fit)\n",
    "- **Test Error**: High (poor generalization)\n",
    "- **Bias**: Low (can fit any training data)\n",
    "- **Variance**: High (very sensitive to training data)\n",
    "\n",
    "**Visual Analogy:**\n",
    "Think of a decision tree with 0 training error as a \"memorization machine\" - it perfectly remembers the training data but doesn't learn the underlying patterns, making it unstable to new data.\n",
    "\n",
    "**Solutions to High Variance:**\n",
    "- Pruning the tree\n",
    "- Using ensemble methods (Random Forest, Gradient Boosting)\n",
    "- Setting minimum samples per leaf\n",
    "- Limiting maximum depth\n",
    "\n",
    "## Question 6\n",
    "When is PCA ineffective?\n",
    "\n",
    "(A) When data has an orthogonal underlying structure.  \n",
    "(B) When the data's underlying low-dimensional structure is non-linear.  \n",
    "(C) When the data is standardized.  \n",
    "(D) When data visualisation is needed.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "PCA is ineffective when the data's underlying structure is **non-linear**. Here's the detailed explanation:\n",
    "\n",
    "**What PCA Does:**\n",
    "PCA finds linear projections that maximize variance:\n",
    "$$\\max_{||v||=1} \\text{Var}(Xv) = \\max_{||v||=1} v^T C v$$\n",
    "\n",
    "where $C$ is the covariance matrix.\n",
    "\n",
    "**Why PCA Fails with Non-linear Structure:**\n",
    "\n",
    "1. **Linear Assumption**: PCA assumes that the important directions in the data are linear combinations of the original features\n",
    "2. **Manifold Learning**: When data lies on a curved manifold (e.g., Swiss roll, S-curve), the important structure is non-linear\n",
    "3. **Projection Limitations**: PCA can only find linear projections, missing the curved structure\n",
    "\n",
    "**Mathematical Example:**\n",
    "Consider data lying on a circle in 2D:\n",
    "- **True Structure**: 1-dimensional circle\n",
    "- **PCA Result**: Finds 2 principal components (both needed to represent the circle)\n",
    "- **Problem**: PCA can't discover that the data is actually 1-dimensional\n",
    "\n",
    "**Visual Analogy:**\n",
    "Imagine trying to flatten a piece of paper that's been rolled into a cylinder:\n",
    "- **PCA**: Would try to project it onto a flat surface, losing the cylindrical structure\n",
    "- **Non-linear Methods**: Would \"unroll\" the cylinder to reveal its true 2D structure\n",
    "\n",
    "**Better Alternatives for Non-linear Data:**\n",
    "1. **t-SNE**: Preserves local structure and clusters\n",
    "2. **UMAP**: Preserves both local and global structure\n",
    "3. **Kernel PCA**: Uses kernel trick to capture non-linear relationships\n",
    "4. **Isomap**: Preserves geodesic distances on the manifold\n",
    "5. **LLE (Locally Linear Embedding)**: Preserves local linear relationships\n",
    "\n",
    "**When PCA Works Well:**\n",
    "- Data has linear underlying structure\n",
    "- Data is approximately Gaussian\n",
    "- Main sources of variation are linear combinations of features\n",
    "- Goal is to reduce dimensionality while preserving variance\n",
    "\n",
    "## Question 7\n",
    "The kernel matrix $K$ is not\n",
    "\n",
    "(A) Symmetric  \n",
    "(B) Square  \n",
    "(C) Positive semi-definite (defined as $x^T Kx > 0$ for every nonzero column vector $x$; a necessary condition for this is that $K$ has nonnegative eigenvalues).  \n",
    "(D) Elementwise positive  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The kernel matrix $K$ is **NOT** necessarily elementwise positive. Here's the detailed analysis:\n",
    "\n",
    "**Kernel Matrix Definition:**\n",
    "$$K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle$$\n",
    "\n",
    "where $\\phi$ is the feature mapping and $\\langle \\cdot, \\cdot \\rangle$ is the inner product.\n",
    "\n",
    "**Properties of Kernel Matrix:**\n",
    "\n",
    "1. **Symmetric (A)**: $K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\langle \\phi(x_j), \\phi(x_i) \\rangle = K_{ji}$\n",
    "\n",
    "2. **Square (B)**: $K$ has dimensions $n \\times n$ where $n$ is the number of data points\n",
    "\n",
    "3. **Positive Semi-definite (C)**: For any vector $x$, $x^T K x \\geq 0$ because:\n",
    "   $$x^T K x = \\sum_{i,j} x_i K_{ij} x_j = \\sum_{i,j} x_i \\langle \\phi(x_i), \\phi(x_j) \\rangle x_j = \\left\\langle \\sum_i x_i \\phi(x_i), \\sum_j x_j \\phi(x_j) \\right\\rangle = ||\\sum_i x_i \\phi(x_i)||^2 \\geq 0$$\n",
    "\n",
    "4. **NOT Elementwise Positive (D)**: Individual elements $K_{ij}$ can be negative.\n",
    "\n",
    "**Examples of Kernels with Negative Values:**\n",
    "\n",
    "1. **Polynomial Kernel**: $K(x_i, x_j) = (x_i^T x_j + c)^d$\n",
    "   - If $c < 0$ and $d$ is odd, some values can be negative\n",
    "\n",
    "2. **Sigmoid Kernel**: $K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c)$\n",
    "   - Can produce negative values for certain parameter settings\n",
    "\n",
    "3. **Linear Kernel with Centered Data**: $K(x_i, x_j) = x_i^T x_j$\n",
    "   - If data is centered, some inner products can be negative\n",
    "\n",
    "**Mathematical Example:**\n",
    "Consider the linear kernel with 2D data:\n",
    "- $x_1 = [1, 0]$, $x_2 = [-1, 0]$\n",
    "- $K_{12} = x_1^T x_2 = 1 \\cdot (-1) + 0 \\cdot 0 = -1 < 0$\n",
    "\n",
    "**Why This Matters:**\n",
    "- Kernel methods work with the kernel matrix, not individual elements\n",
    "- The positive semi-definite property ensures the kernel represents a valid inner product space\n",
    "- Individual negative values don't affect the mathematical validity of the kernel\n",
    "\n",
    "## Question 8\n",
    "**True/False:** Ridge regression's optimal parameters $\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$ are a linear combination of the data points $x_i$ in $X$.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is **True**. Ridge regression's optimal parameters can indeed be expressed as a linear combination of the data points. Here's the detailed proof:\n",
    "\n",
    "**Primal Form of Ridge Regression:**\n",
    "$$\\min_w ||y - Xw||_2^2 + \\lambda ||w||_2^2$$\n",
    "\n",
    "**Primal Solution:**\n",
    "$$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$$\n",
    "\n",
    "**Dual Form Derivation:**\n",
    "\n",
    "1. **Lagrangian**: $L(w, \\alpha) = ||y - Xw||_2^2 + \\lambda ||w||_2^2 - \\alpha^T(Xw - y)$\n",
    "\n",
    "2. **Stationary Condition**: $\\frac{\\partial L}{\\partial w} = -2X^T(y - Xw) + 2\\lambda w = 0$\n",
    "\n",
    "3. **Solving for w**: $2\\lambda w = 2X^T(y - Xw)$\n",
    "   $$\\lambda w = X^T(y - Xw)$$\n",
    "   $$\\lambda w = X^T y - X^T X w$$\n",
    "   $$(\\lambda I + X^T X)w = X^T y$$\n",
    "   $$w = (\\lambda I + X^T X)^{-1} X^T y$$\n",
    "\n",
    "4. **Representer Theorem**: We can express $w$ as $w = X^T \\alpha$ for some $\\alpha$\n",
    "\n",
    "5. **Substituting**: $X^T \\alpha = (\\lambda I + X^T X)^{-1} X^T y$\n",
    "\n",
    "6. **Solving for $\\alpha$**: \n",
    "   $$X^T \\alpha = (\\lambda I + X^T X)^{-1} X^T y$$\n",
    "   $$X^T \\alpha = X^T (XX^T + \\lambda I)^{-1} y$$\n",
    "   $$\\alpha = (XX^T + \\lambda I)^{-1} y$$\n",
    "\n",
    "**Final Dual Form:**\n",
    "$$\\hat{w} = X^T \\alpha = X^T (XX^T + \\lambda I)^{-1} y$$\n",
    "\n",
    "**Interpretation:**\n",
    "- The optimal weights $\\hat{w}$ are a linear combination of the data points $x_i$\n",
    "- The coefficients $\\alpha_i$ are the dual variables\n",
    "- This is known as the **Representer Theorem** in kernel methods\n",
    "- The dual form is often more efficient when $d > n$ (more features than samples)\n",
    "\n",
    "**Mathematical Significance:**\n",
    "This result connects ridge regression to kernel methods and shows that the solution lies in the span of the training data points.\n",
    "\n",
    "## Question 9\n",
    "**True/False:** Solving the k-means objective with Lloyd's algorithm (shown in lecture) will always converge to the global optimum of the k-means objective.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is **False**. Lloyd's algorithm does **NOT** guarantee convergence to the global optimum. Here's the detailed explanation:\n",
    "\n",
    "**K-means Objective Function:**\n",
    "$$\\min_{\\{C_1, \\ldots, C_k\\}} \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "where $C_i$ are the clusters and $\\mu_i$ are the centroids.\n",
    "\n",
    "**Lloyd's Algorithm:**\n",
    "1. Initialize centroids randomly\n",
    "2. **Assignment Step**: Assign each point to nearest centroid\n",
    "3. **Update Step**: Update centroids as mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "**Why It's a Local Optimization Method:**\n",
    "\n",
    "1. **Non-Convex Objective**: The k-means objective is non-convex with many local minima\n",
    "2. **Greedy Updates**: Each step only improves the objective locally\n",
    "3. **Initialization Dependent**: Different starting points lead to different final solutions\n",
    "\n",
    "**Mathematical Example:**\n",
    "Consider 4 points in 1D: $[0, 1, 4, 5]$ with $k=2$:\n",
    "- **Global Optimum**: Clusters $[0,1]$ and $[4,5]$ with centroids at $0.5$ and $4.5$\n",
    "- **Local Optimum**: Clusters $[0]$ and $[1,4,5]$ with centroids at $0$ and $3.33$\n",
    "\n",
    "**Visual Analogy:**\n",
    "Think of the objective as a landscape with many valleys (local minima):\n",
    "- Lloyd's algorithm is like a ball rolling downhill\n",
    "- It will reach the bottom of the valley it starts in\n",
    "- But it might not reach the deepest valley (global minimum)\n",
    "\n",
    "**Convergence Properties:**\n",
    "- **Monotonic Convergence**: The objective never increases\n",
    "- **Finite Convergence**: Algorithm stops in finite steps\n",
    "- **Local Optimum**: Converges to a local minimum, not necessarily global\n",
    "\n",
    "**Practical Solutions:**\n",
    "1. **Multiple Initializations**: Run k-means many times with different random starts\n",
    "2. **K-means++**: Better initialization strategy\n",
    "3. **Global Optimization**: Use methods like genetic algorithms (computationally expensive)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Different runs can give different results\n",
    "- Need to run multiple times to find good solutions\n",
    "- Important to report the best result from multiple initializations\n",
    "\n",
    "## Question 10\n",
    "When might it be appropriate to use ridge regression instead of (unregularized) least square regression?\n",
    "\n",
    "(A) When the data is linearly separable.  \n",
    "(B) When the number of predictor variables is very large relative to the number of observations ($d > n$).  \n",
    "(C) When there are categorical or one-hot features in the input dataset.  \n",
    "(D) When the data is not standardized.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Ridge regression is most appropriate when **$d > n$** (more features than samples). Here's the detailed analysis:\n",
    "\n",
    "**The Problem with $d > n$:**\n",
    "\n",
    "1. **Underdetermined System**: When $d > n$, the system $Xw = y$ has more unknowns than equations\n",
    "2. **Infinite Solutions**: There are infinitely many solutions that perfectly fit the training data\n",
    "3. **Overfitting**: The model can memorize the training data but won't generalize\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "**Unregularized Least Squares:**\n",
    "$$\\min_w ||y - Xw||_2^2$$\n",
    "\n",
    "**Normal Equations:**\n",
    "$$X^T X w = X^T y$$\n",
    "\n",
    "**When $d > n$:**\n",
    "- $X^T X$ is singular (not invertible)\n",
    "- Multiple solutions exist\n",
    "- Solution is unstable\n",
    "\n",
    "**Ridge Regression Solution:**\n",
    "$$\\min_w ||y - Xw||_2^2 + \\lambda ||w||_2^2$$\n",
    "\n",
    "**Ridge Normal Equations:**\n",
    "$$(X^T X + \\lambda I) w = X^T y$$\n",
    "\n",
    "**Benefits of Ridge Regression:**\n",
    "\n",
    "1. **Regularization**: The $\\lambda ||w||_2^2$ term prevents overfitting\n",
    "2. **Stability**: $(X^T X + \\lambda I)$ is always invertible for $\\lambda > 0$\n",
    "3. **Shrinkage**: Coefficients are shrunk toward zero\n",
    "4. **Unique Solution**: Guarantees a unique, stable solution\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "- **Bias-Variance Tradeoff**: Ridge increases bias but reduces variance\n",
    "- **Shrinkage Effect**: $\\hat{w}_{\\text{ridge}} = \\frac{1}{1 + \\lambda} \\hat{w}_{\\text{OLS}}$ (for orthogonal features)\n",
    "- **Effective Degrees of Freedom**: $\\text{df}(\\lambda) = \\sum_{i=1}^d \\frac{d_i^2}{d_i^2 + \\lambda}$ where $d_i$ are singular values\n",
    "\n",
    "**When Ridge is Less Useful:**\n",
    "- $n \\gg d$: Standard least squares works well\n",
    "- Linear separability: Other methods might be better\n",
    "- Categorical features: Need different preprocessing\n",
    "\n",
    "**Practical Example:**\n",
    "- **Genomics**: $d = 20,000$ genes, $n = 100$ patients\n",
    "- **Text Classification**: $d = 10,000$ words, $n = 1,000$ documents\n",
    "- **Image Features**: $d = 1,000$ features, $n = 100$ images\n",
    "\n",
    "## Question 11\n",
    "**True/False:** For PCA, the objective function can equivalently be thought of as (1) variance-maximization or (2) reconstruction error-minimization.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** This is a fundamental result in PCA. The first principal component maximizes the variance of the projected data: $\\max_{||v||=1} \\text{Var}(Xv)$. Equivalently, it minimizes the reconstruction error: $\\min_{||v||=1} ||X - Xvv^T||_F^2$. These two objectives are mathematically equivalent - maximizing the variance of projections is the same as minimizing the squared distance from the data to its projection onto the principal component.\n",
    "\n",
    "## Question 12\n",
    "Which of the following activation functions can be used in the output layer of a neural network if we wish to predict the probabilities of $k$ classes $\\hat{p} = (p_1, p_2,..., p_k)$ such that sum of $\\hat{p}$ over all $k$ equals to 1? (Assume $k \\ge 2$.)\n",
    "\n",
    "(A) Tanh  \n",
    "(B) Leaky ReLU  \n",
    "(C) Sigmoid  \n",
    "(D) Softmax  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** Softmax is the appropriate activation function for multi-class classification because it outputs a probability distribution that sums to 1. The softmax function is defined as $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$, which ensures that all outputs are positive and sum to 1. Tanh and Leaky ReLU don't guarantee this property, and sigmoid is typically used for binary classification.\n",
    "\n",
    "## Question 13\n",
    "**True/False:** For decision tree algorithms, small perturbation in the training data can result in large differences in the resulting classifiers.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** Decision trees are highly sensitive to small changes in the training data because they make decisions based on threshold splits at each node. A small change in the data can cause a completely different split to be chosen at a node, which can cascade through the entire tree structure, leading to very different decision boundaries. This is why ensemble methods like random forests are often used to improve stability.\n",
    "\n",
    "## Question 14\n",
    "**True/False:** The unfairness with respect to race of a model trained on a dataset can be completely resolved by removing race as a feature.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Simply removing race as a feature does not eliminate bias because race can be correlated with other features in the dataset (proxy variables). For example, if race is correlated with zip code, income, or other demographic factors, the model can still learn discriminatory patterns through these proxy variables. True fairness requires more sophisticated approaches like fairness-aware training, data preprocessing, or post-processing techniques.\n",
    "\n",
    "## Question 15\n",
    "For the following code snippet on the bootstrap algorithm, determine whether it is correct or select the function that is possibly buggy.\n",
    "\n",
    "```python\n",
    "1 import random\n",
    "2\n",
    "3 def sample(data):\n",
    "4     sample = random.sample(data, len(data)) # samples len(data) elements without replacement\n",
    "5     return sample\n",
    "6\n",
    "7 def mean(numbers):\n",
    "8     return sum(numbers) / len(numbers)\n",
    "9\n",
    "10 def bootstrap(data, num_samples):\n",
    "11     samples = [sample(data) for _ in range(num_samples)]\n",
    "12     sample_means = [mean(sample) for sample in samples]\n",
    "13     return sample_means\n",
    "14\n",
    "15 data = [1, 2, 3, 4, 5]\n",
    "16 bootstrap_means = bootstrap(data, 10)\n",
    "```\n",
    "\n",
    "(A) Code is correct.  \n",
    "(B) sample function is buggy.  \n",
    "(C) mean function is buggy.  \n",
    "(D) bootstrap function is buggy (assuming sample function and mean function is correct).  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** The `sample` function is buggy because it uses `random.sample()` which samples without replacement, but bootstrap sampling should sample with replacement. Bootstrap samples should allow the same data point to appear multiple times in a single sample. The correct implementation should use `random.choices()` or `numpy.random.choice()` with `replace=True`.\n",
    "\n",
    "## Question 16\n",
    "**True/False:** The bootstrap method can be applied to other statistics, not just variance.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** The bootstrap method is a general resampling technique that can be used to estimate the sampling distribution of any statistic, not just variance. It can be applied to estimate confidence intervals, standard errors, bias, and other properties of estimators for means, medians, regression coefficients, and many other statistics.\n",
    "\n",
    "## Question 17\n",
    "**True/False:** The cluster centers that are calculated during each iteration of Lloyd's algorithm are always actual data points.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** In Lloyd's algorithm, cluster centers (centroids) are calculated as the mean of all data points assigned to that cluster. These centroids are typically not actual data points from the original dataset, but rather the arithmetic mean of the cluster members. Only in special cases (like when a cluster contains only one data point) would the centroid coincide with an actual data point.\n",
    "\n",
    "## Question 18\n",
    "What kind of method can be used to tune models and hyperparameter selection so as to optimize the bias-variance tradeoff?\n",
    "\n",
    "(A) Bootstrap.  \n",
    "(B) $k$-means.  \n",
    "(C) Cross validation.  \n",
    "(D) All of the above.  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** Cross-validation is specifically designed for model selection and hyperparameter tuning to optimize the bias-variance tradeoff. It provides an unbiased estimate of model performance on unseen data, allowing us to compare different model configurations. While bootstrap can be used for some model selection tasks, it's not the primary tool for hyperparameter tuning. K-means is a clustering algorithm, not a model selection method.\n",
    "\n",
    "## Question 19\n",
    "**True/False:** The expected error on unseen samples is at least the irreducible error.\n",
    "\n",
    "(A) True  \n",
    "(B) False  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** The irreducible error represents the inherent noise in the data that cannot be eliminated by any model. It sets a lower bound on the achievable error rate. The expected error on unseen samples can be decomposed as: $\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$. Since bias and variance are non-negative, the expected error must be at least as large as the irreducible error.\n",
    "\n",
    "## Question 20\n",
    "Which of the following methods would not help when a model suffers from high bias?\n",
    "\n",
    "(A) Add more input features.  \n",
    "(B) Standardizing the data (to have mean 0, variance 1).  \n",
    "(C) Decrease regularization.  \n",
    "(D) Increase the complexity of the hypothesis class.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** High bias indicates that the model is too simple to capture the underlying patterns in the data. Standardizing the data (scaling to mean 0, variance 1) doesn't change the model's capacity or complexity - it only changes the scale of the features. The other options directly address high bias: adding features increases model capacity, decreasing regularization allows the model to fit the data more closely, and increasing hypothesis class complexity gives the model more flexibility.\n",
    "\n",
    "## Question 21\n",
    "Which of the following would be the most appropriate loss function to use when training a neural network on a multi-class classification problem?\n",
    "\n",
    "(A) Mean Absolute Error  \n",
    "(B) Mean Squared Error  \n",
    "(C) Cross Entropy  \n",
    "(D) Hinge loss  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** Cross-entropy loss is the most appropriate for multi-class classification because it measures the difference between the predicted probability distribution and the true distribution. It's defined as $-\\sum_{i=1}^k y_i \\log(\\hat{y}_i)$ where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability. Mean Absolute Error and Mean Squared Error are typically used for regression problems, while hinge loss is used for binary classification with SVMs.\n",
    "\n",
    "## Question 22\n",
    "Which of the following does not increase the complexity of a neural network?\n",
    "\n",
    "(A) Adding more layers  \n",
    "(B) Increasing the hidden layer size  \n",
    "(C) Reducing the strength of the regularizer  \n",
    "(D) Reducing the learning rate  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** Reducing the learning rate affects the training process (how quickly the model learns) but doesn't change the model's capacity or complexity. The learning rate is a hyperparameter that controls the step size in gradient descent, not the model's representational power. Adding layers, increasing layer sizes, and reducing regularization all increase the model's capacity to fit complex patterns.\n",
    "\n",
    "## Question 23\n",
    "Which of the following is NOT an advantage of SVMs?\n",
    "\n",
    "(A) SVMs can guarantee that the solution is a global minimum.  \n",
    "(B) SVMs can be used for both linearly-separable and non-linearly-separable data.  \n",
    "(C) The SVM objective can be solved in closed form.  \n",
    "(D) SVMs can be combined with the kernel trick to learn feature mappings.  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** The SVM objective cannot be solved in closed form. It requires iterative optimization algorithms like Sequential Minimal Optimization (SMO) or gradient-based methods. The other statements are true: SVMs have a convex objective function that guarantees global optimality, they can handle both separable and non-separable data (with slack variables), and the kernel trick allows them to work in high-dimensional feature spaces.\n",
    "\n",
    "## Question 24\n",
    "Which of the following would be the most appropriate model for an image classification problem?\n",
    "\n",
    "(A) Neural network with fully-connected layers  \n",
    "(B) Neural network with convolutional layers  \n",
    "(C) Kernel SVM  \n",
    "(D) Random forest  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Convolutional Neural Networks (CNNs) are specifically designed for image classification because they can capture spatial relationships and local patterns in images through convolutional operations. They are translation-invariant and can learn hierarchical features from low-level edges to high-level semantic concepts. Fully-connected networks don't exploit spatial structure, kernel SVMs don't scale well to high-dimensional image data, and random forests lack the ability to learn complex spatial patterns.\n",
    "\n",
    "## Question 25\n",
    "Assume you train a neural network with SGD with a batch size of 10 on a dataset consisting of 500 samples. How many cumulative backward passes will your neural network perform on any given epoch?\n",
    "\n",
    "(A) 5000  \n",
    "(B) 500  \n",
    "(C) 50  \n",
    "(D) 10  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** In one epoch, the model processes all 500 samples. With a batch size of 10, the number of batches per epoch is $\\frac{500}{10} = 50$. Each batch requires one forward pass and one backward pass, so the total number of backward passes in one epoch is 50.\n",
    "\n",
    "![NN](./nn.png)\n",
    "\n",
    "**Figure 3:** This figure depicts a 2-layer neural network.\n",
    "\n",
    "## Question 26\n",
    "Assuming the neural network in figure 3 has no bias parameters, how many trainable parameters does it have?\n",
    "\n",
    "(A) 9  \n",
    "(B) 20  \n",
    "(C) 24  \n",
    "(D) 29  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Looking at the neural network in Figure 3, we can count the parameters: 2 input nodes connected to 2 hidden nodes = 4 weights, plus 2 hidden nodes connected to 1 output node = 2 weights, plus 1 bias connection to the output = 1 parameter. Total: 4 + 2 + 1 = 7 parameters. However, if we're counting all connections including the bias connections to hidden layers, we get: 2×2 + 2×1 + 2 + 1 = 4 + 2 + 2 + 1 = 9 parameters. The answer of 20 suggests a different interpretation of the network structure.\n",
    "\n",
    "## Question 27\n",
    "Which of the following is NOT a potential benefit of using ridge regression?\n",
    "\n",
    "(A) It can reduce the variance of the model.  \n",
    "(B) It can improve the interpretability of the model.  \n",
    "(C) It can help to reduce overfitting.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Ridge regression does not improve interpretability. In fact, it can make interpretation more difficult because it shrinks coefficients toward zero but doesn't set them exactly to zero. Ridge regression's benefits are reducing variance and preventing overfitting through regularization, but it doesn't provide feature selection like Lasso regression, which can improve interpretability by setting some coefficients to exactly zero.\n",
    "\n",
    "## Question 28\n",
    "How many parameters do we need to learn a mixture of two Gaussian distributions with N samples in $\\mathbb{R}^1$?\n",
    "\n",
    "(A) 2  \n",
    "(B) 4  \n",
    "(C) N x 2  \n",
    "(D) N  \n",
    "\n",
    "**Solution:** The answer is actually 5 (not an answer choice). Everyone received a point for this problem.\n",
    "\n",
    "**Explanation:** For a mixture of two Gaussian distributions in $\\mathbb{R}^1$, we need to learn: 2 means ($\\mu_1, \\mu_2$), 2 variances ($\\sigma_1^2, \\sigma_2^2$), and 1 mixing weight ($\\pi_1$, with $\\pi_2 = 1 - \\pi_1$). This gives a total of 5 parameters. The number of samples N doesn't affect the number of parameters in the model.\n",
    "\n",
    "## Question 29\n",
    "Which of the following statements is true about the bootstrap method?\n",
    "\n",
    "(A) The bootstrap estimator for prediction error offers better generalization than cross-validation.  \n",
    "(B) A bootstrap sample can be obtained from the empirical distribution of the data.  \n",
    "(C) All of the above.  \n",
    "(D) None of the above.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Statement (B) is true - bootstrap samples are drawn from the empirical distribution of the observed data. Statement (A) is false - cross-validation generally provides better estimates of prediction error than bootstrap because it uses non-overlapping training and validation sets, while bootstrap samples overlap significantly with the original data.\n",
    "\n",
    "## Question 30\n",
    "What does the generalization error of an SVM measure?\n",
    "\n",
    "(A) How far the hyperplane is from the support vectors.  \n",
    "(B) How accurately the SVM can predict outcomes for unseen data.  \n",
    "(C) The threshold amount of error in an SVM.  \n",
    "(D) The complexity of the decision boundary.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Generalization error measures how well a model performs on unseen data, which is the true test of a model's utility. For an SVM, this means how accurately it can classify new data points that weren't used during training. The other options describe different aspects: (A) describes the margin, (C) describes the slack variables, and (D) describes the model complexity.\n",
    "\n",
    "## Question 31\n",
    "**True/False:** Support vectors are the data points that lie closest to the decision boundary.\n",
    "\n",
    "(A) True.  \n",
    "(B) False.  \n",
    "\n",
    "**Solution:** The answer is (A) or (B) (both answers accepted due to ambiguity of the question).\n",
    "\n",
    "**Explanation:** This question is ambiguous because it depends on the context. In the case of linearly separable data, support vectors are the data points that lie exactly on the margin (at distance $1/||w||$ from the decision boundary). However, for non-separable data with slack variables, support vectors can include points that are misclassified or lie within the margin. The precise definition depends on whether we're considering the hard-margin or soft-margin SVM.\n",
    "\n",
    "## Question 32\n",
    "Which of the following is a potential advantage of using Lasso regression over unregularized linear regression?\n",
    "\n",
    "(A) It can decrease the bias of the model.  \n",
    "(B) It will always be more computationally efficient to train.  \n",
    "(C) It always produces the best results (in terms of test error).  \n",
    "(D) It can make the model more interpretable.  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** Lasso regression can improve interpretability through feature selection - it can set some coefficients to exactly zero, effectively removing those features from the model. This creates a sparse model that's easier to interpret. Lasso doesn't decrease bias (it typically increases it), isn't always more computationally efficient, and doesn't guarantee better test error (though it often helps with overfitting).\n",
    "\n",
    "## Question 33\n",
    "Consider a neural network with $L$ layers. How many forward passes through the entire network are needed in a run of the backpropagation algorithm?\n",
    "\n",
    "(A) $L$  \n",
    "(B) $L^2$  \n",
    "(C) 1  \n",
    "(D) 2  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** Backpropagation requires only one forward pass through the entire network to compute all the activations and outputs. This is followed by one backward pass to compute gradients. The forward pass computes $a^{(l)} = f(W^{(l)}a^{(l-1)} + b^{(l)})$ for each layer $l$, and the backward pass computes gradients using the chain rule.\n",
    "\n",
    "## Question 34\n",
    "Suppose you have trained an SVM with a quadratic kernel. After training the SVM, you correctly infer that your SVM model is underfitting. Which of the following option should you consider for (re)training this SVM next time to address the underfitting?\n",
    "\n",
    "(A) Increase the number of training data points.  \n",
    "(B) Decrease the number of training data points.  \n",
    "(C) Increase the degree of the kernel used (e.g., fit an SVM with a cubic kernel instead).  \n",
    "(D) Decrease the degree of a kernel used (e.g., fit a linear SVM instead).  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** Underfitting means the model is too simple to capture the underlying patterns in the data. To address this, we need to increase the model's complexity. Increasing the kernel degree (from quadratic to cubic) increases the model's capacity to learn more complex decision boundaries. The other options would either not help (A, B) or would make the problem worse by further reducing complexity (D).\n",
    "\n",
    "## Question 35\n",
    "Consider a fully connected layer with input size $M$, an offset, and output size $N$. What is the total number of parameters of this layer?\n",
    "\n",
    "(A) $M + N$  \n",
    "(B) $M^2 \\times N$  \n",
    "(C) $(M+1) \\times N$  \n",
    "(D) $M^2 \\times N^2$  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** A fully connected layer with input size $M$ and output size $N$ has a weight matrix of size $M \\times N$ and a bias vector of size $N$. Including the bias term, the total number of parameters is $M \\times N + N = (M+1) \\times N$. The \"+1\" accounts for the bias term that's added to each output neuron.\n",
    "\n",
    "## Question 36\n",
    "You have a batch of size $N$ 512 x 512 RGB images as your input. The input tensor your neural network has the shape $(N, 3, 512, 512)$. You pass your input through a convolutional layer like below: `Conv2D(in_channels=3, out_channels=32, kernel_size=9, stride=1, padding=1)` What is the shape of your output tensor?\n",
    "\n",
    "(A) $(N, 32, 512, 512)$  \n",
    "(B) $(N, 32, 506, 506)$  \n",
    "(C) $(N, 32, 505, 505)$  \n",
    "(D) $(N, 32, 253, 253)$  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** For a convolutional layer with kernel size $k$, stride $s$, and padding $p$, the output size is calculated as: $\\text{output\\_size} = \\frac{\\text{input\\_size} + 2p - k}{s} + 1$. With input size 512, kernel size 9, stride 1, and padding 1: $\\frac{512 + 2(1) - 9}{1} + 1 = \\frac{512 + 2 - 9}{1} + 1 = 505 + 1 = 506$. So the output shape is $(N, 32, 506, 506)$.\n",
    "\n",
    "## Question 37\n",
    "Which of the following sets is NOT convex?\n",
    "\n",
    "(A) $\\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid ||\\mathbf{x}||_2 < 1\\}$  \n",
    "(B) $\\{(x, y) \\in \\mathbb{R}^2 \\mid |x| > y\\}$  \n",
    "(C) $\\{(x, y) \\in \\mathbb{R}^2 \\mid x < y^2\\}$  \n",
    "(D) $\\{\\mathbf{x} \\in \\mathbb{R}^3 \\mid \\mathbf{x} \\cdot (1, 2, 3) \\geq 0\\}$  \n",
    "\n",
    "**Solution:** The answer is (C) or (B).\n",
    "\n",
    "**Explanation:** Set (C) is not convex because it's the region below a parabola, which is not convex. For example, the line segment between two points below the parabola may not lie entirely within the set. Set (B) is also not convex because it's the union of two half-planes that don't form a convex set. Sets (A) and (D) are convex - (A) is the interior of a circle and (D) is a half-space.\n",
    "\n",
    "## Question 38\n",
    "What is the correct order of a training step of a neural network?\n",
    "\n",
    "(A) compute loss $\\rightarrow$ forward pass $\\rightarrow$ backward pass  \n",
    "(B) backward pass $\\rightarrow$ compute loss $\\rightarrow$ forward pass  \n",
    "(C) forward pass $\\rightarrow$ compute loss $\\rightarrow$ backward pass  \n",
    "(D) backward pass $\\rightarrow$ forward pass $\\rightarrow$ compute loss  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** The correct order is: (1) Forward pass to compute predictions, (2) Compute loss to measure how well the predictions match the targets, (3) Backward pass to compute gradients of the loss with respect to the parameters. This order is necessary because the loss is needed to compute gradients, and the forward pass is needed to compute the loss.\n",
    "\n",
    "## Question 39\n",
    "Which of the following is true for random forest algorithm?\n",
    "\n",
    "(A) Random forests generally have low bias and low variance.  \n",
    "(B) The trees for the random forest are only trained on a subset of the training data.  \n",
    "(C) Random forests can be used for both classification and regression  \n",
    "(D) All of the above.  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** All statements are true. Random forests use ensemble learning to reduce both bias and variance by averaging predictions from multiple trees. Each tree is trained on a bootstrap sample (subset) of the training data, and random forests can indeed be used for both classification and regression tasks. The combination of bagging and feature randomization helps achieve low bias and low variance.\n",
    "\n",
    "## Question 40\n",
    "Which of the following functions is convex on the given interval?\n",
    "\n",
    "(A) $y = -|x|$ on $[0,2]$  \n",
    "(B) $y = x^3$ on $[-1,1]$  \n",
    "(C) $y = \\max(-x^2, -x)$ on $[0,2]$  \n",
    "(D) $y = \\cos(x)$ on $[0, \\pi]$.  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** On the interval $[0,2]$, the function $y = -|x|$ becomes $y = -x$, which is linear and therefore convex. Function (B) $y = x^3$ is concave on $[-1,0]$ and convex on $[0,1]$, so it's not convex on the entire interval. Function (C) is the maximum of two concave functions, which is not necessarily convex. Function (D) $y = \\cos(x)$ is concave on $[0,\\pi]$.\n",
    "\n",
    "## Question 41\n",
    "How does lasso regression differ from ridge regression?\n",
    "\n",
    "(A) Lasso regression adds a penalty term to the regression equation, while ridge regression does not.  \n",
    "(B) Lasso regression can achieve feature selection (by setting feature weights to exactly zero), while ridge regression can not.  \n",
    "(C) Lasso regression is a type of unregularized linear regression, while ridge regression is regularized.  \n",
    "(D) Lasso regression is a type of regularized linear regression, while ridge regression is unregularized.  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Both Lasso and Ridge regression add penalty terms, but they use different norms. Lasso uses $L_1$ regularization ($\\lambda ||w||_1$) which can set coefficients exactly to zero, enabling feature selection. Ridge uses $L_2$ regularization ($\\lambda ||w||_2^2$) which shrinks coefficients toward zero but doesn't set them exactly to zero. Both are types of regularized linear regression.\n",
    "\n",
    "## Question 42\n",
    "In which of the following situations would it be appropriate to use logistic regression?\n",
    "\n",
    "(A) Predicting whether a credit card transaction is fraudulent based on some attributes.  \n",
    "(B) Predicting the number of cars that will pass through a particular intersection during rush hour.  \n",
    "(C) Predicting the annual income of a person based on their education and employment history.  \n",
    "(D) Predicting the price of a stock based on historical data.  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** Logistic regression is designed for binary classification problems where the output is a probability between 0 and 1. Predicting fraud (fraudulent or not) is a binary classification task. The other options are regression problems where the output is a continuous value: number of cars, annual income, and stock price are all continuous variables that require regression models.\n",
    "\n",
    "## Question 43\n",
    "Which of the following statements about principal component analysis (PCA) is false?\n",
    "\n",
    "(A) All principal components are always orthogonal to each other.  \n",
    "(B) Before using PCA, it's important to preprocess data by demeaning the data matrix.  \n",
    "(C) The first $q$ principal components are the first $q$ eigenvectors of the demeaned data matrix  \n",
    "(D) PCA produces a linear transformation of the data.  \n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "**Explanation:** Statement (C) is false. The first $q$ principal components are the first $q$ eigenvectors of the covariance matrix (or equivalently, the right singular vectors of the centered data matrix), not the eigenvectors of the demeaned data matrix itself. The other statements are true: principal components are orthogonal, data should be centered before PCA, and PCA produces a linear transformation.\n",
    "\n",
    "## Question 44\n",
    "Which of the following is NOT a convex optimization problem?\n",
    "\n",
    "(A) Logistic regression  \n",
    "(B) neural network training  \n",
    "(C) Gaussian kernel SVM  \n",
    "(D) minimizing least squares with polynomial features  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Neural network training is generally not a convex optimization problem because the loss function is non-convex with respect to the weights. This is due to the non-linear activation functions and the multi-layer structure. The other options are convex: logistic regression has a convex loss function, kernel SVM has a convex quadratic programming objective, and least squares with polynomial features is convex.\n",
    "\n",
    "## Question 45\n",
    "Which of the following facts can lead to misleading correlational statistics in criminal justice datasets?\n",
    "\n",
    "(A) Different reported crime rates across different neighborhoods.  \n",
    "(B) Different wrongful arrest rates across different demographic groups.  \n",
    "(C) Difference in local laws, i.e., different laws regarding indoor and outdoor drug sales in Seattle  \n",
    "(D) All of the above.  \n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** All of these factors can lead to misleading correlations in criminal justice data. Different reporting rates can create artificial patterns, differential arrest rates can introduce bias, and varying local laws can affect what behaviors are classified as crimes. These factors can create spurious correlations that don't reflect actual underlying relationships, making it difficult to draw valid conclusions from the data.\n",
    "\n",
    "## Question 46\n",
    "Which of the following functions is the logistic loss for label $y = +1$?\n",
    "\n",
    "![Loss](./loss.png)\n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "**Explanation:** The logistic loss for $y = +1$ is defined as $-\\log(\\frac{1}{1 + e^{-z}}) = \\log(1 + e^{-z})$, where $z$ is the model's output. This function is convex, approaches 0 as $z \\rightarrow \\infty$, and approaches $\\infty$ as $z \\rightarrow -\\infty$. Looking at the plot, function (D) matches this behavior.\n",
    "\n",
    "## Question 47\n",
    "Which of the following methods would **not** help when a model suffers from high variance?\n",
    "\n",
    "(A) Reduce training data.  \n",
    "(B) Decrease model size.  \n",
    "(C) Increase the amount of regularization.  \n",
    "(D) Perform feature selection.  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** Reducing training data would make the high variance problem worse, not better. With less data, the model has fewer examples to learn from, making it more likely to overfit to the limited training data. The other options all help reduce variance: decreasing model size reduces capacity, increasing regularization prevents overfitting, and feature selection reduces the number of parameters.\n",
    "\n",
    "## Question 48\n",
    "In linear regression, the loss function is $L(\\alpha) = ||y - K\\alpha||_2^2 + \\lambda\\alpha^T K\\alpha$, where the kernel matrix $K$ is given by $K_{ij} = \\langle\\phi(x_i), \\phi(x_j)\\rangle$ for a kernel map $\\phi$, inner product $\\langle\\cdot, \\cdot\\rangle$, and data samples $x_i, x_j \\in \\mathbb{R}^d$. What is the closed form solution for $\\alpha$ that minimizes $L(\\alpha)$?\n",
    "\n",
    "(A) $(K^T K + \\lambda I)^{-1} K^T y$  \n",
    "(B) $(K + \\lambda I)^{-1} y$  \n",
    "(C) $K^T (K + \\lambda I)^{-1} y$  \n",
    "(D) $(K + \\lambda I)^{-1} K^T y$  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Taking the derivative of $L(\\alpha)$ with respect to $\\alpha$ and setting it to zero: $\\frac{\\partial L}{\\partial \\alpha} = -2K^T(y - K\\alpha) + 2\\lambda K\\alpha = 0$. Rearranging: $K^T y = K^T K\\alpha + \\lambda K\\alpha = K(K\\alpha + \\lambda\\alpha) = K(K + \\lambda I)\\alpha$. Since $K$ is symmetric, this gives $(K + \\lambda I)\\alpha = y$, so $\\alpha = (K + \\lambda I)^{-1} y$.\n",
    "\n",
    "![Figure 4](./figure_4.png)\n",
    "\n",
    "**Figure 4:** This figure depicts a neural network diagram with input nodes $x_1$ and $x_2$, hidden layer nodes $z_1$ and $z_2$, a bias node $b$, and an output node $\\hat{y}$. Connections are shown from $x_1$ to $z_1$ and $z_2$, from $x_2$ to $z_1$ and $z_2$, and from $z_1$, $z_2$, and $b$ to $\\hat{y}$.\n",
    "\n",
    "Use this figure to answer the following two questions.\n",
    "\n",
    "## Question 49\n",
    "Consider the following equations:\n",
    "\n",
    "(1) $z_1(x_1,x_2) = (x_1)^2 + 4x_2$  \n",
    "(2) $z_2(x_1,x_2) = (x_1+x_2)^2$  \n",
    "(3) $\\hat{y}(z_1, z_2, b) = b + z_1 \\cdot z_2$  \n",
    "\n",
    "which can be combined to form the network shown in Figure 4. What is the formula for $\\frac{\\partial \\hat{y}}{\\partial x_1}$?\n",
    "\n",
    "(A) $4x_1$  \n",
    "(B) $2(x_1+x_2) + 2z_1x_2$  \n",
    "(C) $2z_1x_1 + 2z_2(x_1+2x_2)$  \n",
    "(D) $2z_2x_1 + 2z_1(x_1+x_2)$  \n",
    "\n",
    "**Solution:** The answer is (D). (Note that the answer choices were changed during the exam. The original answer choices are not shown here.)\n",
    "\n",
    "**Explanation:** Using the chain rule, $\\frac{\\partial \\hat{y}}{\\partial x_1} = \\frac{\\partial \\hat{y}}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_1} + \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_1}$. We have: $\\frac{\\partial \\hat{y}}{\\partial z_1} = z_2$, $\\frac{\\partial \\hat{y}}{\\partial z_2} = z_1$, $\\frac{\\partial z_1}{\\partial x_1} = 2x_1$, and $\\frac{\\partial z_2}{\\partial x_1} = 2(x_1+x_2)$. Substituting: $\\frac{\\partial \\hat{y}}{\\partial x_1} = z_2 \\cdot 2x_1 + z_1 \\cdot 2(x_1+x_2) = 2z_2x_1 + 2z_1(x_1+x_2)$.\n",
    "\n",
    "## Question 50\n",
    "If $x_1 = 2$ and $x_2 = -1$, then what is the value of $\\frac{\\partial \\hat{y}}{\\partial x_2}$?\n",
    "\n",
    "(A) 0  \n",
    "(B) 4  \n",
    "(C) 2  \n",
    "(D) 1  \n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "**Explanation:** Using the chain rule, $\\frac{\\partial \\hat{y}}{\\partial x_2} = \\frac{\\partial \\hat{y}}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_2} + \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_2}$. We have: $\\frac{\\partial z_1}{\\partial x_2} = 4$ and $\\frac{\\partial z_2}{\\partial x_2} = 2(x_1+x_2) = 2(2+(-1)) = 2$. With $x_1 = 2$ and $x_2 = -1$, we get $z_1 = 2^2 + 4(-1) = 4 - 4 = 0$ and $z_2 = (2+(-1))^2 = 1^2 = 1$. So $\\frac{\\partial \\hat{y}}{\\partial x_2} = z_2 \\cdot 4 + z_1 \\cdot 2 = 1 \\cdot 4 + 0 \\cdot 2 = 4$.\n",
    "\n",
    "## Question 51\n",
    "When reviewing the grade for an assignment in CSE 446, TAs found out those grades follows a gamma distribution. The probability density function $f(x)$ for the gamma distribution with parameters $k$ and $\\theta$ is\n",
    "\n",
    "$f(x; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}$, $\\quad \\Gamma(x) = (x - 1)!$.\n",
    "\n",
    "What is the Maximum Likelihood Estimator (MLE) for the parameter $\\theta$ in terms of the number of students $n$, the student grades $x_1, \\dots, x_n$, and the parameter $k$?\n",
    "\n",
    "(A) $\\frac{1}{kn} \\sum_{i=1}^{n} x_i$  \n",
    "(B) $\\frac{n}{(k-1)!} \\sum_{i=1}^{n} x_i e^{-\\frac{x_i}{k}}$  \n",
    "(C) $\\ln(\\frac{1}{n} \\sum_{i=1}^{n} x_i) - n(k-1)!$  \n",
    "(D) $\\frac{\\ln(k)-(k-1)!}{\\frac{1}{k}}$  \n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "**Explanation:** To find the MLE for $\\theta$, we maximize the log-likelihood function. The log-likelihood is: $\\ell(\\theta) = \\sum_{i=1}^n \\log f(x_i; k, \\theta) = \\sum_{i=1}^n \\left[-\\log(\\Gamma(k)) - k\\log(\\theta) + (k-1)\\log(x_i) - \\frac{x_i}{\\theta}\\right]$. Taking the derivative with respect to $\\theta$ and setting to zero: $\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{nk}{\\theta} + \\frac{\\sum_{i=1}^n x_i}{\\theta^2} = 0$. Solving: $\\frac{nk}{\\theta} = \\frac{\\sum_{i=1}^n x_i}{\\theta^2}$, so $\\theta = \\frac{1}{kn} \\sum_{i=1}^n x_i$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
