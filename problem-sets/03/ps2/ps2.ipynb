{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e165055-1d82-4845-9447-e4080af97da7",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "\n",
    "## Short Answer and “True or False” Conceptual questions\n",
    "\n",
    "**A1.** The answers to these questions should be answerable without\n",
    "referring to external materials. Briefly justify your answers with a few\n",
    "words.\n",
    "\n",
    "1.  In your own words, describe what bias and variance are? What is\n",
    "    bias-variance tradeoff?\n",
    "\n",
    "2.  What typically happens to bias and variance when the model\n",
    "    complexity increases/decreases?\n",
    "\n",
    "3.  True or False: A learning algorithm will always generalize better if\n",
    "    we use fewer features to represent our data.\n",
    "\n",
    "4.  True or False: Hyperparameters should be tuned on the test set.\n",
    "    Explain your choice and detail a procedure for hyperparameter\n",
    "    tuning.\n",
    "\n",
    "5.  True or False: The training error of a function on the training set\n",
    "    provides an overestimate of the true error of that function.\n",
    "\n",
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**A2.** You’re the Reign FC manager, and the team is five games into its\n",
    "2021 season. The number of goals scored by the team in each game so far\n",
    "are given below:\n",
    "\n",
    "$[2, 4, 6, 0, 1]$.\n",
    "\n",
    "Let’s call these scores $x_1, \\dots, x_5$. Based on your (assumed iid)\n",
    "data, you’d like to build a model to understand how many goals the Reign\n",
    "are likely to score in their next game. You decide to model the number\n",
    "of goals scored per game using a Poisson distribution. Recall that the\n",
    "Poisson distribution with parameter $\\lambda$ assigns every non-negative\n",
    "integer $x = 0, 1, 2, \\dots$ a probability given by\n",
    "\n",
    "$$\\text{Poi}(x|\\lambda) = e^{-\\lambda}\\frac{\\lambda^x}{x!}.$$\n",
    "\n",
    "1.  Derive an expression for the maximum-likelihood estimate of the\n",
    "    parameter $\\lambda$ governing the Poisson distribution in terms of\n",
    "    goal counts for the first $n$ games: $x_1, \\dots, x_n$. (Hint:\n",
    "    remember that the log of the likelihood has the same maximizer as\n",
    "    the likelihood function itself.)\n",
    "\n",
    "2.  Give a numerical estimate of $\\lambda$ after the first five games.\n",
    "    Given this $\\lambda$, what is the probability that the Reign score 6\n",
    "    goals in their next game?\n",
    "\n",
    "3.  Suppose the Reign score 8 goals in their 6th game. Give an updated\n",
    "    numerical estimate of $\\lambda$ after six games and compute the\n",
    "    probability that the Reign score 6 goals in their 7th game.\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "**Relevant Files¹** \\* `polyreg.py` \\* `linreg_closedform.py` \\*\n",
    "`plot_polyreg_univariate.py` \\* `plot_polyreg_learningCurve.py`\n",
    "\n",
    "**A3.** Recall that polynomial regression learns a function\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\dots + \\theta_dx^d$,\n",
    "where $d$ represents the polynomial’s highest degree. We can\n",
    "equivalently write this in the form of a linear model with $d$ features\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1\\phi_1(x) + \\theta_2\\phi_2(x) + \\dots + \\theta_d\\phi_d(x) \\quad (1)$$\n",
    "using the basis expansion that $\\phi_j(x) = x^j$. Notice that, with this\n",
    "basis expansion, we obtain a linear model where the features are various\n",
    "powers of the single univariate $x$. We’re still solving a linear\n",
    "regression problem, but are fitting a polynomial function of the input.\n",
    "\n",
    "Implement regularized polynomial regression in `polyreg.py`. You may\n",
    "implement it however you like, using gradient descent or a closed-form\n",
    "solution. However, we recommend the closed-form solution since the data\n",
    "sets are small; for this reason, we’ve included an example closed-form\n",
    "implementation of linear regression in `linreg_closedform.py` (you are\n",
    "welcome to build upon this implementation, but make CERTAIN you\n",
    "understand it, since you’ll need to change several lines of it). You are\n",
    "also welcome to build upon your implementation from the previous\n",
    "assignment, but you must follow the API below. Note that all matrices\n",
    "are actually 2D numpy arrays in the implementation.\n",
    "\n",
    "- `__init__(degree=1, regLambda=1E-8)`: constructor with arguments of\n",
    "  $d$ and $\\lambda$\n",
    "- `fit(X,Y)`: method to train the polynomial regression model\n",
    "- `predict(X)`: method to use the trained polynomial regression model\n",
    "  for prediction\n",
    "- `polyfeatures(X, degree)`: expands the given $n \\times 1$ matrix $X$\n",
    "  into an $n \\times d$ matrix of polynomial features of degree $d$. Note\n",
    "  that the returned matrix will not include the zero-th power.\n",
    "\n",
    "Note that the `polyfeatures(X, degree)` function maps the original\n",
    "univariate data into its higher order powers. Specifically, $X$ will be\n",
    "an $n \\times 1$ matrix ($X \\in \\mathbb{R}^{n \\times 1}$) and this\n",
    "function will return the polynomial expansion of this data, a\n",
    "$n \\times d$ matrix. Note that this function will **not** add in the\n",
    "zero-th order feature (i.e., $x_0 = 1$). You should add the $x_0$\n",
    "feature separately, outside of this function, before training the model.\n",
    "\n",
    "By not including the $x_0$ column in the matrix `polyfeatures()`, this\n",
    "allows the `polyfeatures` function to be more general, so it could be\n",
    "applied to multi-variate data as well. (If it did add the $x_0$ feature,\n",
    "we’d end up with multiple columns of 1’s for multivariate data.)\n",
    "\n",
    "Also, notice that the resulting features will be badly scaled if we use\n",
    "them in raw form. For example, with a polynomial of degree $d = 8$ and\n",
    "$x = 20$, the basis expansion yields $x^1 = 20$ while\n",
    "$x^8 = 2.56 \\times 10^{10}$ – an absolutely huge difference in range.\n",
    "Consequently, we will need to standardize the data before solving linear\n",
    "regression. Standardize the data in `fit()` after you perform the\n",
    "polynomial feature expansion. You’ll need to apply the same\n",
    "standardization transformation in `predict()` before you apply it to new\n",
    "data.\n",
    "\n",
    "![Polynomial Regression](./polynomial-regression.png)\n",
    "\n",
    "**Figure 1: Fit of polynomial regression with $\\lambda = 0$ and\n",
    "$d = 8$** The figure displays a 2D plot titled “PolyRegression with d =\n",
    "8”. The x-axis ranges from 0 to 10, and the y-axis ranges from 2.0 to\n",
    "5.0. Several red ‘x’ marks are scattered across the plot, representing\n",
    "data points. A blue curve, representing the fitted polynomial regression\n",
    "model, passes through or near these data points. The curve shows\n",
    "significant oscillations, indicating a high degree of fit to the\n",
    "training data, which might suggest overfitting given the context of\n",
    "$\\lambda = 0$ (no regularization) and $d = 8$ (high polynomial degree).\n",
    "\n",
    "Run `plot_polyreg_univariate.py` to test your implementation, which will\n",
    "plot the learned function. In this case, the script fits a polynomial of\n",
    "degree $d = 8$ with no regularization $\\lambda = 0$. From the plot, we\n",
    "see that the function fits the data well, but will not generalize well\n",
    "to new data points. Try increasing the amount of regularization, and in\n",
    "1-2 sentences, describe the resulting effect on the function (you may\n",
    "also provide an additional plot to support your analysis).\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "¹Bold text indicates files or functions that you will need to complete;\n",
    "you should not need to modify any of the other files.\n",
    "\n",
    "## Learning Curves and Bias-Variance Tradeoff\n",
    "\n",
    "**A4.** In this problem we will examine the bias-variance tradeoff\n",
    "through learning curves. Learning curves provide a valuable mechanism\n",
    "for evaluating the bias-variance tradeoff. Implement the\n",
    "`learningCurve()` function in `polyreg.py` to compute the learning\n",
    "curves for a given training/test set. The\n",
    "`learningCurve(Xtrain, ytrain, Xtest, ytest, degree, regLambda)`\n",
    "function should take in the training data (`Xtrain`, `ytrain`), the\n",
    "testing data (`Xtest`, `ytest`), and values for the polynomial degree\n",
    "$d$ and regularization parameter $\\lambda$. The function should return\n",
    "two arrays, `errorTrain` (the array of training errors) and `errorTest`\n",
    "(the array of testing errors). The $i^{th}$ index (start from 0) of each\n",
    "array should return the training error (or testing error) for learning\n",
    "with $i + 1$ training instances. Note that the $0^{th}$ index actually\n",
    "won’t matter, since we typically start displaying the learning curves\n",
    "with two or more instances.\n",
    "\n",
    "When computing the learning curves, you should learn on `Xtrain[0:i]`\n",
    "for $i = 1, \\dots, \\text{numInstances(Xtrain)}$, each time computing the\n",
    "testing error over the **entire** test set. There is no need to shuffle\n",
    "the training data, or to average the error over multiple trials – just\n",
    "produce the learning curves for the given training/testing sets with the\n",
    "instances in their given order. Recall that the error for regression\n",
    "problems is given by\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} (h_{\\theta}(x_i) - y_i)^2. \\quad (2)$$\n",
    "\n",
    "Once the function is written to compute the learning curves, run the\n",
    "`plot_polyreg_learningCurve.py` script to plot the learning curves for\n",
    "various values of $\\lambda$ and $d$. You should see plots similar to the\n",
    "following: Notice the following:\n",
    "\n",
    "- The y-axis is using a log-scale and the ranges of the y-scale are all\n",
    "  different for the plots. The dashed black line indicates the $y = 1$\n",
    "  line as a point of reference between the plots.\n",
    "- The plot of the unregularized model with $d = 1$ shows poor training\n",
    "  error, indicating a high bias (i.e., it is a standard univariate\n",
    "  linear regression fit).\n",
    "- The plot of the (almost) unregularized model ($\\lambda = 10^{-6}$)\n",
    "  with $d = 8$ shows that the training error is low, but that the\n",
    "  testing error is high. There is a huge gap between the training and\n",
    "  testing errors caused by the model overfitting the training data,\n",
    "  indicating a high variance problem.\n",
    "- As the regularization parameter increases (e.g., $\\lambda = 1$) with\n",
    "  $d = 8$, we see that the gap between the training and testing error\n",
    "  narrows, with both the training and testing errors converging to a low\n",
    "  value. We can see that the model fits the data well and generalizes\n",
    "  well, and therefore does not have either a high bias or a high\n",
    "  variance problem. Effectively, it has a good tradeoff between bias and\n",
    "  variance.\n",
    "- Once the regularization parameter is too high ($\\lambda = 100$), we\n",
    "  see that the training and testing errors are once again high,\n",
    "  indicating a poor fit. Effectively, there is too much regularization,\n",
    "  resulting in high bias.\n",
    "\n",
    "Create plots for the same values of $d$ and $\\lambda$ shown here. Make\n",
    "absolutely certain that you understand these observations, and how they\n",
    "relate to the learning curve plots. In practice, we can choose the value\n",
    "for $\\lambda$ via cross-validation to achieve the best bias-variance\n",
    "tradeoff.\n",
    "\n",
    "Plots:\n",
    "\n",
    "![Plots](./sample-plots.png)\n",
    "\n",
    "## Ridge Regression on MNIST\n",
    "\n",
    "**A5.** In this problem we will implement a regularized least squares\n",
    "classifier for the MNIST data set. The task is to classify handwritten\n",
    "images of numbers between 0 to 9.\n",
    "\n",
    "You are **NOT** allowed to use any of the pre-built classifiers in\n",
    "`sklearn`. Feel free to use any method from `numpy` or `scipy`.\n",
    "**Remember**: if you are inverting a matrix in your code, you are\n",
    "probably doing something wrong (Hint: look at `scipy.linalg.solve`).\n",
    "\n",
    "Each example has features $x_i \\in \\mathbb{R}^d$ (with\n",
    "$d = 28*28 = 784$) and label $z_j \\in \\{0,...,9\\}$. You can visualize a\n",
    "single example $x_i$ with `imshow` after reshaping it to its original\n",
    "$28 \\times 28$ image shape (and noting that the label $z_j$ is\n",
    "accurate). We wish to learn a predictor $\\hat{f}$ that takes as input a\n",
    "vector in $\\mathbb{R}^d$ and outputs an index in $\\{0,...,9\\}$. We\n",
    "define our training and testing classification error on a predictor $f$\n",
    "as\n",
    "\n",
    "$$\\hat{\\epsilon}_{\\text{train}}(f) = \\frac{1}{N_{\\text{train}}} \\sum_{(x,z) \\in \\text{Training Set}} \\mathbf{1}\\{f(x) \\neq z\\}$$\n",
    "\n",
    "$$\\hat{\\epsilon}_{\\text{test}}(f) = \\frac{1}{N_{\\text{test}}} \\sum_{(x,z) \\in \\text{Test Set}} \\mathbf{1}\\{f(x) \\neq z\\}$$\n",
    "\n",
    "We will use one-hot encoding of the labels: for each observation\n",
    "$(x, z)$, the original label $z \\in \\{0,...,9\\}$ is mapped to the\n",
    "standard basis vector $e_{z+1}$ where $e_i$ is a vector of size $k$\n",
    "containing all zeros except for a 1 in the $i^{\\text{th}}$ position\n",
    "(positions in these vectors are indexed starting at one, hence the\n",
    "$z + 1$ offset for the digit labels). We adopt the notation where we\n",
    "have $n$ data points in our training objective with features\n",
    "$x_i \\in \\mathbb{R}^d$ and label one-hot encoded as $y_i \\in \\{0,1\\}^k$.\n",
    "Here, $k = 10$ since there are 10 digits.\n",
    "\n",
    "**a.** In this problem, we will choose a linear classifier to minimize\n",
    "the regularized least squares objective:\n",
    "$$\\hat{W} = \\operatorname{argmin}_{W \\in \\mathbb{R}^{d \\times k}} \\sum_{i=1}^{n} \\|W^T x_i - y_i\\|^2 + \\lambda \\|W\\|_F^2$$\n",
    "Note that $\\|W\\|_F$ corresponds to the Frobenius norm of $W$, i.e.,\n",
    "$\\|W\\|_F^2 = \\sum_{i=1}^{d} \\sum_{j=1}^{k} W_{i,j}^2$. To classify a\n",
    "point $x_i$, we will use the rule\n",
    "$\\operatorname{argmax}_{j=0,...,9} e_{j+1}^T \\hat{W}^T x_i$. Note that\n",
    "if $W = [w_1 \\quad ... \\quad w_k]$ then\n",
    "$$\\sum_{i=1}^{n} \\|W^T x_i - y_i\\|^2 + \\lambda \\|W\\|_F^2 = \\sum_{j=1}^{k} \\left[ \\sum_{i=1}^{n} (e_j^T W^T x_i - e_j^T y_i)^2 + \\lambda \\|W e_j\\|^2 \\right]$$\n",
    "$$= \\sum_{j=1}^{k} \\left[ \\sum_{i=1}^{n} (w_j^T x_i - e_j^T y_i)^2 + \\lambda \\|w_j\\|^2 \\right]$$\n",
    "$$= \\sum_{j=1}^{k} \\left[ \\|X w_j - Y e_j\\|^2 + \\lambda \\|w_j\\|^2 \\right]$$\n",
    "where $X = [x_1 \\quad ... \\quad x_n]^T \\in \\mathbb{R}^{n \\times d}$ and\n",
    "$Y = [y_1 \\quad ... \\quad y_n]^T \\in \\mathbb{R}^{n \\times k}$. Show that\n",
    "$$\\hat{W} = (X^T X + \\lambda I)^{-1} X^T Y$$\n",
    "\n",
    "**b.** \\* Implement a function `train` that takes as input\n",
    "$X \\in \\mathbb{R}^{n \\times d}$, $Y \\in \\{0,1\\}^{n \\times k}$,\n",
    "$\\lambda > 0$ and returns $\\hat{W} \\in \\mathbb{R}^{d \\times k}$. \\*\n",
    "Implement a function `one_hot` that takes as input\n",
    "$Y \\in \\{0,..., k-1\\}^n$, and returns $Y \\in \\{0,1\\}^{n \\times k}$. \\*\n",
    "Implement a function `predict` that takes as input\n",
    "$\\hat{W} \\in \\mathbb{R}^{d \\times k}$, $X' \\in \\mathbb{R}^{m \\times d}$\n",
    "and returns an $m$-length vector with the $i$th entry equal to\n",
    "$\\operatorname{argmax}_{j=0,...,9} e_j^T \\hat{W}^T x_i'$ where\n",
    "$x_i' \\in \\mathbb{R}^d$ is a column vector representing the $i$th\n",
    "example from $X'$. \\* Using the functions you coded above, train a model\n",
    "to estimate $\\hat{W}$ on the MNIST training data with\n",
    "$\\lambda = 10^{-4}$, and make label predictions on the test data. This\n",
    "behavior is implemented in main function provided in zip file. **What is\n",
    "the training and testing error?** Note that they should both be about\n",
    "15%."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
