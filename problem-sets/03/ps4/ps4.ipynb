{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n",
    "## Conceptual Questions\n",
    "\n",
    "**A1.** The answers to these questions should be answerable without\n",
    "referring to external materials. Briefly justify your answers with a few\n",
    "words.\n",
    "\n",
    "1.  True or False: Training deep neural networks requires minimizing a\n",
    "    convex loss function, and therefore gradient descent will provide\n",
    "    the best result.\n",
    "\n",
    "2.  True or False: It is a good practice to initialize all weights to\n",
    "    zero when training a deep neural network.\n",
    "\n",
    "3.  True or False: We use non-linear activation functions in a neural\n",
    "    network’s hidden layers so that the network learns non-linear\n",
    "    decision boundaries.\n",
    "\n",
    "4.  True or False: Given a neural network, the time complexity of the\n",
    "    backward pass step in the backpropagation algorithm can be\n",
    "    prohibitively larger compared to the relatively low time complexity\n",
    "    of the forward pass step.\n",
    "\n",
    "5.  True or False: Neural Networks are the most extensible model and\n",
    "    therefore the best choice for any circumstance.\n",
    "\n",
    "## Kernels\n",
    "\n",
    "**A2.** Suppose that our inputs $x$ are one-dimensional and that our\n",
    "feature map is infinite-dimensional: $\\phi(x)$ is a vector whose $i$th\n",
    "component is: $$\\frac{1}{\\sqrt{i!}}e^{-x^2/2}x^i$$ for all nonnegative\n",
    "integers $i$. (Thus, $\\phi$ is an infinite-dimensional vector.) Show\n",
    "that $K(x,x') = e^{-\\frac{(x-x')^2}{2}}$ is a kernel function for this\n",
    "feature map, i.e., $$\\phi(x) \\cdot \\phi(x') = e^{-\\frac{(x-x')^2}{2}}$$\n",
    "Hint: Use the Taylor expansion of $z \\mapsto e^z$. (This is the one\n",
    "dimensional version of the Gaussian (RBF) kernel).\n",
    "\n",
    "## Kernel Ridge Regression\n",
    "\n",
    "**A3.** This problem will get you familiar with kernel ridge regression\n",
    "using the polynomial and RBF kernels. First, let’s generate some data.\n",
    "Let $n = 30$ and $f_*(x) = 6 \\sin(\\pi x) \\cos(4\\pi x^2)$. For\n",
    "$i = 1,...,n$ let each $x_i$ be drawn uniformly at random from $[0, 1]$,\n",
    "and let $y_i = f_*(x_i) + \\epsilon_i$ where $\\epsilon_i \\sim N(0, 1)$.\n",
    "For any function $f$, the true error and the train error are\n",
    "respectively defined as:\n",
    "$$\\mathcal{E}_{\\text{true}}(f) = \\mathbb{E}_{X,Y} \\left[(f(X) - Y)^2\\right], \\quad \\hat{\\mathcal{E}}_{\\text{train}}(f) = \\frac{1}{n} \\sum_{i=1}^n (f(x_i) - y_i)^2.$$\n",
    "Now, our goal is, using kernel ridge regression, to construct a\n",
    "predictor:\n",
    "$$\\hat{\\alpha} = \\operatorname{argmin}_{\\alpha} \\|K\\alpha - y\\|_2^2 + \\lambda \\alpha^T K \\alpha, \\quad \\hat{f}(x) = \\sum_{i=1}^n \\hat{\\alpha}_i k(x_i, x)$$\n",
    "where $K \\in \\mathbb{R}^{n \\times n}$ is the kernel matrix such that\n",
    "$K_{i,j} = k(x_i, x_j)$, and $\\lambda \\ge 0$ is the regularization\n",
    "constant.\n",
    "\n",
    "1.  Use leave-one-out cross-validation to find optimal $\\lambda$ and\n",
    "    hyperparameter settings for the following two kernels:\n",
    "\n",
    "- Polynomial kernel: $k_{poly}(x, z) = (1+x^Tz)^d$, where\n",
    "  $d \\in \\mathbb{N}$ is a hyperparameter.\n",
    "- Radial Basis Function (RBF) kernel:\n",
    "  $k_{rbf}(x, z) = \\exp(-\\gamma||x-z||^2_2)$, where $\\gamma > 0$ is a\n",
    "  hyperparameter.\n",
    "\n",
    "You may implement either grid search or random search. Do not use\n",
    "`sklearn`. Reasonable ranges for hyperparameters are\n",
    "$\\lambda \\in [10^{-5}, 10^{-1}]$ and $d \\in [5, 25]$. For $\\gamma$, you\n",
    "can use the heuristic¹: the inverse of the median of all $\\binom{n}{2}$\n",
    "squared distances $||x_i - x_j||^2_2$ for a dataset\n",
    "$x_1, \\dots, x_n \\in \\mathbb{R}^d$. You do not need to search over\n",
    "$\\gamma$ but can sample from a narrow Gaussian distribution centered at\n",
    "this heuristic value if you wish.\n",
    "\n",
    "Report the values of $d, \\lambda$, and $\\gamma$ for both kernels.\n",
    "\n",
    "1.  Using the hyperparameters found in part a, plot the learned\n",
    "    functions $\\hat{f}_{poly}(x)$ and $\\hat{f}_{rbf}(x)$. For each\n",
    "    function, make a single plot that includes:\n",
    "\n",
    "- The original data points $\\{(x_i, y_i)\\}_{i=1}^n$.\n",
    "- The true function $f(x)$.\n",
    "- The learned function $\\hat{f}(x)$, plotted on a fine grid over the\n",
    "  interval $[0, 1]$.\n",
    "\n",
    "## Introduction to PyTorch\n",
    "\n",
    "For questions A.4 and A.5, you will use PyTorch. Please refer to\n",
    "“Section materials (Week 6)” for a useful notebook and PyTorch\n",
    "Documentation.\n",
    "\n",
    "**A4.** PyTorch is a powerful tool for developing neural networks. In\n",
    "this problem, we will explore how PyTorch is built and re-implement some\n",
    "of its core components. Start by reading the `README.md` file in the\n",
    "`intro_pytorch` subfolder, as problem statements may overlap with its\n",
    "content and function comments.\n",
    "\n",
    "1.  Implement components of custom PyTorch modules. These components are\n",
    "    located in folders named `layers`, `losses`, and `optimizers`. Each\n",
    "    file in these folders should contain at least one problem function\n",
    "    with specific directions. Finally, implement functions in the\n",
    "    `train.py` file.\n",
    "\n",
    "2.  Perform hyperparameter search using the modules implemented in A4.a.\n",
    "    The loss function is also treated as a hyperparameter. Due to\n",
    "    different input shapes for cross-entropy and Mean Squared Error\n",
    "    (MSE), two separate files are provided: `crossentropy_search.py` and\n",
    "    `mean_squared_error_search.py`.\n",
    "\n",
    "For each file, build and train 6 specific models in the provided\n",
    "order: 1. Linear neural network (single layer, no activation function).\n",
    "2. Neural network with one hidden layer (2 units) and sigmoid activation\n",
    "function after the hidden layer. 3. Neural network with one hidden layer\n",
    "(2 units) and ReLU activation function after the hidden layer. 4. Neural\n",
    "network with two hidden layers (each with 2 units) and Sigmoid and ReLU\n",
    "activation functions after the first and second hidden layers,\n",
    "respectively. 5. NN with two hidden layer (each with 2 units) and ReLU,\n",
    "Sigmoid activation functions after first and second hidden layers,\n",
    "respectively. 6. NN with two hidden layer (each with 2 units) and ReLu\n",
    "activation functions after first and second hidden layers.\n",
    "\n",
    "For each loss function, submit a plot of losses from training and\n",
    "validation sets. All models should be on the same plot (12 lines per\n",
    "plot), with two plots total (1 for MSE, 1 for cross-entropy).\n",
    "\n",
    "1.  For each loss function, report the best performing architecture\n",
    "    (best performing is defined here as achieving the lowest validation\n",
    "    loss at any point during the training), and plot its guesses on test\n",
    "    set. You should use function `plot_model_guesses` from `train.py`\n",
    "    file. Lastly, report accuracy of that model on a test set.\n",
    "\n",
    "## The Softmax function\n",
    "\n",
    "One of the activation functions we ask you to implement is softmax. For\n",
    "a prediction $\\hat{y} \\in \\mathbb{R}^k$ corresponding to single\n",
    "datapoint (in a problem with $k$ classes):\n",
    "$$\\text{softmax}(\\hat{y}_i) = \\frac{\\exp(\\hat{y}_i)}{\\sum_j \\exp(\\hat{y}_j)}$$\n",
    "\n",
    "## Neural Networks for MNIST\n",
    "\n",
    "**A5.** In Homework 1, we used ridge regression to train a classifier\n",
    "for the MNIST dataset. In Homework 2, we used logistic regression to\n",
    "distinguish between the digits 2 and 7. Now, in this problem, we will\n",
    "use PyTorch to build a simple neural network classifier for MNIST to\n",
    "further improve our accuracy.\n",
    "\n",
    "We will implement two different architectures: a shallow but wide\n",
    "network, and a narrow but deeper network. For both architectures, we use\n",
    "$d$ to refer to the number of input features (in MNIST,\n",
    "$d = 28^2 = 784$), $h_i$ to refer to the dimension of the $i$-th hidden\n",
    "layer and $k$ for the number of target classes (in MNIST, $k = 10$). For\n",
    "the non-linear activation, use ReLU. Recall from lecture that\n",
    "$$\\text{ReLU}(x) = \\begin{cases} x, & x \\ge 0 \\\\ 0, & x < 0. \\end{cases}$$\n",
    "\n",
    "## Weight Initialization\n",
    "\n",
    "Consider a weight matrix $W \\in \\mathbb{R}^{n \\times m}$ and\n",
    "$b \\in \\mathbb{R}^n$. Note that here $m$ refers to the input dimension\n",
    "and $n$ to the output dimension of the transformation $x \\mapsto Wx+b$.\n",
    "Define $\\alpha = \\frac{1}{\\sqrt{m}}$. Initialize all your weight\n",
    "matrices and biases according to $\\text{Unif}(-\\alpha, \\alpha)$.\n",
    "\n",
    "## Training\n",
    "\n",
    "For this assignment, use the Adam optimizer from `torch.optim`. Adam is\n",
    "a more advanced form of gradient descent that combines momentum and\n",
    "learning rate scaling. It often converges faster than regular gradient\n",
    "descent in practice. You can use either Gradient Descent or any form of\n",
    "Stochastic Gradient Descent. Note that you are still using Adam, but\n",
    "might pass either the full data, a single datapoint or a batch of data\n",
    "to it. Use cross entropy for the loss function and ReLU for the\n",
    "non-linearity.\n",
    "\n",
    "## Implementing the Neural Networks\n",
    "\n",
    "1.  Let $W_0 \\in \\mathbb{R}^{h \\times d}$, $b_0 \\in \\mathbb{R}^h$,\n",
    "    $W_1 \\in \\mathbb{R}^{k \\times h}$, $b_1 \\in \\mathbb{R}^k$ and\n",
    "    $\\sigma(z): \\mathbb{R} \\rightarrow \\mathbb{R}$ some non-linear\n",
    "    activation function applied element-wise. Given some\n",
    "    $x \\in \\mathbb{R}^d$, the forward pass of the wide, shallow network\n",
    "    can be formulated as:\n",
    "\n",
    "$$F_1(x) := W_1\\sigma(W_0x + b_0) + b_1$$\n",
    "\n",
    "Use $h = 64$ for the number of hidden units and choose an appropriate\n",
    "learning rate. Train the network until it reaches 99% accuracy on the\n",
    "training data and provide a training plot (loss vs. epoch). Finally\n",
    "evaluate the model on the test data and report both the accuracy and the\n",
    "loss.\n",
    "\n",
    "1.  Let $W_0 \\in \\mathbb{R}^{h_0 \\times d}$, $b_0 \\in \\mathbb{R}^{h_0}$,\n",
    "    $W_1 \\in \\mathbb{R}^{h_1 \\times h_0}$, $b_1 \\in \\mathbb{R}^{h_1}$,\n",
    "    $W_2 \\in \\mathbb{R}^{k \\times h_1}$, $b_2 \\in \\mathbb{R}^k$ and\n",
    "    $\\sigma(z): \\mathbb{R} \\rightarrow \\mathbb{R}$ some non-linear\n",
    "    activation function. Given some $x \\in \\mathbb{R}^d$, the forward\n",
    "    pass of the network can be formulated as:\n",
    "\n",
    "$$F_2(x) := W_2\\sigma(W_1\\sigma(W_0x + b_0) + b_1) + b_2$$\n",
    "\n",
    "Use $h_0 = h_1 = 32$ and perform the same steps as in part a.\n",
    "\n",
    "1.  Compute the total number of parameters of each network and report\n",
    "    them. Then compare the number of parameters as well as the test\n",
    "    accuracies the networks achieved. Is one of the approaches (wide,\n",
    "    shallow vs. narrow, deeper) better than the other? Give an intuition\n",
    "    for why or why not.\n",
    "\n",
    "## Using PyTorch:\n",
    "\n",
    "For your solution, you may not use any functionality from the `torch.nn`\n",
    "module except for `torch.nn.functional.relu` and\n",
    "`torch.nn.functional.cross_entropy`. You must implement the networks\n",
    "$F_1$ and $F_2$ from scratch. For starter code and a tutorial on PyTorch\n",
    "refer to the sections 6 and 7 material."
   ],
   "id": "783d5b15-038f-4ddc-9892-2374a75ce13d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
