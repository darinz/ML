{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5\n",
    "\n",
    "## Conceptual Questions\n",
    "\n",
    "**A1.** The answers to these questions should be answerable without\n",
    "referring to external materials. Briefly justify your answers with a few\n",
    "words.\n",
    "\n",
    "1.  True or False: Given a data matrix $X \\in \\mathbb{R}^{n \\times d}$\n",
    "    where $d$ is much smaller than $n$ and $k = \\text{rank}(X)$, if we\n",
    "    project our data onto a $k$-dimensional subspace using PCA, our\n",
    "    projection will have zero reconstruction error (in other words, we\n",
    "    find a perfect representation of our data, with no information\n",
    "    loss).\n",
    "\n",
    "2.  True or False: Suppose that an $n \\times n$ matrix $X$ has a\n",
    "    singular value decomposition of $USV^T$, where $S$ is a diagonal\n",
    "    $n \\times n$ matrix. Then, the rows of $V$ are equal to the\n",
    "    eigenvectors of $X^T X$.\n",
    "\n",
    "3.  True or False: choosing $k$ to minimize the $k$-means objective (see\n",
    "    Equation (4) below) is a good way to find meaningful clusters.\n",
    "\n",
    "4.  True or False: The singular value decomposition of a matrix is\n",
    "    unique.\n",
    "\n",
    "5.  True or False: The rank of a square matrix equals the number of its\n",
    "    unique nonzero eigenvalues.\n",
    "\n",
    "## Think before you train\n",
    "\n",
    "**A2.** The first part of this problem (part a) explores how you would\n",
    "apply machine learning theory and techniques to a real-world problem.\n",
    "There is one scenario detailing a setting, a dataset, and a specific\n",
    "result we hope to achieve. Your job is to describe how you would handle\n",
    "the scenario with the tools we’ve learned in this class. Your response\n",
    "should include: (1) any pre-processing steps you would take (e.g. any\n",
    "data processing), (2) the specific machine learning pipeline you would\n",
    "use (i.e., algorithms and techniques learned in this class), (3) how\n",
    "your setup acknowledges the constraints and achieves the desired result.\n",
    "You should also aim to leverage some of the theory we have covered in\n",
    "this class. Some things to consider may be: the nature of the data\n",
    "(i.e., *How hard is it to learn? Do we need more data? Are the data\n",
    "sources good?*), the effectiveness of the pipeline (i.e., *How strong is\n",
    "the model when properly trained and tuned?*), and the time needed to\n",
    "effectively perform the pipeline.\n",
    "\n",
    "1.  Scenario: Disease Susceptibility Predictor\n",
    "\n",
    "- Setting: You are tasked by a research institute to create an algorithm\n",
    "  that learns the factors that contribute most to acquiring a specific\n",
    "  disease.\n",
    "- Dataset: A rich dataset of personal demographic information, location\n",
    "  information, risk factors, and whether a person has the disease or\n",
    "  not.\n",
    "- Result: The company wants a system that can determine how susceptible\n",
    "  someone is to this disease when they enter in their own personal\n",
    "  information. The pipeline should take limited amount of personal data\n",
    "  from a new user and infer more detailed metrics about the person.\n",
    "\n",
    "The second part of this problem (parts b, c) focuses on exploring\n",
    "possible shortcomings of machine learning models, and what real-world\n",
    "implications might follow from ignoring these issues.\n",
    "\n",
    "1.  Briefly describe (1) some potential shortcomings of your training\n",
    "    process from the disease susceptibility predictor scenario above\n",
    "    that may result in your algorithm having different accuracy on\n",
    "    different populations, and (2) how you may modify your procedure to\n",
    "    address these shortcomings.\n",
    "\n",
    "2.  Recall in Homework 2 we trained models to predict crime rates using\n",
    "    various features. It is important to note that datasets describing\n",
    "    crime have many shortcomings in describing the **entire landscape of\n",
    "    illegal behavior in a city**, and that these shortcomings often fall\n",
    "    disproportionately on minority communities. Some of these\n",
    "    shortcomings include that crimes are reported at different rates in\n",
    "    different neighborhoods, that police respond differently to the same\n",
    "    crime reported or observed in different neighborhoods, and that\n",
    "    police spend more time patrolling in some neighborhoods than others.\n",
    "    What real-world implications might follow from ignoring these\n",
    "    issues?\n",
    "\n",
    "## Image Classification on CIFAR-10\n",
    "\n",
    "**A3.** In this problem we will explore different deep learning\n",
    "architectures for image classification on the CIFAR-10 dataset. Make\n",
    "sure that you are familiar with `torch.Tensors`, two-dimensional\n",
    "convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`),\n",
    "ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor\n",
    "reshaping (`view`).\n",
    "\n",
    "A few preliminaries: \\* Make sure to read the “Tips for HW4” EdStem post\n",
    "for additional tips about training your models. \\* Each network $f$ maps\n",
    "an image $x^{\\text{in}} \\in \\mathbb{R}^{32 \\times 32 \\times 3}$ (3\n",
    "channels for RGB) to an output\n",
    "$f(x^{\\text{in}}) = x^{\\text{out}} \\in \\mathbb{R}^{10}$. The class label\n",
    "is predicted as $\\operatorname{arg max}_{i=0,1,...,9} x_i^{\\text{out}}$.\n",
    "An error occurs if the predicted label differs from the true label for a\n",
    "given image. \\* The network is trained via multiclass cross-entropy\n",
    "loss. \\* Create a validation dataset by appropriately partitioning the\n",
    "train dataset. Hint: look at the documentation for\n",
    "`torch.utils.data.random_split`. Make sure to tune hyperparameters like\n",
    "network architecture and step size on the validation dataset. Do **NOT**\n",
    "validate your hyperparameters on the test dataset. \\* At the end of each\n",
    "epoch (one pass over the training data), compute and print the training\n",
    "and validation classification accuracy. \\* While one could train a\n",
    "network for hundreds of epochs to reach convergence and maximize\n",
    "accuracy, this can be prohibitively time-consuming, so feel free to\n",
    "train for just a dozen or so epochs.\n",
    "\n",
    "For parts (a) and (b), apply a hyperparameter tuning method (e.g. random\n",
    "search, grid search, etc.) using the validation set, report the\n",
    "hyperparameter configurations you evaluated and the best set of\n",
    "hyperparameters from this set, and plot the training and validation\n",
    "classification accuracy as a function of epochs. Produce a separate line\n",
    "or plot for each hyperparameter configuration evaluated (top 3\n",
    "configurations is sufficient to keep the plots clean). Finally, evaluate\n",
    "your best set of hyperparameters on the test data and report the test\n",
    "accuracy.\n",
    "\n",
    "**Note 1:** Please refer to the provided notebook with starter code for\n",
    "this problem, on the course website. That notebook provides a complete\n",
    "end-to-end example of loading data, training a model using a simple\n",
    "network with a fully-connected output and no hidden layers (this is\n",
    "equivalent to logistic regression), and performing evaluation using\n",
    "canonical Pytorch. We recommend using this as a template for your\n",
    "implementations of the models below.\n",
    "\n",
    "**Note 2:** If you are attempting this problem and do not have access to\n",
    "GPU we highly recommend using Google Colab. The provided notebook\n",
    "includes instructions on how to use GPU in Google Colab.\n",
    "\n",
    "Here are the network architectures you will construct and compare.\n",
    "\n",
    "1.  Fully-connected output, 1 fully-connected hidden layer: this network\n",
    "    has one hidden layer denoted as $x^{\\text{hidden}} \\in \\mathbb{R}^M$\n",
    "    where $M$ will be a hyperparameter you choose ($M$ could be in the\n",
    "    hundreds). The nonlinearity applied to the hidden layer will be the\n",
    "    relu ($\\operatorname{relu}(x) = \\operatorname{max}\\{0, x\\}$). This\n",
    "    network can be written as\n",
    "    $$x^{\\text{out}} = W_2 \\operatorname{relu}(W_1 x^{\\text{in}} + b_1) + b_2$$\n",
    "    where $W_1 \\in \\mathbb{R}^{M \\times 3072}$, $b_1 \\in \\mathbb{R}^M$,\n",
    "    $W_2 \\in \\mathbb{R}^{10 \\times M}$, $b_2 \\in \\mathbb{R}^{10}$. Tune\n",
    "    the different hyperparameters and train for a sufficient number of\n",
    "    epochs to achieve a validation accuracy of at least 50%. Provide the\n",
    "    hyperparameter configuration used to achieve this performance.\n",
    "\n",
    "2.  Convolutional layer with max-pool and fully-connected output: for a\n",
    "    convolutional layer $W_1$ with filters of size\n",
    "    $k \\times k \\times 3$, and $M$ filters (reasonable choices are\n",
    "    $M = 100$, $k = 5$), we have that\n",
    "    $\\operatorname{Conv2d}(x^{\\text{in}}, W_1) \\in \\mathbb{R}^{(33-k) \\times (33-k) \\times M}$.\n",
    "\n",
    "- Each convolution will have its own offset applied to each of the\n",
    "  output pixels of the convolution; we denote this as\n",
    "  $\\text{Conv2d}(x^{\\text{in}}, W) + b_1$ where $b_1$ is parameterized\n",
    "  in $\\mathbb{R}^M$. Apply a relu activation to the result of the\n",
    "  convolutional layer.\n",
    "- Next, use a max-pool of size $N \\times N$ (a reasonable choice is\n",
    "  $N = 14$ to pool to $2 \\times 2$ with $k = 5$) we have that\n",
    "  $\\text{MaxPool}(\\text{relu}(\\text{Conv2d}(x^{\\text{in}}, W_1) + b_1)) \\in \\mathbb{R}^{[\\frac{33-k}{N}] \\times [\\frac{33-k}{N}] \\times M}$.\n",
    "- We will then apply a fully-connected layer to the output to get a\n",
    "  final network given as\n",
    "\n",
    "$$x^{\\text{output}} = W_2(\\text{MaxPool}(\\text{relu}(\\text{Conv2d}(x^{\\text{input}}, W_1) + b_1))) + b_2$$\n",
    "\n",
    "where $W_2 \\in \\mathbb{R}^{10 \\times M([\\frac{33-k}{N}])^2}$,\n",
    "$b_2 \\in \\mathbb{R}^{10}$.\n",
    "\n",
    "The parameters $M, k, N$ (in addition to the step size and momentum) are\n",
    "all hyperparameters, but you can choose a reasonable value. Tune the\n",
    "different hyperparameters (number of convolutional filters, filter\n",
    "sizes, dimensionality of the fully-connected layers, step size, etc.)\n",
    "and train for a sufficient number of epochs to achieve a validation\n",
    "accuracy of at least 65%. Provide the hyperparameter configuration used\n",
    "to achieve this performance.\n",
    "\n",
    "The number of hyperparameters to tune, combined with the slow training\n",
    "times, will hopefully give you a taste of how difficult it is to\n",
    "construct networks with good generalization performance.\n",
    "State-of-the-art networks can have dozens of layers, each with their own\n",
    "hyperparameters to tune. Additional hyperparameters you are welcome to\n",
    "play with, if you are so inclined, include: changing the activation\n",
    "function, replace max-pool with average-pool, adding more convolutional\n",
    "or fully connected layers, and experimenting with batch normalization or\n",
    "dropout.\n",
    "\n",
    "## Matrix Completion and Recommendation System\n",
    "\n",
    "**A4.** Note: Please refer to the provided notebook with starter code\n",
    "for this problem, on the course website. The notebook includes a\n",
    "template and code for data loading. We recommend copying the starter\n",
    "notebook and filling out the template.\n",
    "\n",
    "In this problem, you will build a personalized movie recommendation\n",
    "system using the 100K MovieLens dataset, available at\n",
    "`https://grouplens.org/datasets/movielens/100k/`. The dataset contains\n",
    "$m = 1682$ movies and $n = 943$ users, with a total of 100,000 ratings.\n",
    "Each user has rated at least 20 movies. The goal is to recommend movies\n",
    "that users haven’t seen.\n",
    "\n",
    "We can think of this as a matrix $R \\in \\mathbb{R}^{m \\times n}$ where\n",
    "the entry $R_{i,j} \\in \\{1, \\dots, 5\\}$ represents the $j$-th user’s\n",
    "rating on movie $i$. A higher value indicates greater user satisfaction.\n",
    "The historical data is considered as observed entries in this matrix,\n",
    "with many unknown entries that need to be estimated to recommend the\n",
    "“best” movies.\n",
    "\n",
    "For this problem, ignore user and movie metadata, and only use the\n",
    "ratings from the `u.data` file. Construct a training and test set from\n",
    "this file using the following code:\n",
    "\n",
    "``` python\n",
    "import csv\n",
    "import numpy as np\n",
    "data = []\n",
    "with open('u.data') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in spamreader:\n",
    "        data.append((int(row[0])-1, int(row[1])-1, int(row[2])))\n",
    "data = np.array(data)\n",
    "\n",
    "num_observations = len(data) # num_observations = 100,000\n",
    "num_users = max(data[:,0])+1 # num_users = 943, indexed 0,...,942\n",
    "num_items = max(data[:,1])+1 # num_items = 1682 indexed 0,...,1681\n",
    "\n",
    "np.random.seed(1)\n",
    "num_train = int(0.8*num_observations)\n",
    "perm = np.random.permutation(data.shape[0])\n",
    "train = data[perm[0:num_train],:]\n",
    "test = data[perm[num_train::],:]\n",
    "```\n",
    "\n",
    "The `train` and `test` arrays contain $R_{train}$ and $R_{test}$\n",
    "respectively. Each line in these arrays takes the form “j, i, s”, where\n",
    "$j$ is the user index, $i$ is the movie index, and $s$ is the user’s\n",
    "score.\n",
    "\n",
    "Your task is to train a model using `train` to predict\n",
    "$\\hat{R} \\in \\mathbb{R}^{m \\times n}$, representing how every user would\n",
    "rate every movie. The model will be evaluated based on the average\n",
    "squared error on the test set, defined by the formula:\n",
    "\n",
    "$$\\mathcal{E}_{\\text{test}}(\\hat{R}) = \\frac{1}{|\\text{test}|} \\sum_{(i,j, R_{i,j}) \\in \\text{test}} (\\hat{R}_{i,j} - R_{i,j})^2.$$\n",
    "\n",
    "As a baseline method for personalized recommendation, we will use\n",
    "low-rank matrix factorization. This method learns a vector\n",
    "representation $u_i \\in \\mathbb{R}^d$ for each movie and a vector\n",
    "representation $v_j \\in \\mathbb{R}^d$ for each user, such that their\n",
    "inner product $\\langle u_i, v_j \\rangle$ approximates the rating\n",
    "$R_{i,j}$. You will build a simple latent factor model.\n",
    "\n",
    "You will implement multiple estimators and use the inner product\n",
    "$\\langle u_i, v_j \\rangle$ to predict if user $j$ likes movie $i$ in the\n",
    "test data. For simplicity, we will put aside best practices and choose\n",
    "hyperparameters by using those that minimize the test error. You may use\n",
    "fundamental operators from numpy or pytorch in this problem\n",
    "(numpy.linalg.lstsq, SVD, autograd, etc.) but not any precooked\n",
    "algorithm from a package like scikit-learn. If there is a question\n",
    "whether some package is not allowed for use in this problem, it probably\n",
    "is not appropriate.\n",
    "\n",
    "1.  Our first estimator pools all users together and, for each movie,\n",
    "    outputs as its prediction the average user rating of that movie in\n",
    "    train. That is, if $\\mu \\in \\mathbb{R}^m$ is a vector where $\\mu_i$\n",
    "    is the average rating of the users that rated the $i$th movie, write\n",
    "    this estimator $\\hat{R}$ as a rank-one matrix. Compute the estimate\n",
    "    $\\hat{R}$. What is $\\mathcal{E}_{\\text{test}}(\\hat{R})$ for this\n",
    "    estimate?\n",
    "\n",
    "2.  Allocate a matrix $R_{i,j} \\in \\mathbb{R}^{m \\times n}$ and set its\n",
    "    entries equal to the known values in the training set, and 0\n",
    "    otherwise. Let $\\hat{R}^{(d)}$ be the best rank-$d$ approximation\n",
    "    (in terms of squared error) approximation to $R$. This is equivalent\n",
    "    to computing the singular value decomposition (SVD) and using the\n",
    "    top $d$ singular values. This learns a lower-dimensional vector\n",
    "    representation for users and movies, assuming that each user would\n",
    "    give a rating of 0 to any movie they have not reviewed.\n",
    "\n",
    "- For each $d = 1, 2, 5, 10, 20, 50$, compute the estimator\n",
    "  $\\hat{R}^{(d)}$. We recommend using an efficient solver such as\n",
    "  `scipy.sparse.linalg.svds`.\n",
    "- Plot the average squared error of predictions on the training set and\n",
    "  test set on a single plot, as a function of $d$.\n",
    "\n",
    "Note that, in most applications, we would not actually allocate a full\n",
    "$m \\times n$ matrix. We do so here only because our data is relatively\n",
    "small and it is instructive.\n",
    "\n",
    "1.  Replacing all missing values by a constant may impose strong and\n",
    "    potentially incorrect assumptions on the unobserved entries of $R$.\n",
    "    A more reasonable choice is to minimize the MSE (mean squared error)\n",
    "    only on rated movies. Define a loss function:\n",
    "    $$\\mathcal{L}(\\{u_i\\}_{i=1}^m, \\{v_j\\}_{j=1}^n) := \\sum_{(i,j, R_{i,j}) \\in \\text{train}} (\\langle u_i, v_j \\rangle - R_{i,j})^2 + \\lambda \\sum_{i=1}^m \\|u_i\\|^2 + \\lambda \\sum_{j=1}^n \\|v_j\\|^2 \\quad (1)$$\n",
    "    where $\\lambda > 0$ is the regularization coefficient. We will\n",
    "    implement algorithms to learn vector representations by minimizing\n",
    "    (1). Note: we define the loss function here as the sum of squared\n",
    "    errors; be careful to calculate and plot the mean squared error for\n",
    "    your results.\n",
    "\n",
    "Since this is a non-convex optimization problem, the initial starting\n",
    "point and hyperparameters may affect the quality of $\\hat{R}$. You may\n",
    "need to tune $\\lambda$ and $\\sigma$ to optimize the loss you see.\n",
    "\n",
    "Alternating minimization: First, randomly initialize $\\{u_i\\}$ and\n",
    "$\\{v_j\\}$. Then, alternate between (1) minimizing the loss function with\n",
    "respect to $\\{u_i\\}$ by treating $\\{v_j\\}$ as fixed; and (2) minimizing\n",
    "the loss function with respect to $\\{v_j\\}$ by treating $\\{u_i\\}$ as\n",
    "fixed. Repeat (1) and (2) until both $\\{u_i\\}$ and $\\{v_j\\}$ converge.\n",
    "Note that when one of $\\{u_i\\}$ or $\\{v_j\\}$ is given, minimizing the\n",
    "loss function with respect to the other part has a closed-form solution.\n",
    "Indeed, it can be shown that when minimizing with respect to a single\n",
    "$u_i$ (with $\\{v_j\\}$ fixed), the gradient is given by:\n",
    "$$\\nabla_{u_i} \\mathcal{L}(\\{u_i\\}_{i=1}^m, \\{v_j\\}_{j=1}^n) = 2 \\left( \\sum_{j \\in r(i)} v_j v_j^T + \\lambda I \\right) u_i - 2 \\sum_{j \\in r(i)} R_{i,j} v_j \\quad (2)$$\n",
    "where here $r(i)$ is a shorthand for the set of users who have reviewed\n",
    "movie $i$ in the training set, or more formally,\n",
    "$r(i) = \\{j : (j, i, R_{i,j}) \\in \\text{train}\\}$. Setting the overall\n",
    "gradient to be equal to 0 gives us that\n",
    "\n",
    "$$\\arg \\min_{u_i} L(\\{u_i\\}_{i=1}^m, \\{v_j\\}_{j=1}^n) = \\left( \\sum_{j \\in r(i)} v_j v_j^T + \\lambda I \\right)^{-1} \\left( \\sum_{j \\in r(i)} R_{i,j} v_j \\right) \\quad (3)$$\n",
    "\n",
    "Note that this update rule is for a single vector $u_i$, whereas you\n",
    "should update all of the $\\{u_i\\}_{i=1}^m$ in one round. When it comes\n",
    "to the alternate step which involves fixing $\\{u_i\\}$ and minimizing\n",
    "$\\{v_j\\}$, an analogous calculation will give you a very similar update\n",
    "rule.\n",
    "\n",
    "- Try $d \\in \\{1, 2, 5, 10, 20, 50\\}$ and plot the mean squared error of\n",
    "  train and test as a function of $d$.\n",
    "\n",
    "Some hints: \\* Common choices for initializing the vectors\n",
    "$\\{u_i\\}_{i=1}^m, \\{v_j\\}_{j=1}^n$ include: entries drawn from\n",
    "`np.random.rand()` scaled by some scale factor $\\sigma > 0$ ($\\sigma$ is\n",
    "an additional hyperparameter), or using one of the solutions from part b\n",
    "or c. \\* The only $m \\times n$ matrix you need to allocate is probably\n",
    "for $\\tilde{R}$. \\* It is crucial that the squared error part of the\n",
    "loss is only defined w.r.t. $R_{i,j}$ that actually exist in the\n",
    "training set. Consider implementing some type of data structures that\n",
    "allow you to keep track of $r(i)$ as well as the reverse mapping\n",
    "$r^{-1}(j)$ from movies to relevant users. \\* In computing\n",
    "$\\mathcal{E}_{\\text{test}}(\\hat{R})$ from part a, feel free to use the\n",
    "average train rating as the prediction of movies found in the test set\n",
    "but not in the training set.\n",
    "\n",
    "## k-means clustering\n",
    "\n",
    "**A5.** Given a dataset\n",
    "$\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$ and an integer\n",
    "$1 \\le k \\le n$, recall the following k-means objective function\n",
    "\n",
    "$$\\min_{\\pi_1, \\dots, \\pi_k} \\sum_{i=1}^k \\sum_{j \\in \\pi_i} \\|\\mathbf{x}_j - \\boldsymbol{\\mu}_i\\|^2, \\quad \\boldsymbol{\\mu}_i = \\frac{1}{|\\pi_i|} \\sum_{j \\in \\pi_i} \\mathbf{x}_j \\quad (4)$$\n",
    "\n",
    "Above, $\\{\\pi_i\\}_{i=1}^k$ is a partition of $\\{1,2,..., n\\}$. The\n",
    "objective (4) is NP-hard¹ to find a global minimizer of. Nevertheless,\n",
    "Lloyd’s algorithm (discussed in lecture) typically works well in\n",
    "practice.²\n",
    "\n",
    "1.  Implement Lloyd’s algorithm for solving the k-means objective (4).\n",
    "    Do not use any off-the-shelf implementations, such as those found in\n",
    "    scikit-learn.\n",
    "\n",
    "2.  Run Lloyd’s algorithm on the *training* dataset of MNIST with\n",
    "    $k = 10$. Show the image representing the center of each cluster, as\n",
    "    a set of $k$ $28 \\times 28$ images.\n",
    "\n",
    "**Note on Time to Run:** The runtime of a good implementation for this\n",
    "problem should be fairly fast (a few minutes); if you find it taking\n",
    "upwards of one hour, please check your implementation! (Hint: **For\n",
    "loops are costly**. Can you vectorize it or use Numpy operations to make\n",
    "it faster in some ways? If not, is looping through data-points or\n",
    "through centers faster?)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "¹ To be more precise, it is both NP-hard in d when k = 2 and k when d =\n",
    "2.\n",
    "\n",
    "² See the references on the Wikipedia page for k-means and k-means++ for\n",
    "more details."
   ],
   "id": "e8444c99-fedc-44c3-9fa7-d1186a8217ff"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
