{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376e3ec8-08db-4430-a348-2f3e6bd97b6e",
   "metadata": {},
   "source": [
    "# Problem Set 2 Solutions\n",
    "\n",
    "## Problem 1: Least Squares\n",
    "\n",
    "### Problem 1(a) [2 points]\n",
    "\n",
    "**Problem:** Let $\\hat{\\theta} = \\arg \\min_{\\theta} J(\\theta)$ be the minimizer of the original least squares objective (using the original design matrix $X$). Using the orthonormality assumption, show that $J(\\hat{\\theta}) = (XX^T \\bar{y} - \\bar{y})^T (XX^T \\bar{y} - \\bar{y})$. I.e., show that this is the value of $\\min_{\\theta} J(\\theta)$ (the value of the objective at the minimum).\n",
    "\n",
    "**Answer:** We know from lecture that the least squares minimizer is $\\hat{\\theta} = (X^T X)^{-1} X^T \\bar{y}$ but because of the orthonormality assumption, this simplifies to $\\hat{\\theta} = X^T \\bar{y}$. Substituting this expression into the normal equation for $J(\\theta)$ gives the final expression $J(\\hat{\\theta}) = (XX^T \\bar{y} - \\bar{y})^T (XX^T \\bar{y} - \\bar{y})$.\n",
    "\n",
    "### Problem 1(b) [5 points]\n",
    "\n",
    "**Problem:** Now let $\\hat{\\theta}_{\\text{new}}$ be the minimizer for $\\tilde{J}(\\theta_{\\text{new}}) = (\\tilde{X}\\theta_{\\text{new}} - \\vec{y})^T(\\tilde{X}\\theta_{\\text{new}} - \\vec{y})$. Find the new minimized objective $\\tilde{J}(\\hat{\\theta}_{\\text{new}})$ and write this expression in the form: $\\tilde{J}(\\hat{\\theta}_{\\text{new}}) = J(\\hat{\\theta}) + f(X, \\vec{v}, \\vec{y})$ where $J(\\hat{\\theta})$ is as derived in part (a) and $f$ is some function of $X, \\vec{v}$, and $\\vec{y}$.\n",
    "\n",
    "**Answer:** Just like we had in part (a), the minimizer for the new objective is $\\hat{\\theta}_{\\text{new}} = \\tilde{X}^T\\vec{y}$. Now we solve for the new minimized objective:\n",
    "\n",
    "$\\tilde{J}(\\hat{\\theta}_{\\text{new}}) = (\\tilde{X}\\hat{\\theta}_{\\text{new}} - \\vec{y})^T(\\tilde{X}\\hat{\\theta}_{\\text{new}} - \\vec{y})$\n",
    "$= (\\tilde{X}\\tilde{X}^T\\vec{y} - \\vec{y})^T(\\tilde{X}\\tilde{X}^T\\vec{y} - \\vec{y})$\n",
    "$= ((XX^T + \\vec{v}\\vec{v}^T)\\vec{y} - \\vec{y})^T((XX^T + \\vec{v}\\vec{v}^T)\\vec{y} - \\vec{y})$\n",
    "$= ((XX^T\\vec{y} - \\vec{y}) + \\vec{v}\\vec{v}^T\\vec{y})^T((XX^T\\vec{y} - \\vec{y}) + \\vec{v}\\vec{v}^T\\vec{y})$\n",
    "$= (XX^T\\vec{y} - \\vec{y})^T(XX^T\\vec{y} - \\vec{y}) + 2(XX^T\\vec{y} - \\vec{y})^T(\\vec{v}\\vec{v}^T\\vec{y}) + (\\vec{v}\\vec{v}^T\\vec{y})^T(\\vec{v}\\vec{v}^T\\vec{y})$\n",
    "$= J(\\hat{\\theta}) + 2(XX^T\\vec{y} - \\vec{y})^T(\\vec{v}\\vec{v}^T\\vec{y}) + (\\vec{v}\\vec{v}^T\\vec{y})^T(\\vec{v}\\vec{v}^T\\vec{y})$\n",
    "\n",
    "### Problem 1(c) [6 points]\n",
    "\n",
    "**Problem:** Prove that the optimal objective value does not increase upon adding a feature to the design matrix. That is, show $\\tilde{J}(\\hat{\\theta}_{\\text{new}}) \\le J(\\hat{\\theta})$.\n",
    "\n",
    "**Answer:** Using the final result of part (b), we can continue simplifying the expression for $J(\\hat{\\theta}_{\\text{new}})$ as follows:\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\tilde{J}(\\hat{\\theta}_{\\text{new}}) &= J(\\hat{\\theta}) + 2(XX^T\\tilde{y} - \\tilde{y})^T(vv^T\\tilde{y}) + (vv^T\\tilde{y})^T(vv^T\\tilde{y}) \\\\\n",
    "&= J(\\hat{\\theta}) + 2(XX^T\\tilde{y})^T(vv^T\\tilde{y}) - 2\\tilde{y}^T(vv^T\\tilde{y}) + (vv^T\\tilde{y})^T(vv^T\\tilde{y}) \\\\\n",
    "&= J(\\hat{\\theta}) + 2(\\tilde{y}^T XX^T vv^T\\tilde{y}) - 2(\\tilde{y}^T vv^T\\tilde{y}) + (\\tilde{y}^T vv^T vv^T\\tilde{y}) \\\\\n",
    "&= J(\\hat{\\theta}) - \\tilde{y}^T vv^T\\tilde{y} \\\\\n",
    "&= J(\\hat{\\theta}) - (v^T\\tilde{y})^2 \\\\\n",
    "&\\le J(\\hat{\\theta})\n",
    "\\end{aligned}\n",
    "\\]\n",
    "From the third to last equality to the second to last equality, we use the two facts that $X^T v = 0$ and $v^T v = 1$.\n",
    "\n",
    "The proof is complete.\n",
    "\n",
    "Note for this problem we also accepted solutions where parts (b) and (c) overlapped.\n",
    "\n",
    "### Problem 1(d) [3 points]\n",
    "\n",
    "**Problem:** Does the above result show that if we keep increasing the number of features, we can always get a model that generalizes better than a model with fewer features? Explain why or why not.\n",
    "\n",
    "**Answer:** The result shows that we can either maintain or decrease the minimized square error objective by adding more features. However, remember that the error objective is computed only on the training samples and not the true data distribution. As a result, reducing training error does not guarantee a reduction in error on the true distribution. In fact, after a certain point adding features will likely lead to overfitting, increasing our generalization error. Therefore, adding features does not actually always result in a model that generalizes better.\n",
    "\n",
    "## Problem 2: Decision Boundaries for Generative Models\n",
    "\n",
    "### Problem 2(a) [7 points]\n",
    "\n",
    "**Problem:** Consider the multinomial event model of Naive Bayes. Our goal in this problem is to show that this is a linear classifier.\n",
    "\n",
    "For a given text document $x$, let $c_1, \\dots, c_V$ indicate the number of times each word (out of $V$ words) appears in the document. Thus, $c_i \\in \\{0, 1, 2, \\dots\\}$ counts the occurrences of word $i$. Recall that the Naive Bayes model uses parameters $\\phi_y = p(y = 1)$, $\\phi_{i|y=1} = p(\\text{word i appears in a specific document position } | y = 1)$ and $\\phi_{i|y=0} = p(\\text{word i appears in a specific document position } | y = 0)$.\n",
    "\n",
    "We say a classifier is linear if it assigns a label $y = 1$ using a decision rule of the form\n",
    "$$ \\sum_{i=1}^V w_i c_i + b \\ge 0 $$\n",
    "I.e., the classifier predicts \"$y = 1$\" if $\\sum_{i=1}^V w_i c_i + b \\ge 0$, and predicts \"$y = 0$\" otherwise.\n",
    "\n",
    "Show that Naive Bayes is a linear classifier, and clearly state the values of $w_i$ and $b$ in terms of the Naive Bayes parameters. (Don't worry about whether the decision rule uses \"$\\ge$\" or \"$>$.\") Hint: consider using log-probabilities.\n",
    "\n",
    "**Answer:**\n",
    "The decision boundary for Naive Bayes can be stated as\n",
    "$$ P(y = 1|c; \\Phi) > P(y = 0|c; \\Phi) $$\n",
    "$$ \\log p(y = 1|c; \\Phi) > \\log p(y = 0|c; \\Phi) $$\n",
    "$$ \\log p(y = 1|c; \\Phi) - \\log p(y = 0|c; \\Phi) > 0 $$\n",
    "$$ \\log \\frac{p(y = 1|c; \\Phi)}{p(y = 0|c; \\Phi)} > 0 $$\n",
    "$$ \\log \\frac{p(y = 1) \\prod_{i=1}^V p(E_i|y = 1)^{c_i}}{p(y = 0) \\prod_{i=1}^V p(E_i|y = 0)^{c_i}} > 0 $$\n",
    "$$ \\log \\frac{p(y = 1)}{p(y = 0)} + \\sum_{i=1}^V \\log p(E_i|y = 1)^{c_i} - \\log p(E_i|y = 0)^{c_i} > 0 $$\n",
    "$$ \\log \\frac{p(y = 1)}{p(y = 0)} + \\sum_{i=1}^V c_i \\log \\frac{p(E_i|y = 1)}{p(E_i|y = 0)} > 0 $$\n",
    "Using the given parameters:\n",
    "$$ \\log \\frac{\\phi_y}{1 - \\phi_y} + \\sum_{i=1}^V c_i \\log \\frac{\\phi_{i|y=1}}{\\phi_{i|y=0}} > 0 $$\n",
    "Thus, Naive Bayes is a linear classifier with\n",
    "$$ w_i = \\log \\frac{\\phi_{i|y=1}}{\\phi_{i|y=0}} $$\n",
    "$$ b = \\log \\frac{\\phi_y}{1 - \\phi_y} $$\n",
    "\n",
    "### Problem 2(b) [7 points]\n",
    "\n",
    "**Problem:** In Problem Set 1, you showed that Gaussian Discriminant Analysis (GDA) is a linear classifier. In this problem, we will show that a modified version of GDA has a quadratic decision boundary.\n",
    "\n",
    "Recall that GDA models $p(x|y)$ using a multivariate normal distribution, where $(x|y = 0) \\sim \\mathcal{N}(\\mu_0, \\Sigma)$ and $(x|y = 1) \\sim \\mathcal{N}(\\mu_1, \\Sigma)$, where we used the same $\\Sigma$ for both Gaussians.\n",
    "\n",
    "For this question, we will instead use two covariance matrices $\\Sigma_0, \\Sigma_1$ for the two labels. So, $(x|y = 0) \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$ and $(x|y = 1) \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)$.\n",
    "\n",
    "<img src=\"./q2b_problem.png\" width=\"350\">\n",
    "\n",
    "The model distributions can now be written as:\n",
    "$p(y) = \\phi^y (1 - \\phi)^{1-y}$\n",
    "$p(x|y = 0) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_0)^T \\Sigma_0^{-1} (x - \\mu_0)\\right)$\n",
    "$p(x|y = 1) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma_1|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_1)^T \\Sigma_1^{-1} (x - \\mu_1)\\right)$\n",
    "\n",
    "Let's follow a binary decision rule, where we predict $y = 1$ if $p(y = 1|x) \\ge p(y = 0|x)$, and $y = 0$ otherwise. Show that if $\\Sigma_0 \\ne \\Sigma_1$, then the separating boundary is quadratic in $x$.\n",
    "That is, simplify the decision rule \"$p(y = 1|x) \\ge p(y = 0|x)$\" to the form \"$x^T Ax + B^T x + C \\ge 0$\" (supposing that $x \\in \\mathbb{R}^{n+1}$), for some $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$, $B \\in \\mathbb{R}^{n+1}$, $C \\in \\mathbb{R}$ and $A \\ne 0$. Please clearly state your values for $A, B$ and $C$.\n",
    "\n",
    "**Answer:** Examining the log-probabilities yields:\n",
    "\n",
    "$\\log p(y = 1|x) \\ge \\log p(y = 0|x)$\n",
    "\n",
    "$0 \\le \\log \\left(\\frac{p(y = 1|x)}{p(y = 0|x)}\\right)$\n",
    "\n",
    "$0 \\le \\log \\left(\\frac{p(y = 1)p(x|y = 1)}{p(y = 0)p(x|y = 0)}\\right)$\n",
    "\n",
    "$0 \\le \\log \\left(\\frac{\\phi}{1 - \\phi}\\right) - \\log \\left(\\frac{|\\Sigma_1|^{1/2}}{|\\Sigma_0|^{1/2}}\\right) - \\frac{1}{2}(x - \\mu_1)^T \\Sigma_1^{-1}(x - \\mu_1) - \\frac{1}{2}(x - \\mu_0)^T \\Sigma_0^{-1}(x - \\mu_0)$\n",
    "\n",
    "$0 \\le -\\frac{1}{2}x^T(\\Sigma_1^{-1} - \\Sigma_0^{-1})x - 2(\\mu_1^T \\Sigma_1^{-1} - \\mu_0^T \\Sigma_0^{-1})x + \\mu_1^T \\Sigma_1^{-1} \\mu_1 - \\mu_0^T \\Sigma_0^{-1} \\mu_0 + \\log \\left(\\frac{\\phi}{1 - \\phi}\\right) - \\log \\left(\\frac{|\\Sigma_1|^{1/2}}{|\\Sigma_0|^{1/2}}\\right)$\n",
    "\n",
    "$0 \\le x^T\\left(\\frac{1}{2}(\\Sigma_0^{-1} - \\Sigma_1^{-1})\\right)x + (\\mu_1^T \\Sigma_1^{-1} - \\mu_0^T \\Sigma_0^{-1})x + \\log \\left(\\frac{\\phi}{1 - \\phi}\\right) + \\log \\left(\\frac{|\\Sigma_0|^{1/2}}{|\\Sigma_1|^{1/2}}\\right) + \\frac{1}{2}(\\mu_0^T \\Sigma_0^{-1} \\mu_0 - \\mu_1^T \\Sigma_1^{-1} \\mu_1)$\n",
    "\n",
    "From the above, we see that $A = \\frac{1}{2}(\\Sigma_0^{-1} - \\Sigma_1^{-1})$, $B^T = \\mu_1^T \\Sigma_1^{-1} - \\mu_0^T \\Sigma_0^{-1}$, and $C = \\log\\left(\\frac{\\phi}{1 - \\phi}\\right) + \\log\\left(\\frac{|\\Sigma_0|^{1/2}}{|\\Sigma_1|^{1/2}}\\right) + \\frac{1}{2}(\\mu_0^T \\Sigma_0^{-1} \\mu_0 - \\mu_1^T \\Sigma_1^{-1} \\mu_1)$. Furthermore, $A \\ne 0$ since $\\Sigma_0 \\ne \\Sigma_1$ implies that $\\Sigma_0^{-1} - \\Sigma_1^{-1} \\ne 0$. Therefore, the decision boundary is quadratic.\n",
    "\n",
    "## Problem 3: Generalized Linear Models\n",
    "\n",
    "### Problem 3(a) i. [5 points]\n",
    "\n",
    "**Problem:** Show that the geometric distribution is an exponential family distribution. Explicitly specify the components $b(y)$, $\\eta$, $T(y)$, and $\\alpha(\\eta)$, as well as express $\\phi$ in terms of $\\eta$.\n",
    "\n",
    "**Answer:**\n",
    "The given probability mass function:\n",
    "$$p(y; \\phi) = (1 - \\phi)^{y-1} \\phi$$\n",
    "$$= \\exp\\left((y - 1) \\log(1 - \\phi) + \\log \\phi\\right)$$\n",
    "$$= \\exp\\left((\\log(1 - \\phi))y + \\log \\phi - \\log(1 - \\phi)\\right)$$\n",
    "$$= \\exp\\left((\\log(1 - \\phi))y - \\log \\frac{1 - \\phi}{\\phi}\\right)$$\n",
    "\n",
    "Therefore:\n",
    "$$b(y) = 1$$\n",
    "$$\\eta = \\log(1 - \\phi)$$\n",
    "$$T(y) = y$$\n",
    "$$\\alpha(\\eta) = \\log \\frac{1 - \\phi}{\\phi}$$\n",
    "$$\\phi = 1 - e^\\eta$$\n",
    "\n",
    "### Problem 3(a) ii. [5 points]\n",
    "\n",
    "**Problem:** Suppose that we have an IID training set $\\{(x^{(i)}, y^{(i)}), i = 1, ..., m\\}$ and we wish to model this using a GLM based on a geometric distribution. Find the log-likelihood $\\log \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}; \\theta)$ defined with respect to the entire training set.\n",
    "\n",
    "**Answer:**\n",
    "We calculate the log-likelihood for 1 sample as well as for the entire training set:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y^{(i)}|x^{(i)}; \\theta) &= \\log \\left( (1 - \\phi)^{y^{(i)}-1} \\phi \\right) \\\\\n",
    "&= (y^{(i)} - 1) \\log (1 - \\phi) + \\log \\phi \\\\\n",
    "&= (\\log (1 - \\phi))y^{(i)} - \\log \\frac{1 - \\phi}{\\phi} \\\\\n",
    "&= y^{(i)} \\log e^{\\eta} - \\log \\frac{e^{\\eta}}{1 - e^{\\eta}} \\\\\n",
    "&= \\eta y^{(i)} - \\eta + \\log (1 - e^{\\eta}) \\\\\n",
    "&= \\theta^T x^{(i)} y^{(i)} - \\theta^T x^{(i)} + \\log (1 - \\exp (\\theta^T x^{(i)})) \\\\\n",
    "&= \\theta^T x^{(i)} (y^{(i)} - 1) + \\log (1 - \\exp (\\theta^T x^{(i)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\theta) &= \\log p(y|x; \\theta) \\\\\n",
    "&= \\log \\left( \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}; \\theta) \\right) \\\\\n",
    "&= \\sum_{i=1}^{m} \\log (p(y^{(i)}|x^{(i)}; \\theta)) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( \\theta^T x^{(i)} (y^{(i)} - 1) + \\log (1 - \\exp (\\theta^T x^{(i)})) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Observe that since we use the substitution $\\eta = \\log (1 - \\phi) = \\theta^T x$ above, it follows that $\\forall x, \\theta^T x < 0$ in order for the prediction to be valid. This is obviously not feasible as a prediction rule, and so in practice one could use the function $g(\\theta^T x) = -\\log (\\theta^T x)$ to map these linear predictions to a valid interval when dealing with real data.\n",
    "\n",
    "### Problem 3(b) [6 points]\n",
    "\n",
    "**Problem:** Derive the Hessian $H$ and the gradient vector of the log likelihood with respect to $\\theta$, and state what one step of Newton's method for maximizing the log likelihood would be.\n",
    "\n",
    "**Answer:** To apply Newton's method, we need to find the gradient and Hessian of the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta l(\\theta) &= \\nabla_\\theta \\sum_{i=1}^{m} \\left(\\theta^T x^{(i)} y^{(i)} - \\theta^T x^{(i)} + \\log (1 - \\exp (\\theta^T x^{(i)}))\\right) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left(x^{(i)} (y^{(i)} - 1) - \\frac{x^{(i)} \\exp (\\theta^T x^{(i)})}{(1 - \\exp (\\theta^T x^{(i)}))}\\right) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left(y^{(i)} - \\frac{1}{(1 - \\exp (\\theta^T x^{(i)}))}\\right) x^{(i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H &= \\nabla_\\theta (\\nabla_\\theta l(\\theta))^T \\\\\n",
    "&= -\\nabla_\\theta \\sum_{i=1}^{m} \\frac{1}{(1 - \\exp (\\theta^T x^{(i)}))} x^{(i)T} \\\\\n",
    "&= -\\sum_{i=1}^{m} \\frac{\\exp (\\theta^T x^{(i)})}{(1 - \\exp (\\theta^T x^{(i)}))^2} x^{(i)} x^{(i)T}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The Newton's method update rule is then: $\\theta := \\theta - H^{-1} \\nabla_\\theta l(\\theta)$\n",
    "\n",
    "### Problem 3(c) [2 points]\n",
    "\n",
    "**Problem:** Show that the Hessian is negative semi-definite, which implies the optimization objective is concave and Newton's method maximizes log-likelihood.\n",
    "\n",
    "**Answer:** \n",
    "$$z^T H z = - \\sum_{i=1}^{m} \\frac{\\exp(\\theta^T x^{(i)})}{(1 - \\exp(\\theta^T x^{(i)}))^2} z^T x^{(i)} x^{(i)T} z$$\n",
    "$$= - \\sum_{i=1}^{m} \\frac{\\exp(\\theta^T x^{(i)})}{(1 - \\exp(\\theta^T x^{(i)}))^2} \\|z^T x^{(i)}\\|^2 \\le 0$$\n",
    "\n",
    "Therefore, $H$ is negative semidefinite, which means $l(\\theta)$ is concave. So we are maximizing it.\n",
    "\n",
    "## Problem 4: Support Vector Regression\n",
    "\n",
    "### Problem 4(a) [4 points]\n",
    "\n",
    "**Problem:** Write down the Lagrangian for the optimization problem above. We suggest you use two sets of Lagrange multipliers $\\alpha_i$ and $\\alpha_i^*$, corresponding to the two inequality constraints (labeled (1) and (2) above), so that the Lagrangian would be written $\\mathcal{L}(w, b, \\alpha, \\alpha^*)$.\n",
    "\n",
    "**Answer:**\n",
    "Let $\\alpha_i, \\alpha_i^* \\ge 0$ ($i = 1,\\dots,m$) be the Lagrange multiplier for (1)-(4) respectively. Then, the Lagrangian can be written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b, \\alpha, \\alpha^*) \\\\\n",
    "= \\frac{1}{2}\\|w\\|^2 \\\\\n",
    "- \\sum_{i=1}^m \\alpha_i (\\epsilon - y^{(i)} + w^T x^{(i)} + b) \\\\\n",
    "- \\sum_{i=1}^m \\alpha_i^* (\\epsilon + y^{(i)} - w^T x^{(i)} - b)\n",
    "$$\n",
    "\n",
    "### Problem 4(b) [10 points]\n",
    "\n",
    "**Problem:** Derive the dual optimization problem. You will have to take derivatives of the Lagrangian with respect to $w$ and $b$.\n",
    "\n",
    "**Answer:**\n",
    "First, the dual objective function can be written as:\n",
    "$$ \\theta_D(\\alpha, \\alpha^*) = \\min_{w,b} L(w, b, \\alpha, \\alpha^*) $$\n",
    "\n",
    "Now, taking the derivatives of Lagrangian with respect to all primal variables, we have:\n",
    "$$ \\partial_w L = w - \\sum_{i=1}^m (\\alpha_i - \\alpha_i^*)x^{(i)} = 0 $$\n",
    "$$ \\partial_b L = \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i) = 0 $$\n",
    "\n",
    "Substituting the above two relations back into the Lagrangian, we have:\n",
    "$$ \\theta_D(\\alpha, \\alpha^*) = \\frac{1}{2}\\|w\\|^2 - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) + b \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i) + \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i)w^T x^{(i)} $$\n",
    "Since $\\sum_{i=1}^m (\\alpha_i^* - \\alpha_i) = 0$, the term $b \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i)$ vanishes.\n",
    "$$ \\theta_D(\\alpha, \\alpha^*) = \\frac{1}{2}\\|w\\|^2 - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) + \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i)w^T x^{(i)} $$\n",
    "Substitute $w = \\sum_{i=1}^m (\\alpha_i - \\alpha_i^*)x^{(i)}$:\n",
    "\\begin{align*} &= \\frac{1}{2}\\left\\|\\sum_{i=1}^m (\\alpha_i - \\alpha_i^*)x^{(i)}\\right\\|^2 - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) \\\\ & \\quad + \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i)\\left(\\sum_{j=1}^m (\\alpha_j - \\alpha_j^*)x^{(j)}\\right)^T x^{(i)} \\\\ &= \\frac{1}{2}\\left(\\sum_{i=1}^m (\\alpha_i - \\alpha_i^*)x^{(i)}\\right)^T\\left(\\sum_{k=1}^m (\\alpha_k - \\alpha_k^*)x^{(k)}\\right) - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) \\\\ & \\quad - \\sum_{i=1}^m (\\alpha_i - \\alpha_i^*)\\left(\\sum_{j=1}^m (\\alpha_j - \\alpha_j^*)x^{(j)}\\right)^T x^{(i)} \\\\ &= \\frac{1}{2} \\sum_{i=1}^m \\sum_{k=1}^m (\\alpha_i - \\alpha_i^*)(\\alpha_k - \\alpha_k^*)x^{(i)T}x^{(k)} - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) \\\\ & \\quad - \\sum_{i=1}^m \\sum_{j=1}^m (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*)x^{(j)T}x^{(i)} \\\\ &= -\\frac{1}{2} \\sum_{i=1,j=1}^m (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*)x^{(i)T}x^{(j)} - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) \\end{align*}\n",
    "\n",
    "Now the dual problem can be formulated as:\n",
    "$$ \\max_{\\alpha_i, \\alpha_i^*} -\\frac{1}{2} \\sum_{i=1,j=1}^m (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*)x^{(i)T}x^{(j)} - \\epsilon \\sum_{i=1}^m (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^m y^{(i)}(\\alpha_i - \\alpha_i^*) $$\n",
    "s.t.\n",
    "$$ \\sum_{i=1}^m (\\alpha_i^* - \\alpha_i) = 0 $$\n",
    "$$ \\alpha_i, \\alpha_i^* \\ge 0 $$\n",
    "\n",
    "### Problem 4(c) [4 points]\n",
    "\n",
    "**Problem:** Show that this algorithm can be kernelized. For this, you have to show that (i) the dual optimization objective can be written in terms of inner-products of training examples; and (ii) at test time, given a new $x$ the hypothesis $h_{w,b}(x)$ can also be computed in terms of inner products.\n",
    "\n",
    "**Answer:** This algorithm can be kernelized because when making prediction at $x$, we have:\n",
    "\n",
    "$$f(w, x) = w^T x + b = \\sum_{i=1}^{m} (\\alpha_i - \\alpha_i^*) x^{(i)T} x + b = \\sum_{i=1}^{m} (\\alpha_i - \\alpha_i^*) k(x^{(i)}, x) + b$$\n",
    "\n",
    "This shows that predicting function can be written in a kernel form.\n",
    "\n",
    "## Problem 5: Learning Theory\n",
    "\n",
    "Suppose you are given a hypothesis $h_0 \\in \\mathcal{H}$, and your goal is to determine whether $h_0$ has generalization error within $\\eta > 0$ of the best hypothesis, $h^* = \\arg \\min_{h \\in \\mathcal{H}} \\varepsilon(h)$. More specifically, we say that a hypothesis $h$ is $\\eta$-optimal if $\\varepsilon(h) \\le \\varepsilon(h^*) + \\eta$. Here, we wish to answer the following question:\n",
    "\n",
    "Given a hypothesis $h_0$, is $h_0$ $\\eta$-optimal?\n",
    "\n",
    "Let $\\delta > 0$ be some fixed constant, and consider a finite hypothesis class $\\mathcal{H}$ of size $|\\mathcal{H}| = k$. For each $h \\in \\mathcal{H}$, let $\\hat{\\varepsilon}(h)$ denote the training error of $h$ with respect to some training set of $m$ IID examples, and let $\\hat{h} = \\arg \\min_{h \\in \\mathcal{H}} \\hat{\\varepsilon}(h)$ denote the hypothesis that minimizes training error.\n",
    "\n",
    "Now, consider the following algorithm:\n",
    "\n",
    "1. Set $\\gamma := \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}$\n",
    "2. If $\\hat{\\varepsilon}(h_0) > \\hat{\\varepsilon}(\\hat{h}) + \\eta + 2\\gamma$, then return NO.\n",
    "3. If $\\hat{\\varepsilon}(h_0) < \\hat{\\varepsilon}(\\hat{h}) + \\eta - 2\\gamma$, then return YES.\n",
    "4. Otherwise, return UNSURE.\n",
    "\n",
    "Intuitively, the algorithm works by comparing the training error of $h_0$ to the training error of the hypothesis $\\hat{h}$ with the minimum training error, and returns NO or YES only when $\\hat{\\varepsilon}(h_0)$ is either significantly larger than or significantly smaller than $\\hat{\\varepsilon}(\\hat{h})+\\eta$.\n",
    "\n",
    "### Problem 5(a) [6 points]\n",
    "\n",
    "**Problem:** First, show that if $\\varepsilon(h_0) \\le \\varepsilon(h^*) + \\eta$ (i.e., $h_0$ is $\\eta$-optimal), then the probability that the algorithm returns NO is at most $\\delta$.\n",
    "\n",
    "**Answer:** Suppose that $\\varepsilon(h_0) \\le \\varepsilon(h^*) + \\eta$. Using the Hoeffding inequality, we have that for\n",
    "\n",
    "$$ \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}} $$\n",
    "\n",
    "then with probability at least $1 - \\delta$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\varepsilon}(h_0) &\\le \\varepsilon(h_0) + \\gamma \\\\\n",
    "&\\le \\varepsilon(h^*) + \\eta + \\gamma \\\\\n",
    "&\\le \\varepsilon(\\hat{h}) + \\eta + \\gamma \\\\\n",
    "&\\le \\hat{\\varepsilon}(\\hat{h}) + \\eta + 2\\gamma.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, the first and last inequalities follow from the fact that under the stated uniform convergence conditions, all hypotheses in $\\mathcal{H}$ have empirical errors within $\\gamma$ of their true generalization errors. The second inequality follows from our assumption, and the third inequality follows from the fact that $h^*$ minimizes the true generalization error. Therefore, the reverse condition, $\\hat{\\varepsilon}(h_0) > \\hat{\\varepsilon}(\\hat{h}) + \\eta + 2\\gamma$, occurs with probability at most $\\delta$.\n",
    "\n",
    "### Problem 5(b) [6 points]\n",
    "\n",
    "**Problem:** Second, show that if $\\varepsilon(h_0) > \\varepsilon(h^*) + \\eta$ (i.e., $h_0$ is not $\\eta$-optimal), then the probability that the algorithm returns YES is at most $\\delta$.\n",
    "\n",
    "**Answer:** Suppose that $\\varepsilon(h_0) > \\varepsilon(h^*) + \\eta$. Using the Hoeffding inequality, we have that for\n",
    "$$ \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}} $$\n",
    "then with probability at least $1 - \\delta$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\varepsilon}(h_0) &\\ge \\varepsilon(h_0) - \\gamma \\\\\n",
    "&> \\varepsilon(h^*) + \\eta - \\gamma \\\\\n",
    "&\\ge \\hat{\\varepsilon}(h^*) + \\eta - 2\\gamma \\\\\n",
    "&\\ge \\hat{\\varepsilon}(\\hat{h}) + \\eta - 2\\gamma.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here, the first and third inequalities follow from the fact that under the stated uniform convergence conditions, all hypotheses in $\\mathcal{H}$ have empirical errors within $\\gamma$ of their true generalization errors. The second inequality follows from our assumption, and the last inequality follows from the fact that $\\hat{h}$ minimizes the empirical error. Therefore, the reverse condition, $\\hat{\\varepsilon}(h_0) < \\hat{\\varepsilon}(\\hat{h}) + \\eta - 2\\gamma$ occurs with probability at most $\\delta$.\n",
    "\n",
    "### Problem 5(c) [8 points]\n",
    "\n",
    "**Problem:** Finally, suppose that $h_0 = h^*$, and let $\\eta > 0$ and $\\delta > 0$ be fixed. Show that if $m$ is sufficiently large, then the probability that the algorithm returns YES is at least $1 - \\delta$.\n",
    "\n",
    "Hint: observe that for fixed $\\eta$ and $\\delta$, as $m \\to \\infty$, we have\n",
    "$$ \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}} \\to 0. $$\n",
    "This means that there are values of $m$ for which $2\\gamma < \\eta - 2\\gamma$.\n",
    "\n",
    "**Answer:** Suppose that $h_0 = h^*$. Using the Hoeffding inequality, we have that for\n",
    "$$ \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}} $$\n",
    "then with probability at least $1 - \\delta$,\n",
    "$$ \\hat{\\varepsilon}(h_0) \\le \\varepsilon(h_0) + \\gamma $$\n",
    "$$ = \\varepsilon(h^*) + \\gamma $$\n",
    "$$ \\le \\varepsilon(\\hat{h}) + \\gamma $$\n",
    "$$ \\le \\hat{\\varepsilon}(\\hat{h}) + 2\\gamma. $$\n",
    "Here, the first and last inequalities follow from the fact that under the stated uniform convergence conditions, all hypotheses in $\\mathcal{H}$ have empirical errors within $\\gamma$ of their true generalization errors. The equality in the second step follows from our assumption, and the inequality in the third step follows from the fact that $h^*$ minimizes the true generalization error. But, observe that for fixed $\\eta$ and $\\delta$, as $m \\to \\infty$, we have\n",
    "$$ \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}} \\to 0. $$\n",
    "This implies that for $m$ sufficiently large, $4\\gamma < \\eta$, or equivalently, $2\\gamma < \\eta - 2\\gamma$. It follows that with probability at least $1 - \\delta$, if $m$ is sufficiently large, then\n",
    "$$ \\hat{\\varepsilon}(h_0) \\le \\hat{\\varepsilon}(\\hat{h}) + \\eta - 2\\gamma, $$\n",
    "so the algorithm returns YES.\n",
    "\n",
    "## Problem 6: Short answers\n",
    "\n",
    "The following questions require a reasonably short answer (usually at most 2-3 sentences or a figure, though some questions may require longer or shorter explanations). To discourage random guessing, one point will be deducted for a wrong answer on true/false or multiple choice questions! Also, no credit will be given for answers without a correct explanation.\n",
    "\n",
    "### Problem 6(a) [3 points]\n",
    "\n",
    "**Problem:** You have an implementation of Newton's method and gradient descent. Suppose that one iteration of Newton's method takes twice as long as one iteration of gradient descent. Then, this implies that gradient descent will converge to the optimal objective faster. True/False?\n",
    "\n",
    "**Answer:** False. Newton's method may take fewer steps.\n",
    "\n",
    "**Note:** Note: we received solutions that pointed out the objective function could be non-convex. Technically, it is difficult to reason about the performance of algorithms under this setting and so we intended the problem to deal with convex objective functions. However, we still awarded credit for this observation since we did not state the convexity of the objective function in the problem.\n",
    "\n",
    "### Problem 6(b) [3 points]\n",
    "\n",
    "**Problem:** A stochastic gradient descent algorithm for training logistic regression with a fixed learning rate will always converge to exactly the optimal setting of the parameters $\\theta^* = \\arg \\max_{\\theta} \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}; \\theta)$, assuming a reasonable choice of the learning rate. True/False?\n",
    "\n",
    "**Answer:** False. A fixed learning rate means that we are always taking a finite step towards improving the log-probability of any single training example in the update equation. Unless the examples are somehow aligned, we will continue jumping from side to side of the optimal solution, and will not be able to get arbitrarily close to it. The learning rate has to approach to zero in the course of the updates for the weights to converge robustly.\n",
    "\n",
    "### Problem 6(c) [3 points]\n",
    "\n",
    "**Problem:** Given a valid kernel $K(x, y)$ over $\\mathbb{R}^m$, is $K_{norm}(x, y) = \\frac{K(x,y)}{\\sqrt{K(x,x)K(y,y)}}$ a valid kernel?\n",
    "\n",
    "**Answer:** We assume that $K(x,x) > 0, \\forall x$. Yes. If we write $K(x, y) = \\Phi(x)^T\\Phi(y)$, then we have:\n",
    "$$K_{norm}(x, y) = \\frac{\\Phi(x)^T\\Phi(y)}{\\sqrt{(\\Phi(x)^T\\Phi(x))(\\Phi(y)^T\\Phi(y))}} = \\Psi(x)^T\\Psi(y)$$\n",
    "with\n",
    "$$\\Psi(x) = \\frac{\\Phi(x)}{\\sqrt{\\Phi(x)^T\\Phi(x)}}$$\n",
    "So $K_{norm}$ (as normalized) is a valid kernel.\n",
    "\n",
    "### Problem 6(d) [3 points]\n",
    "\n",
    "**Problem:** Consider a 2 class classification problem with a dataset of inputs $\\{x^{(1)} = (-1,-1), x^{(2)} = (-1, +1), x^{(3)} = (+1,-1), x^{(4)} = (+1,+1)\\}$. Can a linear SVM (with no kernel trick) shatter this set of 4 points?\n",
    "\n",
    "**Answer:** No we cannot, since the decision boundary is linear. Let the labels be represented by $y$. See that we cannot classify in the case $y^{(1)} = +1, y^{(2)} = y^{(3)} = -1, y^{(4)} = +1$.\n",
    "\n",
    "### Problem 6(e) [3 points]\n",
    "\n",
    "**Problem:** The vector of learned weights $w$ for linear hypotheses of the form $h(x) = w^T x + b$ is always perpendicular to the separating hyperplane. True/False? Justify your answer.\n",
    "\n",
    "**Answer:** True. For a linear separating boundary, the hyperplane is defined by the set $\\{x|w^T x = -b\\}$. The inner product $w^T x$ geometrically represents the projection of $x$ onto $w$. The set of all points whose projection onto $w$ is constant $(-b)$ forms a line that must be perpendicular to $w$. So $h$ is perpendicular to $w$. This fact is necessary in the formulation of geometric margins for the linear SVM.\n",
    "\n",
    "### Problem 6(f) [3 points]\n",
    "\n",
    "**Problem:** Let $\\mathcal{H}$ be a set of classifiers with a VC dimension of 5. Consider a set of 5 training examples $\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(5)}, y^{(5)}) \\}$. Now we select a classifier $h^*$ from $\\mathcal{H}$ by minimizing the classification error on the training set. Which one of the following is true?\n",
    "\n",
    "i. $x^{(5)}$ will certainly be classified correctly (i.e. $h^*(x^{(5)}) = y^{(5)}$)\n",
    "ii. $x^{(5)}$ will certainly be classified incorrectly (i.e. $h^*(x^{(5)}) \\ne y^{(5)}$)\n",
    "iii. We cannot tell\n",
    "\n",
    "Briefly justify your answer.\n",
    "\n",
    "**Answer:** We cannot tell. Since the VC-dimension of $\\mathcal{H}$ is 5, $\\mathcal{H}$ can shatter some set of points of size five. These points could be $\\{x^{(1)}, \\dots, x^{(5)}\\}$ but do not necessarily have to be. As a result, there is no guarantee that the given set will be shattered and so we can make no claims about the classification of $x^{(5)}$.\n",
    "\n",
    "### Problem 6(g) [6 points]\n",
    "\n",
    "**Problem:** Suppose you would like to use a linear regression model in order to predict the price of houses. In your model, you use the features $x_0 = 1$, $x_1 = \\text{size in square meters}$, $x_2 = \\text{height of roof in meters}$. Now, suppose a friend repeats the same analysis using exactly the same training set, only he represents the data instead using features $x'_0 = 1$, $x'_1 = x_1$, and $x'_2 = \\text{height in cm (so } x'_2 = 100x_2)$.\n",
    "\n",
    "#### Problem 6(g) i. [3 points]\n",
    "\n",
    "**Problem:** Suppose both of you run linear regression, solving for the parameters via the Normal equations. (Assume there are no degeneracies, so this gives a unique solution to the parameters.) You get parameters $\\theta_0, \\theta_1, \\theta_2$; your friend gets $\\theta'_0, \\theta'_1, \\theta'_2$. Then $\\theta'_0 = \\theta_0, \\theta'_1 = \\theta_1, \\theta'_2 = \\frac{1}{100}\\theta_2$. True/False?\n",
    "\n",
    "**Answer:** True. Observe that running a single step of Newton's method, for a linear regression problem, is equivalent to solving the Normal equations. The result then follows from the invariance of Newton's method to linear reparameterizations.\n",
    "\n",
    "#### Problem 6(g) ii. [3 points]\n",
    "\n",
    "**Problem:** Suppose both of you run linear regression, initializing the parameters to 0, and compare your results after running just *one* iteration of batch gradient descent. You get parameters $\\theta_0, \\theta_1, \\theta_2$; your friend gets $\\theta'_0, \\theta'_1, \\theta'_2$. Then $\\theta'_0 = \\theta_0, \\theta'_1 = \\theta_1, \\theta'_2 = \\frac{1}{100}\\theta_2$. True/False?\n",
    "\n",
    "**Answer:** False. Recall that gradient descent is not invariant to linear reparameterizations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
