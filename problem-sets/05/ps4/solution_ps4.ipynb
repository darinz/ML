{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4 Solutions\n",
    "\n",
    "## Problem 1: Short Answers \\[24 points\\]\n",
    "\n",
    "### (a) \\[5 points\\] Optimization Update Rule\n",
    "\n",
    "**Problem:** Given a cost function $J(\\theta)$ that we seek to minimize\n",
    "and $\\alpha \\in \\mathbb{R} > 0$, consider the following update rule:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\arg\\min_{\\theta} \\left\\{ J(\\theta^{(t)}) + \\nabla_{\\theta^{(t)}} J(\\theta^{(t)})^T (\\theta - \\theta^{(t)}) + \\frac{1}{2\\alpha} \\|\\theta - \\theta^{(t)}\\|^2 \\right\\}$$\n",
    "\n",
    "**(i) \\[3 points\\]** Show that this yields the same $\\theta^{(t+1)}$ as\n",
    "the gradient descent update with step size $\\alpha$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Denote\n",
    "$U(\\theta) = J(\\theta^{(t)}) + \\nabla_{\\theta^{(t)}} J(\\theta^{(t)})^T (\\theta - \\theta^{(t)}) + \\frac{1}{2\\alpha} \\|\\theta - \\theta^{(t)}\\|^2$.\n",
    "\n",
    "To find the minimum over $\\theta$, we compute the gradient of\n",
    "$U(\\theta)$ w.r.t. $\\theta$ and set it to 0:\n",
    "\n",
    "$$\\nabla_{\\theta} U(\\theta) = 0$$\n",
    "$$\\nabla_{\\theta^{(t)}} J(\\theta^{(t)}) + \\frac{1}{2\\alpha} (-2\\theta^{(t)} + 2\\theta) = 0$$\n",
    "$$\\alpha\\nabla_{\\theta^{(t)}} J(\\theta^{(t)}) - \\theta^{(t)} + \\theta = 0$$\n",
    "$$\\Rightarrow \\theta = \\theta^{(t)} - \\alpha\\nabla_{\\theta^{(t)}} J(\\theta^{(t)})$$\n",
    "\n",
    "which is the gradient descent update, as desired.\n",
    "\n",
    "To confirm this is a minimum, we compute the Hessian\n",
    "$\\nabla_{\\theta}^2 U = \\frac{1}{\\alpha}I$ which is positive definite as\n",
    "expected.\n",
    "\n",
    "**(ii) \\[2 points\\]** Provide a sketch (i.e. draw a picture) of the\n",
    "above update for the simplified case where $\\theta \\in \\mathbb{R}$,\n",
    "$J(\\theta) = \\theta$, and $\\theta^{(t)} = 1$. Make sure to clearly label\n",
    "$\\theta^{(t)}$, $\\theta^{(t+1)}$ and $\\alpha$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We provide an example sketch for $\\alpha = 1$. Note that\n",
    "$\\alpha = \\theta^{(t)} - \\theta^{(t+1)}$ since $\\nabla J(\\theta) = 1$.\n",
    "\n",
    "<img src=\"./q1-a-ii_solution.png\" width=\"250px\">\n",
    "\n",
    "### (b) \\[4 points\\] Loss Functions in Binary Classification\n",
    "\n",
    "**Problem:** In the binary classification setting where\n",
    "$y \\in \\{-1, +1\\}$, the margin is defined as $z = y\\theta^T x$, where\n",
    "$\\theta$ and $x$ lie in $\\mathbb{R}^n$.\n",
    "\n",
    "Three loss functions are given: i. zero-one loss:\n",
    "$\\varphi_{zo}(z) = \\mathbf{1}\\{z \\le 0\\}$ ii. exponential loss:\n",
    "$\\varphi_{\\exp}(z) = e^{-z}$ iii. hinge loss:\n",
    "$\\varphi_{\\text{hinge}}(z) = \\max\\{1 - z, 0\\}$\n",
    "\n",
    "Suppose that the margin $z < 0$ for the current parameters $\\theta$.\n",
    "\n",
    "1.  Give the expression for\n",
    "    $\\frac{\\partial}{\\partial \\theta_k}\\varphi(y\\theta^T x)$ for each of\n",
    "    the given loss functions.\n",
    "2.  Identify which loss would fail to minimize with gradient descent, no\n",
    "    matter the step size chosen.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The expressions for the partial derivatives are:\n",
    "\n",
    "1.  $\\frac{\\partial}{\\partial \\theta_k}\\varphi_{zo}(y\\theta^T x) = 0$\n",
    "2.  $\\frac{\\partial}{\\partial \\theta_k}\\varphi_{\\exp}(y\\theta^T x) = -yx_k e^{-z}$\n",
    "3.  $\\frac{\\partial}{\\partial \\theta_k}\\varphi_{\\text{hinge}}(y\\theta^T x) = -yx_k$\n",
    "\n",
    "Since the zero-one loss is 0 for margin $z < 0$, no matter the step size\n",
    "our parameter values would remain unchanged, and hence we fail to\n",
    "minimize the loss with gradient descent.\n",
    "\n",
    "### (c) \\[5 points\\] Spam Classification: Naive Bayes vs Boosting\n",
    "\n",
    "**Problem:** Consider performing spam classification where each e-mail\n",
    "is represented as a vector $\\mathbf{x}$ of the same size as the number\n",
    "of words in the vocabulary $|V|$, where $x_i$ is 1 if the e-mail\n",
    "contains word $i$ and 0 otherwise. We saw in class that Naive Bayes with\n",
    "Laplace smoothing is one simple method for performing classification in\n",
    "this setting. For this question, to simplify we set\n",
    "$p(y = 1) = p(y = -1) = 0.5$.\n",
    "\n",
    "Consider classifying $\\mathbf{x}$ by instead using the boosting\n",
    "algorithm with $2|V|$ decision stumps as the weak learners. In this\n",
    "setting, which of the two methods, Naive Bayes or boosting with decision\n",
    "stumps, would you expect to yield lower bias? Explain your reasoning.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "First, note that since $\\mathbf{x}$ is a vector of only 0s and 1s, the\n",
    "decision stump thresholds can all be set to any value strictly between 0\n",
    "and 1 and have the same effect. One possible output of the boosting\n",
    "algorithm would simply be of the form\n",
    "$\\text{sign}(\\theta^T[\\mathbf{x}; \\mathbf{x}])$ for\n",
    "$\\theta \\in \\mathbb{R}^{2|V|}$ (where we replace the 0s in $\\mathbf{x}$\n",
    "with -1s).\n",
    "\n",
    "For each possible word, Naive Bayes learns two parameters,\n",
    "$p(x_j|y = 1)$ and $p(x_j|y = -1)$, and hence also has $2|V|$ parameters\n",
    "(this is crucial for comparing the two classifiers!). The decision rule\n",
    "in log space is also linear: output\n",
    "$\\text{sign}(\\sum_j \\log p(x_j|y = 1) - \\sum_j \\log p(x_j|y = -1))$.\n",
    "However, Naive Bayes makes the generative modeling assumption that\n",
    "$p(\\mathbf{x}|y)$ is modeled by independent word counts. On the other\n",
    "hand, as a discriminative model boosting allows for more possible values\n",
    "of $\\theta$, and hence has a larger hypothesis space and should achieve\n",
    "lower bias.\n",
    "\n",
    "### (d) \\[4 points\\] Linear SVM Decision Boundary Changes\n",
    "\n",
    "**Problem:** Consider a linear SVM classifier trained for binary\n",
    "classification using the hinge loss\n",
    "$L(\\theta^T x, y) = \\max\\{0, 1 - y\\theta^T x\\}$. For each of the\n",
    "following scenarios, does the optimal decision boundary necessarily\n",
    "remain the same? Explain your reasoning and sketch a picture if helpful.\n",
    "Assume that after performing the action in each scenario, there is still\n",
    "at least one training example in both the positive and negative classes.\n",
    "\n",
    "1.  Remove all examples $(x^{(i)}, y^{(i)})$ with margin $> 1$.\n",
    "2.  Remove all examples $(x^{(i)}, y^{(i)})$ with margin $< 1$.\n",
    "3.  Add an $\\ell_2$-regularization term\n",
    "    $\\frac{\\lambda}{2}\\theta^T\\theta = \\frac{\\lambda}{2} \\|\\theta\\|^2$\n",
    "    to the training loss.\n",
    "4.  Scale all $x^{(i)}$ by a constant factor $\\alpha$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1.  **Yes;** the loss is not affected by examples with margin $> 1$.\n",
    "\n",
    "2.  **No;** the loss is affected by these examples and hence we may have\n",
    "    different optimal $\\theta$.\n",
    "\n",
    "3.  **No;** the regularization term directly encourages $\\theta$ with\n",
    "    smaller $\\ell_2$-norm, hence changing the decision boundary.\n",
    "\n",
    "4.  **No;** consider 1-D counter-example with $\\alpha = 2$, $x^{(1)}$ at\n",
    "    the origin, and $x^{(2)}$ at 1; the decision boundary moves from 0.5\n",
    "    to 1.\n",
    "\n",
    "### (e) \\[6 points\\] Bias-Variance Tradeoff Scenarios\n",
    "\n",
    "**Problem:** We consider a binary classification task where we have $m$\n",
    "training examples and our hypothesis $h_\\theta(x)$ is parameterized by\n",
    "$\\theta$. For each of the following scenarios, select whether we should\n",
    "expect bias and variance to increase or decrease. Explain your\n",
    "reasoning.\n",
    "\n",
    "**Scenario i:** Project the values of $\\theta$ to lie between $-1$ and\n",
    "$1$ after each training update, that is\n",
    "$\\theta_j = \\min\\{1, \\max\\{-1, \\theta_j\\}\\}$.\n",
    "\n",
    "**Scenario ii:** Smooth the estimates of our hypotheses by outputting\n",
    "$h(x) = (1/3) \\sum_{x^{(i)} \\in N_3(x)} h_\\theta(x^{(i)})$, where\n",
    "$N_3(x)$ are the 3 points in the training set closest to $x$.\n",
    "\n",
    "**Scenario iii:** Remove one of the feature dimensions of $x$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1.  Bias should increase and variance should decrease since we’re\n",
    "    reducing the hypothesis space of the model.\n",
    "\n",
    "2.  Bias should increase and variance should decrease since smoothing\n",
    "    encourages more similar outputs for different examples. For example,\n",
    "    consider the extreme case where we smooth by outputting the mean\n",
    "    over all $m$ examples; we then have very high bias and 0 variance\n",
    "    since we make the same prediction for every input.\n",
    "\n",
    "3.  Bias should increase and variance should decrease since for the same\n",
    "    reason as in (i); the hypothesis space is now a strict subset of the\n",
    "    previous space.\n",
    "\n",
    "## Problem 2: Linear Regression - First Order Convergence for Least Squares\n",
    "\n",
    "**Problem:** Consider the least squares problem, where we pick $\\theta$\n",
    "to minimize the objective\n",
    "$J(\\theta) = \\frac{1}{2}(X^T\\theta-y)^T(X^T\\theta-y)$. The solution to\n",
    "this problem is given by the normal equation, where\n",
    "$\\theta = (XX^T)^{-1}Xy$. In Problem Set 1, we showed that a single\n",
    "Newton step will converge to the correct solution. Now we will examine\n",
    "how gradient descent performs on the same problem.\n",
    "\n",
    "### (a) \\[4 points\\] Gradient Descent Update\n",
    "\n",
    "**Problem:** Find the gradient of $J$ with respect to $\\theta$, and\n",
    "write the gradient descent update step for $\\theta^{(t+1)}$, given\n",
    "$\\theta^{(t)}$ and step size $\\alpha$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$\\nabla_\\theta J = XX^T\\theta - Xy$;\n",
    "$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha(XX^T\\theta^{(t)} - Xy)$\n",
    "\n",
    "### (b) \\[8 points\\] Convergence to Optimal Solution\n",
    "\n",
    "**Problem:** Show that as $t \\to \\infty$,\n",
    "$\\theta^{(t+1)} \\to (XX^T)^{-1}Xy$, for gradient descent with step size\n",
    "$\\alpha$ and initial condition $\\theta^{(0)} = 0$. You may use the fact\n",
    "that $(\\alpha A)^{-1} = \\sum_{i=0}^{\\infty} (I - \\alpha A)^i$ for small\n",
    "$\\alpha > 0$, and assume that the choice of $\\alpha$ is small enough.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From (a), we have the gradient descent update:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha XX^T \\theta^{(t)} + \\alpha Xy$$\n",
    "\n",
    "$$= (I - \\alpha XX^T) \\theta^{(t)} + \\alpha Xy$$\n",
    "\n",
    "$$\\theta^{(t+1)} = (I - \\alpha XX^T)^{t+1} \\theta^{(0)} + \\sum_{i=1}^{t+1} (I - \\alpha XX^T)^{t+1-i} \\alpha Xy$$\n",
    "\n",
    "Given $\\theta^{(0)} = 0$ and adjusting the summation index:\n",
    "\n",
    "$$\\theta^{(t+1)} = 0 + \\alpha \\sum_{i=0}^{t} (I - \\alpha XX^T)^i Xy$$\n",
    "\n",
    "As $t \\to \\infty$,\n",
    "$\\sum_{i=0}^{t} (I - \\alpha XX^T)^i = (\\alpha XX^T)^{-1}$. Using this,\n",
    "we now have:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\alpha \\alpha^{-1} (XX^T)^{-1} Xy$$\n",
    "\n",
    "$$= (XX^T)^{-1} Xy$$\n",
    "\n",
    "## Problem 3: Generative Models - Gaussian Discriminant Analysis \\[12 points\\]\n",
    "\n",
    "**Problem:** Consider the 1-dimensional Gaussian discriminant analysis\n",
    "model where $x \\in \\mathbb{R}$ and we assume\n",
    "\n",
    "$$p(y) = \\phi^{1\\{y=1\\}} (1-\\phi)^{1\\{y=-1\\}}$$\n",
    "\n",
    "$$p(x|y = -1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_{-1})^2\\right)$$\n",
    "\n",
    "$$p(x|y = 1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_1)^2\\right)$$\n",
    "\n",
    "In this problem we will assume that $\\sigma$ is a fixed quantity that we\n",
    "have been given and is therefore not a parameter of the model.\n",
    "\n",
    "Recall from Problem Set 1 that we can express\n",
    "$p(y|x; \\phi, \\mu_{-1}, \\mu_1)$ in the form\n",
    "\n",
    "$$p(y|x; \\theta) = \\frac{1}{1 + \\exp(-y(\\theta_1 x + \\theta_0))}$$\n",
    "\n",
    "where for the model described above we have,\n",
    "\n",
    "$$\\theta_0 = \\frac{1}{2\\sigma^2}(\\mu_{-1}^2 - \\mu_1^2) - \\log \\frac{1-\\phi}{\\phi}$$\n",
    "\n",
    "$$\\theta_1 = \\frac{1}{\\sigma^2}(\\mu_1 - \\mu_{-1})$$\n",
    "\n",
    "### (a) \\[2 points\\] Joint Log-Likelihood\n",
    "\n",
    "**Problem:** Write the joint log-likelihood\n",
    "$\\ell(\\phi, \\mu_{-1}, \\mu_1) = \\log p(x, y; \\phi, \\mu_{-1}, \\mu_1)$ for\n",
    "a single example $(x, y)$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$p(x, y; \\phi, \\mu_{-1}, \\mu_1) = p(y; \\phi)p(x|y; \\mu_{-1}, \\mu_1)$$\n",
    "\n",
    "$$\\log p(x, y; \\phi, \\mu_{-1}, \\mu_1) = \\log p(y|\\phi) + \\log p(x|y; \\mu_{-1}, \\mu_1)$$\n",
    "\n",
    "$$= \\log(1 - \\phi)^{1\\{y=-1\\}} \\log(\\phi)^{1\\{y=1\\}} + \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2}(x - \\mu_y)^2$$\n",
    "\n",
    "### (b) \\[7 points\\] Concavity of Log-Likelihood\n",
    "\n",
    "**Problem:** Show that the log-likelihood of all training examples\n",
    "$\\{(x^{(i)},y^{(i)})\\}_{i=1}^m$ is concave (and hence any maximum we\n",
    "find must be the global maximum) by first computing\n",
    "$\\frac{\\partial^2 \\ell}{\\partial \\phi^2}$,\n",
    "$\\frac{\\partial^2 \\ell}{\\partial \\mu_{-1}^2}$, and\n",
    "$\\frac{\\partial^2 \\ell}{\\partial \\mu_1^2}$ for a single example\n",
    "$(x, y)$. Then make an argument that the total log-likelihood is\n",
    "concave. Hint: Recall a function is concave if its Hessian is negative\n",
    "semidefinite. A one-dimensional function $f$ is concave if\n",
    "$f''(x) \\le 0$ for all $x$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "First we show that the log-likelihood is concave for a single $(x, y)$.\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\phi} = -1\\{y = -1\\}\\frac{1}{1 - \\phi} + 1\\{y = 1\\}\\frac{1}{\\phi}$$\n",
    "\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\phi^2} = \\begin{cases} -\\phi^{-2} & y = 1 \\\\ -(1 - \\phi)^{-2} & y = -1 \\end{cases}$$\n",
    "\n",
    "which is negative for both cases.\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu_y} = \\frac{1}{\\sigma^2}(x - \\mu_y)$$\n",
    "\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\mu_y^2} = -\\frac{1}{\\sigma^2}$$\n",
    "\n",
    "and negative as well.\n",
    "\n",
    "Since $\\phi$ and $\\mu_y$ are in separate terms, the Hessian $H$ must be\n",
    "diagonal and negative along the diagonal. Hence $H$ is negative\n",
    "semidefinite, and $\\ell$ is concave in both $\\phi$ and $\\mu_y$. Due to\n",
    "linearity of differentiation, the sum of concave functions is concave,\n",
    "and thus log-likelihood over all training $m$ examples must be concave\n",
    "as well.\n",
    "\n",
    "### (c) \\[3 points\\] Decision Boundary\n",
    "\n",
    "**Problem:** Derive an expression for the decision boundary for\n",
    "classifying $x$ as either $y = -1$ or $1$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We want $p(y = -1|x;\\theta) = p(y = 1|x;\\theta) = 0.5$ and hence set\n",
    "$\\theta_1x + \\theta_0 = 0$ where $\\theta_1$ and $\\theta_0$ are given in\n",
    "the problem statement.\n",
    "\n",
    "Solving, we find:\n",
    "\n",
    "$$x = \\frac{2\\sigma^2 \\log \\frac{1-\\phi}{\\phi} + (\\mu_1^2 - \\mu_{-1}^2)}{2(\\mu_1 - \\mu_{-1})}$$\n",
    "\n",
    "Note that setting $p(x|y = -1) = p(x|y = 1)$ does not work, since this\n",
    "does not take into account $p(y)$.\n",
    "\n",
    "## Problem 4: Generalized Linear Models - Gaussian Distribution\n",
    "\n",
    "**Problem:** Assume we are given $x_1, x_2, \\dots, x_n$ drawn i.i.d.\n",
    "$\\sim N(\\mu, \\sigma^2)$, that is,\n",
    "\n",
    "$$p(x_i; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{1}{2\\sigma^2}(x_i - \\mu)^2 \\right)$$\n",
    "\n",
    "Define $s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2$ where\n",
    "$\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}$.\n",
    "\n",
    "### (a) \\[3 points\\] Unbiased Estimator\n",
    "\n",
    "**Problem:** Prove $g(x) = \\frac{s^2}{n-1}$ is an unbiased estimator of\n",
    "$\\sigma^2$, that is\n",
    "\n",
    "$$E[g(x)] = \\sigma^2$$\n",
    "\n",
    "Hint: $E[x_i] = \\mu$, $Var(x_i) = \\sigma^2$, $Cov(x_i, x_j) = 0$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$E[g(x)] = \\frac{1}{n-1} E\\left[ \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\right]$$\n",
    "\n",
    "$$= \\frac{1}{n-1} \\left( n(\\sigma^2 + \\mu^2) - \\frac{1}{n}(n(\\sigma^2 + \\mu^2) + \\mu^2n(n - 1)) \\right)$$\n",
    "\n",
    "$$= \\frac{1}{n-1} \\left( (n - 1)(\\sigma^2 + \\mu^2) - (n - 1)\\mu^2 \\right)$$\n",
    "\n",
    "$$= \\sigma^2$$\n",
    "\n",
    "### (b) \\[5 points\\] Maximum Likelihood Estimation\n",
    "\n",
    "**Problem:** Find the maximum-likelihood estimate of $\\mu$ and\n",
    "$\\sigma^2$. Hint: You should be able to express your final expression\n",
    "for $\\sigma^2$ in terms of $s^2$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$L = \\prod_{i=1}^{n} p(x_i; \\mu, \\sigma^2)$$\n",
    "\n",
    "$$= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "$$l = -\\frac{1}{2} \\sum_{i=1}^{n} \\left(\\log 2\\pi\\sigma^2 + \\frac{(x_i - \\mu)^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "$$\\nabla_{\\sigma^2}l = -\\frac{1}{2} \\sum_{i=1}^{n} \\left(\\frac{1}{\\sigma^2} - \\frac{(x_i - \\mu)^2}{\\sigma^4}\\right)$$\n",
    "\n",
    "$$\\nabla_{\\mu}l = \\frac{1}{2} \\sum_{i=1}^{n} \\left(\\frac{2(x_i - \\mu)}{\\sigma^2}\\right)$$\n",
    "\n",
    "Setting $\\nabla_{\\sigma^2}l = 0$ and $\\nabla_{\\mu}l = 0$, we have:\n",
    "\n",
    "$$\\mu = \\frac{\\sum_{i=1}^{n} x_i}{n}$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2 = \\frac{s^2}{n}$$\n",
    "\n",
    "### (c) \\[6 points\\] Exponential Family Form\n",
    "\n",
    "**Problem:** Show that the general form of the Gaussian distribution is\n",
    "a member of the exponential family by finding $b(x)$, $\\eta$, $T(x)$,\n",
    "and $a(\\eta)$. Hint: Since both $\\mu$ and $\\sigma^2$ are parameters,\n",
    "$\\eta$ and $T(x)$ will now be two dimensional vectors. Denote\n",
    "$\\eta = [\\eta_1, \\eta_2]^T$ and try to express $a(\\eta)$ in terms of\n",
    "$\\eta_1$ and $\\eta_2$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$b(x) = \\frac{1}{\\sqrt{2\\pi}}$$\n",
    "\n",
    "$$\\eta = \\left[\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}\\right]^T$$\n",
    "\n",
    "$$T(x) = [x, x^2]^T$$\n",
    "\n",
    "$$a(\\eta) = \\frac{\\mu^2}{2\\sigma^2} + \\log \\sigma = -\\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2}\\log(-2\\eta_2)$$\n",
    "\n",
    "### (d) \\[4 points\\] Verification of Exponential Family Properties\n",
    "\n",
    "**Problem:** Verify that $\\nabla_\\eta a(\\eta) = E[T(x); \\eta]$ for the\n",
    "Gaussian distribution. Hint: You can prove this either by using the\n",
    "general form of exponential families, or by computing\n",
    "$\\nabla_\\eta a(\\eta)$ directly from part (c).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "In general for an exponential family,\n",
    "\n",
    "$$\\int h(x) \\exp (\\eta^T T(x) - a(\\eta)) dx = 1$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$a(\\eta) = \\log \\int h(x) \\exp (\\eta^T T(x)) dx$$\n",
    "\n",
    "$$\\nabla_\\eta a(\\eta) = \\frac{\\int h(x) \\exp (\\eta^T T(x)) T(x)dx}{\\int h(x) \\exp (\\eta^T T(x)) dx}$$\n",
    "\n",
    "$$= \\frac{\\int h(x) \\exp (\\eta^T T(x) - a(\\eta)) T(x)dx}{\\int h(x) \\exp (\\eta^T T(x) - a(\\eta)) dx}$$\n",
    "\n",
    "$$= E[T(x); \\eta]$$\n",
    "\n",
    "We consider the two components of $\\eta$ separately for the case of the\n",
    "Gaussian distribution:\n",
    "\n",
    "$$\\nabla_{\\eta_1} a(\\eta) = \\nabla_{\\eta_1} \\left( -\\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2} \\log(-2\\eta_2) \\right)$$\n",
    "\n",
    "$$= -\\frac{2\\eta_1}{4\\eta_2}$$\n",
    "\n",
    "$$= \\mu$$\n",
    "\n",
    "$$= E[x]$$\n",
    "\n",
    "$$\\nabla_{\\eta_2} a(\\eta) = \\nabla_{\\eta_2} \\left( -\\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2} \\log(-2\\eta_2) \\right)$$\n",
    "\n",
    "$$= \\frac{\\eta_1^2}{4\\eta_2^2} - \\frac{1}{2\\eta_2}$$\n",
    "\n",
    "$$= \\mu^2 + \\sigma^2$$\n",
    "\n",
    "$$= E[x^2]$$\n",
    "\n",
    "### (e) \\[4 points\\] Positive Semidefiniteness of Hessian\n",
    "\n",
    "**Problem:** Show that $\\nabla_\\eta^2 a(\\eta)$ is positive semidefinite.\n",
    "Hint: You can compute $\\nabla_\\eta^2 a(\\eta)$ using the results from\n",
    "part (c) and (d). Or instead you may use the following fact: In general\n",
    "for exponential families,\n",
    "\n",
    "$$\\nabla_\\eta^2 a(\\eta) = E [T(x)T(x)^T] - E[T(x)]E[T(x)]^T$$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Applying the above formula, we have:\n",
    "\n",
    "$$\\nabla_\\eta^2 a(\\eta) = \\begin{bmatrix} \\sigma^2 & 2\\mu\\sigma^2 \\\\ 2\\mu\\sigma^2 & 4\\mu^2\\sigma^2 + 2\\sigma^4 \\end{bmatrix}$$\n",
    "\n",
    "We can then confirm the Hessian (covariance of $T(x)$) is positive\n",
    "semidefinite:\n",
    "\n",
    "$$z^T[\\nabla_\\eta^2 a(\\eta)]z = \\sigma^2 z_1^2 + 4\\mu\\sigma^2 z_1 z_2 + 4\\mu^2 \\sigma^2 z_2^2 + 2\\sigma^4 z_2^2$$\n",
    "\n",
    "$$= (\\sigma z_1 + 2\\mu\\sigma z_2)^2 + 2\\sigma^4 z_2^2$$\n",
    "\n",
    "$$\\ge 0.$$"
   ],
   "id": "d1b5edb2-db6b-4681-8e42-14432b39a6f0"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
