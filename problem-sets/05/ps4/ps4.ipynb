{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0495053d-64ce-44f6-8db9-4eb00d92e98c",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n",
    "## Problem 1: Short Answers [24 points]\n",
    "\n",
    "### (a) [5 points] Optimization Update Rule\n",
    "\n",
    "**Problem:** Given a cost function $J(\\theta)$ that we seek to minimize and $\\alpha \\in \\mathbb{R} > 0$, consider the following update rule:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\arg\\min_{\\theta} \\left\\{ J(\\theta^{(t)}) + \\nabla_{\\theta^{(t)}} J(\\theta^{(t)})^T (\\theta - \\theta^{(t)}) + \\frac{1}{2\\alpha} \\|\\theta - \\theta^{(t)}\\|^2 \\right\\}$$\n",
    "\n",
    "**(i) [3 points]** Show that this yields the same $\\theta^{(t+1)}$ as the gradient descent update with step size $\\alpha$.\n",
    "\n",
    "**(ii) [2 points]** Provide a sketch (i.e. draw a picture) of the above update for the simplified case where $\\theta \\in \\mathbb{R}$, $J(\\theta) = \\theta$, and $\\theta^{(t)} = 1$. Make sure to clearly label $\\theta^{(t)}$, $\\theta^{(t+1)}$ and $\\alpha$.\n",
    "\n",
    "### (b) [4 points] Loss Functions in Binary Classification\n",
    "\n",
    "**Problem:** In the binary classification setting where $y \\in \\{-1, +1\\}$, the margin is defined as $z = y\\theta^T x$, where $\\theta$ and $x$ lie in $\\mathbb{R}^n$.\n",
    "\n",
    "Three loss functions are given:\n",
    "i. zero-one loss: $\\varphi_{zo}(z) = \\mathbf{1}\\{z \\le 0\\}$\n",
    "ii. exponential loss: $\\varphi_{\\exp}(z) = e^{-z}$\n",
    "iii. hinge loss: $\\varphi_{\\text{hinge}}(z) = \\max\\{1 - z, 0\\}$\n",
    "\n",
    "Suppose that the margin $z < 0$ for the current parameters $\\theta$.\n",
    "\n",
    "1. Give the expression for $\\frac{\\partial}{\\partial \\theta_k}\\varphi(y\\theta^T x)$ for each of the given loss functions.\n",
    "2. Identify which loss would fail to minimize with gradient descent, no matter the step size chosen.\n",
    "\n",
    "### (c) [5 points] Spam Classification: Naive Bayes vs Boosting\n",
    "\n",
    "**Problem:** Consider performing spam classification where each e-mail is represented as a vector $\\mathbf{x}$ of the same size as the number of words in the vocabulary $|V|$, where $x_i$ is 1 if the e-mail contains word $i$ and 0 otherwise. We saw in class that Naive Bayes with Laplace smoothing is one simple method for performing classification in this setting. For this question, to simplify we set $p(y = 1) = p(y = -1) = 0.5$.\n",
    "\n",
    "Consider classifying $\\mathbf{x}$ by instead using the boosting algorithm with $2|V|$ decision stumps as the weak learners. In this setting, which of the two methods, Naive Bayes or boosting with decision stumps, would you expect to yield lower bias? Explain your reasoning.\n",
    "\n",
    "### (d) [4 points] Linear SVM Decision Boundary Changes\n",
    "\n",
    "**Problem:** Consider a linear SVM classifier trained for binary classification using the hinge loss $L(\\theta^T x, y) = \\max\\{0, 1 - y\\theta^T x\\}$. For each of the following scenarios, does the optimal decision boundary necessarily remain the same? Explain your reasoning and sketch a picture if helpful. Assume that after performing the action in each scenario, there is still at least one training example in both the positive and negative classes.\n",
    "\n",
    "i. Remove all examples $(x^{(i)}, y^{(i)})$ with margin $> 1$.\n",
    "ii. Remove all examples $(x^{(i)}, y^{(i)})$ with margin $< 1$.\n",
    "iii. Add an $\\ell_2$-regularization term $\\frac{\\lambda}{2}\\theta^T\\theta = \\frac{\\lambda}{2} \\|\\theta\\|^2$ to the training loss.\n",
    "iv. Scale all $x^{(i)}$ by a constant factor $\\alpha$.\n",
    "\n",
    "### (e) [6 points] Bias-Variance Tradeoff Scenarios\n",
    "\n",
    "**Problem:** We consider a binary classification task where we have $m$ training examples and our hypothesis $h_\\theta(x)$ is parameterized by $\\theta$. For each of the following scenarios, select whether we should expect bias and variance to increase or decrease. Explain your reasoning.\n",
    "\n",
    "**Scenario i:** Project the values of $\\theta$ to lie between $-1$ and $1$ after each training update, that is $\\theta_j = \\min\\{1, \\max\\{-1, \\theta_j\\}\\}$.\n",
    "\n",
    "**Scenario ii:** Smooth the estimates of our hypotheses by outputting\n",
    "$h(x) = (1/3) \\sum_{x^{(i)} \\in N_3(x)} h_\\theta(x^{(i)})$,\n",
    "where $N_3(x)$ are the 3 points in the training set closest to $x$.\n",
    "\n",
    "**Scenario iii:** Remove one of the feature dimensions of $x$.\n",
    "\n",
    "## Problem 2: Linear Regression - First Order Convergence for Least Squares\n",
    "\n",
    "**Problem:** Consider the least squares problem, where we pick $\\theta$ to minimize the objective $J(\\theta) = \\frac{1}{2}(X^T\\theta-y)^T(X^T\\theta-y)$. The solution to this problem is given by the normal equation, where $\\theta = (XX^T)^{-1}Xy$. In Problem Set 1, we showed that a single Newton step will converge to the correct solution. Now we will examine how gradient descent performs on the same problem.\n",
    "\n",
    "### (a) [4 points] Gradient Descent Update\n",
    "\n",
    "**Problem:** Find the gradient of $J$ with respect to $\\theta$, and write the gradient descent update step for $\\theta^{(t+1)}$, given $\\theta^{(t)}$ and step size $\\alpha$.\n",
    "\n",
    "### (b) [8 points] Convergence to Optimal Solution\n",
    "\n",
    "**Problem:** Show that as $t \\to \\infty$, $\\theta^{(t+1)} \\to (XX^T)^{-1}Xy$, for gradient descent with step size $\\alpha$ and initial condition $\\theta^{(0)} = 0$. You may use the fact that $(\\alpha A)^{-1} = \\sum_{i=0}^{\\infty} (I - \\alpha A)^i$ for small $\\alpha > 0$, and assume that the choice of $\\alpha$ is small enough.\n",
    "\n",
    "## Problem 3: Generative Models - Gaussian Discriminant Analysis [12 points]\n",
    "\n",
    "**Problem:** Consider the 1-dimensional Gaussian discriminant analysis model where $x \\in \\mathbb{R}$ and we assume\n",
    "\n",
    "$$p(y) = \\phi^{1\\{y=1\\}} (1-\\phi)^{1\\{y=-1\\}}$$\n",
    "\n",
    "$$p(x|y = -1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_{-1})^2\\right)$$\n",
    "\n",
    "$$p(x|y = 1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_1)^2\\right)$$\n",
    "\n",
    "In this problem we will assume that $\\sigma$ is a fixed quantity that we have been given and is therefore not a parameter of the model.\n",
    "\n",
    "Recall from Problem Set 1 that we can express $p(y|x; \\phi, \\mu_{-1}, \\mu_1)$ in the form\n",
    "\n",
    "$$p(y|x; \\theta) = \\frac{1}{1 + \\exp(-y(\\theta_1 x + \\theta_0))}$$\n",
    "\n",
    "where for the model described above we have,\n",
    "\n",
    "$$\\theta_0 = \\frac{1}{2\\sigma^2}(\\mu_{-1}^2 - \\mu_1^2) - \\log \\frac{1-\\phi}{\\phi}$$\n",
    "\n",
    "$$\\theta_1 = \\frac{1}{\\sigma^2}(\\mu_1 - \\mu_{-1})$$\n",
    "\n",
    "### (a) [2 points] Joint Log-Likelihood\n",
    "\n",
    "**Problem:** Write the joint log-likelihood $\\ell(\\phi, \\mu_{-1}, \\mu_1) = \\log p(x, y; \\phi, \\mu_{-1}, \\mu_1)$ for a single example $(x, y)$.\n",
    "\n",
    "### (b) [7 points] Concavity of Log-Likelihood\n",
    "\n",
    "**Problem:** Show that the log-likelihood of all training examples $\\{(x^{(i)},y^{(i)})\\}_{i=1}^m$ is concave (and hence any maximum we find must be the global maximum) by first computing $\\frac{\\partial^2 \\ell}{\\partial \\phi^2}$, $\\frac{\\partial^2 \\ell}{\\partial \\mu_{-1}^2}$, and $\\frac{\\partial^2 \\ell}{\\partial \\mu_1^2}$ for a single example $(x, y)$. Then make an argument that the total log-likelihood is concave. Hint: Recall a function is concave if its Hessian is negative semidefinite. A one-dimensional function $f$ is concave if $f''(x) \\le 0$ for all $x$.\n",
    "\n",
    "### (c) [3 points] Decision Boundary\n",
    "\n",
    "**Problem:** Derive an expression for the decision boundary for classifying $x$ as either $y = -1$ or $1$.\n",
    "\n",
    "## Problem 4: Generalized Linear Models - Gaussian Distribution\n",
    "\n",
    "**Problem:** Assume we are given $x_1, x_2, \\dots, x_n$ drawn i.i.d. $\\sim N(\\mu, \\sigma^2)$, that is,\n",
    "\n",
    "$$p(x_i; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{1}{2\\sigma^2}(x_i - \\mu)^2 \\right)$$\n",
    "\n",
    "Define $s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2$ where $\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}$.\n",
    "\n",
    "### (a) [3 points] Unbiased Estimator\n",
    "\n",
    "**Problem:** Prove $g(x) = \\frac{s^2}{n-1}$ is an unbiased estimator of $\\sigma^2$, that is\n",
    "\n",
    "$$E[g(x)] = \\sigma^2$$\n",
    "\n",
    "Hint: $E[x_i] = \\mu$, $Var(x_i) = \\sigma^2$, $Cov(x_i, x_j) = 0$.\n",
    "\n",
    "### (b) [5 points] Maximum Likelihood Estimation\n",
    "\n",
    "**Problem:** Find the maximum-likelihood estimate of $\\mu$ and $\\sigma^2$. Hint: You should be able to express your final expression for $\\sigma^2$ in terms of $s^2$.\n",
    "\n",
    "### (c) [6 points] Exponential Family Form\n",
    "\n",
    "**Problem:** Show that the general form of the Gaussian distribution is a member of the exponential family by finding $b(x)$, $\\eta$, $T(x)$, and $a(\\eta)$. Hint: Since both $\\mu$ and $\\sigma^2$ are parameters, $\\eta$ and $T(x)$ will now be two dimensional vectors. Denote $\\eta = [\\eta_1, \\eta_2]^T$ and try to express $a(\\eta)$ in terms of $\\eta_1$ and $\\eta_2$.\n",
    "\n",
    "### (d) [4 points] Verification of Exponential Family Properties\n",
    "\n",
    "**Problem:** Verify that $\\nabla_\\eta a(\\eta) = E[T(x); \\eta]$ for the Gaussian distribution. Hint: You can prove this either by using the general form of exponential families, or by computing $\\nabla_\\eta a(\\eta)$ directly from part (c).\n",
    "\n",
    "### (e) [4 points] Positive Semidefiniteness of Hessian\n",
    "\n",
    "**Problem:** Show that $\\nabla_\\eta^2 a(\\eta)$ is positive semidefinite. Hint: You can compute $\\nabla_\\eta^2 a(\\eta)$ using the results from part (c) and (d). Or instead you may use the following fact: In general for exponential families,\n",
    "\n",
    "$$\\nabla_\\eta^2 a(\\eta) = E [T(x)T(x)^T] - E[T(x)]E[T(x)]^T$$\n",
    "\n",
    "## Problem 5: Shift Invariant Kernels\n",
    "\n",
    "**Problem:** A kernel $K$ on $\\mathbb{R}^n$ is said to be shift invariant if:\n",
    "$\\forall \\delta \\in \\mathbb{R}^n, \\forall x, z \\in \\mathbb{R}^n, K(x, z) = K(x + \\delta, z + \\delta)$\n",
    "\n",
    "### (a) [4 points] Examples of Shift Invariant and Non-Shift Invariant Kernels\n",
    "\n",
    "**Problem:** Give an example of a shift invariant and a non-shift invariant kernels seen in lectures (no need to prove they are kernels). For the rest of this problem, we will simplify a bit and consider the case where $n = 1$.\n",
    "\n",
    "### (b) [6 points] Kernel Property Proof\n",
    "\n",
    "**Problem:** Let $p(\\omega)$ be a probability density over $\\mathbb{R}$. Let $\\phi : \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}^d$. Define $F : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ as:\n",
    "\n",
    "$$F(x, z) = \\int_{-\\infty}^{\\infty} \\phi(x, \\omega)^T \\phi(z, \\omega) p(\\omega) d\\omega$$\n",
    "\n",
    "Show that $F$ is a kernel for all $x, z \\in \\mathbb{R}^n$.\n",
    "\n",
    "### (c) [4 points] Trigonometric Kernel Construction\n",
    "\n",
    "**Problem:** Let's suppose $n = 1$. Let $h: \\mathbb{R} \\to \\mathbb{R}$ be a function such that\n",
    "$$ \\forall z \\in \\mathbb{R}, h(z) = \\int_{-\\infty}^{\\infty} \\cos(\\omega z) p(\\omega) d\\omega $$\n",
    "Show that there exists $\\phi$ such that $h(x - z) = \\int_{-\\infty}^{\\infty} \\phi(x, \\omega)^T \\phi(z, \\omega) p(\\omega) d\\omega$. Provide an explicit definition of $\\phi$. Hint: Use the trigonometric identity $\\cos(a-b) = \\cos(a) \\cos(b) + \\sin(a) \\sin(b)$, valid for all $a, b \\in \\mathbb{R}$.\n",
    "\n",
    "### (d) [2 points] Kernel Verification\n",
    "\n",
    "**Problem:** Show that $K(x, z) = h(x - z)$ is indeed a kernel.\n",
    "\n",
    "## Problem 6: Learning Theory - Relaxed Generalization Bounds [10 points]\n",
    "\n",
    "**Problem:** Let $Z_1, Z_2, \\dots, Z_m$ be independent and identically distributed random variables drawn from a Bernoulli($\\phi$) distribution where $P(Z_i = 1) = \\phi$ and $P(Z_i = 0) = 1 - \\phi$. Let $\\hat{\\phi} = (1/m) \\sum_{i=1}^m Z_i$, and let any $\\gamma > 0$ be fixed. Hoeffding's inequality, as we saw in class, states\n",
    "$$P(|\\phi - \\hat{\\phi}| > \\gamma) \\le 2 \\exp(-2\\gamma^2 m)$$\n",
    "However, this relies on the assumption that the random variables $Z_1, \\dots, Z_m$ are all *jointly independent*. In this problem we will relax this assumption by only assuming *pairwise independence* among the $Z_i$. In this case we cannot apply Hoeffding's inequality, but the following inequality (Chebyshev's inequality) holds:\n",
    "$$P(|\\phi - \\hat{\\phi}| > \\gamma) \\le \\frac{\\text{Var}(Z_i)}{m\\gamma^2}$$\n",
    "where $\\text{Var}(Z_i)$ denotes the variance of the random variable $Z_i$ and for $Z_i \\sim \\text{Bernoulli}(\\phi)$ we have $\\text{Var}(Z_i) = \\phi(1 - \\phi)$.\n",
    "\n",
    "Given our hypothesis set $\\mathcal{H} = \\{h_1, \\dots, h_k\\}$ and $m$ pairwise but not necessarily jointly independent data samples $(x, y) \\sim \\mathcal{D}$, we now derive guarantees on the generalization error of our best hypothesis\n",
    "$$\\hat{h} = \\arg\\min_{h \\in \\mathcal{H}} \\hat{\\epsilon}(h)$$\n",
    "where as usual we define $\\hat{\\epsilon}(h) = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}\\{h(x^{(i)}) \\ne y^{(i)}\\}$, where $(x^{(i)}, y^{(i)})$ are examples from the training set.\n",
    "\n",
    "### (a) [2 points] Maximum Variance\n",
    "\n",
    "**Problem:** What is the maximum possible value of $\\text{Var}(Z_i) = \\phi(1 - \\phi)$? From now on we will instead use this maximal value such that the bounds we derive hold for all possible $\\phi$.\n",
    "\n",
    "### (b) [4 points] Sample Size Requirements\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "**i. [2 points]** Give a non-trivial upper bound on $P(|\\epsilon(h) - \\hat{\\epsilon}(h)| > \\gamma)$ for a hypothesis $h$.\n",
    "\n",
    "**ii. [1 point]** Given a fixed $\\delta \\in (0,1)$, how large must the sample size $m$ be to guarantee that $P(|\\epsilon(\\hat{h}) - \\hat{\\epsilon}(\\hat{h})| > \\gamma) \\le \\delta$? (In other words, to ensure that the training error and generalization error are within $\\gamma$ of one another with probability at least $1 - \\delta$.)\n",
    "\n",
    "**iii. [1 point]** Compare this sample size to what is achievable using Hoeffding's inequality.\n",
    "\n",
    "### (c) [4 points] Generalization Error Bound\n",
    "\n",
    "**Problem:** Show that with probability at least $1 - \\delta$, the difference between the generalization error of $\\hat{h}$ and the generalization error of the best hypothesis in $\\mathcal{H}$ (i.e. the hypothesis $h^* = \\arg\\min_{h \\in \\mathcal{H}} \\mathcal{E}(h)$) is bounded by $\\sqrt{k/(m\\delta)}$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
