{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 Solutions\n",
    "\n",
    "## Problem 1: Generalized Linear Models\n",
    "\n",
    "### (a) \\[10 points\\] Log-likelihood Concavity\n",
    "\n",
    "**Problem:** Given a training set $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$, the\n",
    "loglikelihood is given by\n",
    "$$\\ell(\\theta) = \\sum_{i=1}^m \\log p(y^{(i)} | x^{(i)}; \\theta).$$\n",
    "\n",
    "Give a set of conditions on $b(y)$, $T(y)$, and $a(\\eta)$ which ensure\n",
    "that the loglikelihood is a concave function of $\\theta$ (and thus has a\n",
    "unique maximum). Your conditions must be reasonable, and should be as\n",
    "weak as possible.\n",
    "\n",
    "**Answer:** The log-likelihood is given by\n",
    "$$\\ell(\\theta) = \\sum_{k=1}^M \\log(b(y)) + \\eta^{(k)}T(y) - a(\\eta^{(k)})$$\n",
    "\n",
    "where $\\eta^{(k)} = \\theta^T x^{(k)}$. Find the Hessian by taking the\n",
    "partials with respect to $\\theta_i$ and $\\theta_j$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i} \\ell(\\theta) = \\sum_{k=1}^M T(y)x_i^{(k)} - \\frac{\\partial}{\\partial \\eta} a(\\eta^{(k)})x_i^{(k)}$$\n",
    "\n",
    "$$\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ell(\\theta) = \\sum_{k=1}^M - \\frac{\\partial^2}{\\partial \\eta^2} a(\\eta^{(k)}) x_i^{(k)} x_j^{(k)}$$\n",
    "\n",
    "The Hessian is:\n",
    "$$H = -\\sum_{k=1}^{M} \\frac{\\partial^2}{\\partial \\eta^2} a(\\eta^{(k)}) x^{(k)} x^{(k)T}$$\n",
    "\n",
    "For any vector $z$:\n",
    "$$z^T H z = -\\sum_{k=1}^{M} \\frac{\\partial^2}{\\partial \\eta^2} a(\\eta^{(k)}) (z^T x^{(k)})^2$$\n",
    "\n",
    "If $\\frac{\\partial^2}{\\partial \\eta^2} a(\\eta) \\ge 0$ for all $\\eta$,\n",
    "then $z^T H z \\le 0$. If H is negative semi-definite, then the original\n",
    "optimization problem is concave.\n",
    "\n",
    "### (b) \\[3 points\\] Normal Distribution Verification\n",
    "\n",
    "**Problem:** When the response variable is distributed according to a\n",
    "Normal distribution (with unit variance), we have\n",
    "$b(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{y^2}{2}}$, $T(y) = y$, and\n",
    "$a(\\eta) = \\frac{\\eta^2}{2}$. Verify that the condition(s) you gave in\n",
    "part (a) hold for this setting.\n",
    "\n",
    "**Answer:** $$\\frac{\\partial^2}{\\partial \\eta^2} a(\\eta) = 1 \\ge 0.$$\n",
    "\n",
    "## Problem 2: Bayesian Linear Regression\n",
    "\n",
    "**Problem:** Consider Bayesian linear regression using a Gaussian prior\n",
    "on the parameters $\\theta \\in \\mathbb{R}^{n+1}$. Thus, in our prior,\n",
    "$\\theta \\sim N(0, \\tau^2 I_{n+1})$, where $\\tau^2 \\in \\mathbb{R}$, and\n",
    "$I_{n+1}$ is the $n+1$-by-$n+1$ identity matrix. Also let the\n",
    "conditional distribution of $y^{(i)}$ given $x^{(i)}$ and $\\theta$ be\n",
    "$N(\\theta^T x^{(i)}, \\sigma^2)$, as in our usual linear least-squares\n",
    "model.$^1$ Let a set of $m$ IID training examples be given (with\n",
    "$x^{(i)} \\in \\mathbb{R}^{n+1}$). Recall that the MAP estimate of the\n",
    "parameters $\\theta$ is given by:\n",
    "$$\\theta_{MAP} = \\arg \\max_{\\theta} \\left( \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}, \\theta) \\right) p(\\theta)$$\n",
    "\n",
    "Find, in closed form, the MAP estimate of the parameters $\\theta$. For\n",
    "this problem, you should treat $\\tau^2$ and $\\sigma^2$ as fixed, known,\n",
    "constants. \\[Hint: Your solution should involve deriving something that\n",
    "looks a bit like the Normal equations.\\]\n",
    "\n",
    "**Answer:**\n",
    "$$\\theta_{MAP} = \\arg \\max_{\\theta} \\left( \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}, \\theta) \\right) p(\\theta)$$\n",
    "$$= \\arg \\max_{\\theta} \\log \\left[ \\left( \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}, \\theta) \\right) p(\\theta) \\right]$$\n",
    "$$= \\arg \\min_{\\theta} \\left( -\\log p(\\theta) - \\sum_{i=1}^{m} \\log p(y^{(i)}|x^{(i)}, \\theta) \\right) \\quad (1)$$\n",
    "\n",
    "Substituting expressions for $p(\\theta)$ and\n",
    "$p(y^{(i)}|x^{(i)}, \\theta)$, and dropping terms that don’t affect the\n",
    "optimization, we get:\n",
    "$$\\theta_{MAP} = \\arg \\min_{\\theta} \\left( \\frac{\\sigma^2}{\\tau^2} \\theta^T \\theta + (Y - X\\theta)^T (Y - X\\theta) \\right) \\quad (2)$$\n",
    "\n",
    "In the above expression, $Y$ is an $m$-vector containing the training\n",
    "labels $y^{(i)}$, $X$ is an $m$-by-$n$ matrix with the data $x^{(i)}$,\n",
    "and $\\theta$ is our vector of parameters. Taking derivatives with\n",
    "respect to $\\theta$, equating to zero, and solving, we get:\n",
    "$$\\theta_{MAP} = (X^T X + \\frac{\\sigma^2}{\\tau^2}I_n)^{-1}X^T Y \\quad (3)$$\n",
    "\n",
    "Observe the similarity between this expression, and the least squares\n",
    "solution derived in the notes.\n",
    "\n",
    "$^1$Equivalently, $y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$, where\n",
    "the $\\epsilon^{(i)}$’s are distributed IID $N(0, \\sigma^2)$.\n",
    "\n",
    "## Problem 3: Kernels\n",
    "\n",
    "**Problem:** In this problem, you will prove that certain functions $K$\n",
    "give valid kernels. Be careful to justify every step in your proofs.\n",
    "Specifically, if you use a result proved either in the lecture notes or\n",
    "homeworks, be careful to state exactly which result you’re using.\n",
    "\n",
    "### (a) \\[8 points\\] Exponential Kernel\n",
    "\n",
    "**Problem:** Let $K(x, z)$ be a valid (Mercer) kernel over\n",
    "$\\mathbb{R}^n \\times \\mathbb{R}^n$. Consider the function given by\n",
    "$$K_e(x, z) = \\exp(K(x, z)).$$\n",
    "\n",
    "Show that $K_e$ is a valid kernel. \\[Hint: There are many ways of\n",
    "proving this result, but you might find the following two facts useful:\n",
    "(i) The Taylor expansion of $e^x$ is given by\n",
    "$e^x = \\sum_{j=0}^\\infty \\frac{1}{j!}x^j$ (ii) If a sequence of\n",
    "non-negative numbers $a_i \\ge 0$ has a limit\n",
    "$a = \\lim_{i \\to \\infty} a_i$, then $a \\ge 0$.\\]\n",
    "\n",
    "**Answer:** Let $K_i(x, z) = \\sum_{j=0}^i \\frac{1}{j!}K(x, z)^j$. $K_i$\n",
    "is a polynomial in $K(x, z)$ with positive coefficients. As proved in\n",
    "the homework, $K_i(x, z)$ is also a kernel, so $z^T K_i z \\ge 0$. Thus,\n",
    "$$\\lim_{i \\to \\infty} z^T K_i z \\ge 0$$\n",
    "$$z^T (\\lim_{i \\to \\infty} K_i)z \\ge 0$$\n",
    "\n",
    "Since $\\lim_{i \\to \\infty} K_i = K_e$, $K_e$ is positive semi-definite,\n",
    "and thus a valid kernel.\n",
    "\n",
    "### (b) \\[8 points\\] Gaussian Kernel\n",
    "\n",
    "**Problem:** The Gaussian kernel is given by the function\n",
    "$$K(x,z) = e^{-\\frac{||x-z||^2}{\\sigma^2}},$$\n",
    "\n",
    "where $\\sigma^2 > 0$ is some fixed, positive constant. We said in class\n",
    "that this is a valid kernel, but did not prove it. Prove that the\n",
    "Gaussian kernel is indeed a valid kernel. \\[Hint: The following fact may\n",
    "be useful. $||x-z||^2 = ||x||^2 - 2x^Tz + ||z||^2$.\\]\n",
    "\n",
    "**Answer:** We can rewrite the Gaussian kernel as\n",
    "$$K(x, z) = e^{-\\frac{||x||^2}{\\sigma^2}} e^{-\\frac{||z||^2}{\\sigma^2}} e^{\\frac{2}{\\sigma^2}x^T z}$$\n",
    "\n",
    "The first two terms together form a kernel by the fact proved in the\n",
    "homework that $K(x,z) = f(x)f(z)$ is a valid kernel. The third term is\n",
    "$e^{K(x,z)}$, which we’ve already shown to be a valid kernel. By the\n",
    "result proved in the homework, the product of two kernels is also a\n",
    "kernel.\n",
    "\n",
    "## Problem 4: One-class SVM\n",
    "\n",
    "**Problem:** Given an unlabeled set of examples\n",
    "$\\{x^{(1)}, \\dots, x^{(m)}\\}$ the one-class SVM algorithm tries to find\n",
    "a direction $w$ that maximally separates the data from the origin.$^2$\n",
    "More precisely, it solves the (primal) optimization problem:\n",
    "$$\\min_w \\frac{1}{2} w^T w$$\n",
    "$$\\text{s.t.} \\quad w^T x^{(i)} \\ge 1 \\quad \\text{for all } i = 1, \\dots, m$$\n",
    "\n",
    "A new test example $x$ is labeled 1 if $w^T x \\ge 1$, and 0 otherwise.\n",
    "\n",
    "### (a) \\[9 points\\] Dual Formulation\n",
    "\n",
    "**Problem:** The primal optimization problem for the one-class SVM was\n",
    "given above. Write down the corresponding dual optimization problem.\n",
    "Simplify your answer as much as possible. In particular, $w$ should not\n",
    "appear in your answer.\n",
    "\n",
    "**Answer:** The Lagrangian is given by\n",
    "$$L(w, \\alpha) = \\frac{1}{2} w^T w + \\sum_{i=1}^m \\alpha_i (1 - w^T x^{(i)}) \\quad (4)$$\n",
    "\n",
    "Setting the gradient of the Lagrangian with respect to $w$ to zero, we\n",
    "obtain $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$. It follows that\n",
    "$$\\max_{\\alpha \\ge 0} \\min_w \\left( \\frac{1}{2} w^T w + \\sum_{i=1}^m \\alpha_i (1 - w^T x^{(i)}) \\right) \\quad (5)$$\n",
    "$$= \\max_{\\alpha \\ge 0} \\left( \\frac{1}{2} \\left( \\sum_{i=1}^m \\alpha_i x^{(i)} \\right)^T \\left( \\sum_{i=1}^m \\alpha_i x^{(i)} \\right) + \\sum_{i=1}^m \\alpha_i \\left( 1 - \\left( \\sum_{i=1}^m \\alpha_i x^{(i)} \\right)^T x^{(i)} \\right) \\right) \\quad (6)$$\n",
    "$$= \\max_{\\alpha \\ge 0} \\left( \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j x^{(i)^T} x^{(j)} \\right) \\quad (7)$$\n",
    "\n",
    "The first equality follows from setting the gradient w.r.t. $w$ equal to\n",
    "zero, and solving for $w$, which gives\n",
    "$w = \\sum_{i=1}^m \\alpha_i x^{(i)}$ and substituting this expression for\n",
    "$w$. The second equality follows from simplifying the expression.\n",
    "\n",
    "### (b) \\[4 points\\] Kernelization\n",
    "\n",
    "**Problem:** Can the one-class SVM be kernelized (both in training and\n",
    "testing)? Justify your answer.\n",
    "\n",
    "**Answer:** Yes. For training we can use the dual formulation, in which\n",
    "only inner products of the data appear. For testing at a point $z$ we\n",
    "just need to evaluate\n",
    "$w^T z = (\\sum_{i=1}^m \\alpha_i x^{(i)})^T z = \\sum_{i=1}^m \\alpha_i x^{(i)^T} z$\n",
    "in which the training data and the test point $z$ only appear in inner\n",
    "products.\n",
    "\n",
    "### (c) \\[5 points\\] SMO-like Algorithm\n",
    "\n",
    "**Problem:** Give an SMO-like algorithm to optimize the dual. I.e., give\n",
    "an algorithm that in every optimization step optimizes over the smallest\n",
    "possible subset of variables. Also give in closed-form the update\n",
    "equation for this subset of variables. You should also justify why it is\n",
    "sufficient to consider this many variables at a time in each step.\n",
    "\n",
    "**Answer:** Since we have convex optimization problem with only\n",
    "independent coordinate wise constraints ($\\alpha_i \\ge 0$), we can\n",
    "optimize iteratively over 1 variable at a time. Optimizing w.r.t.\n",
    "$\\alpha_i$ is done by setting\n",
    "$$\\alpha_i = \\max \\left\\{ 0, \\frac{1}{K_{i,i}} \\left( 1 - \\sum_{j \\ne i} \\alpha_j K_{i,j} \\right) \\right\\}$$\n",
    "\n",
    "(Set the derivative w.r.t. $\\alpha_i$ equal to zero and solve for\n",
    "$\\alpha_i$. And take into account the constraint. Here, we defined\n",
    "$K_{i,j} = x^{(i)\\text{T}} x^{(j)}$.)\n",
    "\n",
    "$^2$This turns out to be useful for anomaly detection. In anomaly\n",
    "detection you are given a set of data points that are all considered to\n",
    "be ‘normal’. Given these ‘normal’ data points, the task is to decide for\n",
    "a new data point whether it is also ‘normal’ or not. Adding slack\n",
    "variables allows for training data that are not necessarily all\n",
    "‘normal’. The most common formulation with slack variables is not the\n",
    "most direct adaptation of the soft margin SVM formulation seen in class.\n",
    "Instead the $\\nu$-SVM formulation is often used. This formulation allows\n",
    "you to specify the fraction of outliers (instead of the constant $C$\n",
    "which is harder to interpret). See the literature for details.\n",
    "\n",
    "## Problem 5: Uniform Convergence\n",
    "\n",
    "**Problem:** In this problem, we consider trying to estimate the mean of\n",
    "a biased coin toss. We will repeatedly toss the coin and keep a running\n",
    "estimate of the mean. We would like to prove that (with high\n",
    "probability), after some initial set of $N$ tosses, the running estimate\n",
    "from that point on will always be accurate and never deviate too much\n",
    "from the true value.\n",
    "\n",
    "More formally, let $X_i \\sim \\text{Bernoulli}(\\phi)$ be IID random\n",
    "variables. Let $\\hat{\\phi}_n$ be our estimate for $\\phi$ after $n$\n",
    "observations: $$\\hat{\\phi}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i.$$\n",
    "\n",
    "We’d like to show that after a certain number of coin flips, our\n",
    "estimates will stay close to the true value of $\\phi$. More formally,\n",
    "we’d like to show that for all $\\gamma, \\delta \\in (0,1/2]$, there\n",
    "exists a value $N$ such that\n",
    "$$P\\left( \\max_{n \\ge N} |\\phi - \\hat{\\phi}_n| > \\gamma \\right) \\le \\delta.$$\n",
    "\n",
    "Show that in order to make the guarantee above, it suffices to have\n",
    "$N = O\\left(\\frac{1}{\\gamma^2} \\log\\left(\\frac{1}{\\delta\\gamma}\\right)\\right)$.\n",
    "You may need to use the fact that for $\\gamma \\in (0,1/2]$,\n",
    "$\\log\\left(\\frac{1}{1-\\exp(-2\\gamma^2)}\\right) = O(\\log(\\frac{1}{\\gamma}))$.\n",
    "\n",
    "\\[Hint: Let $A_n$ be the event that $|\\phi - \\hat{\\phi}_n| > \\gamma$ and\n",
    "consider taking a union bound over the set of events\n",
    "$A_N, A_{N+1}, A_{N+2}, \\dots$\\]\n",
    "\n",
    "**Answer:** $$\\text{Pr}(\\max_{n \\ge N} |\\phi - \\hat{\\phi}_n| > \\gamma)$$\n",
    "$$= \\text{Pr}(\\bigcup_{n \\ge N} \\{|\\phi - \\hat{\\phi}_n| > \\gamma\\})$$\n",
    "$$\\le \\sum_{n \\ge N} \\text{Pr}(|\\phi - \\hat{\\phi}_n| > \\gamma)$$\n",
    "$$\\le \\sum_{n \\ge N} 2e^{-2\\gamma^2 n}$$\n",
    "$$= \\frac{2(e^{-2\\gamma^2})^N}{1 - e^{-2\\gamma^2}}$$\n",
    "\n",
    "Hence, in order to guarantee that\n",
    "$\\text{Pr}(\\max_{n \\ge N} |\\theta - \\hat{\\theta}_n| > \\gamma) \\le \\delta$,\n",
    "we only need to choose $N$ such that\n",
    "$$\\frac{2(e^{-2\\gamma^2})^N}{1 - e^{-2\\gamma^2}} \\le \\delta$$\n",
    "$$(e^{-2\\gamma^2})^N \\le \\delta(1 - e^{-2\\gamma^2})/2$$\n",
    "$$\\log((e^{-2\\gamma^2})^N) \\le \\log(\\delta(1 - e^{-2\\gamma^2})/2)$$\n",
    "$$(-2\\gamma^2)N \\le \\log \\delta + \\log(1 - e^{-2\\gamma^2}) - \\log 2$$\n",
    "$$N \\ge \\frac{1}{2\\gamma^2}(-\\log \\delta - \\log(1 - e^{-2\\gamma^2}) + \\log 2)$$\n",
    "$$N \\ge \\frac{1}{2\\gamma^2}\\left(\\log \\frac{1}{\\delta} + \\log\\left(\\frac{1}{1 - e^{-2\\gamma^2}}\\right) + \\log 2\\right)$$\n",
    "\n",
    "Thus, it is sufficient to have\n",
    "$$N = O\\left(\\frac{1}{\\gamma^2}\\left(\\log \\frac{1}{\\delta} + \\log\\left(\\frac{1}{1 - e^{-2\\gamma^2}}\\right)\\right)\\right)$$\n",
    "$$N = O\\left(\\frac{1}{\\gamma^2}\\left(\\log \\frac{1}{\\delta} + \\log \\frac{1}{\\gamma}\\right)\\right)$$\n",
    "$$N = O\\left(\\frac{1}{\\gamma^2} \\log \\frac{1}{\\delta\\gamma}\\right)$$\n",
    "\n",
    "## Problem 6: Short Answers\n",
    "\n",
    "The following questions require a true/false accompanied by one sentence\n",
    "of explanation, or a reasonably short answer (usually at most 1-2\n",
    "sentences or a figure). To discourage random guessing, one point will be\n",
    "deducted for a wrong answer on multiple choice questions! Also, no\n",
    "credit will be given for answers without a correct explanation.\n",
    "\n",
    "### (a) \\[5 points\\] Decision Boundary with Separate Covariance Matrices\n",
    "\n",
    "**Problem:** Let there be a binary classification problem with\n",
    "continuous-valued features. In Problem Set #1, you showed if we apply\n",
    "Gaussian discriminant analysis using the same covariance matrix $\\Sigma$\n",
    "for both classes, then the resulting decision boundary will be linear.\n",
    "What will the decision boundary look like if we modeled the two classes\n",
    "using separate covariance matrices $\\Sigma_0$ and $\\Sigma_1$? (I.e.,\n",
    "$x^{(i)}|y^{(i)} = b \\sim \\mathcal{N}(\\mu_b, \\Sigma_b)$, for $b = 0$ or\n",
    "$1$.)\n",
    "\n",
    "**Answer:** The decision boundary is given by the following equation\n",
    "(then using Bayes rule and replacing terms independent of $x$ by\n",
    "constants $C_1, C_2$ on the next line)\n",
    "$$\\log p(y = 0|x) = \\log p(y = 1|x)$$\n",
    "$$\\log p(x|y = 0) + \\log p(y = 0) - \\log p(x) = \\log p(x|y = 1) + \\log p(y = 1) - \\log p(x)$$\n",
    "$$C_1 - \\frac{1}{2}(x - \\mu_0)^\\top \\Sigma_0^{-1}(x - \\mu_0) = C_2 - \\frac{1}{2}(x - \\mu_1)^\\top \\Sigma_1^{-1}(x - \\mu_1)$$\n",
    "\n",
    "This is a quadratic decision boundary in $x$. (In particular, if we\n",
    "consolidate all terms on the left side, the quadratic term in $x$ does\n",
    "not cancel.)\n",
    "\n",
    "### (b) \\[5 points\\] Kernel Perceptron Mistakes\n",
    "\n",
    "**Problem:** Consider a sequence of examples\n",
    "$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(m)}, y^{(m)})$.\n",
    "Assume that for all $i$ we have $||x^{(i)}|| \\le D$ and that the data\n",
    "are linearly separated with a margin $\\gamma$. Suppose that the\n",
    "perceptron algorithm makes exactly $(D/\\gamma)^2$ mistakes on this\n",
    "sequence of examples. Now, suppose we use a feature mapping\n",
    "$\\phi(\\cdot)$ to a higher dimensional space and use the corresponding\n",
    "kernel perceptron algorithm on the same sequence of data (now in the\n",
    "higher-dimensional feature space). Then the kernel perceptron\n",
    "(implicitly operating in this higher dimensional feature space) will\n",
    "make a number of mistakes that is\n",
    "\n",
    "1.  strictly less than $(D/\\gamma)^2$.\n",
    "2.  equal to $(D/\\gamma)^2$.\n",
    "3.  strictly more than $(D/\\gamma)^2$.\n",
    "4.  impossible to say from the given information.\n",
    "\n",
    "**Answer:** Impossible to say from the given information, since the\n",
    "number of mistakes depends on the configuration of the points in the\n",
    "higher dimensional feature space, about which no information is given.\n",
    "\n",
    "### (c) \\[5 points\\] Mercer Kernel Construction\n",
    "\n",
    "**Problem:** Let any $x^{(1)}, x^{(2)}, x^{(3)} \\in \\mathbb{R}^p$ be\n",
    "given ($x^{(1)} \\ne x^{(2)}, x^{(1)} \\ne x^{(3)}, x^{(2)} \\ne x^{(3)}$).\n",
    "Also let any $z^{(1)}, z^{(2)}, z^{(3)} \\in \\mathbb{R}^q$ be fixed. Then\n",
    "there exists a valid Mercer kernel\n",
    "$K: \\mathbb{R}^p \\times \\mathbb{R}^p \\to \\mathbb{R}$ such that for all\n",
    "$i, j \\in \\{1,2,3\\}$ we have\n",
    "$K(x^{(i)}, x^{(j)}) = (z^{(i)})^T z^{(j)}$. True or False?\n",
    "\n",
    "**Answer:** True. Consider any feature mapping that satisfies\n",
    "$\\phi(x^{(i)}) = z^{(i)}$ for $i \\in \\{1,2,3\\}$. \\[E.g. extend\n",
    "$\\phi(\\cdot)$ to be identically $\\vec{0}$ for any argument different\n",
    "from $x^{(1)}, x^{(2)}, x^{(3)}$.\\] The kernel $K(\\cdot,\\cdot)$ that\n",
    "satisfies for all $u, v$ $K(u,v) = \\phi(u)^T \\phi(v)$ is a Mercer kernel\n",
    "and satisfies the desired properties.\n",
    "\n",
    "### (d) \\[5 points\\] Newton’s Method Convergence\n",
    "\n",
    "**Problem:** Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be defined according\n",
    "to $f(x) = \\frac{1}{2}x^T Ax + b^T x + c$, where $A$ is symmetric\n",
    "positive definite. Suppose we use Newton’s method to minimize $f$. Show\n",
    "that Newton’s method will find the optimum in exactly one iteration. You\n",
    "may assume that Newton’s method is initialized with $\\vec{0}$.\n",
    "\n",
    "**Answer:** Using the formulas from the lecture notes, we have that\n",
    "$$\\nabla_x f(x) = Ax + b \\quad (8)$$ $$\\nabla_x^2 f(x) = A \\quad (9)$$\n",
    "\n",
    "Setting the gradient to zero, we find that the optimum of the function\n",
    "is given by $$x = -A^{-1}b$$\n",
    "\n",
    "Newton’s method will perform the update\n",
    "$$x = x - (\\nabla_x^2 f(x))^{-1} \\nabla_x f(x) \\quad (10)$$\n",
    "$$= \\vec{0} - A^{-1}(A\\vec{0} + b) \\quad (11)$$\n",
    "$$= -A^{-1}b \\quad (12)$$\n",
    "\n",
    "So it will move to the optimum on the first iteration.\n",
    "\n",
    "### (e) \\[5 points\\] Boolean Functions VC Dimension\n",
    "\n",
    "**Problem:** Consider binary classification, and let the input domain be\n",
    "$\\mathcal{X} = \\{0, 1\\}^n$, i.e., the space of all $n$-dimensional bit\n",
    "vectors. Thus, each sample $x$ has $n$ binary-valued features. Let\n",
    "$\\mathcal{H}_n$ be the class of all boolean functions over the input\n",
    "space. What is $|\\mathcal{H}_n|$ and $VC(\\mathcal{H}_n)$?\n",
    "\n",
    "**Answer:** $|\\mathcal{H}_n| = 2^{2^n}$. There are exactly $2^n$\n",
    "distinct points in $\\mathcal{X}$, and $\\mathcal{H}$ can realize all\n",
    "$2^{2^n}$ labellings on those $2^n$ points; thus\n",
    "$VC(\\mathcal{H}_n) \\ge 2^n$. Since there can be no more than $2^n$\n",
    "distinct points in the input space, $VC(\\mathcal{H}_n) \\le 2^n$, and\n",
    "thus $VC(\\mathcal{H}_n) = 2^n$.\n",
    "\n",
    "### (f) \\[5 points\\] L1-Regularized SVM on Linearly Separable Data\n",
    "\n",
    "**Problem:** Suppose an $l_1$-regularized SVM (with regularization\n",
    "parameter $C > 0$) is trained on a dataset that is linearly separable.\n",
    "Because the data is linearly separable, to minimize the primal\n",
    "objective, the SVM algorithm will set all the slack variables to zero.\n",
    "Thus, the weight vector $w$ obtained will be the same no matter what\n",
    "regularization parameter $C$ is used (so long as it is strictly bigger\n",
    "than zero). True or false?\n",
    "\n",
    "**Answer:** No - outliers can still affect the choice of separating\n",
    "line. We may choose to misclassify a point if it makes the margin\n",
    "larger. The value of $C$ will affect how we choose to make this\n",
    "tradeoff.\n",
    "\n",
    "### (g) \\[5 points\\] Locally Weighted Linear Regression Bandwidth\n",
    "\n",
    "**Problem:** Consider using hold-out cross validation (using 70% of the\n",
    "data for training, 30% for hold-out CV) to select the bandwidth\n",
    "parameter $\\tau$ for locally weighted linear regression. As the number\n",
    "of training examples $m$ increases, would you expect the value of $\\tau$\n",
    "selected by the algorithm to generally become larger, smaller, or\n",
    "neither of the above? For this problem, assume that (the expected value\n",
    "of) $y$ is a non-linear function of $x$.\n",
    "\n",
    "**Answer:** Smaller. For any fixed $\\tau$, as the amount of training\n",
    "data increases, the prediction of locally weighted regression for a\n",
    "given $x$ will converge to the prediction of the line that best fits\n",
    "(the expected value of) $y$ in the $\\tau$-sized region around $x$. Since\n",
    "(the expected value of) $y$ is a non-linear function of $x$, the smaller\n",
    "$\\tau$, the better the performance for sufficiently large training data\n",
    "set.\n",
    "\n",
    "### (h) \\[5 points\\] Feature Selection Algorithms\n",
    "\n",
    "**Problem:** Consider a feature selection problem in which the mutual\n",
    "information $MI(x_i, y) = 0$ for all features $x_i$. Also for every\n",
    "subset of features $S_i = \\{x_{i_1}, \\dots, x_{i_k}\\}$ of size $< n/2$\n",
    "we have $MI(S_i, y) = 0.^3$ However there is a subset $S^*$ of size\n",
    "exactly $n/2$ such that $MI(S^*, y) = 1$. I.e. this subset of features\n",
    "allows us to predict $y$ correctly. Of the three feature selection\n",
    "algorithms listed below, which one do you expect to work best on this\n",
    "dataset?\n",
    "\n",
    "1.  Forward Search.\n",
    "2.  Backward Search.\n",
    "3.  Filtering using mutual information $MI(x_i, y)$.\n",
    "4.  All three are expected to perform reasonably well.\n",
    "\n",
    "**Answer:** Backward Search, since it will be able to maintain a subset\n",
    "of features that predicts $y$ correctly down to only $n/2$ features.\n",
    "Forward Search has no way to distinguish between any subset of $< n/2$\n",
    "features and will thus end up with arbitrary subsets of $< n/2$\n",
    "features, which will be good subsets with very low probability;\n",
    "resulting in also having inferior subsets of features of size $\\ge n/2$.\n",
    "Filtering using mutual information cannot distinguish between any of the\n",
    "features, and will thus pick random subsets of the features.\n",
    "\n",
    "$^3 MI(S_i, y) = \\sum_{S_i} \\sum_y P(S_i, y) \\log(P(S_i, y)/P(S_i) P(y))$,\n",
    "where the first summation is over all possible values of the features in\n",
    "$S_i$."
   ],
   "id": "933320c4-4a29-4a29-85da-2233bbd89314"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
