# Generalization Code Examples

This directory contains Python code examples that demonstrate key concepts in machine learning generalization, including bias-variance tradeoff, double descent, and complexity bounds.

## Overview

The code examples in this directory are designed to help you understand:

1. **Bias-Variance Tradeoff**: The fundamental tension between model complexity and generalization
2. **Double Descent**: Modern phenomena that challenge classical bias-variance wisdom
3. **Complexity Bounds**: Theoretical foundations for understanding generalization
4. **Practical Implementation**: How to apply these concepts in real machine learning problems

## Files

### Bias-Variance Tradeoff

- **`bias_variance_tradeoff_demo.py`** - Demonstrates the bias-variance tradeoff using polynomial regression
- **`bias_variance_decomposition_examples.py`** - Advanced examples of bias-variance decomposition

### Double Descent

- **`double_descent_demo.py`** - Comprehensive demonstration of the double descent phenomenon
- **`double_descent_examples.py`** - Advanced examples demonstrating the double descent phenomenon

### Complexity Bounds

- **`complexity_bounds_examples.py`** - Theoretical foundations and practical examples of complexity bounds

## Learning Objectives

1. **Bias-Variance Understanding**: Learn to decompose error into bias and variance components
2. **Model Selection**: Understand how to choose optimal model complexity
3. **Overfitting Detection**: Learn to identify and diagnose overfitting vs underfitting
4. **Modern Phenomena**: Understand double descent and other modern generalization phenomena
5. **Theoretical Foundations**: Learn the mathematical foundations of generalization theory
6. **Practical Application**: Apply these concepts to real machine learning problems

## Dependencies

The code examples require the following Python packages:

```
numpy
matplotlib
scikit-learn
scipy
torch (for some examples)
```

## Usage

Each Python file can be run independently to demonstrate specific concepts:

```bash
# Run bias-variance tradeoff demonstration
python bias_variance_tradeoff_demo.py

# Run double descent examples
python double_descent_examples.py

# Run complexity bounds examples
python complexity_bounds_examples.py
```

## Key Concepts Demonstrated

### Bias-Variance Tradeoff

1. **U-Shaped Curve**: How model complexity affects training and test error
2. **Error Decomposition**: Breaking down total error into biasÂ², variance, and irreducible error
3. **Overfitting vs Underfitting**: Visual and quantitative analysis of model behavior
4. **Optimal Complexity**: Finding the sweet spot between bias and variance

### Double Descent

1. **Model-Wise Double Descent**: Three-regime phenomenon (underparameterized, interpolation threshold, overparameterized)
2. **Sample-Wise Double Descent**: How varying training sample size affects generalization
3. **Minimum Norm Solution**: Why gradient descent finds solutions that generalize well
4. **Complexity Measures**: Parameter count vs model norm as complexity measures
5. **Regularization Effects**: How regularization mitigates peaks at interpolation threshold

### Complexity Bounds

1. **Theoretical Foundations**: Mathematical bounds on generalization error
2. **VC Dimension**: Measuring model complexity
3. **Rademacher Complexity**: Alternative complexity measures
4. **Practical Implications**: How theory guides practice

## Expected Outcomes

After working through these examples, you should be able to:

- **Diagnose Model Problems**: Identify whether a model is overfitting or underfitting
- **Select Model Complexity**: Choose appropriate model complexity for given data
- **Understand Modern ML**: Appreciate why modern deep learning works despite classical theory
- **Apply Theory**: Use theoretical insights to improve practical machine learning systems
- **Communicate Results**: Explain generalization behavior to stakeholders

## Contributing

Feel free to add new examples or improve existing ones. When adding new code:

1. Include comprehensive docstrings
2. Add visualizations where appropriate
3. Provide clear explanations of the concepts being demonstrated
4. Ensure code is well-commented and educational

## References

- "Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman
- "Understanding Machine Learning" by Shalev-Shwartz and Ben-David
- Recent papers on double descent and modern generalization theory
