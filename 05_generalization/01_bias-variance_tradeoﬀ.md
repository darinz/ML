# Generalization

This chapter discusses tools to analyze and understand the generalization of machine learning models, i.e., their performances on unseen test examples. Recall that for supervised learning problems, given a training dataset $` \{(x^{(i)}, y^{(i)})\}_{i=1}^n `$ , we typically learn a model $` h_\theta `$ by minimizing a loss/cost function $` J(\theta) `$ , which encourages $` h_\theta `$ to fit the data. E.g., when the loss function is the least square loss (aka mean squared error), we have $` J(\theta) = \frac{1}{n} \sum_{i=1}^n (y^{(i)} - h_\theta(x^{(i)}))^2 `$ . This loss function for training purposes is oftentimes referred to as the training loss/error/cost.

However, minimizing the training loss is not our ultimate goal—it is merely our approach towards the goal of learning a predictive model. The most important evaluation metric of a model is the loss on unseen test examples, which is oftentimes referred to as the test error. Formally, we sample a test example $`(x, y)`$ from the so-called test distribution $` \mathcal{D} `$ , and measure the model's error on it by, e.g., the mean squared error, $`(h_\theta(x) - y)^2 `$ . The expected loss/error over the randomness of the test example is called the test loss/error:

```math
L(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}}[(y - h_\theta(x))^2]
```

Note that the measurement of the error involves computing the expectation, and in practice, it can be approximated by the average error on many sampled test examples, which are referred to as the test dataset. Note that the key difference here between training and test datasets is that the test examples are *unseen*, in the sense that the training procedure has not used the test examples. In classical statistical learning settings, the training examples are also drawn from the same distribution as the test distribution $` \mathcal{D} `$ , but still the test examples are unseen by the learning procedure whereas the training examples are seen.

Because of this key difference between training and test datasets, even if they are both drawn from the same distribution $` \mathcal{D} `$ , the test error is not necessarily always close to the training error. As a result, successfully minimizing the training error may not always lead to a small test error. We typically say the model *overfits* the data if the model predicts accurately on the training dataset but doesn't generalize well to other test examples, that is, if the training error is small but the test error is large. We say the model *underfits* the data if the training error is relatively large (and in this case, typically the test error is also relatively large).

This chapter studies how the test error is influenced by the learning procedure, especially the choice of model parameterizations. We will decompose the test error into "bias" and "variance" terms and study how each of them is affected by the choice of model parameterizations and their tradeoffs. Using the bias-variance tradeoff, we will discuss when overfitting and underfitting will occur and be avoided. We will also discuss the double descent phenomenon in Section 8.2 and some classical theoretical results in Section 8.3.

## 8.1 Bias-variance tradeoff

<img src="./img/bias_variance_example.png" width="700px"/>

Figure 8.1: A running example of training and test dataset for this section.

As an illustrating example, we consider the following training dataset and test dataset, which are also shown in Figure 8.1. The training inputs $`x^{(i)}`$ are randomly chosen and the outputs $`y^{(i)}`$ are generated by $`y^{(i)} = h^*(x^{(i)}) + \xi^{(i)}`$ where the function $`h^*(\cdot)`$ is a quadratic function and is shown in Figure 8.1 as the solid line, and $`\xi^{(i)}`$ is the observation noise assumed to be generated from $`\sim N(0, \sigma^2)`$. A test example $`(x, y)`$ also has the same input-output relationship $`y = h^*(x) + \xi`$ where $`\xi \sim N(0, \sigma^2)`$. It's impossible to predict the noise $`\xi`$, and therefore essentially our goal is to recover the function $`h^*(\cdot)`$.

We will consider the test error of learning various types of models. When talking about linear regression, we discussed the problem of whether to fit a "simple" model such as the linear $`y = \theta_0 + \theta_1 x`$, or a more "complex" model such as the polynomial $`y = \theta_0 + \theta_1 x + \cdots + \theta_5 x^5`$.

We start with fitting a linear model, as shown in Figure 8.2. The best fitted linear model cannot predict $`y`$ from $`x`$ accurately even on the training dataset, let alone on the test dataset. This is because the true relationship between $`y`$ and $`x`$ is not linear—any linear model is far away from the true function $`h^*(\cdot)`$. As a result, the training error is large and this is a typical situation of *underfitting*.

<img src="./img/linear_fit_train_test.png" width="700px"/>

Figure 8.2: The best fit linear model has large training and test errors.

The issue cannot be mitigated with more training examples—even with a very large amount of, or even infinite training examples, the best fitted linear model is still inaccurate and fails to capture the structure of the data (Figure 8.3). Even if the noise is not present in the training data, the issue still occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the linear model family's inability to capture the structure in the data—linear models cannot represent the true quadratic function $`h^*`$, but not the lack of the data. Informally, we define the **bias** of a model to be the test error even if we were to fit it to a very (say, infinitely) large training dataset. Thus, in this case, the linear model suffers from large bias, and underfits (i.e., fails to capture structure exhibited by) the data.

<img src="./img/linear_fit_large_dataset.png" width="340px"/> 

Figure 8.3: The best fit linear model on a much larger dataset still has a large training error. 

<img src="./img/linear_fit_noiseless.png" width="340px"/>
 
Figure 8.4: The best fit linear model on a noiseless dataset also has a large training/test error.

Next, we fit a 5th-degree polynomial to the data. Figure 8.5 shows that it fails to learn a good model either. However, the failure pattern is different from the linear model case. Specifically, even though the learnt 5th-degree polynomial did a very good job predicting $`y^{(i)}`$ from $`x^{(i)}`$ for training examples, it does not work well on test examples (Figure 8.5). In other words, the model learnt from the training set does not *generalize* well to other test examples—the test error is high. Contrary to the behavior of linear models, the bias of the 5-th degree polynomials is small—if we were to fit a 5-th degree polynomial to an extremely large dataset, the resulting model would be close to a quadratic function and be accurate (Figure 8.6). This is because the family of 5-th degree polynomials contains all the quadratic functions (setting $`\theta_5 = \theta_4 = \theta_3 = 0`$ results in a quadratic function), and, therefore, 5-th degree polynomials are in principle capable of capturing the structure of the data.

<img src="./img/poly5_fit_train_test.png" width="700px"/>

Figure 8.5: Best fit 5-th degree polynomial has zero training error, but still has a large test error and does not recover the the ground truth. This is a classic situation of overfitting.

<img src="./img/poly5_fit_large_dataset.png" width="350px"/>

Figure 8.6: The best fit 5-th degree polynomial on a huge dataset nearly recovers the ground-truth—suggesting that the culprit in Figure 8.5 is the variance (or lack of data) but not bias.

The failure of fitting 5-th degree polynomials can be captured by another component of the test error, called **variance** of a model fitting procedure. Specifically, when fitting a 5-th degree polynomial as in Figure 8.7 there is a large risk that we're fitting patterns in the data that happened to be present in our *small, finite* training set, but that do not reflect the wider pattern of the relationship between $`x`$ and $`y`$. These "spurious" patterns in the training set are (mostly) due to the observation noise $`\xi^{(i)}`$, and fitting these spurious patters results in a model with large test error. In this case, we say the model has a large variance.

<img src="./img/poly5_fit_different_datasets.png" width="700px"/>

Figure 8.7: The best fit 5-th degree models on three different datasets generated from the same distribution behave quite differently, suggesting the existence of a large variance.

The variance can be intuitively (and mathematically, as shown in Section 8.1.1) characterized by the amount of variations across models learnt on multiple different training datasets (drawn from the same underlying distribution). The "spurious patterns" are specific to the randomness of the noise (and inputs) in a particular dataset, and thus are different across multiple training datasets. Therefore, overfitting to the "spurious patterns" of multiple datasets should result in very different models. Indeed, as shown in Figure 8.7, the models learned on the three different training datasets are quite different, overfitting to the "spurious patterns" of each datasets.

Often, there is a tradeoff between bias and variance. If our model is too "simple" and has very few parameters, then it may have large bias (but small variance), and it typically may suffer from underfitting. If it is too "complex" and has very many parameters, then it may suffer from large variance (but have smaller bias), and thus overfitting. See Figure 8.8 for a typical tradeoff between bias and variance.

<img src="./img/bias_variance_tradeoff.png" width="500px"/>

Figure 8.8: An illustration of the typical bias-variance tradeoff.

As we will see formally in Section 8.1.1, the test error can be decomposed as a summation of bias and variance. This means that the test error will have a convex curve as the model complexity increases, and in practice we should tune the model complexity to achieve the best tradeoff. For instance, in the example above, fitting a quadratic function does better than either of the extremes of a first or a 5-th degree polynomial, as shown in Figure 8.9.

<img src="./img/quadratic_fit_train_test.png" width="700px"/>

Figure 8.9: Best fit quadratic model has small training and test error because quadratic model achieves a better tradeoff.

Interestingly, the bias-variance tradeoff curves or the test error curves do not universally follow the shape in Figure 8.8, at least not universally when the model complexity is simply measured by the number of parameters. (We will discuss the so-called double descent phenomenon in Section 8.2.) Nevertheless, the principle of bias-variance tradeoff is perhaps still the first resort when analyzing and predicting the behavior of test errors.


### 8.1.1 A mathematical decomposition (for regression)

To formally state the bias-variance tradeoff for regression problems, we consider the following setup (which is an extension of the beginning paragraph of Section 8.1).

- Draw a training dataset $`S = \{x^{(i)}, y^{(i)}\}_{i=1}^n `$ such that $`y^{(i)} = h^*(x^{(i)}) + \xi^{(i)}`$ where $`\xi^{(i)} \in N(0, \sigma^2)`$.
- Train a model on the dataset $`S`$, denoted by $`\hat{h}_S`$.
- Take a test example $`(x, y)`$ such that $`y = h^*(x) + \xi`$ where $`\xi \sim N(0, \sigma^2)`$, and measure the expected test error (averaged over the random draw of the training set $`S`$ and the randomness of $`\xi`$):

```math
\mathrm{MSE}(x) = \mathbb{E}_{S, \xi}[(y - \hat{h}_S(x))^2]
```

We will decompose the MSE into a bias and variance term. We start by assembling some simple mathematical notation that will be used twice below.

**Claim 8.1.1:** Suppose $`A`$ and $`B`$ are two independent real random variables and $`\mathbb{E}[A] = 0`$. Then, $`\mathbb{E}[(A+B)^2] = \mathbb{E}[A^2] + \mathbb{E}[B^2]`$.

As a corollary, because a random variable $`A`$ is independent with a constant $`c`$, when $`\mathbb{E}[A] = 0`$, we have $`\mathbb{E}[(A + c)^2] = \mathbb{E}[A^2] + c^2`$.

The proof of the claim follows from expanding the square: $`\mathbb{E}[(A+B)^2] = \mathbb{E}[A^2] + \mathbb{E}[B^2] + 2\mathbb{E}[AB] = \mathbb{E}[A^2] + \mathbb{E}[B^2]`$. Here we used the independence to show that $`\mathbb{E}[AB] = \mathbb{E}[A]\mathbb{E}[B] = 0`$.

Using Claim 8.1.1 with $`A = \xi`$ (the noise) and $`B = h^*(x) - \hat{h}_S(x)`$ (the difference between the true function and the model's prediction), we have:

```math
\mathrm{MSE}(x) = \mathbb{E}[(y - \hat{h}_S(x))^2] = \mathbb{E}[(\xi + (h^*(x) - \hat{h}_S(x)))^2] \tag{8.3}
```
**Explanation:**
- Here, $`y = h^*(x) + \xi`$ by our data generation process, and $`\hat{h}_S(x)`$ is the prediction from the model trained on dataset $`S`$.
- The error $`y - \hat{h}_S(x)`$ can be rewritten as $`(h^*(x) + \xi) - \hat{h}_S(x) = (h^*(x) - \hat{h}_S(x)) + \xi`$.
- We are interested in the expected squared error, which is the mean squared error (MSE) at $`x`$.

```math
= \mathbb{E}[\xi^2] + \mathbb{E}[(h^*(x) - \hat{h}_S(x))^2] \quad \text{(by Claim 8.1.1)} \tag{8.4}
```
**Explanation:**
- Claim 8.1.1 tells us that for two independent random variables $`A`$ and $`B`$ with $`\mathbb{E}[A]=0`$, $`\mathbb{E}[(A+B)^2] = \mathbb{E}[A^2] + \mathbb{E}[B^2]`$.
- In our case, $`\xi`$ (the noise) is independent of $`h^*(x) - \hat{h}_S(x)`$ (the model error), and $`\mathbb{E}[\xi]=0`$.
- So, the expected squared error splits into two parts: the expected noise squared ($`\mathbb{E}[\xi^2] = \sigma^2`$) and the expected squared model error ($`\mathbb{E}[(h^*(x) - \hat{h}_S(x))^2]`$).

```math
= \sigma^2 + \mathbb{E}[(h^*(x) - \hat{h}_S(x))^2]
```
**Explanation:**
- $`\mathbb{E}[\xi^2]`$ is just the variance of the noise, which is $`\sigma^2`$ by our data generation assumption.
- The second term is the expected squared difference between the true function and the model's prediction, averaged over all possible training sets $`S`$.

Now, let's introduce the concept of the "average model":

- Define $`h_{avg}(x) = \mathbb{E}_S[\hat{h}_S(x)]`$ as the average prediction at $`x`$ if we could train on infinitely many datasets $`S`$ and average their predictions.
- This is a hypothetical model, but it helps us analyze the error.

We can further decompose the second term by writing $`h^*(x) - \hat{h}_S(x) = (h^*(x) - h_{avg}(x)) + (h_{avg}(x) - \hat{h}_S(x))`$ (i.e., add and subtract $`h_{avg}(x)`$):

```math
\mathrm{MSE}(x) = \sigma^2 + \mathbb{E}[(h^*(x) - \hat{h}_S(x))^2] \tag{8.5}
```
**Explanation:**
- This is just restating the previous result, but now we are about to break down the second term further.

```math
= \sigma^2 + (h^*(x) - h_{avg}(x))^2 + \mathbb{E}[(h_{avg}(x) - \hat{h}_S(x))^2] \tag{8.6}
```
**Explanation:**
- We use the same trick as before (Claim 8.1.1):
  - $`h^*(x) - \hat{h}_S(x)`$ is split into $`(h^*(x) - h_{avg}(x))`$ (a constant, the difference between the true function and the average model) and $`(h_{avg}(x) - \hat{h}_S(x))`$ (a random variable, the difference between the average model and the model trained on a particular dataset).
  - The cross-term vanishes because $`\mathbb{E}_S[h_{avg}(x) - \hat{h}_S(x)] = 0`$ by definition of $`h_{avg}(x)`$.
- The first new term, $`(h^*(x) - h_{avg}(x))^2`$, is called the **squared bias**: it measures how far the average model is from the true function.
- The second new term, $`\mathbb{E}[(h_{avg}(x) - \hat{h}_S(x))^2]`$, is the **variance**: it measures how much the model's prediction varies as we change the training set.

```math
= \underbrace{\sigma^2}_{\text{unavoidable}} + \underbrace{(h^*(x) - h_{avg}(x))^2}_{\triangleq \text{bias}^2} + \underbrace{\mathrm{var}(\hat{h}_S(x))}_{\triangleq \text{variance}} \tag{8.7}
```
**Explanation:**
- The first term, $`\sigma^2`$, is the **irreducible error** or **noise**: it is the error due to the inherent randomness in the data generation process. No model can reduce this part.
- The second term, $`(h^*(x) - h_{avg}(x))^2`$, is the **bias squared**: it is the error due to the model's inability to represent the true function, even with infinite data.
- The third term, $`\mathrm{var}(\hat{h}_S(x))`$, is the **variance**: it is the error due to the model's sensitivity to the particular dataset it was trained on. It decreases as the dataset size increases.

In summary, the total expected squared error at $`x`$ is the sum of:
- The irreducible noise ($`\sigma^2`$),
- The squared bias (systematic error from model assumptions),
- The variance (error from sensitivity to the training data).

This decomposition helps us understand the tradeoff between bias and variance when choosing model complexity and training set size.

### Python Code: Bias-Variance Decomposition and Related Calculations

Below are Python code snippets that demonstrate the calculations and concepts discussed in the sections above. These examples use numpy and simulate data to illustrate the bias-variance decomposition, MSE, bias, and variance.

#### 1. Simulate Data Generation
```python
import numpy as np

# True function (quadratic)
def h_star(x):
    return 2 * x**2 + 0.5

# Parameters
sigma = 0.2  # Standard deviation of noise
n_train = 8  # Number of training points
n_test = 1000  # Number of test points

# Generate training data
np.random.seed(42)
x_train = np.random.rand(n_train)
y_train = h_star(x_train) + np.random.normal(0, sigma, n_train)

# Generate test data (no noise for true function)
x_test = np.linspace(0, 1, n_test)
y_test_true = h_star(x_test)
```

#### 2. Fit Models and Compute Predictions
```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Fit a linear model
linear_model = LinearRegression().fit(x_train.reshape(-1, 1), y_train)
y_pred_linear = linear_model.predict(x_test.reshape(-1, 1))

# Fit a 5th-degree polynomial model
poly5_model = make_pipeline(PolynomialFeatures(5), LinearRegression())
poly5_model.fit(x_train.reshape(-1, 1), y_train)
y_pred_poly5 = poly5_model.predict(x_test.reshape(-1, 1))

# Fit a quadratic model (degree 2)
poly2_model = make_pipeline(PolynomialFeatures(2), LinearRegression())
poly2_model.fit(x_train.reshape(-1, 1), y_train)
y_pred_poly2 = poly2_model.predict(x_test.reshape(-1, 1))
```

#### 3. Mean Squared Error (MSE)
Equation (8.2):
```python
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Example: MSE for linear, quadratic, and 5th-degree polynomial models
mse_linear = mse(y_test_true, y_pred_linear)
mse_poly2 = mse(y_test_true, y_pred_poly2)
mse_poly5 = mse(y_test_true, y_pred_poly5)
print(f"MSE (Linear): {mse_linear:.4f}")
print(f"MSE (Quadratic): {mse_poly2:.4f}")
print(f"MSE (5th-degree): {mse_poly5:.4f}")
```

#### 4. Bias, Variance, and Noise Decomposition (Equations 8.3–8.7)
We estimate bias and variance at a single $`x`$ value by repeatedly resampling training sets, fitting models, and evaluating predictions.

```python
# Settings for bias-variance estimation
n_repeats = 500
x0 = 0.5  # Test at a single point

preds_poly5 = []
for _ in range(n_repeats):
    x_train = np.random.rand(n_train)
    y_train = h_star(x_train) + np.random.normal(0, sigma, n_train)
    model = make_pipeline(PolynomialFeatures(5), LinearRegression())
    model.fit(x_train.reshape(-1, 1), y_train)
    preds_poly5.append(model.predict(np.array([[x0]]))[0])

preds_poly5 = np.array(preds_poly5)

# True value and average model prediction
true_val = h_star(x0)
avg_pred = np.mean(preds_poly5)

# Bias^2
bias2 = (true_val - avg_pred) ** 2
# Variance
variance = np.var(preds_poly5)
# Noise (irreducible error)
noise = sigma ** 2
# Total expected MSE
expected_mse = bias2 + variance + noise

print(f"At x = {x0}")
print(f"Bias^2: {bias2:.4f}")
print(f"Variance: {variance:.4f}")
print(f"Noise: {noise:.4f}")
print(f"Expected MSE: {expected_mse:.4f}")
```

#### 5. Visualizing Bias-Variance Tradeoff
You can visualize the bias-variance tradeoff by plotting bias$^2$, variance, and total error as a function of model complexity (polynomial degree):

```python
import matplotlib.pyplot as plt

degrees = range(1, 10)
bias2_list = []
variance_list = []
mse_list = []

for deg in degrees:
    preds = []
    for _ in range(n_repeats):
        x_train = np.random.rand(n_train)
        y_train = h_star(x_train) + np.random.normal(0, sigma, n_train)
        model = make_pipeline(PolynomialFeatures(deg), LinearRegression())
        model.fit(x_train.reshape(-1, 1), y_train)
        preds.append(model.predict(np.array([[x0]]))[0])
    preds = np.array(preds)
    avg_pred = np.mean(preds)
    bias2 = (true_val - avg_pred) ** 2
    variance = np.var(preds)
    bias2_list.append(bias2)
    variance_list.append(variance)
    mse_list.append(bias2 + variance + noise)

plt.plot(degrees, bias2_list, label="Bias$^2$")
plt.plot(degrees, variance_list, label="Variance")
plt.plot(degrees, mse_list, label="Total Error (MSE)")
plt.xlabel("Model Complexity (Polynomial Degree)")
plt.ylabel("Error at x = {:.2f}".format(x0))
plt.legend()
plt.title("Bias-Variance Tradeoff")
plt.show()
```

---

These code snippets allow you to:
- Simulate the data generation process described in the text
- Fit models of different complexity
- Compute and visualize MSE, bias, variance, and noise
- See the bias-variance tradeoff in action

You can adapt these examples to experiment with different true functions, noise levels, and model classes.