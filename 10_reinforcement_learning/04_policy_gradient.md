# Policy Gradient (REINFORCE)

We will present a model-free algorithm called **REINFORCE** that does not require the notion of value functions and $Q$ functions. It turns out to be more convenient to introduce REINFORCE in the finite horizon case, which will be assumed throughout this note: we use $\tau = (s_0, a_0, \ldots, s_{T-1}, a_{T-1}, s_T)$ to denote a trajectory, where $T < \infty$ is the length of the trajectory. Moreover, REINFORCE only applies to learning a **randomized policy**. We use $\pi_\theta(a|s)$ to denote the probability of the policy $\pi_\theta$ outputting the action $a$ at state $s$. The other notations will be the same as in previous lecture notes.

The advantage of applying REINFORCE is that we only need to assume that we can sample from the transition probabilities $\{P_{sa}\}$ and can query the reward function $R(s, a)$ at state $s$ and action $a$\(^1\), but we do not need to know the analytical form of the transition probabilities or the reward function. We do not explicitly learn the transition probabilities or the reward function either.

Let $s_0$ be sampled from some distribution $\mu$. We consider optimizing the expected total payoff of the policy $\pi_\theta$ over the parameter $\theta$ defined as:

```math
\eta(\theta) \triangleq \mathbb{E} \left[ \sum_{t=0}^{T-1} \gamma^t R(s_t, a_t) \right] \tag{17.1}
```

Recall that $s_t \sim P_{s_{t-1} a_{t-1}}$ and $a_t \sim \pi_\theta(\cdot|s_t)$. Also note that $\eta(\theta) = \mathbb{E}_{s_0 \sim \mu} \left[ V^{\pi_\theta}(s_0) \right]$ if we ignore the difference between finite and infinite horizon.

---

\(^1\) *In this notes we will work with the general setting where the reward depends on both the state and the action.*

---

We aim to use gradient ascent to maximize $\eta(\theta)$. The main challenge we face here is to compute (or estimate) the gradient of $\eta(\theta)$ without the knowledge of the form of the reward function and the transition probabilities.

Let $P_\theta(\tau)$ denote the distribution of $\tau$ (generated by the policy $\pi_\theta$), and let $f(\tau) = \sum_{t=0}^{T-1} \gamma^t R(s_t, a_t)$. We can rewrite $\eta(\theta)$ as

```math
\eta(\theta) = \mathbb{E}_{\tau \sim P_\theta} [f(\tau)] \tag{17.2}
```

We face a similar situation in the variational auto-encoder (VAE) setting covered in previous lectures, where we need to take the gradient with respect to a variable that shows up under the expectation—the distribution $P_\theta$ depends on $\theta$. Recall that in VAE, we used re-parameterization techniques to address this problem. However, it does not apply here because we do not know how to compute the gradient of the function $f$. (We only have an efficient way to evaluate the function $f$ by taking a weighted sum of the observed rewards, but we do not necessarily know the reward function itself to compute the gradient.)

The REINFORCE algorithm uses another approach to estimate the gradient of $\eta(\theta)$. We start with the following derivation:

```math
\nabla_\theta \mathbb{E}_{\tau \sim P_\theta} [f(\tau)] = \nabla_\theta \int P_\theta(\tau) f(\tau) d\tau
= \int \nabla_\theta (P_\theta(\tau) f(\tau)) d\tau \quad \text{(swap integration with gradient)}
= \int (\nabla_\theta P_\theta(\tau)) f(\tau) d\tau \quad \text{(because $f$ does not depend on $\theta$)}
= \int P_\theta(\tau) (\nabla_\theta \log P_\theta(\tau)) f(\tau) d\tau
\quad \text{(because $\nabla \log P_\theta(\tau) = \frac{\nabla P_\theta(\tau)}{P_\theta(\tau)}$)}
= \mathbb{E}_{\tau \sim P_\theta} \left[ (\nabla_\theta \log P_\theta(\tau)) f(\tau) \right] \tag{17.3}
```

Now we have a sample-based estimator for $\nabla_\theta \mathbb{E}_{\tau \sim P_\theta} [f(\tau)]$. Let $\tau^{(1)}, \ldots, \tau^{(n)}$ be $n$ empirical samples from $P_\theta$ (which are obtained by running the policy $\pi_\theta$ for $n$ times, with $T$ steps for each run). We can estimate the gradient of $\eta(\theta)$ by

```math
\nabla_\theta \mathbb{E}_{\tau \sim P_\theta} [f(\tau)] = \mathbb{E}_{\tau \sim P_\theta} \left[ (\nabla_\theta \log P_\theta(\tau)) f(\tau) \right] \tag{17.4}
```

```math
\approx \frac{1}{n} \sum_{i=1}^n (\nabla_\theta \log P_\theta(\tau^{(i)})) f(\tau^{(i)}) \tag{17.5}
```

The next question is how to compute $\log P_\theta(\tau)$. We derive an analytical formula for $\log P_\theta(\tau)$ and compute its gradient with respect to $\theta$ (using auto-differentiation). Using the definition of $\tau$, we have

```math
P_\theta(\tau) = \mu(s_0) \pi_\theta(a_0|s_0) P_{s_0 a_0}(s_1) \pi_\theta(a_1|s_1) P_{s_1 a_1}(s_2) \cdots P_{s_{T-1} a_{T-1}}(s_T) \tag{17.6}
```

Here recall that $\mu$ is used to denote the density of the distribution of $s_0$. It follows that

```math
\log P_\theta(\tau) = \log \mu(s_0) + \log \pi_\theta(a_0|s_0) + \log P_{s_0 a_0}(s_1) + \log \pi_\theta(a_1|s_1)
+ \log P_{s_1 a_1}(s_2) + \cdots + \log P_{s_{T-1} a_{T-1}}(s_T) \tag{17.7}
```

Taking gradient with respect to $\theta$, we obtain

```math
\nabla_\theta \log P_\theta(\tau) = \nabla_\theta \log \pi_\theta(a_0|s_0) + \nabla_\theta \log \pi_\theta(a_1|s_1) + \cdots + \nabla_\theta \log \pi_\theta(a_{T-1}|s_{T-1})
```

Note that many of the terms disappear because they don't depend on $\theta$ and thus have zero gradients. (This is somewhat important—we don't know how to evaluate those terms such as $\log P_{s_0 a_0}(s_1)$ because we don't have access to the transition probabilities, but luckily those terms have zero gradients!)

Plugging the equation above into equation (17.4), we conclude that

```math
\nabla_\theta \eta(\theta) = \nabla_\theta \mathbb{E}_{\tau \sim P_\theta} [f(\tau)] = \mathbb{E}_{\tau \sim P_\theta} \left[ \left( \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \cdot f(\tau) \right]
= \mathbb{E}_{\tau \sim P_\theta} \left[ \left( \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \cdot \left( \sum_{t=0}^{T-1} \gamma^t R(s_t, a_t) \right) \right] \tag{17.8}
```

We estimate the RHS of the equation above by empirical sample trajectories, and the estimate is unbiased. The vanilla REINFORCE algorithm iteratively updates the parameter by gradient ascent using the estimated gradients.

---

## Intuitive Explanation and Step-by-Step Guide: Policy Gradient and REINFORCE

### Why Policy Gradient Methods?
- In many reinforcement learning problems, we want to find a good policy (a way of choosing actions) that maximizes the expected reward over time.
- Sometimes, the environment is too complex to model exactly, or we don't know the transition probabilities or reward function in advance.
- **Policy gradient methods** like REINFORCE allow us to directly optimize the parameters of a policy by following the gradient of expected reward, even when we only have access to samples (trajectories) from the environment.

### What is a Randomized Policy?
- Instead of always picking the same action in a given state, a randomized policy assigns probabilities to each possible action.
- This is useful for exploration and for problems where the best strategy is not deterministic.
- We write $\pi_\theta(a|s)$ for the probability of taking action $a$ in state $s$ under policy parameters $\theta$.

### The Objective: Expected Return
- We want to maximize the expected total reward (return) over a trajectory:

  ```math
  \eta(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T-1} \gamma^t R(s_t, a_t) \right]
  ```
- Here, $\gamma$ is a discount factor (how much we care about future rewards), and $R(s_t, a_t)$ is the reward at time $t$.
- The expectation is over all possible trajectories generated by following the policy $\pi_\theta$.

### Why Not Use Value Functions?
- REINFORCE is **model-free**: it doesn't require learning value functions or transition models.
- We only need to be able to sample trajectories and observe rewards.

### The Policy Gradient Trick
- The main challenge is to compute the gradient of $\eta(\theta)$ with respect to $\theta$.
- We use a mathematical trick (the "log-derivative trick") to move the gradient inside the expectation:

  ```math
  \nabla_\theta \eta(\theta) = \mathbb{E}_{\tau \sim P_\theta} \left[ (\nabla_\theta \log P_\theta(\tau)) f(\tau) \right]
  ```
- This means we can estimate the gradient using only samples from the policy.

### What is $\log P_\theta(\tau)$?
- $P_\theta(\tau)$ is the probability of a trajectory $\tau$ under the policy.
- It factors into the product of initial state probability, policy probabilities, and transition probabilities.
- When we take the gradient with respect to $\theta$, only the policy terms matter (the environment transitions don't depend on $\theta$).

### The REINFORCE Estimator
- The gradient estimator becomes:

  ```math
  \nabla_\theta \eta(\theta) = \mathbb{E}_{\tau \sim P_\theta} \left[ \left( \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \cdot f(\tau) \right]
  ```
- In practice, we estimate this expectation by averaging over a batch of sampled trajectories.
- If a trajectory has high total reward, we increase the probability of the actions taken; if it has low reward, we decrease them.

### Why Use a Baseline?
- The variance of the estimator can be very high, making learning slow or unstable.
- We can subtract a **baseline** $B(s_t)$ from the reward without changing the expected value of the gradient, but potentially reducing its variance.
- A good choice for the baseline is the value function $V^{\pi_\theta}(s_t)$, i.e., the expected future reward from state $s_t$.
- Subtracting the baseline means we only reinforce actions that do better than average.

### The Policy Gradient with Baseline
- The estimator with baseline is:

  ```math
  \nabla_\theta \eta(\theta) = \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_{\geq t} - B(s_t)) \right]
  ```
- Here, $R_{\geq t}$ is the sum of discounted rewards from time $t$ onward.
- The baseline $B(s_t)$ is typically fit by regression to minimize the squared error to $R_{\geq t}$ over the sampled data.

### Step-by-Step: Vanilla Policy Gradient with Baseline
1. **Collect trajectories:** Run the current policy in the environment and record states, actions, and rewards.
2. **Compute returns:** For each time step $t$ in each trajectory, compute $R_{\geq t}$ (the sum of discounted future rewards).
3. **Fit the baseline:** Use regression to fit $B(s_t)$ to $R_{\geq t}$, minimizing the squared error.
4. **Compute gradients:** For each $(s_t, a_t)$, compute $\nabla_\theta \log \pi_\theta(a_t|s_t)$.
5. **Update policy:** Average the gradients weighted by $(R_{\geq t} - B(s_t))$ and take a step in that direction.

### Practical Notes and Intuition
- The baseline does not introduce bias, only reduces variance.
- If the baseline is perfect (equal to the expected return), the variance is minimized.
- In practice, even a rough baseline helps learning.
- The REINFORCE algorithm is simple but can be slow to converge; more advanced policy gradient methods build on these ideas.

### Analogy
- Imagine you are trying to learn which route to take to work. Each day, you try a route (a trajectory), and at the end, you get a score (reward). If a route is much better than your average, you are more likely to try similar routes in the future. If it's worse, you avoid them. The baseline is like your running average of how good your commute usually is.

---

**Interpretation of the policy gradient formula (17.8).**

The quantity $\nabla_\theta P_\theta(\tau) = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$ is intuitively the direction of the change of $\theta$ that will make the trajectory $\tau$ more likely to occur (or increase the probability of choosing action $a_0, \ldots, a_{t-1}$), and $f(\tau)$ is the total payoff of this trajectory. Thus, by taking a gradient step, intuitively we are trying to improve the likelihood of all the trajectories, but with a different emphasis or weight for each $\tau$ (or for each set of actions $a_0, a_1, \ldots, a_{t-1}$). If $\tau$ is very rewarding (that is, $f(\tau)$ is large), we try very hard to move in the direction that can increase the probability of the trajectory τ (or the direction that
increases the probability of choosing $`a_0,...,a_{t−1}`$), and if τ has low payoﬀ,
we try less hard with a smaller weight.

An interesting fact that follows from formula (17.3) is that

```math
\mathbb{E}_{\tau \sim P_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right] = 0 \tag{17.9}
```

To see this, we take $f(\tau) = 1$ (that is, the reward is always a constant), then the LHS of (17.8) is zero because the payoff is always a fixed constant $\sum_{t=0}^T \gamma^t$. Thus the RHS of (17.8) is also zero, which implies (17.9).

In fact, one can verify that $\mathbb{E}_{a_t \sim \pi_\theta(\cdot|s_t)} \nabla_\theta \log \pi_\theta(a_t|s_t) = 0$ for any fixed $t$ and $s_t$\(^2\). This fact has two consequences. First, we can simplify formula (17.8) to

```math
\nabla_\theta \eta(\theta) = \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \left( \sum_{j \geq t}^{T-1} \gamma^j R(s_j, a_j) \right) \right]
= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \left( \sum_{j \geq t}^{T-1} \gamma^j R(s_j, a_j) \right) \right] \tag{17.10}
```

where the second equality follows from

```math
\mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \left( \sum_{0 \leq j < t} \gamma^j R(s_j, a_j) \right) \right]
= \mathbb{E} \left[ \mathbb{E} [\nabla_\theta \log \pi_\theta(a_t|s_t) | s_0, a_0, \ldots, s_{t-1}, a_{t-1}, s_t] \cdot \left( \sum_{0 \leq j < t} \gamma^j R(s_j, a_j) \right) \right]
= 0 \quad \text{(because $\mathbb{E} [\nabla_\theta \log \pi_\theta(a_t|s_t) | s_0, a_0, \ldots, s_{t-1}, a_{t-1}, s_t] = 0$)}
```

Note that here we used the law of total expectation. The outer expectation in the second line above is over the randomness of $s_0, a_0, \ldots, a_{t-1}, s_t$, whereas the inner expectation is over the randomness of $a_t$ (conditioned on $s_0, a_0, \ldots, a_{t-1}, s_t$). We see that we've made the estimator slightly simpler.

The second consequence of $\mathbb{E}_{a_t \sim \pi_\theta(\cdot|s_t)} \nabla_\theta \log \pi_\theta(a_t|s_t) = 0$ is the following: for any value $B(s_t)$ that only depends on $s_t$, it holds that

```math
\mathbb{E}_{\tau \sim P_\theta} [\nabla_\theta \log \pi_\theta(a_t|s_t) \cdot B(s_t)]
= \mathbb{E} [\mathbb{E} [\nabla_\theta \log \pi_\theta(a_t|s_t) | s_0, a_0, \ldots, s_{t-1}, a_{t-1}, s_t] B(s_t)]
= 0 \quad \text{(because $\mathbb{E} [\nabla_\theta \log \pi_\theta(a_t|s_t) | s_0, a_0, \ldots, s_{t-1}, a_{t-1}, s_t] = 0$)}
```

\(^2\) In general, it's true that $\mathbb{E}_{x \sim p_\theta} [\nabla \log p_\theta(x)] = 0$.

Again here we used the law of total expectation. The outer expectation in the second line above is over the randomness of $s_0, a_0, \ldots, a_{t-1}, s_t$, whereas the inner expectation is over the randomness of $a_t$ (conditioned on $s_0, a_0, \ldots, a_{t-1}, s_t$.) It follows from equation (17.10) and the equation above that

```math
\nabla_\theta \eta(\theta) = \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \left( \sum_{j \geq t}^{T-1} \gamma^j R(s_j, a_j) - \gamma^t B(s_t) \right) \right]
= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim P_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \gamma^t \left( \sum_{j \geq t}^{T-1} \gamma^{j-t} R(s_j, a_j) - B(s_t) \right) \right] \tag{17.11}
```

Therefore, we will get a different estimator for estimating the $\nabla_\theta \eta(\theta)$ with a different choice of $B(\cdot)$. The benefit of introducing a proper $B(\cdot)$—which is often referred to as a **baseline**—is that it helps reduce the variance of the estimator.\(^3\) It turns out that a near optimal estimator would be the expected future payoff $\mathbb{E} \left[ \sum_{j \geq t}^{T-1} \gamma^{j-t} R(s_j, a_j) | s_t \right]$, which is pretty much the same as the value function $V^{\pi_\theta}(s_t)$ (if we ignore the difference between finite and infinite horizon.) Here one could estimate the value function $V^{\pi_\theta}(\cdot)$ in a crude way, because its precise value doesn't influence the mean of the estimator but only the variance. This leads to a policy gradient algorithm with baselines stated in Algorithm 7.\(^4\)

---

\(^3\) As a heuristic but illustrating example, suppose for a fixed $t$, the future reward $\sum_{j \geq t}^{T-1} \gamma^{j-t} R(s_j, a_j)$ randomly takes two values $1000 + 1$ and $1000 - 2$ with equal probability, and the corresponding values for $\nabla_\theta \log \pi_\theta(a_t|s_t)$ are vector $z$ and $-z$. (Note that because $\mathbb{E} [\nabla_\theta \log \pi_\theta(a_t|s_t)] = 0$, if $\nabla_\theta \log \pi_\theta(a_t|s_t)$ can only take two values uniformly, then the two values have to two vectors in an opposite direction.) In this case, without subtracting the baseline, the estimators take two values $(1000 + 1)z$ and $-(1000 - 2)z$, whereas after subtracting a baseline of 1000, the estimator has two values $z$ and $2z$. The latter estimator has much lower variance compared to the original estimator.

\(^4\) We note that the estimator of the gradient in the algorithm does not exactly match the equation (17.13). If we multiply $\gamma^t$ in the summand of equation (17.13), then they will exactly match. Removing such discount factors empirically works well because it gives a large update.

---

**Algorithm 7** Vanilla policy gradient with baseline

For $i = 1, \ldots$ do:

1. **Collect a set of trajectories** by executing the current policy. Use $R_{\geq t}$ as a shorthand for $\sum_{j \geq t}^{T-1} \gamma^{j-t} R(s_j, a_j)$.
2. **Fit the baseline** by finding a function $B$ that minimizes

   ```math
   \sum_{\tau} \sum_t (R_{\geq t} - B(s_t))^2 \tag{17.12}
   ```

3. **Update the policy parameter $\theta$** with the gradient estimator

   ```math
   \sum_{\tau} \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_{\geq t} - B(s_t)) \tag{17.13}
   ```

