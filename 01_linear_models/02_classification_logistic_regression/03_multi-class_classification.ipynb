{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f836e7b5-5b68-4ab7-925d-4a3a71abd4b2",
   "metadata": {},
   "source": [
    "# 2.3 Multi-Class Classification\n",
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "In many real-world problems, the task is not just to distinguish between two classes (binary classification), but among three or more possible categories. Here are some compelling examples:\n",
    "\n",
    "- **Email classification:** spam, personal, work, marketing, newsletters\n",
    "- **Image recognition:** cat, dog, car, airplane, bird, fish, etc.\n",
    "- **Handwritten digit recognition:** digits 0 through 9 (MNIST dataset)\n",
    "- **Medical diagnosis:** healthy, disease A, disease B, disease C, etc.\n",
    "- **Language identification:** English, Spanish, French, German, Chinese, etc.\n",
    "- **Product categorization:** electronics, clothing, books, food, etc.\n",
    "- **Sentiment analysis:** very negative, negative, neutral, positive, very positive\n",
    "\n",
    "Multi-class classification is essential in machine learning because most practical problems involve more than two possible outcomes. The response variable $y$ can take on any one of $k$ values, so $y \\in \\{1, 2, \\ldots, k\\}$.\n",
    "\n",
    "## From Binary Classification to Multi-Class Problems\n",
    "\n",
    "So far, we've focused on **binary classification** problems where we need to distinguish between exactly two classes. We explored two approaches: the probabilistic logistic regression with its smooth sigmoid function, and the deterministic perceptron with its hard threshold function.\n",
    "\n",
    "However, the real world is rarely so simple. Most practical classification problems involve **multiple classes** - sometimes dozens or even hundreds of possible categories. Email classification systems need to distinguish between spam, personal, work, marketing, and newsletter emails. Image recognition systems must identify hundreds of different objects. Medical diagnosis systems might need to distinguish among dozens of possible conditions.\n",
    "\n",
    "This motivates our exploration of **multi-class classification**, where we extend the probabilistic framework we developed in logistic regression to handle multiple classes. The key insight is the **softmax function**, which generalizes the sigmoid function to multiple outputs while maintaining the mathematical properties that make logistic regression so effective.\n",
    "\n",
    "This transition represents a natural evolution in our understanding of classification - from simple binary decisions to the complex decision boundaries needed in real-world applications.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "In multi-class classification, we have:\n",
    "- **Input space:** $\\mathcal{X} \\subseteq \\mathbb{R}^d$ (feature vectors)\n",
    "- **Output space:** $\\mathcal{Y} = \\{1, 2, \\ldots, k\\}$ (class labels)\n",
    "- **Training data:** $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$\n",
    "- **Goal:** Learn a function $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that accurately predicts the class label\n",
    "\n",
    "### Challenges in Multi-Class Classification\n",
    "\n",
    "1. **Complexity:** More classes mean more complex decision boundaries\n",
    "2. **Imbalanced Data:** Some classes may have many more examples than others\n",
    "3. **Computational Cost:** Training time scales with the number of classes\n",
    "4. **Evaluation:** More complex metrics needed (beyond accuracy)\n",
    "\n",
    "## The Multinomial Model and Softmax Intuition\n",
    "\n",
    "### From Binary to Multi-Class\n",
    "\n",
    "Recall that in binary classification, we often use the logistic (sigmoid) function to map real-valued scores to probabilities. In the multi-class case, we need a function that:\n",
    "- Outputs a probability for each class\n",
    "- Ensures all probabilities are non-negative and sum to 1\n",
    "\n",
    "This is achieved by the **softmax function**, which generalizes the sigmoid function to multiple classes.\n",
    "\n",
    "### The Softmax Function\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be the input features. We introduce $k$ parameter vectors $\\theta_1, \\ldots, \\theta_k$, each in $\\mathbb{R}^d$. For each class $i$, we compute a score (logit):\n",
    "\n",
    "$$\n",
    "t_i = \\theta_i^\\top x\n",
    "$$\n",
    "\n",
    "The vector $t = (t_1, \\ldots, t_k)$ is then passed through softmax:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(t_1, \\ldots, t_k) = \\left[ \\frac{\\exp(t_1)}{\\sum_{j=1}^k \\exp(t_j)}, \\ldots, \\frac{\\exp(t_k)}{\\sum_{j=1}^k \\exp(t_j)} \\right]\n",
    "$$\n",
    "\n",
    "The output is a probability vector $\\phi = (\\phi_1, \\ldots, \\phi_k)$, where $\\phi_i$ is the probability assigned to class $i$.\n",
    "\n",
    "### Intuitive Understanding of Softmax\n",
    "\n",
    "#### Why Exponentiation?\n",
    "\n",
    "The exponential function $\\exp(t_i)$ has several desirable properties:\n",
    "1. **Always Positive:** $\\exp(t_i) > 0$ for any real $t_i$\n",
    "2. **Monotonic:** Larger $t_i$ leads to larger $\\exp(t_i)$\n",
    "3. **Sensitive to Differences:** Small differences in $t_i$ become amplified\n",
    "\n",
    "#### Why Normalization?\n",
    "\n",
    "Dividing by the sum $\\sum_{j=1}^k \\exp(t_j)$ ensures:\n",
    "1. **Probability Constraint:** $\\sum_{i=1}^k \\phi_i = 1$\n",
    "2. **Non-negative:** $\\phi_i \\geq 0$ for all $i$\n",
    "3. **Relative Scale:** Probabilities depend on relative differences between logits\n",
    "\n",
    "#### Geometric Interpretation\n",
    "\n",
    "Softmax can be viewed as projecting a $k$-dimensional real vector (the logits) onto the $(k-1)$-dimensional probability simplex. The simplex is the set of all probability distributions over $k$ classes.\n",
    "\n",
    "### Properties of Softmax\n",
    "\n",
    "#### Invariance to Translation\n",
    "\n",
    "Softmax is invariant to adding a constant to all logits:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(t_1 + c, \\ldots, t_k + c) = \\mathrm{softmax}(t_1, \\ldots, t_k)\n",
    "$$\n",
    "\n",
    "This means we can subtract the maximum logit for numerical stability:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(t_1, \\ldots, t_k) = \\mathrm{softmax}(t_1 - \\max_j t_j, \\ldots, t_k - \\max_j t_j)\n",
    "$$\n",
    "\n",
    "#### Temperature Scaling\n",
    "\n",
    "We can control the \"sharpness\" of the distribution by introducing a temperature parameter $\\tau$:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}_\\tau(t_1, \\ldots, t_k) = \\left[ \\frac{\\exp(t_1/\\tau)}{\\sum_{j=1}^k \\exp(t_j/\\tau)}, \\ldots, \\frac{\\exp(t_k/\\tau)}{\\sum_{j=1}^k \\exp(t_j/\\tau)} \\right]\n",
    "$$\n",
    "\n",
    "- **$\\tau \\to 0$:** Approaches one-hot encoding (deterministic)\n",
    "- **$\\tau = 1$:** Standard softmax\n",
    "- **$\\tau \\to \\infty$:** Approaches uniform distribution\n",
    "\n",
    "## Probabilistic Model\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "Given input $x$, the model predicts:\n",
    "\n",
    "$$\n",
    "P(y = i \\mid x; \\theta) = \\phi_i = \\frac{\\exp(\\theta_i^\\top x)}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x)}\n",
    "$$\n",
    "\n",
    "This is a generalization of logistic regression to multiple classes, sometimes called **multinomial logistic regression** or **softmax regression**.\n",
    "\n",
    "### Parameter Interpretation\n",
    "\n",
    "- **$\\theta_i$:** Parameter vector for class $i$\n",
    "- **$\\theta_i^\\top x$:** Score (logit) for class $i$\n",
    "- **$\\phi_i$:** Probability of class $i$\n",
    "\n",
    "#### Decision Rule\n",
    "\n",
    "The predicted class is:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{i} P(y = i \\mid x; \\theta) = \\arg\\max_{i} \\theta_i^\\top x\n",
    "$$\n",
    "\n",
    "This shows that the decision boundary between any two classes $i$ and $j$ is linear:\n",
    "$$\n",
    "\\theta_i^\\top x = \\theta_j^\\top x \\implies (\\theta_i - \\theta_j)^\\top x = 0\n",
    "$$\n",
    "\n",
    "### Example: 3-Class Classification\n",
    "\n",
    "Consider a 3-class problem with:\n",
    "- $\\theta_1 = [1, 2]$, $\\theta_2 = [0, 1]$, $\\theta_3 = [-1, 0]$\n",
    "- Input $x = [1, 1]$\n",
    "\n",
    "Then:\n",
    "- $t_1 = \\theta_1^\\top x = 1 \\cdot 1 + 2 \\cdot 1 = 3$\n",
    "- $t_2 = \\theta_2^\\top x = 0 \\cdot 1 + 1 \\cdot 1 = 1$\n",
    "- $t_3 = \\theta_3^\\top x = -1 \\cdot 1 + 0 \\cdot 1 = -1$\n",
    "\n",
    "Softmax probabilities:\n",
    "- $\\phi_1 = \\frac{e^3}{e^3 + e^1 + e^{-1}} \\approx 0.88$\n",
    "- $\\phi_2 = \\frac{e^1}{e^3 + e^1 + e^{-1}} \\approx 0.12$\n",
    "- $\\phi_3 = \\frac{e^{-1}}{e^3 + e^1 + e^{-1}} \\approx 0.00$\n",
    "\n",
    "Prediction: Class 1 (highest probability)\n",
    "\n",
    "## Loss Function: Cross-Entropy and Negative Log-Likelihood\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Given $n$ independent training examples, the likelihood is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n P(y^{(i)} \\mid x^{(i)}; \\theta) = \\prod_{i=1}^n \\frac{\\exp(\\theta_{y^{(i)}}^\\top x^{(i)})}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x^{(i)})}\n",
    "$$\n",
    "\n",
    "### Negative Log-Likelihood\n",
    "\n",
    "Maximizing likelihood is equivalent to minimizing negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = -\\log L(\\theta) = \\sum_{i=1}^n -\\log \\left( \\frac{\\exp(\\theta_{y^{(i)}}^\\top x^{(i)})}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x^{(i)})} \\right)\n",
    "$$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "The cross-entropy loss for a single example is:\n",
    "\n",
    "$$\n",
    "\\ell_{ce}((t_1, \\ldots, t_k), y) = -\\log \\left( \\frac{\\exp(t_y)}{\\sum_{i=1}^k \\exp(t_i)} \\right)\n",
    "$$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The loss penalizes the model when it assigns low probability to the true class:\n",
    "- **Correct prediction with high confidence:** Low loss\n",
    "- **Correct prediction with low confidence:** Higher loss\n",
    "- **Incorrect prediction with high confidence:** Very high loss\n",
    "- **Incorrect prediction with low confidence:** Lower loss\n",
    "\n",
    "#### Example Calculation\n",
    "\n",
    "For the previous example with $t = [3, 1, -1]$ and true class $y = 2$:\n",
    "$$\n",
    "\\ell_{ce} = -\\log\\left(\\frac{e^1}{e^3 + e^1 + e^{-1}}\\right) = -\\log(0.12) \\approx 2.12\n",
    "$$\n",
    "\n",
    "## Step-by-Step Example\n",
    "\n",
    "### Complete Example: 3-Class Classification\n",
    "\n",
    "Suppose we have 3 classes and a single input $x$ with logits $t = (2, 1, 0)$. Let's compute the softmax probabilities step by step:\n",
    "\n",
    "1. **Compute exponentials:** \n",
    "   - $\\exp(2) \\approx 7.39$\n",
    "   - $\\exp(1) \\approx 2.72$\n",
    "   - $\\exp(0) = 1$\n",
    "\n",
    "2. **Sum the exponentials:** \n",
    "   - $7.39 + 2.72 + 1 = 11.11$\n",
    "\n",
    "3. **Compute probabilities:** \n",
    "   - $\\phi_1 = 7.39/11.11 \\approx 0.665$\n",
    "   - $\\phi_2 = 2.72/11.11 \\approx 0.245$\n",
    "   - $\\phi_3 = 1/11.11 \\approx 0.090$\n",
    "\n",
    "4. **Verify probability constraint:** \n",
    "   - $0.665 + 0.245 + 0.090 = 1.000$ ✓\n",
    "\n",
    "If the true class is 2 (indexing from 1), the cross-entropy loss is:\n",
    "$$\n",
    "-\\log(0.245) \\approx 1.40\n",
    "$$\n",
    "\n",
    "### Numerical Stability Example\n",
    "\n",
    "Consider logits $t = [1000, 1001, 1002]$:\n",
    "- **Naive computation:** $\\exp(1000) \\approx \\infty$ (overflow!)\n",
    "- **Stable computation:** Subtract max logit first\n",
    "  - $t' = [1000-1002, 1001-1002, 1002-1002] = [-2, -1, 0]$\n",
    "  - $\\exp(-2) \\approx 0.135$, $\\exp(-1) \\approx 0.368$, $\\exp(0) = 1$\n",
    "  - Probabilities: $[0.090, 0.245, 0.665]$\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "When computing softmax, subtract the maximum logit from all logits before exponentiating to avoid overflow:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(t)_i = \\frac{\\exp(t_i - \\max_j t_j)}{\\sum_{j=1}^k \\exp(t_j - \\max_j t_j)}\n",
    "$$\n",
    "\n",
    "This is mathematically equivalent but numerically stable.\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "Labels should be integers $1, \\ldots, k$ (or $0, \\ldots, k-1$ depending on convention). Common approaches:\n",
    "- **One-hot encoding:** Convert to binary vectors\n",
    "- **Integer encoding:** Use class indices directly\n",
    "- **Ordinal encoding:** For ordered classes\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Most ML libraries have built-in softmax and cross-entropy loss functions:\n",
    "- **NumPy:** `np.softmax()`, `np.log_softmax()`\n",
    "- **PyTorch:** `torch.softmax()`, `torch.nn.CrossEntropyLoss()`\n",
    "- **TensorFlow:** `tf.nn.softmax()`, `tf.keras.losses.SparseCategoricalCrossentropy()`\n",
    "- **scikit-learn:** `LogisticRegression(multi_class='multinomial')`\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, add regularization terms:\n",
    "- **L2 regularization:** $\\lambda \\sum_{i=1}^k \\|\\theta_i\\|_2^2$\n",
    "- **L1 regularization:** $\\lambda \\sum_{i=1}^k \\|\\theta_i\\|_1$\n",
    "- **Dropout:** Randomly zero some inputs during training\n",
    "\n",
    "## Gradient Derivation and Intuition\n",
    "\n",
    "### Gradient of Cross-Entropy Loss\n",
    "\n",
    "The cross-entropy loss has a simple and elegant gradient. For a single example:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_{ce}(t, y)}{\\partial t_i} = \\phi_i - 1\\{y = i\\}\n",
    "$$\n",
    "\n",
    "where $1\\{y = i\\}$ is the indicator function (1 if $y = i$, 0 otherwise).\n",
    "\n",
    "#### Intuitive Understanding\n",
    "\n",
    "- **$\\phi_i - 1\\{y = i\\}$:** Difference between predicted and true probability\n",
    "- **Positive gradient:** Predicted probability too high, decrease it\n",
    "- **Negative gradient:** Predicted probability too low, increase it\n",
    "- **Zero gradient:** Perfect prediction\n",
    "\n",
    "#### Vectorized Form\n",
    "\n",
    "In vectorized form: $\\nabla_t \\ell_{ce}(t, y) = \\phi - e_y$\n",
    "where $e_y$ is the one-hot encoding of $y$.\n",
    "\n",
    "### Gradient for Model Parameters\n",
    "\n",
    "For the model parameters $\\theta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} = \\sum_{j=1}^n (\\phi_i^{(j)} - 1\\{y^{(j)} = i\\}) x^{(j)}\n",
    "$$\n",
    "\n",
    "This form is efficient to compute and is the basis for gradient descent and backpropagation in neural networks.\n",
    "\n",
    "### Gradient Descent Update\n",
    "\n",
    "The parameter update rule is:\n",
    "\n",
    "$$\n",
    "\\theta_i := \\theta_i - \\alpha \\sum_{j=1}^n (\\phi_i^{(j)} - 1\\{y^{(j)} = i\\}) x^{(j)}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "## Applications and Extensions\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Softmax is used as the final layer in multi-class classification networks:\n",
    "1. **Hidden layers:** Learn feature representations\n",
    "2. **Output layer:** Linear transformation + softmax\n",
    "3. **Training:** Backpropagate gradients through the network\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "- **MNIST:** 10-class digit recognition\n",
    "- **CIFAR-10:** 10-class image classification\n",
    "- **ImageNet:** 1000-class image classification\n",
    "- **Object detection:** Multi-class with bounding boxes\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "- **Text classification:** Topic classification, sentiment analysis\n",
    "- **Part-of-speech tagging:** Grammatical role classification\n",
    "- **Named entity recognition:** Person, organization, location, etc.\n",
    "- **Language modeling:** Next word prediction\n",
    "\n",
    "### Medical Applications\n",
    "\n",
    "- **Disease diagnosis:** Multiple possible conditions\n",
    "- **Drug discovery:** Compound classification\n",
    "- **Medical imaging:** Tissue type classification\n",
    "- **Patient stratification:** Risk group classification\n",
    "\n",
    "### Extensions and Variants\n",
    "\n",
    "#### Hierarchical Softmax\n",
    "\n",
    "For large numbers of classes ($k \\gg 100$), hierarchical softmax organizes classes in a tree structure:\n",
    "- **Advantages:** Faster training and inference\n",
    "- **Disadvantages:** Requires hierarchical structure\n",
    "- **Applications:** Large vocabulary language models\n",
    "\n",
    "#### Label Smoothing\n",
    "\n",
    "Replace hard targets with soft targets to improve generalization:\n",
    "$$\n",
    "y_i' = (1 - \\epsilon) \\cdot 1\\{y = i\\} + \\frac{\\epsilon}{k}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a small constant (e.g., 0.1).\n",
    "\n",
    "#### Multi-Label Classification\n",
    "\n",
    "When an example can belong to multiple classes simultaneously:\n",
    "- **Approach:** Use sigmoid activation for each class\n",
    "- **Loss:** Binary cross-entropy for each class\n",
    "- **Applications:** Tag prediction, multi-disease diagnosis\n",
    "\n",
    "#### Ordinal Classification\n",
    "\n",
    "When classes have a natural ordering:\n",
    "- **Approach:** Model cumulative probabilities\n",
    "- **Loss:** Ordinal regression loss\n",
    "- **Applications:** Rating prediction, severity assessment\n",
    "\n",
    "## Comparison with Other Approaches\n",
    "\n",
    "### One-vs-Rest (OvR)\n",
    "\n",
    "Train $k$ binary classifiers, one for each class:\n",
    "- **Advantages:** Simple, can use any binary classifier\n",
    "- **Disadvantages:** Imbalanced training sets, no probability calibration\n",
    "- **When to use:** Small number of classes, existing binary classifiers\n",
    "\n",
    "### One-vs-One (OvO)\n",
    "\n",
    "Train $\\binom{k}{2}$ binary classifiers for each pair of classes:\n",
    "- **Advantages:** Balanced training sets, can handle non-linear boundaries\n",
    "- **Disadvantages:** More classifiers, complex prediction\n",
    "- **When to use:** Small number of classes, non-linear boundaries\n",
    "\n",
    "### Softmax vs. Other Methods\n",
    "\n",
    "| Method | Advantages | Disadvantages |\n",
    "|--------|------------|---------------|\n",
    "| **Softmax** | Probabilistic, efficient, convex | Linear boundaries only |\n",
    "| **OvR** | Simple, flexible | Imbalanced, no calibration |\n",
    "| **OvO** | Balanced, non-linear | Many classifiers |\n",
    "| **Decision Trees** | Non-linear, interpretable | No probabilities |\n",
    "| **Random Forest** | Robust, feature importance | Black box |\n",
    "| **SVM** | Non-linear (with kernels) | No probabilities |\n",
    "\n",
    "## Summary\n",
    "\n",
    "Multi-class classification with softmax and cross-entropy is a foundational technique in machine learning, generalizing logistic regression to multiple classes. Its mathematical simplicity, interpretability, and efficient gradient make it the default choice for many practical problems.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Softmax Function:** Smooth, differentiable generalization of sigmoid\n",
    "2. **Cross-Entropy Loss:** Natural loss function for probability estimation\n",
    "3. **Linear Decision Boundaries:** Between any pair of classes\n",
    "4. **Numerical Stability:** Always subtract max logit before exponentiating\n",
    "5. **Efficient Gradients:** Simple, interpretable gradient expressions\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "For more advanced topics, see:\n",
    "- **Neural networks:** Multi-layer architectures\n",
    "- **Kernel methods:** Non-linear feature spaces\n",
    "- **Ensemble methods:** Combining multiple classifiers\n",
    "- **Calibration:** Improving probability estimates\n",
    "- **Active learning:** Selecting informative examples\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** There are some ambiguities in naming conventions. Some people call the cross-entropy loss the function that maps the probability vector (the $\\phi$ in our language) and label $y$ to the final real number, and call our version of cross-entropy loss softmax-cross-entropy loss. We choose our current naming convention because it's consistent with the naming of most modern deep learning libraries such as PyTorch and Jax.\n",
    "\n",
    "---\n",
    "\n",
    "**Previous: [Perceptron Algorithm](02_perceptron.md)** - Learn about the perceptron learning algorithm and its relationship to linear classification.\n",
    "\n",
    "**Next: [Newton's Method](04_newtons_method.md)** - Explore second-order optimization methods for faster convergence in logistic regression.\n",
    "\n",
    "## From Classification Models to Advanced Optimization\n",
    "\n",
    "We've now built a comprehensive understanding of classification problems, from binary classification with logistic regression and perceptron algorithms to multi-class classification with softmax regression. These models provide powerful tools for making predictions, but they all rely on optimization algorithms to find the best parameters.\n",
    "\n",
    "So far, we've used **gradient ascent** (or gradient descent on the negative log-likelihood) to optimize our classification models. While gradient methods are simple and effective, they have limitations: they may require many iterations to converge, they're sensitive to the learning rate choice, and they don't take advantage of the curvature information available in our models.\n",
    "\n",
    "This motivates our exploration of **Newton's method**, a second-order optimization technique that uses both gradient and curvature information to make more informed parameter updates. Newton's method can converge much faster than gradient methods, especially for well-behaved functions like the logistic regression log-likelihood.\n",
    "\n",
    "The transition from first-order to second-order optimization represents a natural progression in our understanding of machine learning optimization - from simple gradient methods to more sophisticated techniques that leverage additional mathematical structure in our problems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
