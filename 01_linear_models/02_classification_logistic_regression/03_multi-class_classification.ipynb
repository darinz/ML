{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Multi-Class Classification\n",
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "In many real-world problems, the task is not just to distinguish between\n",
    "two classes (binary classification), but among three or more possible\n",
    "categories. Here are some compelling examples:\n",
    "\n",
    "- **Email classification:** spam, personal, work, marketing, newsletters\n",
    "- **Image recognition:** cat, dog, car, airplane, bird, fish, etc.\n",
    "- **Handwritten digit recognition:** digits 0 through 9 (MNIST dataset)\n",
    "- **Medical diagnosis:** healthy, disease A, disease B, disease C, etc.\n",
    "- **Language identification:** English, Spanish, French, German,\n",
    "  Chinese, etc.\n",
    "- **Product categorization:** electronics, clothing, books, food, etc.\n",
    "- **Sentiment analysis:** very negative, negative, neutral, positive,\n",
    "  very positive\n",
    "\n",
    "Multi-class classification is essential in machine learning because most\n",
    "practical problems involve more than two possible outcomes. The response\n",
    "variable $y$ can take on any one of $k$ values, so\n",
    "$y \\in \\{1, 2, \\ldots, k\\}$.\n",
    "\n",
    "## From Binary Classification to Multi-Class Problems\n",
    "\n",
    "So far, we’ve focused on **binary classification** problems where we\n",
    "need to distinguish between exactly two classes. We explored two\n",
    "approaches: the probabilistic logistic regression with its smooth\n",
    "sigmoid function, and the deterministic perceptron with its hard\n",
    "threshold function.\n",
    "\n",
    "However, the real world is rarely so simple. Most practical\n",
    "classification problems involve **multiple classes** - sometimes dozens\n",
    "or even hundreds of possible categories. Email classification systems\n",
    "need to distinguish between spam, personal, work, marketing, and\n",
    "newsletter emails. Image recognition systems must identify hundreds of\n",
    "different objects. Medical diagnosis systems might need to distinguish\n",
    "among dozens of possible conditions.\n",
    "\n",
    "This motivates our exploration of **multi-class classification**, where\n",
    "we extend the probabilistic framework we developed in logistic\n",
    "regression to handle multiple classes. The key insight is the **softmax\n",
    "function**, which generalizes the sigmoid function to multiple outputs\n",
    "while maintaining the mathematical properties that make logistic\n",
    "regression so effective.\n",
    "\n",
    "This transition represents a natural evolution in our understanding of\n",
    "classification - from simple binary decisions to the complex decision\n",
    "boundaries needed in real-world applications.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "In multi-class classification, we have: - **Input space:**\n",
    "$\\mathcal{X} \\subseteq \\mathbb{R}^d$ (feature vectors) - **Output\n",
    "space:** $\\mathcal{Y} = \\{1, 2, \\ldots, k\\}$ (class labels) - **Training\n",
    "data:**\n",
    "$\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$ -\n",
    "**Goal:** Learn a function $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that\n",
    "accurately predicts the class label\n",
    "\n",
    "### Challenges in Multi-Class Classification\n",
    "\n",
    "1.  **Complexity:** More classes mean more complex decision boundaries\n",
    "2.  **Imbalanced Data:** Some classes may have many more examples than\n",
    "    others\n",
    "3.  **Computational Cost:** Training time scales with the number of\n",
    "    classes\n",
    "4.  **Evaluation:** More complex metrics needed (beyond accuracy)\n",
    "\n",
    "## The Multinomial Model and Softmax Intuition\n",
    "\n",
    "### From Binary to Multi-Class\n",
    "\n",
    "Recall that in binary classification, we often use the logistic\n",
    "(sigmoid) function to map real-valued scores to probabilities. In the\n",
    "multi-class case, we need a function that: - Outputs a probability for\n",
    "each class - Ensures all probabilities are non-negative and sum to 1\n",
    "\n",
    "This is achieved by the **softmax function**, which generalizes the\n",
    "sigmoid function to multiple classes.\n",
    "\n",
    "### The Softmax Function\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be the input features. We introduce $k$\n",
    "parameter vectors $\\theta_1, \\ldots, \\theta_k$, each in $\\mathbb{R}^d$.\n",
    "For each class $i$, we compute a score (logit):\n",
    "\n",
    "$$t_i = \\theta_i^\\top x$$\n",
    "\n",
    "The vector $t = (t_1, \\ldots, t_k)$ is then passed through softmax:\n",
    "\n",
    "$$\\mathrm{softmax}(t_1, \\ldots, t_k) = \\left[ \\frac{\\exp(t_1)}{\\sum_{j=1}^k \\exp(t_j)}, \\ldots, \\frac{\\exp(t_k)}{\\sum_{j=1}^k \\exp(t_j)} \\right]$$\n",
    "\n",
    "The output is a probability vector $\\phi = (\\phi_1, \\ldots, \\phi_k)$,\n",
    "where $\\phi_i$ is the probability assigned to class $i$.\n",
    "\n",
    "### Intuitive Understanding of Softmax\n",
    "\n",
    "#### Why Exponentiation?\n",
    "\n",
    "The exponential function $\\exp(t_i)$ has several desirable\n",
    "properties: 1. **Always Positive:** $\\exp(t_i) > 0$ for any real $t_i$\n",
    "2. **Monotonic:** Larger $t_i$ leads to larger $\\exp(t_i)$ 3.\n",
    "**Sensitive to Differences:** Small differences in $t_i$ become\n",
    "amplified\n",
    "\n",
    "#### Why Normalization?\n",
    "\n",
    "Dividing by the sum $\\sum_{j=1}^k \\exp(t_j)$ ensures: 1. **Probability\n",
    "Constraint:** $\\sum_{i=1}^k \\phi_i = 1$ 2. **Non-negative:**\n",
    "$\\phi_i \\geq 0$ for all $i$ 3. **Relative Scale:** Probabilities depend\n",
    "on relative differences between logits\n",
    "\n",
    "#### Geometric Interpretation\n",
    "\n",
    "Softmax can be viewed as projecting a $k$-dimensional real vector (the\n",
    "logits) onto the $(k-1)$-dimensional probability simplex. The simplex is\n",
    "the set of all probability distributions over $k$ classes.\n",
    "\n",
    "### Properties of Softmax\n",
    "\n",
    "#### Invariance to Translation\n",
    "\n",
    "Softmax is invariant to adding a constant to all logits:\n",
    "\n",
    "$$\\mathrm{softmax}(t_1 + c, \\ldots, t_k + c) = \\mathrm{softmax}(t_1, \\ldots, t_k)$$\n",
    "\n",
    "This means we can subtract the maximum logit for numerical stability:\n",
    "\n",
    "$$\\mathrm{softmax}(t_1, \\ldots, t_k) = \\mathrm{softmax}(t_1 - \\max_j t_j, \\ldots, t_k - \\max_j t_j)$$\n",
    "\n",
    "#### Temperature Scaling\n",
    "\n",
    "We can control the “sharpness” of the distribution by introducing a\n",
    "temperature parameter $\\tau$:\n",
    "\n",
    "$$\\mathrm{softmax}_\\tau(t_1, \\ldots, t_k) = \\left[ \\frac{\\exp(t_1/\\tau)}{\\sum_{j=1}^k \\exp(t_j/\\tau)}, \\ldots, \\frac{\\exp(t_k/\\tau)}{\\sum_{j=1}^k \\exp(t_j/\\tau)} \\right]$$\n",
    "\n",
    "- **$\\tau \\to 0$:** Approaches one-hot encoding (deterministic)\n",
    "- **$\\tau = 1$:** Standard softmax\n",
    "- **$\\tau \\to \\infty$:** Approaches uniform distribution\n",
    "\n",
    "## Probabilistic Model\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "Given input $x$, the model predicts:\n",
    "\n",
    "$$P(y = i \\mid x; \\theta) = \\phi_i = \\frac{\\exp(\\theta_i^\\top x)}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x)}$$\n",
    "\n",
    "This is a generalization of logistic regression to multiple classes,\n",
    "sometimes called **multinomial logistic regression** or **softmax\n",
    "regression**.\n",
    "\n",
    "### Parameter Interpretation\n",
    "\n",
    "- **$\\theta_i$:** Parameter vector for class $i$\n",
    "- **$\\theta_i^\\top x$:** Score (logit) for class $i$\n",
    "- **$\\phi_i$:** Probability of class $i$\n",
    "\n",
    "#### Decision Rule\n",
    "\n",
    "The predicted class is:\n",
    "$$\\hat{y} = \\arg\\max_{i} P(y = i \\mid x; \\theta) = \\arg\\max_{i} \\theta_i^\\top x$$\n",
    "\n",
    "This shows that the decision boundary between any two classes $i$ and\n",
    "$j$ is linear:\n",
    "$$\\theta_i^\\top x = \\theta_j^\\top x \\implies (\\theta_i - \\theta_j)^\\top x = 0$$\n",
    "\n",
    "### Example: 3-Class Classification\n",
    "\n",
    "Consider a 3-class problem with: - $\\theta_1 = [1, 2]$,\n",
    "$\\theta_2 = [0, 1]$, $\\theta_3 = [-1, 0]$ - Input $x = [1, 1]$\n",
    "\n",
    "Then: - $t_1 = \\theta_1^\\top x = 1 \\cdot 1 + 2 \\cdot 1 = 3$ -\n",
    "$t_2 = \\theta_2^\\top x = 0 \\cdot 1 + 1 \\cdot 1 = 1$ -\n",
    "$t_3 = \\theta_3^\\top x = -1 \\cdot 1 + 0 \\cdot 1 = -1$\n",
    "\n",
    "Softmax probabilities: -\n",
    "$\\phi_1 = \\frac{e^3}{e^3 + e^1 + e^{-1}} \\approx 0.88$ -\n",
    "$\\phi_2 = \\frac{e^1}{e^3 + e^1 + e^{-1}} \\approx 0.12$ -\n",
    "$\\phi_3 = \\frac{e^{-1}}{e^3 + e^1 + e^{-1}} \\approx 0.00$\n",
    "\n",
    "Prediction: Class 1 (highest probability)\n",
    "\n",
    "## Loss Function: Cross-Entropy and Negative Log-Likelihood\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Given $n$ independent training examples, the likelihood is:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^n P(y^{(i)} \\mid x^{(i)}; \\theta) = \\prod_{i=1}^n \\frac{\\exp(\\theta_{y^{(i)}}^\\top x^{(i)})}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x^{(i)})}$$\n",
    "\n",
    "### Negative Log-Likelihood\n",
    "\n",
    "Maximizing likelihood is equivalent to minimizing negative\n",
    "log-likelihood:\n",
    "\n",
    "$$\\ell(\\theta) = -\\log L(\\theta) = \\sum_{i=1}^n -\\log \\left( \\frac{\\exp(\\theta_{y^{(i)}}^\\top x^{(i)})}{\\sum_{j=1}^k \\exp(\\theta_j^\\top x^{(i)})} \\right)$$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "The cross-entropy loss for a single example is:\n",
    "\n",
    "$$\\ell_{ce}((t_1, \\ldots, t_k), y) = -\\log \\left( \\frac{\\exp(t_y)}{\\sum_{i=1}^k \\exp(t_i)} \\right)$$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The loss penalizes the model when it assigns low probability to the true\n",
    "class: - **Correct prediction with high confidence:** Low loss -\n",
    "**Correct prediction with low confidence:** Higher loss - **Incorrect\n",
    "prediction with high confidence:** Very high loss - **Incorrect\n",
    "prediction with low confidence:** Lower loss\n",
    "\n",
    "#### Example Calculation\n",
    "\n",
    "For the previous example with $t = [3, 1, -1]$ and true class $y = 2$:\n",
    "$$\\ell_{ce} = -\\log\\left(\\frac{e^1}{e^3 + e^1 + e^{-1}}\\right) = -\\log(0.12) \\approx 2.12$$\n",
    "\n",
    "## Step-by-Step Example\n",
    "\n",
    "### Complete Example: 3-Class Classification\n",
    "\n",
    "Suppose we have 3 classes and a single input $x$ with logits\n",
    "$t = (2, 1, 0)$. Let’s compute the softmax probabilities step by step:\n",
    "\n",
    "1.  **Compute exponentials:**\n",
    "    - $\\exp(2) \\approx 7.39$\n",
    "    - $\\exp(1) \\approx 2.72$\n",
    "    - $\\exp(0) = 1$\n",
    "2.  **Sum the exponentials:**\n",
    "    - $7.39 + 2.72 + 1 = 11.11$\n",
    "3.  **Compute probabilities:**\n",
    "    - $\\phi_1 = 7.39/11.11 \\approx 0.665$\n",
    "    - $\\phi_2 = 2.72/11.11 \\approx 0.245$\n",
    "    - $\\phi_3 = 1/11.11 \\approx 0.090$\n",
    "4.  **Verify probability constraint:**\n",
    "    - $0.665 + 0.245 + 0.090 = 1.000$ ✓\n",
    "\n",
    "If the true class is 2 (indexing from 1), the cross-entropy loss is:\n",
    "$$-\\log(0.245) \\approx 1.40$$\n",
    "\n",
    "### Numerical Stability Example\n",
    "\n",
    "Consider logits $t = [1000, 1001, 1002]$: - **Naive computation:**\n",
    "$\\exp(1000) \\approx \\infty$ (overflow!) - **Stable computation:**\n",
    "Subtract max logit first -\n",
    "$t' = [1000-1002, 1001-1002, 1002-1002] = [-2, -1, 0]$ -\n",
    "$\\exp(-2) \\approx 0.135$, $\\exp(-1) \\approx 0.368$, $\\exp(0) = 1$ -\n",
    "Probabilities: $[0.090, 0.245, 0.665]$\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "When computing softmax, subtract the maximum logit from all logits\n",
    "before exponentiating to avoid overflow:\n",
    "\n",
    "$$\\mathrm{softmax}(t)_i = \\frac{\\exp(t_i - \\max_j t_j)}{\\sum_{j=1}^k \\exp(t_j - \\max_j t_j)}$$\n",
    "\n",
    "This is mathematically equivalent but numerically stable.\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "Labels should be integers $1, \\ldots, k$ (or $0, \\ldots, k-1$ depending\n",
    "on convention). Common approaches: - **One-hot encoding:** Convert to\n",
    "binary vectors - **Integer encoding:** Use class indices directly -\n",
    "**Ordinal encoding:** For ordered classes\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Most ML libraries have built-in softmax and cross-entropy loss\n",
    "functions: - **NumPy:** `np.softmax()`, `np.log_softmax()` -\n",
    "**PyTorch:** `torch.softmax()`, `torch.nn.CrossEntropyLoss()` -\n",
    "**TensorFlow:** `tf.nn.softmax()`,\n",
    "`tf.keras.losses.SparseCategoricalCrossentropy()` - **scikit-learn:**\n",
    "`LogisticRegression(multi_class='multinomial')`\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, add regularization terms: - **L2\n",
    "regularization:** $\\lambda \\sum_{i=1}^k \\|\\theta_i\\|_2^2$ - **L1\n",
    "regularization:** $\\lambda \\sum_{i=1}^k \\|\\theta_i\\|_1$ - **Dropout:**\n",
    "Randomly zero some inputs during training\n",
    "\n",
    "## Gradient Derivation and Intuition\n",
    "\n",
    "### Gradient of Cross-Entropy Loss\n",
    "\n",
    "The cross-entropy loss has a simple and elegant gradient. For a single\n",
    "example:\n",
    "\n",
    "$$\\frac{\\partial \\ell_{ce}(t, y)}{\\partial t_i} = \\phi_i - 1\\{y = i\\}$$\n",
    "\n",
    "where $1\\{y = i\\}$ is the indicator function (1 if $y = i$, 0\n",
    "otherwise).\n",
    "\n",
    "#### Intuitive Understanding\n",
    "\n",
    "- **$\\phi_i - 1\\{y = i\\}$:** Difference between predicted and true\n",
    "  probability\n",
    "- **Positive gradient:** Predicted probability too high, decrease it\n",
    "- **Negative gradient:** Predicted probability too low, increase it\n",
    "- **Zero gradient:** Perfect prediction\n",
    "\n",
    "#### Vectorized Form\n",
    "\n",
    "In vectorized form: $\\nabla_t \\ell_{ce}(t, y) = \\phi - e_y$ where $e_y$\n",
    "is the one-hot encoding of $y$.\n",
    "\n",
    "### Gradient for Model Parameters\n",
    "\n",
    "For the model parameters $\\theta_i$:\n",
    "\n",
    "$$\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} = \\sum_{j=1}^n (\\phi_i^{(j)} - 1\\{y^{(j)} = i\\}) x^{(j)}$$\n",
    "\n",
    "This form is efficient to compute and is the basis for gradient descent\n",
    "and backpropagation in neural networks.\n",
    "\n",
    "### Gradient Descent Update\n",
    "\n",
    "The parameter update rule is:\n",
    "\n",
    "$$\\theta_i := \\theta_i - \\alpha \\sum_{j=1}^n (\\phi_i^{(j)} - 1\\{y^{(j)} = i\\}) x^{(j)}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "## Applications and Extensions\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Softmax is used as the final layer in multi-class classification\n",
    "networks: 1. **Hidden layers:** Learn feature representations 2.\n",
    "**Output layer:** Linear transformation + softmax 3. **Training:**\n",
    "Backpropagate gradients through the network\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "- **MNIST:** 10-class digit recognition\n",
    "- **CIFAR-10:** 10-class image classification\n",
    "- **ImageNet:** 1000-class image classification\n",
    "- **Object detection:** Multi-class with bounding boxes\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "- **Text classification:** Topic classification, sentiment analysis\n",
    "- **Part-of-speech tagging:** Grammatical role classification\n",
    "- **Named entity recognition:** Person, organization, location, etc.\n",
    "- **Language modeling:** Next word prediction\n",
    "\n",
    "### Medical Applications\n",
    "\n",
    "- **Disease diagnosis:** Multiple possible conditions\n",
    "- **Drug discovery:** Compound classification\n",
    "- **Medical imaging:** Tissue type classification\n",
    "- **Patient stratification:** Risk group classification\n",
    "\n",
    "### Extensions and Variants\n",
    "\n",
    "#### Hierarchical Softmax\n",
    "\n",
    "For large numbers of classes ($k \\gg 100$), hierarchical softmax\n",
    "organizes classes in a tree structure: - **Advantages:** Faster training\n",
    "and inference - **Disadvantages:** Requires hierarchical structure -\n",
    "**Applications:** Large vocabulary language models\n",
    "\n",
    "#### Label Smoothing\n",
    "\n",
    "Replace hard targets with soft targets to improve generalization:\n",
    "$$y_i' = (1 - \\epsilon) \\cdot 1\\{y = i\\} + \\frac{\\epsilon}{k}$$\n",
    "\n",
    "where $\\epsilon$ is a small constant (e.g., 0.1).\n",
    "\n",
    "#### Multi-Label Classification\n",
    "\n",
    "When an example can belong to multiple classes simultaneously: -\n",
    "**Approach:** Use sigmoid activation for each class - **Loss:** Binary\n",
    "cross-entropy for each class - **Applications:** Tag prediction,\n",
    "multi-disease diagnosis\n",
    "\n",
    "#### Ordinal Classification\n",
    "\n",
    "When classes have a natural ordering: - **Approach:** Model cumulative\n",
    "probabilities - **Loss:** Ordinal regression loss - **Applications:**\n",
    "Rating prediction, severity assessment\n",
    "\n",
    "## Comparison with Other Approaches\n",
    "\n",
    "### One-vs-Rest (OvR)\n",
    "\n",
    "Train $k$ binary classifiers, one for each class: - **Advantages:**\n",
    "Simple, can use any binary classifier - **Disadvantages:** Imbalanced\n",
    "training sets, no probability calibration - **When to use:** Small\n",
    "number of classes, existing binary classifiers\n",
    "\n",
    "### One-vs-One (OvO)\n",
    "\n",
    "Train $\\binom{k}{2}$ binary classifiers for each pair of classes: -\n",
    "**Advantages:** Balanced training sets, can handle non-linear\n",
    "boundaries - **Disadvantages:** More classifiers, complex prediction -\n",
    "**When to use:** Small number of classes, non-linear boundaries\n",
    "\n",
    "### Softmax vs. Other Methods\n",
    "\n",
    "| Method | Advantages | Disadvantages |\n",
    "|-----------------|-------------------------|-------------------------------|\n",
    "| **Softmax** | Probabilistic, efficient, convex | Linear boundaries only |\n",
    "| **OvR** | Simple, flexible | Imbalanced, no calibration |\n",
    "| **OvO** | Balanced, non-linear | Many classifiers |\n",
    "| **Decision Trees** | Non-linear, interpretable | No probabilities |\n",
    "| **Random Forest** | Robust, feature importance | Black box |\n",
    "| **SVM** | Non-linear (with kernels) | No probabilities |\n",
    "\n",
    "## Summary\n",
    "\n",
    "Multi-class classification with softmax and cross-entropy is a\n",
    "foundational technique in machine learning, generalizing logistic\n",
    "regression to multiple classes. Its mathematical simplicity,\n",
    "interpretability, and efficient gradient make it the default choice for\n",
    "many practical problems.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1.  **Softmax Function:** Smooth, differentiable generalization of\n",
    "    sigmoid\n",
    "2.  **Cross-Entropy Loss:** Natural loss function for probability\n",
    "    estimation\n",
    "3.  **Linear Decision Boundaries:** Between any pair of classes\n",
    "4.  **Numerical Stability:** Always subtract max logit before\n",
    "    exponentiating\n",
    "5.  **Efficient Gradients:** Simple, interpretable gradient expressions\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "For more advanced topics, see: - **Neural networks:** Multi-layer\n",
    "architectures - **Kernel methods:** Non-linear feature spaces -\n",
    "**Ensemble methods:** Combining multiple classifiers - **Calibration:**\n",
    "Improving probability estimates - **Active learning:** Selecting\n",
    "informative examples\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "> **Note:** There are some ambiguities in naming conventions. Some\n",
    "> people call the cross-entropy loss the function that maps the\n",
    "> probability vector (the $\\phi$ in our language) and label $y$ to the\n",
    "> final real number, and call our version of cross-entropy loss\n",
    "> softmax-cross-entropy loss. We choose our current naming convention\n",
    "> because it’s consistent with the naming of most modern deep learning\n",
    "> libraries such as PyTorch and Jax.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Previous: [Perceptron Algorithm](02_perceptron.md)** - Learn about the\n",
    "perceptron learning algorithm and its relationship to linear\n",
    "classification.\n",
    "\n",
    "**Next: [Newton’s Method](04_newtons_method.md)** - Explore second-order\n",
    "optimization methods for faster convergence in logistic regression.\n",
    "\n",
    "## From Classification Models to Advanced Optimization\n",
    "\n",
    "We’ve now built a comprehensive understanding of classification\n",
    "problems, from binary classification with logistic regression and\n",
    "perceptron algorithms to multi-class classification with softmax\n",
    "regression. These models provide powerful tools for making predictions,\n",
    "but they all rely on optimization algorithms to find the best\n",
    "parameters.\n",
    "\n",
    "So far, we’ve used **gradient ascent** (or gradient descent on the\n",
    "negative log-likelihood) to optimize our classification models. While\n",
    "gradient methods are simple and effective, they have limitations: they\n",
    "may require many iterations to converge, they’re sensitive to the\n",
    "learning rate choice, and they don’t take advantage of the curvature\n",
    "information available in our models.\n",
    "\n",
    "This motivates our exploration of **Newton’s method**, a second-order\n",
    "optimization technique that uses both gradient and curvature information\n",
    "to make more informed parameter updates. Newton’s method can converge\n",
    "much faster than gradient methods, especially for well-behaved functions\n",
    "like the logistic regression log-likelihood.\n",
    "\n",
    "The transition from first-order to second-order optimization represents\n",
    "a natural progression in our understanding of machine learning\n",
    "optimization - from simple gradient methods to more sophisticated\n",
    "techniques that leverage additional mathematical structure in our\n",
    "problems."
   ],
   "id": "f836e7b5-5b68-4ab7-925d-4a3a71abd4b2"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
