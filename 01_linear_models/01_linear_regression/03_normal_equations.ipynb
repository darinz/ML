{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 The normal equations\n",
    "\n",
    "Gradient descent gives one way of minimizing $J$. However, gradient\n",
    "descent requires iterative updates and careful tuning of the learning\n",
    "rate. In contrast, the normal equations approach allows us to directly\n",
    "solve for the optimal parameters in one step, provided the problem is\n",
    "well-posed and the necessary matrix inverses exist. This is especially\n",
    "useful for linear regression problems where the cost function is\n",
    "quadratic and differentiable. In this method, we will minimize $J$ by\n",
    "explicitly taking its derivatives with respect to the $\\theta_j$’s, and\n",
    "setting them to zero. To enable us to do this without having to write\n",
    "reams of algebra and pages full of matrices of derivatives, let’s\n",
    "introduce some notation for doing calculus with matrices.\n",
    "\n",
    "## Why Normal Equations?\n",
    "\n",
    "Before diving into the mathematics, let’s understand why we might prefer\n",
    "normal equations over gradient descent:\n",
    "\n",
    "**Advantages of Normal Equations:** 1. **Exact solution**: No need for\n",
    "iterative optimization 2. **No hyperparameters**: No learning rate to\n",
    "tune 3. **Guaranteed convergence**: Always finds the global minimum (if\n",
    "it exists) 4. **Theoretical insight**: Provides understanding of the\n",
    "optimal solution structure\n",
    "\n",
    "**Disadvantages of Normal Equations:** 1. **Computational cost**:\n",
    "$O(n^3)$ for matrix inversion vs $O(n^2)$ per iteration for gradient\n",
    "descent 2. **Memory usage**: Requires storing $X^T X$ matrix 3.\n",
    "**Numerical instability**: Matrix inversion can be numerically unstable\n",
    "4. **Non-invertible matrices**: Fails when $X^T X$ is singular\n",
    "\n",
    "**When to use each method:** - **Normal equations**: Small to medium\n",
    "datasets (\\< 10,000 examples), when you need exact solution - **Gradient\n",
    "descent**: Large datasets, when approximate solution is acceptable, or\n",
    "when $X^T X$ is singular\n",
    "\n",
    "## 1.2.1 Matrix derivatives\n",
    "\n",
    "When working with functions that take matrices as inputs, we generalize\n",
    "the concept of derivatives to matrices. For a function\n",
    "$f : \\mathbb{R}^{n \\times d} \\mapsto \\mathbb{R}$ mapping from $n$-by-$d$\n",
    "matrices to the real numbers, we define the derivative of $f$ with\n",
    "respect to $A$ to be:\n",
    "\n",
    "$$\\nabla_A f(A) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1d}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial A_{n1}} & \\cdots & \\frac{\\partial f}{\\partial A_{nd}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Understanding Matrix Derivatives\n",
    "\n",
    "The gradient of a scalar-valued function with respect to a matrix is\n",
    "itself a matrix, where each entry is the partial derivative of the\n",
    "function with respect to the corresponding entry in the input matrix.\n",
    "This allows us to perform calculus operations in a compact, vectorized\n",
    "form, which is essential for efficient computation in machine learning.\n",
    "\n",
    "**Key properties:** 1. **Linearity**:\n",
    "$\\nabla_A (f(A) + g(A)) = \\nabla_A f(A) + \\nabla_A g(A)$ 2. **Chain\n",
    "rule**: $\\nabla_A f(g(A)) = \\nabla_{g(A)} f(g(A)) \\cdot \\nabla_A g(A)$\n",
    "3. **Transpose rule**: $\\nabla_A f(A^T) = (\\nabla_{A^T} f(A^T))^T$\n",
    "\n",
    "### Example: Matrix Derivative Computation\n",
    "\n",
    "For example, suppose\n",
    "\n",
    "$$A = \n",
    "\\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ \n",
    "A_{21} & A_{22} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "is a 2-by-2 matrix, and the function\n",
    "$f : \\mathbb{R}^{2 \\times 2} \\mapsto \\mathbb{R}$ is given by\n",
    "\n",
    "$$f(A) = \\frac{3}{2}A_{11} + 5A_{12}^2 + A_{21}A_{22}.$$\n",
    "\n",
    "Here, $A_{ij}$ denotes the $(i, j)$ entry of the matrix $A$. We then\n",
    "have\n",
    "\n",
    "$$\\nabla_A f(A) = \\begin{bmatrix}\n",
    "\\frac{3}{2} & 10A_{12} \\\\\n",
    "A_{22} & A_{21}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "**Step-by-step computation:** 1.\n",
    "$\\frac{\\partial f}{\\partial A_{11}} = \\frac{3}{2}$ (derivative of\n",
    "$\\frac{3}{2}A_{11}$) 2. $\\frac{\\partial f}{\\partial A_{12}} = 10A_{12}$\n",
    "(derivative of $5A_{12}^2$) 3.\n",
    "$\\frac{\\partial f}{\\partial A_{21}} = A_{22}$ (derivative of\n",
    "$A_{21}A_{22}$ with respect to $A_{21}$) 4.\n",
    "$\\frac{\\partial f}{\\partial A_{22}} = A_{21}$ (derivative of\n",
    "$A_{21}A_{22}$ with respect to $A_{22}$)\n",
    "\n",
    "In this example, we see how to compute the gradient of a function with\n",
    "respect to a matrix. Each entry in the resulting gradient matrix is\n",
    "obtained by differentiating the function with respect to the\n",
    "corresponding entry in $A$. This process is analogous to taking partial\n",
    "derivatives with respect to each variable in multivariable calculus, but\n",
    "extended to matrices.\n",
    "\n",
    "### Important Matrix Calculus Rules\n",
    "\n",
    "For our derivation of the normal equations, we’ll need these key rules:\n",
    "\n",
    "1.  **Linear term**: $\\nabla_x (a^T x) = a$\n",
    "2.  **Quadratic term**: $\\nabla_x (x^T A x) = 2A x$ (for symmetric\n",
    "    matrix $A$)\n",
    "3.  **Transpose property**: $(A^T)^T = A$\n",
    "4.  **Matrix multiplication**: $(AB)^T = B^T A^T$\n",
    "\n",
    "These rules will allow us to efficiently compute the gradient of our\n",
    "cost function.\n",
    "\n",
    "## 1.2.2 Least squares revisited\n",
    "\n",
    "Armed with the tools of matrix derivatives, let us now proceed to find\n",
    "in closed-form the value of $\\theta$ that minimizes $J(\\theta)$. We\n",
    "begin by re-writing $J$ in matrix-vectorial notation. The design matrix\n",
    "$X$ is a convenient way to represent all the input features of your\n",
    "training data in a single matrix. Each row corresponds to one training\n",
    "example, and each column corresponds to a feature (including the\n",
    "intercept term if present). This matrix formulation allows us to express\n",
    "the entire dataset and the linear model compactly, making it easier to\n",
    "apply linear algebra techniques.\n",
    "\n",
    "### The Design Matrix\n",
    "\n",
    "Given a training set, define the **design matrix** $X$ to be the\n",
    "`n-by-d` matrix (actually `n-by-(d+1)`, if we include the intercept\n",
    "term) that contains the training examples’ input values in its rows:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "--- (x^{(1)})^T --- \\\\\n",
    "--- (x^{(2)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (x^{(n)})^T ---\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "**Understanding the design matrix:** - **Rows**: Each row represents one\n",
    "training example - **Columns**: Each column represents one feature -\n",
    "**First column**: Usually all ones (intercept term) - **Dimensions**:\n",
    "$n \\times (d+1)$ where $n$ is number of examples, $d$ is number of\n",
    "features\n",
    "\n",
    "**Example**: For our housing dataset with living area and bedrooms:\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & 2104 & 3 \\\\\n",
    "1 & 1600 & 3 \\\\\n",
    "1 & 2400 & 3 \\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### The Target Vector\n",
    "\n",
    "Also, let $\\vec{y}$ be the $n$-dimensional vector containing all the\n",
    "target values from the training set:\n",
    "\n",
    "$$\\vec{y} = \\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "The vector $\\vec{y}$ stacks all the target values (labels) from the\n",
    "training set into a single column vector. This matches the structure of\n",
    "$X$, so that we can perform matrix operations between $X$ and $\\vec{y}$.\n",
    "\n",
    "**Example**: For our housing dataset: $$\\vec{y} = \\begin{bmatrix}\n",
    "400 \\\\\n",
    "330 \\\\\n",
    "369 \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Vectorized Predictions and Errors\n",
    "\n",
    "Now, since $h_\\theta(x^{(i)}) = (x^{(i)})^T \\theta$, we can easily\n",
    "verify that\n",
    "\n",
    "$$X\\theta - \\vec{y} = \\begin{bmatrix}\n",
    "(x^{(1)})^T \\theta \\\\\n",
    "\\vdots \\\\\n",
    "(x^{(n)})^T \\theta\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h_\\theta(x^{(1)}) - y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta(x^{(n)}) - y^{(n)}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "**Understanding this expression:** - $X\\theta$ computes all predictions\n",
    "at once using matrix multiplication - $X\\theta - \\vec{y}$ gives the\n",
    "vector of prediction errors (residuals) - This is much more efficient\n",
    "than computing each prediction individually\n",
    "\n",
    "The expression $X\\theta$ computes the predicted values for all training\n",
    "examples at once, using matrix multiplication. Subtracting $\\vec{y}$\n",
    "gives the vector of residuals (errors) for each example. This vectorized\n",
    "form is much more efficient than computing each prediction and error\n",
    "individually.\n",
    "\n",
    "### Vectorized Cost Function\n",
    "\n",
    "Thus, using the fact that for a vector $z$, we have that\n",
    "$z^T z = \\sum_i z_i^2$:\n",
    "\n",
    "$$\\frac{1}{2}(X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) = \\frac{1}{2} \\sum_{i=1}^n (h_\\theta(x^{(i)}) - y^{(i)})^2 = J(\\theta)$$\n",
    "\n",
    "**Understanding the vectorized form:** -\n",
    "$(X\\theta - \\vec{y})^T (X\\theta - \\vec{y})$ computes the dot product of\n",
    "the error vector with itself - This is equivalent to summing the squared\n",
    "errors: $\\sum_i (h_\\theta(x^{(i)}) - y^{(i)})^2$ - The factor\n",
    "$\\frac{1}{2}$ is included for mathematical convenience\n",
    "\n",
    "The cost function $J(\\theta)$ for linear regression is the mean squared\n",
    "error (up to a factor of $1/2$ for convenience in differentiation). The\n",
    "matrix form $\\frac{1}{2}(X\\theta - \\vec{y})^T (X\\theta - \\vec{y})$ is\n",
    "equivalent to summing the squared errors for all training examples, but\n",
    "is more compact and enables efficient computation and differentiation.\n",
    "\n",
    "### Computing the Gradient\n",
    "\n",
    "Finally, to minimize $J$, let’s find its derivatives with respect to\n",
    "$\\theta$. Hence,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "    &= \\nabla_\\theta \\frac{1}{2}(X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) \\\\\n",
    "    &= \\frac{1}{2} \\nabla_\\theta \\left( (X\\theta)^T X\\theta - (X\\theta)^T \\vec{y} - \\vec{y}^T X\\theta + \\vec{y}^T \\vec{y} \\right) \\\\\n",
    "    &= \\frac{1}{2} \\nabla_\\theta \\left( \\theta^T X^T X \\theta - \\vec{y}^T X\\theta - \\vec{y}^T X\\theta \\right) \\\\\n",
    "    &= \\frac{1}{2} \\nabla_\\theta \\left( \\theta^T X^T X \\theta - 2 (X^T \\vec{y})^T \\theta \\right) \\\\\n",
    "    &= \\frac{1}{2} \\left( 2 X^T X \\theta - 2 X^T \\vec{y} \\right) \\\\\n",
    "    &= X^T X \\theta - X^T \\vec{y}\n",
    "\\end{align*}$$\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "#### Step 1: Expand the quadratic form\n",
    "\n",
    "$(X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) = (X\\theta)^T X\\theta - (X\\theta)^T \\vec{y} - \\vec{y}^T X\\theta + \\vec{y}^T \\vec{y}$\n",
    "\n",
    "#### Step 2: Simplify using matrix properties\n",
    "\n",
    "- $(X\\theta)^T = \\theta^T X^T$\n",
    "- $(X\\theta)^T \\vec{y} = \\vec{y}^T X\\theta$ (since both are scalars)\n",
    "- $\\vec{y}^T \\vec{y}$ is constant with respect to $\\theta$\n",
    "\n",
    "#### Step 3: Apply matrix calculus rules\n",
    "\n",
    "- $\\nabla_\\theta (\\theta^T X^T X \\theta) = 2 X^T X \\theta$ (quadratic\n",
    "  term rule)\n",
    "- $\\nabla_\\theta ((X^T \\vec{y})^T \\theta) = X^T \\vec{y}$ (linear term\n",
    "  rule)\n",
    "\n",
    "#### Step 4: Simplify\n",
    "\n",
    "The $\\frac{1}{2}$ factor cancels the 2’s, giving us the final result.\n",
    "\n",
    "Here, we use properties of matrix calculus to differentiate the cost\n",
    "function with respect to $\\theta$. The key steps involve expanding the\n",
    "quadratic form, applying the rules for differentiating with respect to\n",
    "vectors and matrices, and simplifying. The result is a linear equation\n",
    "in $\\theta$.\n",
    "\n",
    "In the third step, we used the fact that $a^T b = b^T a$, and in the\n",
    "fifth step used the facts $\\nabla_x b^T x = b$ and\n",
    "$\\nabla_x x^T A x = 2A x$ for symmetric matrix $A$ (for more details,\n",
    "see Section 4.3 of “Linear Algebra Review and Reference”). To minimize\n",
    "$J$, we set its derivatives to zero, and obtain the **normal\n",
    "equations**:\n",
    "\n",
    "$$X^T X \\theta = X^T \\vec{y}$$\n",
    "\n",
    "### Understanding the Normal Equations\n",
    "\n",
    "Setting the gradient to zero gives us the condition for optimality. The\n",
    "resulting equation, called the normal equation, is a system of linear\n",
    "equations that can be solved directly for $\\theta$ (provided $X^T X$ is\n",
    "invertible). This is the closed-form solution for linear regression.\n",
    "\n",
    "**Geometric interpretation:** - $X^T X$ is the Gram matrix, which\n",
    "measures the correlations between features - $X^T \\vec{y}$ is the\n",
    "correlation between features and target - The normal equations say:\n",
    "“Find $\\theta$ such that the predicted correlations match the actual\n",
    "correlations”\n",
    "\n",
    "**When does this solution exist?** - When $X^T X$ is invertible (i.e.,\n",
    "when $X$ has full column rank) - This means no feature is a perfect\n",
    "linear combination of other features - If $X^T X$ is singular, we need\n",
    "regularization or use gradient descent\n",
    "\n",
    "### The Closed-Form Solution\n",
    "\n",
    "Thus, the value of $\\theta$ that minimizes $J(\\theta)$ is given in\n",
    "closed form by the equation\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T \\vec{y}$$\n",
    "\n",
    "**Understanding this formula:** - $(X^T X)^{-1}$ is the inverse of the\n",
    "Gram matrix - $X^T \\vec{y}$ is the correlation between features and\n",
    "target - The product gives us the optimal parameters\n",
    "\n",
    "This formula gives the optimal parameters for linear regression in one\n",
    "step. It is derived from the normal equations and uses the inverse of\n",
    "$X^T X$. In practice, this approach is efficient for small to\n",
    "medium-sized datasets, but for very large datasets or when $X^T X$ is\n",
    "not invertible, iterative methods like gradient descent or\n",
    "regularization techniques are preferred.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "**Time complexity:** - Matrix multiplication $X^T X$: $O(nd^2)$ - Matrix\n",
    "inversion $(X^T X)^{-1}$: $O(d^3)$ - Final multiplication: $O(d^2)$ -\n",
    "**Total**: $O(nd^2 + d^3)$\n",
    "\n",
    "**Space complexity:** - Storing $X^T X$: $O(d^2)$ - Storing\n",
    "$(X^T X)^{-1}$: $O(d^2)$ - **Total**: $O(d^2)$\n",
    "\n",
    "**Comparison with gradient descent:** - **Normal equations**:\n",
    "$O(nd^2 + d^3)$ one-time cost - **Gradient descent**: $O(nd)$ per\n",
    "iteration, but many iterations needed\n",
    "\n",
    "### Numerical Stability Considerations\n",
    "\n",
    "**Potential issues:** 1. **Singular matrices**: $X^T X$ may not be\n",
    "invertible 2. **Ill-conditioned matrices**: Small changes in data cause\n",
    "large changes in solution 3. **Numerical precision**: Matrix inversion\n",
    "can amplify roundoff errors\n",
    "\n",
    "**Solutions:** 1. **Regularization**: Add $\\lambda I$ to $X^T X$ before\n",
    "inverting 2. **QR decomposition**: More numerically stable than direct\n",
    "inversion 3. **SVD decomposition**: Handles singular matrices gracefully\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "**Direct implementation:**\n",
    "\n",
    "``` python\n",
    "theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "```\n",
    "\n",
    "**More stable implementation:**\n",
    "\n",
    "``` python\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "```\n",
    "\n",
    "**With regularization:**\n",
    "\n",
    "``` python\n",
    "lambda_reg = 0.01\n",
    "theta = np.linalg.solve(X.T @ X + lambda_reg * np.eye(X.shape[1]), X.T @ y)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "The normal equations provide a beautiful closed-form solution to linear\n",
    "regression. They show us that:\n",
    "\n",
    "1.  **The optimal solution exists** when the features are linearly\n",
    "    independent\n",
    "2.  **The solution can be computed directly** without iteration\n",
    "3.  **The solution has a clear geometric interpretation** in terms of\n",
    "    correlations\n",
    "4.  **The method is efficient** for small to medium datasets\n",
    "\n",
    "However, they also have limitations that make gradient descent\n",
    "preferable in many practical scenarios, especially with large datasets\n",
    "or when numerical stability is a concern.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Previous: [LMS Algorithm](02_lms_algorithm.md)** - Learn about\n",
    "gradient descent and the LMS algorithm for optimizing the cost function.\n",
    "\n",
    "**Next: [Probabilistic\n",
    "Interpretation](04_probabilistic_interpretation.md)** - Understand the\n",
    "probabilistic foundations of linear regression and maximum likelihood\n",
    "estimation."
   ],
   "id": "82b5606e-0ec7-45ee-8930-307a228d5d11"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
