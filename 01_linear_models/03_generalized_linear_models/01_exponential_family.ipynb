{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exponential Family: Foundation of Generalized Linear Models\n",
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "So far, we’ve seen a regression example, and a classification example.\n",
    "In the regression example, we had\n",
    "$y|x; \\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$, and in the classification\n",
    "one, $y|x; \\theta \\sim \\text{Bernoulli}(\\phi)$, for some appropriate\n",
    "definitions of $\\mu$ and $\\phi$ as functions of $x$ and $\\theta$.\n",
    "\n",
    "**Key Insight**: These seemingly different models share a deep\n",
    "mathematical connection through the **exponential family** of\n",
    "distributions. This unified framework allows us to understand both\n",
    "regression and classification as special cases of a broader family of\n",
    "models, called **Generalized Linear Models (GLMs)**.\n",
    "\n",
    "**Why This Matters**: The exponential family provides a systematic way\n",
    "to: - Unify diverse probability distributions under one mathematical\n",
    "framework - Derive consistent learning algorithms for different types of\n",
    "data - Understand the relationships between different statistical\n",
    "models - Develop new models for novel data types\n",
    "\n",
    "## 3.1 The Exponential Family: Mathematical Foundation\n",
    "\n",
    "### Definition and Structure\n",
    "\n",
    "To work our way up to GLMs, we will begin by defining exponential family\n",
    "distributions. We say that a class of distributions is in the\n",
    "**exponential family** if it can be written in the form:\n",
    "\n",
    "$$p(y; \\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta)) \\tag{3.1}$$\n",
    "\n",
    "### Understanding Each Component\n",
    "\n",
    "Let’s break down this seemingly complex equation into its intuitive\n",
    "components:\n",
    "\n",
    "#### 1. **Natural Parameter** $\\eta$\n",
    "\n",
    "- **What it is**: The parameter that appears linearly in the exponential\n",
    "  term\n",
    "- **Why “natural”**: It’s the parameter that makes the exponential\n",
    "  family mathematically elegant\n",
    "- **Role**: Controls the shape and location of the distribution\n",
    "- **Example**: For Bernoulli, $\\eta = \\log(\\phi/(1-\\phi))$ (the\n",
    "  log-odds)\n",
    "\n",
    "#### 2. **Sufficient Statistic** $T(y)$\n",
    "\n",
    "- **What it is**: A function of the data that captures all relevant\n",
    "  information\n",
    "- **Why “sufficient”**: Contains all the information needed to estimate\n",
    "  the parameter\n",
    "- **Role**: Transforms the data into the form needed for the exponential\n",
    "  family\n",
    "- **Example**: Often $T(y) = y$ (the identity function)\n",
    "\n",
    "#### 3. **Log Partition Function** $a(\\eta)$\n",
    "\n",
    "- **What it is**: The normalization constant that ensures the\n",
    "  distribution sums/integrates to 1\n",
    "- **Why “log partition”**: It’s the logarithm of the partition function\n",
    "  from statistical physics\n",
    "- **Role**: Makes the distribution a valid probability distribution\n",
    "- **Mathematical role**: $e^{-a(\\eta)}$ is the normalization constant\n",
    "\n",
    "#### 4. **Base Measure** $b(y)$\n",
    "\n",
    "- **What it is**: A function that depends only on the data, not the\n",
    "  parameters\n",
    "- **Role**: Provides the basic structure of the distribution\n",
    "- **Example**: For Gaussian, $b(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-y^2/2)$\n",
    "\n",
    "### Why This Form is Powerful\n",
    "\n",
    "The exponential family form provides several key advantages:\n",
    "\n",
    "1.  **Unified Framework**: Many distributions can be written in this\n",
    "    form\n",
    "2.  **Mathematical Tractability**: Derivatives and expectations are\n",
    "    often simple\n",
    "3.  **Statistical Properties**: Well-understood properties for\n",
    "    estimation and inference\n",
    "4.  **Computational Efficiency**: Algorithms can be written generically\n",
    "\n",
    "### The Normalization Constant\n",
    "\n",
    "The quantity $e^{-a(\\eta)}$ plays a crucial role as the **normalization\n",
    "constant**. It ensures that:\n",
    "\n",
    "$$\\int p(y; \\eta) dy = 1 \\quad \\text{(for continuous distributions)}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\sum_y p(y; \\eta) = 1 \\quad \\text{(for discrete distributions)}$$\n",
    "\n",
    "This is why $a(\\eta)$ is called the log partition function - it’s the\n",
    "logarithm of the integral/sum that normalizes the distribution.\n",
    "\n",
    "## 3.2 Examples: Bernoulli and Gaussian Distributions\n",
    "\n",
    "### The Bernoulli Distribution\n",
    "\n",
    "The Bernoulli distribution is fundamental for binary classification\n",
    "problems. Let’s derive it step-by-step.\n",
    "\n",
    "#### Step 1: Standard Form\n",
    "\n",
    "The Bernoulli distribution with mean $\\phi$ specifies: -\n",
    "$p(y = 1; \\phi) = \\phi$ - $p(y = 0; \\phi) = 1 - \\phi$ - $y \\in \\{0, 1\\}$\n",
    "\n",
    "#### Step 2: Algebraic Manipulation\n",
    "\n",
    "We can write this more compactly as:\n",
    "\n",
    "$$p(y; \\phi) = \\phi^y (1 - \\phi)^{1-y}$$\n",
    "\n",
    "#### Step 3: Taking Logarithms\n",
    "\n",
    "To get into exponential family form, we take the natural logarithm:\n",
    "\n",
    "$$\\log p(y; \\phi) = y \\log \\phi + (1-y) \\log(1-\\phi)$$\n",
    "\n",
    "#### Step 4: Rearranging Terms\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "$$\\log p(y; \\phi) = y \\log \\phi + \\log(1-\\phi) - y \\log(1-\\phi)$$\n",
    "\n",
    "$$\\log p(y; \\phi) = y \\left(\\log \\phi - \\log(1-\\phi)\\right) + \\log(1-\\phi)$$\n",
    "\n",
    "$$\\log p(y; \\phi) = y \\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\log(1-\\phi)$$\n",
    "\n",
    "#### Step 5: Exponential Family Form\n",
    "\n",
    "Now we can write the probability mass function as:\n",
    "\n",
    "$$p(y; \\phi) = \\exp\\left(y \\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\log(1-\\phi)\\right)$$\n",
    "\n",
    "$$p(y; \\phi) = \\exp\\left(y \\log\\left(\\frac{\\phi}{1-\\phi}\\right) - \\log\\left(\\frac{1}{1-\\phi}\\right)\\right)$$\n",
    "\n",
    "#### Step 6: Identifying Components\n",
    "\n",
    "Comparing with the exponential family form\n",
    "$p(y; \\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta))$, we identify:\n",
    "\n",
    "- **Natural parameter**: $\\eta = \\log\\left(\\frac{\\phi}{1-\\phi}\\right)$\n",
    "  (the log-odds)\n",
    "- **Sufficient statistic**: $T(y) = y$\n",
    "- **Log partition function**:\n",
    "  $a(\\eta) = \\log\\left(\\frac{1}{1-\\phi}\\right) = \\log(1 + e^{\\eta})$\n",
    "- **Base measure**: $b(y) = 1$\n",
    "\n",
    "#### The Sigmoid Connection\n",
    "\n",
    "**Key Insight**: If we solve for $\\phi$ in terms of $\\eta$:\n",
    "\n",
    "$$\\eta = \\log\\left(\\frac{\\phi}{1-\\phi}\\right)$$\n",
    "\n",
    "$$e^{\\eta} = \\frac{\\phi}{1-\\phi}$$\n",
    "\n",
    "$$e^{\\eta} = \\frac{\\phi}{1-\\phi}$$\n",
    "\n",
    "$$e^{\\eta}(1-\\phi) = \\phi$$\n",
    "\n",
    "$$e^{\\eta} - e^{\\eta}\\phi = \\phi$$\n",
    "\n",
    "$$e^{\\eta} = \\phi(1 + e^{\\eta})$$\n",
    "\n",
    "$$\\phi = \\frac{e^{\\eta}}{1 + e^{\\eta}} = \\frac{1}{1 + e^{-\\eta}}$$\n",
    "\n",
    "This is the **sigmoid function**! This connection explains why logistic\n",
    "regression uses the sigmoid function - it’s the natural response\n",
    "function for the Bernoulli distribution.\n",
    "\n",
    "### The Gaussian Distribution\n",
    "\n",
    "The Gaussian (normal) distribution is fundamental for regression\n",
    "problems. Let’s derive it step-by-step.\n",
    "\n",
    "#### Step 1: Standard Form\n",
    "\n",
    "The Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ is:\n",
    "\n",
    "$$p(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)$$\n",
    "\n",
    "#### Step 2: Simplification\n",
    "\n",
    "For GLMs, we typically fix $\\sigma^2 = 1$ (this doesn’t affect the\n",
    "learning algorithm). This gives:\n",
    "\n",
    "$$p(y; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(y-\\mu)^2\\right)$$\n",
    "\n",
    "#### Step 3: Expanding the Square\n",
    "\n",
    "We expand $(y-\\mu)^2 = y^2 - 2\\mu y + \\mu^2$:\n",
    "\n",
    "$$p(y; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(y^2 - 2\\mu y + \\mu^2)\\right)$$\n",
    "\n",
    "#### Step 4: Separating Terms\n",
    "\n",
    "We can separate the terms involving $\\mu$ from those involving $y$:\n",
    "\n",
    "$$p(y; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}y^2\\right) \\exp\\left(\\mu y - \\frac{1}{2}\\mu^2\\right)$$\n",
    "\n",
    "#### Step 5: Exponential Family Form\n",
    "\n",
    "This can be written as:\n",
    "\n",
    "$$p(y; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}y^2\\right) \\exp\\left(\\mu y - \\frac{1}{2}\\mu^2\\right)$$\n",
    "\n",
    "#### Step 6: Identifying Components\n",
    "\n",
    "Comparing with the exponential family form, we identify:\n",
    "\n",
    "- **Natural parameter**: $\\eta = \\mu$\n",
    "- **Sufficient statistic**: $T(y) = y$\n",
    "- **Log partition function**:\n",
    "  $a(\\eta) = \\frac{1}{2}\\mu^2 = \\frac{1}{2}\\eta^2$\n",
    "- **Base measure**:\n",
    "  $b(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}y^2\\right)$\n",
    "\n",
    "### Why These Examples Matter\n",
    "\n",
    "These two examples demonstrate the power of the exponential family:\n",
    "\n",
    "1.  **Bernoulli**: Shows how binary classification naturally leads to\n",
    "    the sigmoid function\n",
    "2.  **Gaussian**: Shows how regression naturally leads to linear\n",
    "    predictions\n",
    "\n",
    "Both follow the same mathematical structure, which allows us to develop\n",
    "unified algorithms.\n",
    "\n",
    "## 3.3 Properties of Exponential Family Distributions\n",
    "\n",
    "### Mathematical Properties\n",
    "\n",
    "Exponential family distributions have several important properties:\n",
    "\n",
    "#### 1. **Mean and Variance from Log Partition Function**\n",
    "\n",
    "The derivatives of $a(\\eta)$ give us important moments:\n",
    "\n",
    "$$\\frac{d}{d\\eta} a(\\eta) = \\mathbb{E}[T(y)]$$\n",
    "\n",
    "$$\\frac{d^2}{d\\eta^2} a(\\eta) = \\text{Var}[T(y)]$$\n",
    "\n",
    "#### 2. **Convexity**\n",
    "\n",
    "The log partition function $a(\\eta)$ is convex, which ensures that\n",
    "maximum likelihood estimation is well-behaved.\n",
    "\n",
    "#### 3. **Sufficiency**\n",
    "\n",
    "The sufficient statistic $T(y)$ contains all the information needed to\n",
    "estimate the parameter $\\eta$.\n",
    "\n",
    "### Computational Advantages\n",
    "\n",
    "1.  **Gradient Calculations**: Derivatives are often simple to compute\n",
    "2.  **Optimization**: The convexity of $a(\\eta)$ makes optimization\n",
    "    tractable\n",
    "3.  **Generalization**: Algorithms can be written once and applied to\n",
    "    many distributions\n",
    "\n",
    "## 3.4 Beyond Bernoulli and Gaussian\n",
    "\n",
    "The exponential family includes many other important distributions:\n",
    "\n",
    "### Discrete Distributions\n",
    "\n",
    "- **Multinomial**: For multi-class classification\n",
    "- **Poisson**: For count data (e.g., number of events)\n",
    "- **Negative Binomial**: For overdispersed count data\n",
    "\n",
    "### Continuous Distributions\n",
    "\n",
    "- **Gamma**: For positive continuous data (e.g., waiting times)\n",
    "- **Exponential**: Special case of Gamma for time-to-event data\n",
    "- **Beta**: For probabilities and proportions\n",
    "- **Dirichlet**: For probability vectors (generalization of Beta)\n",
    "\n",
    "### Why This Matters for GLMs\n",
    "\n",
    "Each distribution in the exponential family can be used as the response\n",
    "distribution in a GLM, allowing us to model: - Binary outcomes\n",
    "(Bernoulli) - Continuous outcomes (Gaussian) - Count outcomes\n",
    "(Poisson) - Positive continuous outcomes (Gamma) - And many more…\n",
    "\n",
    "## 3.5 Connection to Generalized Linear Models\n",
    "\n",
    "The exponential family provides the foundation for GLMs because:\n",
    "\n",
    "1.  **Unified Framework**: All exponential family distributions follow\n",
    "    the same mathematical structure\n",
    "2.  **Natural Parameters**: The natural parameter $\\eta$ can be modeled\n",
    "    as a linear function of features\n",
    "3.  **Response Functions**: Each distribution has a natural way to\n",
    "    convert $\\eta$ to the mean\n",
    "4.  **Estimation**: Maximum likelihood estimation follows a consistent\n",
    "    pattern\n",
    "\n",
    "In the next section, we’ll see how to use this foundation to construct\n",
    "GLMs for various prediction problems.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The exponential family provides a powerful mathematical framework that:\n",
    "\n",
    "- **Unifies** diverse probability distributions under one structure\n",
    "- **Simplifies** the development of learning algorithms\n",
    "- **Enables** the construction of GLMs for various data types\n",
    "- **Provides** theoretical guarantees for estimation and inference\n",
    "\n",
    "Understanding the exponential family is crucial for mastering GLMs and\n",
    "appreciating the deep connections between different statistical models.\n",
    "\n",
    "## From Mathematical Foundation to Systematic Construction\n",
    "\n",
    "We’ve now established the **exponential family** as the mathematical\n",
    "foundation that unifies diverse probability distributions under a single\n",
    "elegant framework. This foundation provides the theoretical backbone for\n",
    "understanding how seemingly different models - like linear regression\n",
    "and logistic regression - are actually special cases of a broader\n",
    "family.\n",
    "\n",
    "The exponential family gives us the mathematical tools we need: natural\n",
    "parameters, sufficient statistics, log partition functions, and the\n",
    "beautiful properties that make estimation and inference tractable. We’ve\n",
    "seen how the Bernoulli distribution naturally leads to the sigmoid\n",
    "function and how the Gaussian distribution leads to linear predictions.\n",
    "\n",
    "However, having this mathematical foundation is only the first step. The\n",
    "real power comes from **systematically constructing models** that\n",
    "leverage this foundation. How do we take the exponential family\n",
    "framework and turn it into a practical recipe for building models for\n",
    "any type of response variable?\n",
    "\n",
    "This motivates our next topic: **constructing Generalized Linear\n",
    "Models**. We’ll learn the three fundamental assumptions and the\n",
    "systematic four-step process that allows us to build GLMs for any\n",
    "prediction problem, from count data to binary outcomes to continuous\n",
    "responses.\n",
    "\n",
    "The transition from understanding the exponential family to applying it\n",
    "in GLM construction represents the bridge from mathematical theory to\n",
    "practical modeling - where elegant mathematics meets real-world\n",
    "problem-solving.\n",
    "\n",
    "## Further Reading and Advanced Resources\n",
    "\n",
    "For deeper theoretical understanding and advanced perspectives on\n",
    "exponential families, the `exponential_family/` directory contains\n",
    "comprehensive reference materials from leading institutions:\n",
    "\n",
    "### **Academic Reference Materials**\n",
    "\n",
    "- **MIT Lecture Notes**\n",
    "  (`the-exponential-family_MIT18_655S16_LecNote7.pdf`): Comprehensive\n",
    "  coverage of exponential families with rigorous mathematical treatment\n",
    "- **Princeton Lectures** (`exponential-families_princeton.pdf`,\n",
    "  `lecture11_princeton.pdf`): Clear explanations with practical\n",
    "  applications\n",
    "- **Berkeley Materials** (`exponential_family_chapter8.pdf`,\n",
    "  `the-exponential-family_chapter8_berkeley.pdf`): Advanced probability\n",
    "  theory perspective\n",
    "- **Columbia Lecture**\n",
    "  (`the-exponential-family_lecture12_columbia.pdf`): Focused treatment\n",
    "  of exponential family properties\n",
    "- **Purdue Materials** (`expfamily_purdue.pdf`): Comprehensive treatment\n",
    "  with detailed examples\n",
    "\n",
    "### **Recommended Study Path**\n",
    "\n",
    "1.  **Foundation**: Master the concepts in this document and practice\n",
    "    with `exponential_family_examples.py`\n",
    "2.  **Intermediate**: Study\n",
    "    `the-exponential-family_lecture12_columbia.pdf` for clear\n",
    "    explanations\n",
    "3.  **Advanced**: Dive into\n",
    "    `the-exponential-family_MIT18_655S16_LecNote7.pdf` for comprehensive\n",
    "    coverage\n",
    "4.  **Specialized**: Use institution-specific materials for particular\n",
    "    topics of interest\n",
    "\n",
    "These resources provide multiple perspectives on exponential families,\n",
    "from different teaching approaches to advanced theoretical treatments,\n",
    "complementing the practical implementation focus of this course.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Next: [Constructing GLMs](02_constructing_glm.md)** - Learn the\n",
    "systematic approach to building Generalized Linear Models for any\n",
    "prediction problem."
   ],
   "id": "5dd6399f-4bf2-4821-9053-4149e5ca2a9a"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
