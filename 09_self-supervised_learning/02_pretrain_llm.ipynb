{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e528c1e-bbd8-489b-a261-127e6a33b663",
   "metadata": {},
   "source": [
    "# 14.3 Pretrained large language models\n",
    "\n",
    "## The Big Picture: Why Large Language Models Matter\n",
    "\n",
    "**The Language Understanding Challenge:**\n",
    "Imagine trying to teach a computer to understand and generate human language. Language is incredibly complex - it has grammar, context, ambiguity, idioms, cultural references, and infinite possible combinations. Traditional approaches required hand-crafted rules and extensive labeled data for each specific task.\n",
    "\n",
    "**The Intuitive Analogy:**\n",
    "Think of the difference between:\n",
    "- **Traditional NLP**: Like teaching a computer specific rules for each language task (translation, summarization, question answering)\n",
    "- **Large Language Models**: Like teaching a computer to read and understand language the way humans do - by absorbing massive amounts of text and learning patterns naturally\n",
    "\n",
    "**Why Large Language Models Matter:**\n",
    "- **Universal language understanding**: One model can handle many different language tasks\n",
    "- **Massive knowledge base**: Learns from billions of documents and conversations\n",
    "- **Emergent capabilities**: Develops abilities not explicitly trained for\n",
    "- **Human-like reasoning**: Can understand context, follow instructions, and generate coherent text\n",
    "- **Democratization**: Makes advanced language AI accessible to more applications\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**From Task-Specific to General Language Intelligence:**\n",
    "- **Traditional approach**: Train separate models for each language task\n",
    "- **LLM approach**: Train one massive model that understands language broadly\n",
    "- **Adaptation**: Use the same model for translation, summarization, coding, reasoning, etc.\n",
    "\n",
    "**The Foundation Model Advantage:**\n",
    "- **Scale**: Larger models often perform better and have emergent capabilities\n",
    "- **Knowledge transfer**: What you learn about language helps with all language tasks\n",
    "- **Few-shot learning**: Can learn new tasks with just a few examples\n",
    "- **Zero-shot capabilities**: Can handle tasks it was never explicitly trained for\n",
    "\n",
    "Natural language processing is another area where pretraining models are particularly successful. In language problems, an example typically corresponds to a document or generally a sequence (or trunk) of words,[^5] denoted by $x = (x_1, \\cdots, x_T)$ where $T$ is the length of the document/sequence, $x_i \\in \\{1, \\cdots, V\\}$ are words in the document, and $V$ is the vocabulary size.[^6]\n",
    "\n",
    "**The Language Data Structure:**\n",
    "- **Sequence nature**: Language is inherently sequential (words come in order)\n",
    "- **Variable length**: Documents can be short (tweets) or long (books)\n",
    "- **Vocabulary size**: Large vocabularies (50K-100K+ words) create complexity\n",
    "- **Context dependency**: Meaning depends on surrounding words\n",
    "\n",
    "## From General Foundation Models to Language-Specific Applications\n",
    "\n",
    "We've now explored **self-supervised learning and foundation models** - a paradigm shift that enables learning from vast amounts of unlabeled data. We've seen how contrastive learning works by creating surrogate tasks from the data itself, how foundation models can be adapted to various downstream tasks, and how this approach addresses the fundamental data labeling bottleneck in machine learning.\n",
    "\n",
    "However, while the general principles of self-supervised learning apply across different data modalities, **natural language processing** presents unique challenges and opportunities that require specialized approaches. Language has its own structure, patterns, and characteristics that make it particularly well-suited for certain types of self-supervised learning.\n",
    "\n",
    "**The Language-Specific Challenges:**\n",
    "- **Sequential nature**: Words depend on previous words in complex ways\n",
    "- **Context sensitivity**: Same word can mean different things in different contexts\n",
    "- **Long-range dependencies**: Important information can be far apart in text\n",
    "- **Ambiguity**: Many words and phrases have multiple meanings\n",
    "- **Cultural knowledge**: Understanding requires world knowledge\n",
    "\n",
    "**The Language-Specific Opportunities:**\n",
    "- **Predictive structure**: Language has strong predictive patterns\n",
    "- **Self-supervised tasks**: Many natural tasks can be created from text itself\n",
    "- **Massive data**: Text data is abundant and easily accessible\n",
    "- **Transfer learning**: Language knowledge transfers well across tasks\n",
    "\n",
    "This motivates our exploration of **large language models (LLMs)** - specialized foundation models for text that leverage the sequential and contextual nature of language. We'll see how language modeling works through the chain rule of probability, how Transformer architectures process text, and how these models can generate coherent text and adapt to new tasks through prompting.\n",
    "\n",
    "**The LLM Revolution:**\n",
    "- **Scale**: Models with billions of parameters trained on massive text corpora\n",
    "- **Architecture**: Transformer architecture enables parallel processing and long-range attention\n",
    "- **Training**: Self-supervised learning on next-word prediction\n",
    "- **Capabilities**: Text generation, understanding, reasoning, and task adaptation\n",
    "\n",
    "The transition from general foundation models to language-specific applications represents the bridge from universal principles to domain expertise - taking our understanding of self-supervised learning and applying it to the rich, structured world of natural language.\n",
    "\n",
    "In this section, we'll explore how language models work, how they're trained on massive text corpora, and how they can be adapted to various language tasks through finetuning, zero-shot learning, and in-context learning.\n",
    "\n",
    "## Understanding Language Modeling\n",
    "\n",
    "### The Big Picture: What is Language Modeling?\n",
    "\n",
    "**The Core Problem:**\n",
    "Language modeling is about predicting what comes next in a sequence of words. Given some text, we want to predict the probability of the next word, or the probability of entire sequences.\n",
    "\n",
    "**The Intuitive Analogy:**\n",
    "- **Language modeling**: Like predicting what someone will say next in a conversation\n",
    "- **Probability estimation**: Like guessing how likely different responses are\n",
    "- **Context understanding**: Like using what was said before to make better predictions\n",
    "\n",
    "**Why Language Modeling Matters:**\n",
    "- **Foundation for all NLP**: Most language tasks can be framed as language modeling\n",
    "- **Natural self-supervised task**: No labels needed - just predict the next word\n",
    "- **Universal representation**: Learns general language understanding\n",
    "- **Scalable training**: Can use any text data for training\n",
    "\n",
    "### Introduction to Language Modeling\n",
    "\n",
    "Language modeling is one of the most fundamental tasks in natural language processing. At its core, it involves predicting the probability of sequences of words, which requires understanding the complex patterns and relationships in human language.\n",
    "\n",
    "**The Language Understanding Challenge:**\n",
    "- **Complexity**: Language has infinite possible combinations\n",
    "- **Context**: Meaning depends on surrounding words and world knowledge\n",
    "- **Ambiguity**: Same words can mean different things\n",
    "- **Structure**: Grammar, syntax, and semantic relationships\n",
    "\n",
    "**The Learning Approach:**\n",
    "- **Pattern recognition**: Learn statistical patterns from massive text data\n",
    "- **Context modeling**: Understand how words relate to each other\n",
    "- **Knowledge acquisition**: Absorb factual and conceptual knowledge\n",
    "- **Generalization**: Apply learned patterns to new situations\n",
    "\n",
    "### The Language Modeling Problem\n",
    "\n",
    "A language model is a probabilistic model representing the probability of a document, denoted by $p(x_1, \\cdots, x_T)$. This probability distribution is very complex because its support size is $V^T$â€”exponential in the length of the document. Instead of modeling the distribution of a document itself, we can apply the chain rule of conditional probability to decompose it as follows:\n",
    "\n",
    "$$\n",
    "p(x_1, \\cdots, x_T) = p(x_1) p(x_2|x_1) \\cdots p(x_T|x_1, \\cdots, x_{T-1}).\n",
    "$$\n",
    "\n",
    "**The Mathematical Challenge:**\n",
    "- **Direct modeling**: Would require estimating $V^T$ parameters\n",
    "- **Exponential growth**: Parameters grow exponentially with sequence length\n",
    "- **Intractability**: Impossible to estimate for realistic document lengths\n",
    "- **Solution**: Chain rule decomposition breaks the problem into manageable pieces\n",
    "\n",
    "Now the support size of each of the conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ is $V$.\n",
    "\n",
    "**The Chain Rule Advantage:**\n",
    "- **Linear growth**: Only $T \\times V$ parameters needed\n",
    "- **Conditional structure**: Each prediction depends only on previous words\n",
    "- **Tractable**: Much more manageable computational complexity\n",
    "- **Natural**: Matches how humans process language sequentially\n",
    "\n",
    "#### Understanding the Chain Rule Decomposition\n",
    "\n",
    "The chain rule decomposition is a fundamental concept in probability theory that allows us to break down complex joint distributions into simpler conditional distributions. Here's why this is crucial for language modeling:\n",
    "\n",
    "**The Problem with Direct Modeling:**\n",
    "- The vocabulary size $V$ is typically 50,000-100,000 words\n",
    "- Document length $T$ can be hundreds or thousands of words\n",
    "- Direct modeling would require estimating $V^T$ parameters\n",
    "- For a vocabulary of 50,000 and document length of 100, this is $50,000^{100}$ parameters!\n",
    "\n",
    "**The Exponential Explosion Analogy:**\n",
    "- **Vocabulary**: Like having 50,000 different building blocks\n",
    "- **Document**: Like building a structure with 100 blocks\n",
    "- **Direct modeling**: Like trying to memorize every possible structure\n",
    "- **Chain rule**: Like learning rules for adding one block at a time\n",
    "\n",
    "**The Solution with Chain Rule:**\n",
    "- Each conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ only needs $V$ parameters\n",
    "- Total parameters needed: $T \\times V$ (much more manageable)\n",
    "- Each prediction depends only on the previous words in the sequence\n",
    "\n",
    "**The Sequential Learning Intuition:**\n",
    "- **Step 1**: Learn to predict the first word $p(x_1)$\n",
    "- **Step 2**: Learn to predict the second word given the first $p(x_2|x_1)$\n",
    "- **Step 3**: Learn to predict the third word given the first two $p(x_3|x_1, x_2)$\n",
    "- **Continue**: Build up understanding word by word\n",
    "\n",
    "**Example:**\n",
    "Consider the sentence \"The cat sat on the mat\":\n",
    "- $p(\\text{The})$: Probability of starting with \"The\"\n",
    "- $p(\\text{cat}|\\text{The})$: Probability of \"cat\" given \"The\"\n",
    "- $p(\\text{sat}|\\text{The cat})$: Probability of \"sat\" given \"The cat\"\n",
    "- And so on...\n",
    "\n",
    "**The Context Building Process:**\n",
    "- **\"The\"**: No context, just word frequency\n",
    "- **\"The cat\"**: Context helps predict animal-related words\n",
    "- **\"The cat sat\"**: Context suggests action words\n",
    "- **\"The cat sat on\"**: Context suggests spatial relationships\n",
    "- **\"The cat sat on the\"**: Context suggests objects or surfaces\n",
    "\n",
    "### Parameterizing the Language Model\n",
    "\n",
    "We will model the conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ as a function of $x_1, \\ldots, x_{t-1}$ parameterized by some parameter $\\theta$.\n",
    "\n",
    "**The Parameterization Challenge:**\n",
    "- **Input**: Sequence of discrete word indices\n",
    "- **Output**: Probability distribution over vocabulary\n",
    "- **Function**: Must capture complex language patterns\n",
    "- **Learning**: Must be differentiable for gradient-based optimization\n",
    "\n",
    "A parameterized model takes in numerical inputs and therefore we first introduce embeddings or representations for the words. Let $e_i \\in \\mathbb{R}^d$ be the embedding of the word $i \\in \\{1, 2, \\cdots, V\\}$. We call $[e_1, \\cdots, e_V] \\in \\mathbb{R}^{d \\times V}$ the embedding matrix.\n",
    "\n",
    "#### Word Embeddings: From Discrete to Continuous\n",
    "\n",
    "**The Challenge:**\n",
    "- Words are discrete symbols (e.g., \"cat\", \"dog\", \"house\")\n",
    "- Neural networks work with continuous numerical inputs\n",
    "- We need a way to convert words to numbers\n",
    "\n",
    "**The Discrete vs. Continuous Problem:**\n",
    "- **Discrete words**: \"cat\", \"dog\", \"house\" - no natural numerical relationship\n",
    "- **Neural networks**: Expect continuous, differentiable inputs\n",
    "- **Solution**: Map each word to a continuous vector (embedding)\n",
    "- **Learning**: Embeddings are learned to capture semantic relationships\n",
    "\n",
    "**The Solution:**\n",
    "- Each word is assigned a unique integer ID (e.g., \"cat\" = 42, \"dog\" = 17)\n",
    "- Each word ID is mapped to a continuous vector (embedding)\n",
    "- These embeddings are learned during training\n",
    "\n",
    "**The Word-to-Vector Process:**\n",
    "1. **Vocabulary creation**: Assign unique IDs to all words\n",
    "2. **Embedding initialization**: Start with random vectors\n",
    "3. **Learning**: Update embeddings based on word usage patterns\n",
    "4. **Semantic capture**: Similar words get similar embeddings\n",
    "\n",
    "**Properties of Word Embeddings:**\n",
    "- **Dimensionality**: Typically 256-1024 dimensions\n",
    "- **Semantic Similarity**: Similar words have similar embeddings\n",
    "- **Learnable**: Embeddings are updated during training to capture word relationships\n",
    "- **Context-independent**: Each word has a fixed embedding (in basic models)\n",
    "\n",
    "**The Semantic Space Analogy:**\n",
    "- **Words**: Like points in a high-dimensional space\n",
    "- **Similar words**: Like points that are close together\n",
    "- **Related concepts**: Like clusters of nearby points\n",
    "- **Semantic relationships**: Like geometric relationships between points\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "Word IDs: [1, 2, 3, 4, 5]\n",
    "Embeddings: \n",
    "  e_1 (the) = [0.1, 0.2, 0.3, ...]\n",
    "  e_2 (cat) = [0.4, 0.1, 0.8, ...]\n",
    "  e_3 (sat) = [0.2, 0.9, 0.1, ...]\n",
    "  ...\n",
    "```\n",
    "\n",
    "**The Embedding Learning Process:**\n",
    "- **Initialization**: Random vectors for each word\n",
    "- **Context learning**: Words that appear in similar contexts get similar embeddings\n",
    "- **Semantic capture**: \"cat\" and \"dog\" become similar because they appear in similar contexts\n",
    "- **Relationship learning**: \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"\n",
    "\n",
    "## The Transformer Architecture\n",
    "\n",
    "The most commonly used model is the Transformer [Vaswani et al., 2017]. In this subsection, we will introduce the input-output interface of a Transformer, but treat the intermediate computation in the Transformer as a blackbox. We will cover more of the transformer architecture in [section 12](../12_llm/).\n",
    "\n",
    "### The Big Picture: Why Transformers Matter\n",
    "\n",
    "**The Sequential Processing Problem:**\n",
    "Traditional neural networks (RNNs, LSTMs) process text word by word sequentially. This creates several problems:\n",
    "- **Slow training**: Can't parallelize across sequence length\n",
    "- **Memory issues**: Hard to handle long sequences\n",
    "- **Gradient problems**: Information can get lost over long distances\n",
    "- **Limited context**: Hard to capture long-range dependencies\n",
    "\n",
    "**The Transformer Solution:**\n",
    "- **Parallel processing**: Can process entire sequences at once\n",
    "- **Attention mechanism**: Can directly attend to any position in the sequence\n",
    "- **Long-range dependencies**: Can capture relationships across long distances\n",
    "- **Scalable**: Performance improves with more data and larger models\n",
    "\n",
    "### Transformer Overview\n",
    "\n",
    "The Transformer is a neural network architecture that revolutionized natural language processing. Unlike previous models that processed sequences sequentially (like RNNs), Transformers can process entire sequences in parallel, making them much faster to train and more effective at capturing long-range dependencies.\n",
    "\n",
    "**The Parallel Processing Advantage:**\n",
    "- **RNNs**: Must process word 1, then word 2, then word 3...\n",
    "- **Transformers**: Can process all words simultaneously\n",
    "- **Speed**: Much faster training and inference\n",
    "- **Efficiency**: Better utilization of modern hardware (GPUs)\n",
    "\n",
    "**The Attention Revolution:**\n",
    "- **Traditional models**: Each word only sees previous words\n",
    "- **Transformers**: Each word can attend to any other word\n",
    "- **Context**: Full context available for every prediction\n",
    "- **Relationships**: Can capture complex relationships between distant words\n",
    "\n",
    "#### Key Components of the Transformer\n",
    "\n",
    "1. **Input Embeddings**: Convert word tokens to continuous vectors\n",
    "2. **Positional Encodings**: Add information about word positions in the sequence\n",
    "3. **Multi-Head Self-Attention**: Allow words to attend to all other words in the sequence\n",
    "4. **Feed-Forward Networks**: Process each position independently\n",
    "5. **Layer Normalization**: Stabilize training\n",
    "6. **Residual Connections**: Help with gradient flow\n",
    "\n",
    "**The Transformer Architecture Analogy:**\n",
    "- **Input Embeddings**: Like translating words into a common language\n",
    "- **Positional Encodings**: Like adding timestamps to know word order\n",
    "- **Self-Attention**: Like having a spotlight that can focus on any word\n",
    "- **Feed-Forward**: Like processing each word's meaning independently\n",
    "- **Layer Normalization**: Like keeping the signal strength consistent\n",
    "- **Residual Connections**: Like having shortcuts to preserve information\n",
    "\n",
    "### Input-Output Interface\n",
    "\n",
    "As shown in Figure 14.1, given a document $(x_1, \\cdots, x_T)$, we first translate the sequence of discrete variables into a sequence of corresponding word embeddings ($e_{x_1}, \\cdots, e_{x_T}$).\n",
    "\n",
    "<img src=\"./img/transformer_output.png\" width=\"400px\"/>\n",
    "\n",
    "**Figure 14.1 (description):** The inputs to the Transformer are the embeddings $e_{x_0}, e_{x_1}, \\ldots, e_{x_T}$ corresponding to the tokens $x_0, x_1, \\ldots, x_T$. The Transformer $f_\\theta(x)$ outputs a sequence of vectors $u_1, u_2, \\ldots, u_{T+1}$, each of which is used to predict the next token in the sequence.\n",
    "\n",
    "**The Input Processing Pipeline:**\n",
    "1. **Word tokens**: Discrete word IDs (e.g., [1, 42, 17, 8, 23])\n",
    "2. **Word embeddings**: Convert to continuous vectors\n",
    "3. **Positional encoding**: Add position information\n",
    "4. **Transformer processing**: Apply attention and feed-forward layers\n",
    "5. **Output logits**: Continuous vectors for next-word prediction\n",
    "\n",
    "We also introduce a fixed special token $x_0 = \\perp$ in the vocabulary with corresponding embedding $e_{x_0}$ to mark the beginning of a document. Then, the word embeddings are passed into a Transformer model, which takes in a sequence of vectors ($e_{x_0}, e_{x_1}, \\cdots, e_{x_T}$) and outputs a sequence of vectors ($u_1, u_2, \\cdots, u_{T+1}$), where $u_t \\in \\mathbb{R}^V$ will be interpreted as the logits for the probability distribution of the next word.\n",
    "\n",
    "#### Understanding the Special Token\n",
    "\n",
    "The special token $\\perp$ (often called `<BOS>` for \"beginning of sequence\") serves several important purposes:\n",
    "\n",
    "1. **Sequence Start Marker**: Indicates the beginning of a new sequence\n",
    "2. **Context for First Word**: Provides context for predicting the first actual word\n",
    "3. **Consistent Input Length**: Ensures all sequences have the same structure\n",
    "\n",
    "**The Special Token Analogy:**\n",
    "- **Purpose**: Like a \"start\" button that tells the model a new sequence is beginning\n",
    "- **Context**: Like having a blank slate before the first word\n",
    "- **Consistency**: Like always having the same starting point for every sequence\n",
    "- **Prediction**: Like having a reference point for predicting the first word\n",
    "\n",
    "**Why We Need It:**\n",
    "- **First word prediction**: Without it, the model has no context for predicting the first word\n",
    "- **Sequence boundaries**: Helps the model know when one sequence ends and another begins\n",
    "- **Training consistency**: Ensures all sequences have the same structure\n",
    "- **Generation**: Provides a starting point for text generation\n",
    "\n",
    "#### The Autoregressive Property\n",
    "\n",
    "Here we use the autoregressive version of the Transformer, which by design ensures $u_t$ only depends on $x_1, \\cdots, x_{t-1}$ (note that this property does not hold in masked language models [Devlin et al., 2019] where the losses are also different).\n",
    "\n",
    "**What is Autoregressive?**\n",
    "- Each output depends only on previous inputs\n",
    "- No information from future tokens is used\n",
    "- This is crucial for language generation tasks\n",
    "\n",
    "**The Autoregressive Intuition:**\n",
    "- **Causal structure**: Like predicting the next word in a sentence\n",
    "- **No cheating**: Can't use information from words that haven't been written yet\n",
    "- **Generation**: Perfect for text generation (predict one word at a time)\n",
    "- **Training**: Each position predicts the next word given previous words\n",
    "\n",
    "**Example:**\n",
    "For the sequence \"The cat sat\":\n",
    "- $u_1$ depends on $x_0$ (special token) only\n",
    "- $u_2$ depends on $x_0, x_1$ (\"The\")\n",
    "- $u_3$ depends on $x_0, x_1, x_2$ (\"The cat\")\n",
    "- $u_4$ depends on $x_0, x_1, x_2, x_3$ (\"The cat sat\")\n",
    "\n",
    "**The Prediction Process:**\n",
    "- **Position 1**: Predict word after start token\n",
    "- **Position 2**: Predict word after \"The\"\n",
    "- **Position 3**: Predict word after \"The cat\"\n",
    "- **Position 4**: Predict word after \"The cat sat\"\n",
    "\n",
    "We view the whole mapping from $x$'s to $u$'s as a blackbox in this subsection and call it a Transformer, denoted it by $f_\\theta$, where $\\theta$ include both the parameters in the Transformer and the input embeddings. We write $u_t = f_\\theta(x_0, x_1, \\ldots, x_{t-1})$ where $f_\\theta$ denotes the mapping from the input to the outputs.\n",
    "\n",
    "**The Blackbox Understanding:**\n",
    "- **Input**: Sequence of word embeddings with positional information\n",
    "- **Processing**: Complex attention and feed-forward computations\n",
    "- **Output**: Logits for next-word prediction at each position\n",
    "- **Parameters**: All weights in the Transformer and embeddings\n",
    "\n",
    "---\n",
    "\n",
    "**Next: [From Logits to Probabilities](02_pretrain_llm.md#from-logits-to-probabilities)** - Learn how to convert model outputs into probability distributions.\n",
    "\n",
    "## Introduction to Language Modeling\n",
    "\n",
    "Language modeling is one of the most fundamental tasks in natural language processing. At its core, it involves predicting the probability of sequences of words, which requires understanding the complex patterns and relationships in human language.\n",
    "\n",
    "### The Language Modeling Problem\n",
    "\n",
    "A language model is a probabilistic model representing the probability of a document, denoted by $p(x_1, \\cdots, x_T)$. This probability distribution is very complex because its support size is $V^T$â€”exponential in the length of the document. Instead of modeling the distribution of a document itself, we can apply the chain rule of conditional probability to decompose it as follows:\n",
    "\n",
    "$$\n",
    "p(x_1, \\cdots, x_T) = p(x_1) p(x_2|x_1) \\cdots p(x_T|x_1, \\cdots, x_{T-1}).\n",
    "$$\n",
    "\n",
    "Now the support size of each of the conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ is $V$.\n",
    "\n",
    "#### Understanding the Chain Rule Decomposition\n",
    "\n",
    "The chain rule decomposition is a fundamental concept in probability theory that allows us to break down complex joint distributions into simpler conditional distributions. Here's why this is crucial for language modeling:\n",
    "\n",
    "**The Problem with Direct Modeling:**\n",
    "- The vocabulary size $V$ is typically 50,000-100,000 words\n",
    "- Document length $T$ can be hundreds or thousands of words\n",
    "- Direct modeling would require estimating $V^T$ parameters\n",
    "- For a vocabulary of 50,000 and document length of 100, this is $50,000^{100}$ parameters!\n",
    "\n",
    "**The Solution with Chain Rule:**\n",
    "- Each conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ only needs $V$ parameters\n",
    "- Total parameters needed: $T \\times V$ (much more manageable)\n",
    "- Each prediction depends only on the previous words in the sequence\n",
    "\n",
    "**Example:**\n",
    "Consider the sentence \"The cat sat on the mat\":\n",
    "- $p(\\text{The})$: Probability of starting with \"The\"\n",
    "- $p(\\text{cat}|\\text{The})$: Probability of \"cat\" given \"The\"\n",
    "- $p(\\text{sat}|\\text{The cat})$: Probability of \"sat\" given \"The cat\"\n",
    "- And so on...\n",
    "\n",
    "### Parameterizing the Language Model\n",
    "\n",
    "We will model the conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ as a function of $x_1, \\ldots, x_{t-1}$ parameterized by some parameter $\\theta$.\n",
    "\n",
    "A parameterized model takes in numerical inputs and therefore we first introduce embeddings or representations for the words. Let $e_i \\in \\mathbb{R}^d$ be the embedding of the word $i \\in \\{1, 2, \\cdots, V\\}$. We call $[e_1, \\cdots, e_V] \\in \\mathbb{R}^{d \\times V}$ the embedding matrix.\n",
    "\n",
    "#### Word Embeddings: From Discrete to Continuous\n",
    "\n",
    "**The Challenge:**\n",
    "- Words are discrete symbols (e.g., \"cat\", \"dog\", \"house\")\n",
    "- Neural networks work with continuous numerical inputs\n",
    "- We need a way to convert words to numbers\n",
    "\n",
    "**The Solution:**\n",
    "- Each word is assigned a unique integer ID (e.g., \"cat\" = 42, \"dog\" = 17)\n",
    "- Each word ID is mapped to a continuous vector (embedding)\n",
    "- These embeddings are learned during training\n",
    "\n",
    "**Properties of Word Embeddings:**\n",
    "- **Dimensionality**: Typically 256-1024 dimensions\n",
    "- **Semantic Similarity**: Similar words have similar embeddings\n",
    "- **Learnable**: Embeddings are updated during training to capture word relationships\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "Word IDs: [1, 2, 3, 4, 5]\n",
    "Embeddings: \n",
    "  e_1 (the) = [0.1, 0.2, 0.3, ...]\n",
    "  e_2 (cat) = [0.4, 0.1, 0.8, ...]\n",
    "  e_3 (sat) = [0.2, 0.9, 0.1, ...]\n",
    "  ...\n",
    "```\n",
    "\n",
    "## The Transformer Architecture\n",
    "\n",
    "The most commonly used model is the Transformer [Vaswani et al., 2017]. In this subsection, we will introduce the input-output interface of a Transformer, but treat the intermediate computation in the Transformer as a blackbox. We will cover more of the transformer architecture in [section 12](../12_llm/).\n",
    "\n",
    "### Transformer Overview\n",
    "\n",
    "The Transformer is a neural network architecture that revolutionized natural language processing. Unlike previous models that processed sequences sequentially (like RNNs), Transformers can process entire sequences in parallel, making them much faster to train and more effective at capturing long-range dependencies.\n",
    "\n",
    "#### Key Components of the Transformer\n",
    "\n",
    "1. **Input Embeddings**: Convert word tokens to continuous vectors\n",
    "2. **Positional Encodings**: Add information about word positions in the sequence\n",
    "3. **Multi-Head Self-Attention**: Allow words to attend to all other words in the sequence\n",
    "4. **Feed-Forward Networks**: Process each position independently\n",
    "5. **Layer Normalization**: Stabilize training\n",
    "6. **Residual Connections**: Help with gradient flow\n",
    "\n",
    "### Input-Output Interface\n",
    "\n",
    "As shown in Figure 14.1, given a document $(x_1, \\cdots, x_T)$, we first translate the sequence of discrete variables into a sequence of corresponding word embeddings ($e_{x_1}, \\cdots, e_{x_T}$).\n",
    "\n",
    "<img src=\"./img/transformer_output.png\" width=\"400px\"/>\n",
    "\n",
    "**Figure 14.1 (description):** The inputs to the Transformer are the embeddings $e_{x_0}, e_{x_1}, \\ldots, e_{x_T}$ corresponding to the tokens $x_0, x_1, \\ldots, x_T$. The Transformer $f_\\theta(x)$ outputs a sequence of vectors $u_1, u_2, \\ldots, u_{T+1}$, each of which is used to predict the next token in the sequence.\n",
    "\n",
    "We also introduce a fixed special token $x_0 = \\perp$ in the vocabulary with corresponding embedding $e_{x_0}$ to mark the beginning of a document. Then, the word embeddings are passed into a Transformer model, which takes in a sequence of vectors ($e_{x_0}, e_{x_1}, \\cdots, e_{x_T}$) and outputs a sequence of vectors ($u_1, u_2, \\cdots, u_{T+1}$), where $u_t \\in \\mathbb{R}^V$ will be interpreted as the logits for the probability distribution of the next word.\n",
    "\n",
    "#### Understanding the Special Token\n",
    "\n",
    "The special token $\\perp$ (often called `<BOS>` for \"beginning of sequence\") serves several important purposes:\n",
    "\n",
    "1. **Sequence Start Marker**: Indicates the beginning of a new sequence\n",
    "2. **Context for First Word**: Provides context for predicting the first actual word\n",
    "3. **Consistent Input Length**: Ensures all sequences have the same structure\n",
    "\n",
    "#### The Autoregressive Property\n",
    "\n",
    "Here we use the autoregressive version of the Transformer, which by design ensures $u_t$ only depends on $x_1, \\cdots, x_{t-1}$ (note that this property does not hold in masked language models [Devlin et al., 2019] where the losses are also different).\n",
    "\n",
    "**What is Autoregressive?**\n",
    "- Each output depends only on previous inputs\n",
    "- No information from future tokens is used\n",
    "- This is crucial for language generation tasks\n",
    "\n",
    "**Example:**\n",
    "For the sequence \"The cat sat\":\n",
    "- $u_1$ depends on $x_0$ (special token) only\n",
    "- $u_2$ depends on $x_0, x_1$ (\"The\")\n",
    "- $u_3$ depends on $x_0, x_1, x_2$ (\"The cat\")\n",
    "- $u_4$ depends on $x_0, x_1, x_2, x_3$ (\"The cat sat\")\n",
    "\n",
    "We view the whole mapping from $x$'s to $u$'s as a blackbox in this subsection and call it a Transformer, denoted it by $f_\\theta$, where $\\theta$ include both the parameters in the Transformer and the input embeddings. We write $u_t = f_\\theta(x_0, x_1, \\ldots, x_{t-1})$ where $f_\\theta$ denotes the mapping from the input to the outputs.\n",
    "\n",
    "\n",
    "### From Logits to Probabilities\n",
    "\n",
    "The conditional probability $p(x_t|x_1, \\cdots, x_{t-1})$ is the softmax of the logits:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "p(x_t = 1|x_1 \\cdots, x_{t-1}) \\\\\n",
    "p(x_t = 2|x_1 \\cdots, x_{t-1}) \\\\\n",
    "\\vdots \\\\\n",
    "p(x_t = V|x_1 \\cdots, x_{t-1})\n",
    "\\end{bmatrix}\n",
    "= \\mathrm{softmax}(u_t) \\in \\mathbb{R}^V \\tag{14.6}\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "= \\mathrm{softmax}(f_\\theta(x_0, \\ldots, x_{t-1})) \\tag{14.7}\n",
    "$$\n",
    "\n",
    "#### Understanding Softmax\n",
    "\n",
    "The softmax function converts a vector of logits into a probability distribution:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^V \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "**The Softmax Intuition:**\n",
    "- **Raw scores**: Logits can be any real numbers (positive, negative, large, small)\n",
    "- **Exponentiation**: Makes all values positive and amplifies differences\n",
    "- **Normalization**: Divides by sum to make probabilities add to 1\n",
    "- **Probability distribution**: Output is a valid probability distribution\n",
    "\n",
    "**The Temperature Analogy:**\n",
    "- **Logits**: Like raw scores on a test (could be negative, any magnitude)\n",
    "- **Exponentiation**: Like converting to a positive scale\n",
    "- **Normalization**: Like converting to percentages that sum to 100%\n",
    "- **Probability**: Like the chance of each outcome\n",
    "\n",
    "**Properties:**\n",
    "- All outputs are positive (probabilities)\n",
    "- Sum to 1 (valid probability distribution)\n",
    "- Preserves relative ordering of logits\n",
    "- Amplifies differences between large and small values\n",
    "\n",
    "**The Amplification Effect:**\n",
    "- **Large logits**: Get much larger after exponentiation\n",
    "- **Small logits**: Stay small after exponentiation\n",
    "- **Result**: Sharpens the distribution, making confident predictions more confident\n",
    "- **Purpose**: Helps the model make clear, confident predictions\n",
    "\n",
    "**Example:**\n",
    "If $u_t = [2.0, 1.0, 0.5]$ for a 3-word vocabulary:\n",
    "- $\\exp(u_t) = [7.39, 2.72, 1.65]$\n",
    "- $\\sum \\exp(u_t) = 11.76$\n",
    "- $\\text{softmax}(u_t) = [0.63, 0.23, 0.14]$\n",
    "\n",
    "**The Numerical Process:**\n",
    "1. **Input logits**: [2.0, 1.0, 0.5]\n",
    "2. **Exponentiate**: [e^2.0, e^1.0, e^0.5] = [7.39, 2.72, 1.65]\n",
    "3. **Sum**: 7.39 + 2.72 + 1.65 = 11.76\n",
    "4. **Normalize**: [7.39/11.76, 2.72/11.76, 1.65/11.76] = [0.63, 0.23, 0.14]\n",
    "\n",
    "## Training the Language Model\n",
    "\n",
    "We train the Transformer parameter $\\theta$ by minimizing the negative log-likelihood of seeing the data under the probabilistic model defined by $\\theta$.\n",
    "\n",
    "which is the cross-entropy loss on the logits.\n",
    "\n",
    "$$\n",
    "\\text{loss}(\\theta) = \\frac{1}{T} \\sum_{t=1}^T -\\log(p_\\theta(x_t|x_1, \\ldots, x_{t-1})) \\tag{14.8}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{T} \\sum_{t=1}^T \\ell_{ce}(f_\\theta(x_0, x_1, \\cdots, x_{t-1}), x_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{T} \\sum_{t=1}^T -\\log(\\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_{t-1}))_{x_t})\n",
    "$$\n",
    "\n",
    "### The Big Picture: What is Language Model Training?\n",
    "\n",
    "**The Training Goal:**\n",
    "Language model training is about teaching the model to predict the next word accurately. We want the model to assign high probability to the correct next word and low probability to incorrect words.\n",
    "\n",
    "**The Learning Process:**\n",
    "- **Input**: Sequences of words from a large text corpus\n",
    "- **Target**: Predict the next word at each position\n",
    "- **Loss**: Measure how well the model predicts the correct words\n",
    "- **Optimization**: Update model parameters to improve predictions\n",
    "\n",
    "**The Training Intuition:**\n",
    "- **Teacher**: The training data shows what words actually come next\n",
    "- **Student**: The model tries to predict what comes next\n",
    "- **Feedback**: Loss function tells the model how wrong its predictions are\n",
    "- **Learning**: Model adjusts its parameters to make better predictions\n",
    "\n",
    "#### Understanding the Loss Function\n",
    "\n",
    "**Cross-Entropy Loss:**\n",
    "The cross-entropy loss measures how well our predicted probability distribution matches the true distribution (which is a one-hot vector for the correct word).\n",
    "\n",
    "$$\n",
    "\\ell_{ce}(u, x_t) = -\\log(\\text{softmax}(u)_{x_t})\n",
    "$$\n",
    "\n",
    "**The Cross-Entropy Intuition:**\n",
    "- **Predicted distribution**: Model's guess about next word probabilities\n",
    "- **True distribution**: One-hot vector (1 for correct word, 0 for others)\n",
    "- **Cross-entropy**: Measures how different these distributions are\n",
    "- **Minimization**: Want predicted distribution to match true distribution\n",
    "\n",
    "**Why This Works:**\n",
    "- If the model assigns high probability to the correct word, $-\\log(p)$ is small\n",
    "- If the model assigns low probability to the correct word, $-\\log(p)$ is large\n",
    "- The loss encourages the model to assign high probability to correct words\n",
    "\n",
    "**The Penalty System:**\n",
    "- **Good prediction**: High probability for correct word â†’ small penalty\n",
    "- **Bad prediction**: Low probability for correct word â†’ large penalty\n",
    "- **Perfect prediction**: Probability = 1 â†’ penalty = 0\n",
    "- **Terrible prediction**: Probability â‰ˆ 0 â†’ penalty â†’ âˆž\n",
    "\n",
    "**Example:**\n",
    "For the sequence \"The cat sat\" and $u_2$ (predicting the word after \"The\"):\n",
    "- If the model predicts $p(\\text{cat}) = 0.8$, loss = $-\\log(0.8) = 0.22$\n",
    "- If the model predicts $p(\\text{cat}) = 0.1$, loss = $-\\log(0.1) = 2.30$\n",
    "\n",
    "**The Learning Signal:**\n",
    "- **High confidence, correct**: Small loss, model is doing well\n",
    "- **High confidence, wrong**: Large loss, model is overconfident\n",
    "- **Low confidence, correct**: Medium loss, model is uncertain\n",
    "- **Low confidence, wrong**: Large loss, model is wrong and uncertain\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "1. **Forward Pass**: For each position $t$, compute $u_t = f_\\theta(x_0, \\ldots, x_{t-1})$\n",
    "2. **Loss Computation**: Compute cross-entropy loss for each position\n",
    "3. **Backward Pass**: Compute gradients with respect to $\\theta$\n",
    "4. **Parameter Update**: Update $\\theta$ using gradient descent\n",
    "\n",
    "**The Training Loop:**\n",
    "- **Step 1**: Take a batch of text sequences\n",
    "- **Step 2**: For each position, predict the next word\n",
    "- **Step 3**: Compare predictions with actual next words\n",
    "- **Step 4**: Compute loss (how wrong the predictions are)\n",
    "- **Step 5**: Compute gradients (how to change parameters)\n",
    "- **Step 6**: Update parameters to reduce loss\n",
    "- **Step 7**: Repeat with next batch\n",
    "\n",
    "**The Batch Processing:**\n",
    "- **Single sequence**: Process one text sequence at a time\n",
    "- **Batch processing**: Process multiple sequences simultaneously\n",
    "- **Parallelization**: Can use GPU to process many sequences in parallel\n",
    "- **Efficiency**: Much faster than processing sequences one by one\n",
    "\n",
    "**The Gradient Flow:**\n",
    "- **Loss computation**: How wrong is the model at each position?\n",
    "- **Gradient computation**: How should each parameter change?\n",
    "- **Parameter update**: Adjust parameters to reduce loss\n",
    "- **Convergence**: Parameters converge to values that minimize loss\n",
    "\n",
    "## Text Generation with Language Models\n",
    "\n",
    "**Autoregressive text decoding / generation.** Given an autoregressive Transformer, we can simply sample text from it sequentially. Given a prefix $x_1, \\ldots, x_t$, we generate text completion $x_{t+1}, \\ldots, x_T$ sequentially using the conditional distribution.\n",
    "\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_t)) \\tag{14.9}\n",
    "$$\n",
    "$$\n",
    "x_{t+2} \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_{t+1})) \\tag{14.10}\n",
    "$$\n",
    "$$\n",
    "\\vdots \\tag{14.11}\n",
    "$$\n",
    "$$\n",
    "x_T \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_{T-1})) \\tag{14.12}\n",
    "$$\n",
    "\n",
    "### The Big Picture: How Text Generation Works\n",
    "\n",
    "**The Generation Process:**\n",
    "Text generation is like having a conversation with the model. You give it some starting text (prefix), and it continues the conversation by predicting what comes next, one word at a time.\n",
    "\n",
    "**The Autoregressive Nature:**\n",
    "- **Sequential generation**: Generate one word at a time\n",
    "- **Context building**: Each new word depends on all previous words\n",
    "- **Feedback loop**: Generated words become input for next predictions\n",
    "- **Cumulative context**: Context grows as generation proceeds\n",
    "\n",
    "**The Generation Intuition:**\n",
    "- **Starting point**: Given some initial text (prompt)\n",
    "- **Prediction**: Model predicts probability distribution over next word\n",
    "- **Sampling**: Choose next word based on probabilities\n",
    "- **Extension**: Add chosen word to text and repeat\n",
    "\n",
    "Note that each generated token is used as the input to the model when generating the following tokens. In practice, people often introduce a parameter $\\tau > 0$ named *temperature* to further adjust the entropy/sharpness of the generated distribution,\n",
    "\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_t)/\\tau) \\tag{14.13}\n",
    "$$\n",
    "$$\n",
    "x_{t+2} \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_{t+1})/\\tau) \\tag{14.14}\n",
    "$$\n",
    "$$\n",
    "\\vdots \\tag{14.15}\n",
    "$$\n",
    "$$\n",
    "x_T \\sim \\mathrm{softmax}(f_\\theta(x_0, x_1, \\cdots, x_{T-1})/\\tau) \\tag{14.16}\n",
    "$$\n",
    "\n",
    "### Understanding Temperature\n",
    "\n",
    "The temperature parameter $\\tau$ controls the randomness of text generation:\n",
    "\n",
    "**The Temperature Analogy:**\n",
    "- **Temperature**: Like adjusting the \"creativity\" or \"confidence\" of the model\n",
    "- **Low temperature**: Model is very confident and predictable\n",
    "- **High temperature**: Model is more uncertain and creative\n",
    "- **Optimal temperature**: Balance between coherence and diversity\n",
    "\n",
    "**Low Temperature ($\\tau < 1$):**\n",
    "- Makes the distribution more \"peaked\" (concentrated)\n",
    "- Model becomes more confident in its predictions\n",
    "- Generated text is more deterministic and focused\n",
    "- Risk of repetitive or boring text\n",
    "\n",
    "**The Low Temperature Effect:**\n",
    "- **Sharpening**: Amplifies differences between probabilities\n",
    "- **Confidence**: Model becomes more certain about its predictions\n",
    "- **Consistency**: Generated text is more predictable\n",
    "- **Repetition**: May get stuck in loops or patterns\n",
    "\n",
    "**High Temperature ($\\tau > 1$):**\n",
    "- Makes the distribution more \"flat\" (spread out)\n",
    "- Model becomes less confident in its predictions\n",
    "- Generated text is more diverse and creative\n",
    "- Risk of incoherent or nonsensical text\n",
    "\n",
    "**The High Temperature Effect:**\n",
    "- **Flattening**: Reduces differences between probabilities\n",
    "- **Uncertainty**: Model becomes less certain about predictions\n",
    "- **Diversity**: Generated text is more varied\n",
    "- **Creativity**: May produce unexpected but interesting text\n",
    "\n",
    "**Example:**\n",
    "Consider logits $u = [2.0, 1.0, 0.5]$:\n",
    "- $\\tau = 0.5$: $\\text{softmax}(u/0.5) = [0.88, 0.11, 0.01]$ (very confident)\n",
    "- $\\tau = 1.0$: $\\text{softmax}(u/1.0) = [0.63, 0.23, 0.14]$ (balanced)\n",
    "- $\\tau = 2.0$: $\\text{softmax}(u/2.0) = [0.42, 0.31, 0.27]$ (uncertain)\n",
    "\n",
    "**The Temperature Scaling Process:**\n",
    "1. **Original logits**: [2.0, 1.0, 0.5]\n",
    "2. **Divide by temperature**: [2.0/Ï„, 1.0/Ï„, 0.5/Ï„]\n",
    "3. **Apply softmax**: Convert to probabilities\n",
    "4. **Sample**: Choose next word based on probabilities\n",
    "\n",
    "When $\\tau = 1$, the text is sampled from the original conditional probability defined by the model. With a decreasing $\\tau$, the generated text gradually becomes more \"deterministic\". $\\tau \\to 0$ reduces to greedy decoding, where we generate the most probable next token from the conditional probability.\n",
    "\n",
    "**The Temperature Extremes:**\n",
    "- **$\\tau \\to 0$**: Greedy decoding (always choose most probable word)\n",
    "- **$\\tau = 1$**: Standard sampling (use model's original probabilities)\n",
    "- **$\\tau \\to \\infty$**: Uniform sampling (equal probability for all words)\n",
    "\n",
    "### Generation Strategies\n",
    "\n",
    "**Greedy Decoding ($\\tau \\to 0$):**\n",
    "- Always choose the most probable next word\n",
    "- Fast and deterministic\n",
    "- Often produces repetitive or boring text\n",
    "\n",
    "**The Greedy Decoding Analogy:**\n",
    "- **Strategy**: Always take the safest, most obvious choice\n",
    "- **Speed**: Very fast (no randomness to compute)\n",
    "- **Quality**: Often repetitive or predictable\n",
    "- **Use case**: When you want consistent, predictable output\n",
    "\n",
    "**Random Sampling ($\\tau = 1$):**\n",
    "- Sample according to the model's probability distribution\n",
    "- More diverse output\n",
    "- Can be less coherent\n",
    "\n",
    "**The Random Sampling Analogy:**\n",
    "- **Strategy**: Let the model's confidence guide choices\n",
    "- **Diversity**: More varied output than greedy\n",
    "- **Coherence**: Generally maintains reasonable coherence\n",
    "- **Use case**: When you want natural, varied text\n",
    "\n",
    "**Top-k Sampling:**\n",
    "- Only consider the top $k$ most probable words\n",
    "- Sample from this restricted set\n",
    "- Balances diversity and coherence\n",
    "\n",
    "**The Top-k Sampling Analogy:**\n",
    "- **Strategy**: Consider only the most reasonable options\n",
    "- **Control**: Limits choices to top k candidates\n",
    "- **Balance**: Good balance between diversity and coherence\n",
    "- **Use case**: When you want controlled creativity\n",
    "\n",
    "**Nucleus Sampling (Top-p):**\n",
    "- Consider words until cumulative probability reaches $p$\n",
    "- Sample from this dynamic set\n",
    "- Often produces better results than fixed top-k\n",
    "\n",
    "**The Nucleus Sampling Analogy:**\n",
    "- **Strategy**: Consider options until you have enough probability mass\n",
    "- **Adaptive**: Number of options varies based on model confidence\n",
    "- **Quality**: Often produces better text than fixed top-k\n",
    "- **Use case**: When you want high-quality, diverse text\n",
    "\n",
    "**The Sampling Strategy Comparison:**\n",
    "- **Greedy**: Fastest, most predictable, often repetitive\n",
    "- **Random**: Natural diversity, good coherence\n",
    "- **Top-k**: Controlled diversity, consistent quality\n",
    "- **Nucleus**: Best quality, adaptive diversity\n",
    "\n",
    "---\n",
    "\n",
    "## 14.3.1 Zero-shot learning and in-context learning\n",
    "\n",
    "For language models, there are many ways to adapt a pretrained model to downstream tasks. In this notes, we discuss three of them: finetuning, zero-shot learning, and in-context learning.\n",
    "\n",
    "### The Big Picture: Adaptation Methods for Language Models\n",
    "\n",
    "**The Adaptation Challenge:**\n",
    "Once we have a pretrained language model, we need to use it for specific tasks. Different tasks require different adaptation strategies, depending on the amount of labeled data available and the specific requirements.\n",
    "\n",
    "**The Adaptation Spectrum:**\n",
    "- **Finetuning**: Lots of labeled data, best performance\n",
    "- **In-context learning**: Few labeled examples, good performance\n",
    "- **Zero-shot learning**: No labeled data, variable performance\n",
    "\n",
    "**The Key Insight:**\n",
    "Language models can adapt to new tasks without traditional training by using their general language understanding and the structure of the task itself.\n",
    "\n",
    "### Finetuning\n",
    "\n",
    "**Finetuning** is not very common for the autoregressive language models that we introduced in Section 14.3 but much more common for other variants such as masked language models which has similar input-output interfaces but are pretrained differently [Devlin et al., 2019]. \n",
    "\n",
    "Finetuning means taking a model that has already learned a lot from a huge dataset (pretraining), and then continuing to train it on a smaller, task-specific dataset. Think of it like a student who has read many books (pretraining) and then studies specifically for a test (finetuning). The model adapts its knowledge to do well on the new task.\n",
    "\n",
    "**The Finetuning Analogy:**\n",
    "- **Pretraining**: Like getting a general education (reading many books)\n",
    "- **Finetuning**: Like studying for a specific exam (focused learning)\n",
    "- **Adaptation**: Like using general knowledge to learn specific skills\n",
    "- **Specialization**: Like becoming an expert in a particular area\n",
    "\n",
    "**The Finetuning Process:**\n",
    "1. **Start with pretrained model**: Model already knows general language patterns\n",
    "2. **Add task-specific data**: Small dataset for the specific task\n",
    "3. **Continue training**: Update model parameters on task data\n",
    "4. **Task specialization**: Model becomes good at the specific task\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "The finetuning method is the same as introduced generally in Section 14.1â€”the only question is how we define the prediction task with an additional linear head. One option is to treat $c_{T+1} = \\phi_\\theta(x_1, \\cdots, x_T)$ as the representation and use $w^\\top c_{T+1} = w^\\top \\phi_\\theta(x_1, \\cdots, x_T)$ to predict the task label. As described in Section 14.1, we initialize $\\theta$ to the pretrained model $\\hat{\\theta}$ and then optimize both $w$ and $\\theta$.\n",
    "\n",
    "**The Finetuning Mathematics:**\n",
    "- **Representation extraction**: $c_{T+1} = \\phi_\\theta(x_1, \\cdots, x_T)$\n",
    "- **Task prediction**: $y = w^\\top c_{T+1}$\n",
    "- **Parameter initialization**: $\\theta \\leftarrow \\hat{\\theta}$ (pretrained weights)\n",
    "- **Joint optimization**: Update both $w$ and $\\theta$\n",
    "\n",
    "**The Representation Learning:**\n",
    "- **$c_{T+1}$**: Final representation from the language model\n",
    "- **$w$**: Task-specific weights to be learned\n",
    "- **$y$**: Task prediction (classification, regression, etc.)\n",
    "- **Learning**: Both representation and task weights are updated\n",
    "\n",
    "#### When to Use Finetuning\n",
    "\n",
    "**Advantages:**\n",
    "- **Best Performance**: Can achieve highest accuracy on the target task\n",
    "- **Task-Specific Adaptation**: Model can learn task-specific patterns\n",
    "- **Flexibility**: Can adapt to any task format\n",
    "- **Optimization**: Can optimize for task-specific metrics\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Requires Labeled Data**: Needs significant amount of task-specific data\n",
    "- **Computationally Expensive**: Requires training the entire model\n",
    "- **Risk of Catastrophic Forgetting**: May lose general knowledge\n",
    "- **Task-Specific**: Need separate model for each task\n",
    "\n",
    "**The Catastrophic Forgetting Problem:**\n",
    "- **What happens**: Model forgets general knowledge while learning specific task\n",
    "- **Why it happens**: Parameters optimized for specific task may not preserve general knowledge\n",
    "- **Mitigation**: Use smaller learning rates, regularization, or parameter-efficient methods\n",
    "- **Trade-off**: Task performance vs. general knowledge preservation\n",
    "\n",
    "**When to Use:**\n",
    "- You have a large labeled dataset for your specific task\n",
    "- You want the best possible performance\n",
    "- You have sufficient computational resources\n",
    "- The task is very different from pretraining\n",
    "\n",
    "- **Why does this work?** The pretrained model already knows a lot about language, so it only needs to \"tune\" itself to the specifics of the new task. This is much faster and requires less data than training from scratch.\n",
    "- **Tip:** Use finetuning when you have a moderate or large labeled dataset for your specific task, and you want the best possible performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Zero-shot Learning\n",
    "\n",
    "**Zero-shot** adaptation or zero-shot learning is the setting where there is no input-output pairs from the downstream tasks. For language problems tasks, typically the task is formatted as a question or a cloze test form via natural language. \n",
    "\n",
    "#### How Zero-shot Learning Works\n",
    "\n",
    "- **Analogy:** Imagine you ask a well-read person a question they've never seen before, but they can answer it because they have broad general knowledge.\n",
    "- **How does it work?** The model is given a prompt (like a question) and must generate the answer using only what it learned during pretraining. No additional training is done for the new task.\n",
    "\n",
    "**The Zero-shot Intuition:**\n",
    "- **No examples**: Task is presented without any training examples\n",
    "- **General knowledge**: Model uses its broad understanding from pretraining\n",
    "- **Task understanding**: Model must understand what the task is asking for\n",
    "- **Answer generation**: Model generates appropriate response\n",
    "\n",
    "**The Zero-shot Process:**\n",
    "1. **Task formulation**: Convert task into natural language prompt\n",
    "2. **Model understanding**: Model interprets what the task requires\n",
    "3. **Knowledge application**: Model applies relevant knowledge\n",
    "4. **Answer generation**: Model generates appropriate response\n",
    "\n",
    "#### Task Formatting\n",
    "\n",
    "The key insight is that we can format many tasks as natural language questions or prompts. The model then generates text that answers the question.\n",
    "\n",
    "For example, we can format an example as a question:\n",
    "\n",
    "$x_{\\text{task}} = (x_{\\text{task},1}, \\cdots, x_{\\text{task},R}) = \\text{\"Is the speed of light a universal constant?\"}$\n",
    "\n",
    "Then, we compute the most likely next word predicted by the language model given this question, that is, computing $\\text{argmax}_{x_{T+1}} p(x_{T+1} \\mid x_{\\text{task},1}, \\cdots, x_{\\text{task},R})$. In this case, if the most likely next word $x_{T+1}$ is \"No\", then we solve the task. (The speed of light is only a constant in vacuum.)\n",
    "\n",
    "**The Task Formatting Strategy:**\n",
    "- **Natural language**: Convert task into question or instruction\n",
    "- **Clear instructions**: Make it obvious what the model should do\n",
    "- **Expected format**: Specify the format of the expected answer\n",
    "- **Context provision**: Provide necessary context for the task\n",
    "\n",
    "**The Prompt Engineering:**\n",
    "- **Question format**: \"What is the capital of France?\"\n",
    "- **Instruction format**: \"Translate this to French: Hello world\"\n",
    "- **Cloze format**: \"The capital of France is _____\"\n",
    "- **Multiple choice**: \"Is this positive or negative? The movie was great.\"\n",
    "\n",
    "#### Examples of Zero-shot Tasks\n",
    "\n",
    "**Classification Tasks:**\n",
    "- **Sentiment Analysis**: \"The movie was terrible. Sentiment: [positive/negative]\"\n",
    "- **Topic Classification**: \"This article discusses climate change. Topic: [politics/science/sports]\"\n",
    "- **Language Detection**: \"Bonjour, comment allez-vous? Language: [English/French/Spanish]\"\n",
    "\n",
    "**Generation Tasks:**\n",
    "- **Translation**: \"Translate to French: Hello world â†’\"\n",
    "- **Summarization**: \"Summarize this text: [text] â†’\"\n",
    "- **Question Answering**: \"What is the capital of France? Answer:\"\n",
    "\n",
    "**Reasoning Tasks:**\n",
    "- **Mathematical Reasoning**: \"What is 15 + 27? Answer:\"\n",
    "- **Logical Reasoning**: \"If all A are B, and all B are C, then all A are C. True or false?\"\n",
    "- **Common Sense**: \"Can a person run faster than a car? Answer:\"\n",
    "\n",
    "**The Task Diversity:**\n",
    "- **Language tasks**: Translation, summarization, question answering\n",
    "- **Reasoning tasks**: Math, logic, common sense\n",
    "- **Creative tasks**: Writing, poetry, code generation\n",
    "- **Analysis tasks**: Sentiment, topic classification, fact checking\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- **No Training Required**: Works immediately without any additional training\n",
    "- **No Labeled Data**: Can work on tasks with no labeled examples\n",
    "- **Rapid Deployment**: Can be applied to new tasks instantly\n",
    "- **Cost Effective**: No computational cost for adaptation\n",
    "\n",
    "**The Zero-shot Benefits:**\n",
    "- **Immediate use**: No waiting for training to complete\n",
    "- **No data collection**: Can work with tasks where data is hard to collect\n",
    "- **Scalability**: Can handle many tasks with one model\n",
    "- **Accessibility**: Makes AI accessible to more users\n",
    "\n",
    "**Limitations:**\n",
    "- **Inconsistent Performance**: Results can be unreliable\n",
    "- **Limited Control**: Cannot fine-tune behavior for specific tasks\n",
    "- **Prompt Sensitivity**: Performance heavily depends on prompt formulation\n",
    "- **Knowledge Cutoff**: Limited to knowledge from pretraining\n",
    "\n",
    "**The Zero-shot Challenges:**\n",
    "- **Unpredictable**: Performance varies significantly across tasks\n",
    "- **Prompt engineering**: Requires careful prompt design\n",
    "- **No learning**: Cannot improve with more examples\n",
    "- **Knowledge limits**: Cannot access information beyond pretraining\n",
    "\n",
    "- **Why does this work?** During pretraining, the model has seen so many examples of language that it can often generalize to new questions or tasks, even if it has never seen them before.\n",
    "- **Tip:** Zero-shot is useful when you have no labeled data for your task, or want to quickly test what a model can do \"out of the box.\"\n",
    "\n",
    "---\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "**In-context learning** is mostly used for few-shot settings where we have a few labeled examples $(x^{(1)}_{\\text{task}}, y^{(1)}_{\\text{task}}), \\cdots, (x^{(n_{\\text{task}})}_{\\text{task}}, y^{(n_{\\text{task}})}_{\\text{task}})$. \n",
    "\n",
    "#### The In-context Learning Paradigm\n",
    "\n",
    "- **Analogy:** Imagine you show a person a few examples of a new kind of puzzle, and then ask them to solve a similar one. They use the examples as hints to figure out the pattern.\n",
    "- **How does it work?** Given a test example $x_{\\text{test}}$, we construct a document $(x_1, \\cdots, x_T)$, which is more commonly called a \"prompt\" in this context, by concatenating the labeled examples and the test example in some format. The model is not retrained; instead, it \"reads\" the prompt and tries to continue it in a way that matches the pattern.\n",
    "\n",
    "**The In-context Learning Intuition:**\n",
    "- **Few examples**: Just a handful of labeled examples (1-10)\n",
    "- **Pattern recognition**: Model learns the pattern from examples\n",
    "- **No training**: Model parameters remain unchanged\n",
    "- **Temporary learning**: \"Learning\" happens only during inference\n",
    "\n",
    "**The In-context Process:**\n",
    "1. **Example collection**: Gather a few labeled examples\n",
    "2. **Prompt construction**: Format examples into a coherent prompt\n",
    "3. **Pattern learning**: Model learns pattern from examples\n",
    "4. **Test application**: Apply learned pattern to new example\n",
    "\n",
    "#### Prompt Construction\n",
    "\n",
    "For example, we may construct the prompt as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_1, \\cdots, x_T \\quad = \\quad & \\text{\"Q: 2 ~ 3 = ?\"} \\quad x^{(1)}_{\\text{task}} \\\\\n",
    "& \\text{\"A: 5\"} \\quad y^{(1)}_{\\text{task}} \\\\\n",
    "& \\text{\"Q: 6 ~ 7 = ?\"} \\quad x^{(2)}_{\\text{task}} \\\\\n",
    "& \\text{\"A: 13\"} \\quad y^{(2)}_{\\text{task}} \\\\\n",
    "& \\cdots \\\\\n",
    "& \\text{\"Q: 15 ~ 2 = ?\"} \\quad x_{\\text{test}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, we let the pretrained model generate the most likely $x_{T+1}, x_{T+2}, \\cdots$. In this case, if the model can \"learn\" that the symbol $\\sim$ means addition from the few examples, we will obtain the following which suggests the answer is 17.\n",
    "\n",
    "$$\n",
    "x_{T+1}, x_{T+2}, \\cdots = \\text{\"A: 17\"}.\n",
    "$$\n",
    "\n",
    "**The Prompt Construction Strategy:**\n",
    "- **Consistent format**: Use same format for all examples\n",
    "- **Clear pattern**: Make the pattern obvious to the model\n",
    "- **Relevant examples**: Choose examples similar to the test case\n",
    "- **Appropriate number**: Usually 1-10 examples (more isn't always better)\n",
    "\n",
    "**The Pattern Learning Process:**\n",
    "1. **Input examples**: Model sees several input-output pairs\n",
    "2. **Pattern extraction**: Model identifies the underlying pattern\n",
    "3. **Test application**: Model applies pattern to new input\n",
    "4. **Output generation**: Model generates appropriate output\n",
    "\n",
    "#### Understanding In-context Learning\n",
    "\n",
    "**Key Insights:**\n",
    "1. **Pattern Recognition**: The model learns patterns from the examples in the prompt\n",
    "2. **No Parameter Updates**: The model's parameters remain unchanged\n",
    "3. **Temporary Learning**: The \"learning\" happens only during inference\n",
    "4. **Few-shot Capability**: Works with just a few examples (typically 1-10)\n",
    "\n",
    "**The In-context Learning Mechanism:**\n",
    "- **Attention mechanism**: Model attends to relevant parts of the prompt\n",
    "- **Pattern matching**: Model finds similarities between examples\n",
    "- **Generalization**: Model applies learned pattern to new cases\n",
    "- **Context utilization**: Model uses prompt context for predictions\n",
    "\n",
    "**Why It Works:**\n",
    "- **Pretraining Knowledge**: The model has seen similar patterns during pretraining\n",
    "- **Pattern Completion**: The model tries to complete the pattern in the prompt\n",
    "- **Context Utilization**: The examples provide context for the task\n",
    "- **Attention Power**: Transformer attention can focus on relevant examples\n",
    "\n",
    "**The Learning Without Learning Paradox:**\n",
    "- **No training**: Model parameters don't change\n",
    "- **Learning happens**: Model behavior adapts to the prompt\n",
    "- **Temporary adaptation**: Adaptation only lasts for this prompt\n",
    "- **Pattern completion**: Model completes the pattern it sees\n",
    "\n",
    "#### Examples of In-context Learning\n",
    "\n",
    "**Text Classification:**\n",
    "```\n",
    "Input: \"I love this movie!\"\n",
    "Output: positive\n",
    "\n",
    "Input: \"This is terrible.\"\n",
    "Output: negative\n",
    "\n",
    "Input: \"The food was okay.\"\n",
    "Output: [model generates: negative]\n",
    "```\n",
    "\n",
    "**Translation:**\n",
    "```\n",
    "Input: \"Hello\" â†’ \"Hola\"\n",
    "Input: \"Goodbye\" â†’ \"AdiÃ³s\"\n",
    "Input: \"Thank you\" â†’ [model generates: \"Gracias\"]\n",
    "```\n",
    "\n",
    "**Mathematical Reasoning:**\n",
    "```\n",
    "Input: \"2 + 3 = 5\"\n",
    "Input: \"7 + 4 = 11\"\n",
    "Input: \"15 + 8 = [model generates: 23]\"\n",
    "```\n",
    "\n",
    "**The Example Quality:**\n",
    "- **Relevance**: Examples should be similar to the test case\n",
    "- **Diversity**: Examples should cover different cases\n",
    "- **Clarity**: Examples should be clear and unambiguous\n",
    "- **Consistency**: Examples should follow the same format\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- **No Training Required**: Works without additional training\n",
    "- **Few Examples Needed**: Can work with just a few labeled examples\n",
    "- **Rapid Prototyping**: Easy to test new tasks quickly\n",
    "- **Interpretable**: The prompt shows exactly what the model is learning from\n",
    "\n",
    "**The In-context Benefits:**\n",
    "- **Immediate use**: No training time required\n",
    "- **Low data requirement**: Works with minimal labeled data\n",
    "- **Flexibility**: Can adapt to many different tasks\n",
    "- **Transparency**: Prompt shows exactly what the model learns\n",
    "\n",
    "**Limitations:**\n",
    "- **Prompt Engineering**: Requires careful prompt design\n",
    "- **Inconsistent Performance**: Results can vary significantly\n",
    "- **Limited Context**: Limited by the model's context window\n",
    "- **No Learning**: Cannot improve with more examples (unlike traditional learning)\n",
    "\n",
    "**The In-context Challenges:**\n",
    "- **Prompt sensitivity**: Small changes can affect performance\n",
    "- **Context limits**: Limited by model's maximum sequence length\n",
    "- **No improvement**: Performance doesn't improve with more examples\n",
    "- **Unpredictable**: Results can be inconsistent\n",
    "\n",
    "#### Best Practices for In-context Learning\n",
    "\n",
    "1. **Clear Formatting**: Use consistent formatting for examples\n",
    "2. **Relevant Examples**: Choose examples similar to the target task\n",
    "3. **Appropriate Number**: Use 1-10 examples (more isn't always better)\n",
    "4. **Clear Instructions**: Include explicit instructions when possible\n",
    "5. **Consistent Style**: Maintain consistent style across examples\n",
    "\n",
    "**The Prompt Engineering Guidelines:**\n",
    "- **Format consistency**: Use same format for all examples\n",
    "- **Example quality**: Choose high-quality, relevant examples\n",
    "- **Instruction clarity**: Make instructions clear and specific\n",
    "- **Style consistency**: Maintain consistent writing style\n",
    "- **Length optimization**: Balance between clarity and context limits\n",
    "\n",
    "- **Why does this work?** The model has learned to pick up on patterns in the prompt, even if it is not explicitly trained for the new task. This is a powerful way to use large language models for new problems with very little data.\n",
    "- **Tip:** In-context learning is great for rapid prototyping and for tasks where you have only a handful of labeled examples.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison of Adaptation Methods\n",
    "\n",
    "| Method | Data Required | Performance | Speed | Flexibility | Control |\n",
    "|--------|---------------|-------------|-------|-------------|---------|\n",
    "| **Finetuning** | Large labeled dataset | Best | Slow | High | High |\n",
    "| **Zero-shot** | No data | Variable | Fast | Medium | Low |\n",
    "| **In-context** | Few examples | Good | Fast | High | Medium |\n",
    "\n",
    "**The Method Comparison:**\n",
    "- **Data requirements**: How much labeled data is needed\n",
    "- **Performance**: How well the method works on the task\n",
    "- **Speed**: How quickly the method can be applied\n",
    "- **Flexibility**: How easily it adapts to different tasks\n",
    "- **Control**: How much control you have over the adaptation\n",
    "\n",
    "**Summary:**\n",
    "- **Finetuning:** Best when you have a moderate/large labeled dataset and want top performance. The model is retrained for your task.\n",
    "- **Zero-shot:** No labeled data needed. The model uses its general knowledge to answer new questions.\n",
    "- **In-context learning:** Give the model a few examples in the prompt. The model \"figures out\" the pattern and applies it to the new example, without retraining.\n",
    "\n",
    "These methods make large language models extremely flexible and powerful for a wide range of tasks.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "**When to Use Each Method:**\n",
    "\n",
    "1. **Use Finetuning When:**\n",
    "   - You have >1000 labeled examples\n",
    "   - You need the best possible performance\n",
    "   - You have computational resources\n",
    "   - The task is very different from pretraining\n",
    "\n",
    "2. **Use Zero-shot When:**\n",
    "   - You have no labeled data\n",
    "   - You want to quickly test a model's capabilities\n",
    "   - You're doing rapid prototyping\n",
    "   - The task is similar to what the model saw during pretraining\n",
    "\n",
    "3. **Use In-context Learning When:**\n",
    "   - You have 1-10 labeled examples\n",
    "   - You want to avoid training\n",
    "   - You're doing rapid prototyping\n",
    "   - The task has clear patterns that can be demonstrated\n",
    "\n",
    "**The Decision Framework:**\n",
    "- **Data availability**: More data â†’ finetuning\n",
    "- **Performance needs**: High performance â†’ finetuning\n",
    "- **Speed requirements**: Fast deployment â†’ zero-shot or in-context\n",
    "- **Resource constraints**: Limited compute â†’ zero-shot or in-context\n",
    "\n",
    "**Hybrid Approaches:**\n",
    "- **Prompt Engineering + Zero-shot**: Carefully design prompts for better zero-shot performance\n",
    "- **In-context + Finetuning**: Use in-context learning to bootstrap, then finetune\n",
    "- **Ensemble Methods**: Combine predictions from multiple adaptation methods\n",
    "\n",
    "**The Hybrid Strategy:**\n",
    "- **Bootstrap**: Use in-context learning to get initial performance\n",
    "- **Evaluate**: Assess if performance is sufficient\n",
    "- **Decide**: Choose whether to finetune for better performance\n",
    "- **Optimize**: Use best method for the specific use case\n",
    "\n",
    "## From Theoretical Understanding to Practical Implementation\n",
    "\n",
    "We've now explored **large language models** - specialized foundation models for text that leverage the sequential and contextual nature of language. We've seen how language modeling works through the chain rule of probability, how Transformer architectures process text, and how these models can generate coherent text and adapt to new tasks through finetuning, zero-shot learning, and in-context learning.\n",
    "\n",
    "**The Journey So Far:**\n",
    "- **Foundation models**: General-purpose models trained on massive data\n",
    "- **Language modeling**: Predicting next words in sequences\n",
    "- **Transformer architecture**: Parallel processing with attention\n",
    "- **Text generation**: Creating coherent text with various strategies\n",
    "- **Adaptation methods**: Using models for specific tasks\n",
    "\n",
    "However, while understanding the theoretical foundations of self-supervised learning and large language models is essential, true mastery comes from **practical implementation**. The concepts we've learned - contrastive learning, language modeling, text generation, and adaptation methods - need to be applied to real problems to develop intuition and practical skills.\n",
    "\n",
    "**The Theory-to-Practice Bridge:**\n",
    "- **Understanding**: Theoretical knowledge provides foundation\n",
    "- **Implementation**: Practical coding builds intuition\n",
    "- **Experimentation**: Hands-on work reveals nuances\n",
    "- **Mastery**: Combining theory and practice leads to expertise\n",
    "\n",
    "This motivates our exploration of **hands-on coding** - the practical implementation of all the self-supervised learning and language model concepts we've learned. We'll put our theoretical knowledge into practice by implementing contrastive learning for computer vision, building language models for text generation, and developing the practical skills needed to create foundation models that can adapt to various downstream tasks.\n",
    "\n",
    "**The Practical Learning Goals:**\n",
    "- **Implementation skills**: Ability to code the concepts we've learned\n",
    "- **Intuition building**: Deep understanding through hands-on experience\n",
    "- **Problem solving**: Ability to apply concepts to real problems\n",
    "- **Tool mastery**: Proficiency with modern AI frameworks and tools\n",
    "\n",
    "The transition from theoretical understanding to practical implementation represents the bridge from knowledge to application - taking our understanding of how self-supervised learning and language models work and turning it into practical tools for building powerful AI systems.\n",
    "\n",
    "In the next section, we'll implement complete systems for self-supervised learning and language models, experiment with different techniques, and develop the practical skills needed for real-world applications in computer vision and natural language processing.\n",
    "\n",
    "---\n",
    "\n",
    "**Previous: [Self-Supervised Learning](01_pretraining.md)** - Understand the fundamental techniques for learning from unlabeled data.\n",
    "\n",
    "**Next: [Hands-on Coding](03_hands-on_coding.md)** - Implement self-supervised learning and language model techniques with practical examples.\n",
    "\n",
    "[^5]: In the practical implementations, typically all the data are concatenated into a single sequence in some order, and each example typically corresponds a sub-sequence of consecutive words which may correspond to a subset of a document or may span across multiple documents.\n",
    "\n",
    "[^6]: Technically, words may be decomposed into tokens which could be words or sub-words (combinations of letters), but this note omits this technicality. In fact most common words are a single token themselves.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
