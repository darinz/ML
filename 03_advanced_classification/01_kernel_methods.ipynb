{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6099934d-7369-432d-a217-20891b66aeeb",
   "metadata": {},
   "source": [
    "# Kernel Methods: A Comprehensive Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Kernel methods represent one of the most powerful and elegant ideas in machine learning. They allow us to work in high-dimensional feature spaces without ever explicitly computing the features, enabling us to capture complex non-linear patterns in data efficiently. This guide will take you from the fundamental motivation behind kernels to their practical implementation.\n",
    "\n",
    "**Why Kernels Matter:**\n",
    "- **Non-linearity**: Capture complex patterns that linear models cannot\n",
    "- **Efficiency**: Work in infinite-dimensional spaces with finite computation\n",
    "- **Flexibility**: Apply to any algorithm that can be expressed in terms of inner products\n",
    "- **Theoretical Foundation**: Based on solid mathematical principles from functional analysis\n",
    "\n",
    "## 5.1 Feature Maps and the Motivation for Kernels\n",
    "\n",
    "### 5.1.1 The Linear Model Limitation\n",
    "\n",
    "**The Problem with Linearity**\n",
    "\n",
    "Recall that in our discussion about linear regression, we considered the problem of predicting the price of a house (denoted by $y$) from the living area of the house (denoted by $x$), and we fit a linear function of $x$ to the training data. What if the price $y$ can be more accurately represented as a *non-linear* function of $x$? In this case, we need a more expressive family of models than linear models.\n",
    "\n",
    "**Intuitive Example: Housing Price Prediction**\n",
    "\n",
    "Consider a dataset where house prices follow a non-linear pattern:\n",
    "- **Small houses (500-1000 sq ft)**: Price increases slowly (economies of scale)\n",
    "- **Medium houses (1000-2000 sq ft)**: Price increases rapidly (sweet spot for families)\n",
    "- **Large houses (2000+ sq ft)**: Price increases slowly again (diminishing returns)\n",
    "\n",
    "A linear model $y = \\theta_1 x + \\theta_0$ would fail to capture this pattern, leading to poor predictions. The relationship is inherently non-linear.\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "The linear model assumes that the rate of change (derivative) is constant: $\\frac{dy}{dx} = \\theta_1$. But in reality, the rate of change varies with $x$, suggesting we need a more complex model.\n",
    "\n",
    "### 5.1.2 Polynomial Feature Maps: A Solution\n",
    "\n",
    "**The Polynomial Approach**\n",
    "\n",
    "We start by considering fitting cubic functions $y = \\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0$. This allows us to capture non-linear patterns. It turns out that we can view the cubic function as a linear function over a different set of feature variables.\n",
    "\n",
    "**The Feature Map Transformation**\n",
    "\n",
    "Concretely, let the function $\\phi : \\mathbb{R} \\to \\mathbb{R}^4$ be defined as\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{bmatrix} \\in \\mathbb{R}^4.\n",
    "$$\n",
    "\n",
    "Let $\\theta \\in \\mathbb{R}^4$ be the vector containing $\\theta_0, \\theta_1, \\theta_2, \\theta_3$ as entries. Then we can rewrite the cubic function in $x$ as:\n",
    "\n",
    "$$\n",
    "\\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0 = \\theta^T \\phi(x)\n",
    "$$\n",
    "\n",
    "**Key Insight**: A cubic function of the variable $x$ can be viewed as a linear function over the variables $\\phi(x)$. This is the fundamental idea behind feature maps.\n",
    "\n",
    "**Why This Works:**\n",
    "- We've transformed a non-linear problem in the original space into a linear problem in a higher-dimensional space\n",
    "- The feature map $\\phi$ captures the non-linear structure\n",
    "- We can now use linear learning algorithms in the feature space\n",
    "\n",
    "### 5.1.3 Terminology and Definitions\n",
    "\n",
    "**Distinguishing Concepts**\n",
    "\n",
    "To distinguish between these two sets of variables, in the context of kernel methods, we will call the \"original\" input value the input **attributes** of a problem (in this case, $x$, the living area). When the original input is mapped to some new set of quantities $\\phi(x)$, we will call those new quantities the **features** variables. We will call $\\phi$ a **feature map**, which maps the attributes to the features.\n",
    "\n",
    "**Formal Definition**: A feature map is a function $\\phi : \\mathcal{X} \\to \\mathcal{H}$ where:\n",
    "- $\\mathcal{X}$ is the input space (e.g., $\\mathbb{R}^d$)\n",
    "- $\\mathcal{H}$ is the feature space (e.g., $\\mathbb{R}^p$ where $p \\geq d$)\n",
    "\n",
    "**Intuition Behind the Names:**\n",
    "- **Attributes**: Raw, observable characteristics of the data\n",
    "- **Features**: Transformed representations that capture patterns\n",
    "- **Feature Map**: The transformation that reveals the hidden structure\n",
    "\n",
    "### 5.1.4 Examples of Feature Maps\n",
    "\n",
    "#### Polynomial Feature Maps\n",
    "\n",
    "**General Form**: For degree $k$ polynomials in $d$ dimensions:\n",
    "$$\n",
    "\\phi(x) = [1, x_1, x_2, \\ldots, x_d, x_1^2, x_1x_2, \\ldots, x_d^k]^T\n",
    "$$\n",
    "\n",
    "**Example**: For $d=2$ and $k=2$:\n",
    "$$\n",
    "\\phi(x_1, x_2) = [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]^T\n",
    "$$\n",
    "\n",
    "**Why Polynomials?**\n",
    "- They can approximate any smooth function (Taylor series)\n",
    "- They capture interactions between variables\n",
    "- They're computationally tractable for low degrees\n",
    "\n",
    "#### Radial Basis Function (RBF) Feature Maps\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "\\phi(x) = [\\exp(-\\gamma\\|x - c_1\\|^2), \\exp(-\\gamma\\|x - c_2\\|^2), \\ldots]^T\n",
    "$$\n",
    "where $c_i$ are centers and $\\gamma$ is a parameter.\n",
    "\n",
    "**Intuition**: Each feature measures the similarity to a reference point $c_i$. Points close to $c_i$ have high values, points far away have low values.\n",
    "\n",
    "**Properties**:\n",
    "- **Local**: Each feature is sensitive to a specific region\n",
    "- **Smooth**: The exponential function provides smooth transitions\n",
    "- **Flexible**: Can capture complex non-linear patterns\n",
    "\n",
    "#### Trigonometric Feature Maps\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "\\phi(x) = [1, \\sin(x), \\cos(x), \\sin(2x), \\cos(2x), \\ldots]^T\n",
    "$$\n",
    "\n",
    "**Use Case**: Periodic patterns, signal processing, time series analysis.\n",
    "\n",
    "**Fourier Series Connection**: This is related to Fourier series expansion, where any periodic function can be expressed as a sum of sines and cosines.\n",
    "\n",
    "### 5.1.5 The Curse of Dimensionality\n",
    "\n",
    "**The Problem**\n",
    "\n",
    "As we increase the degree of polynomial features or the dimensionality of the input, the feature space grows exponentially:\n",
    "\n",
    "- **Degree 2 polynomial in $d$ dimensions**: $O(d^2)$ features\n",
    "- **Degree 3 polynomial in $d$ dimensions**: $O(d^3)$ features  \n",
    "- **Degree $k$ polynomial in $d$ dimensions**: $O(d^k)$ features\n",
    "\n",
    "**Concrete Example**: For $d=100$ and $k=3$:\n",
    "- Original space: 100 dimensions\n",
    "- Feature space: $\\binom{100+3}{3} = \\binom{103}{3} = 176,851$ dimensions\n",
    "\n",
    "**The Computational Challenge**\n",
    "\n",
    "This exponential growth makes explicit computation of features computationally prohibitive for high-dimensional data:\n",
    "\n",
    "1. **Memory**: Storing feature vectors becomes impossible\n",
    "2. **Computation**: Computing inner products becomes expensive\n",
    "3. **Storage**: The feature matrix grows quadratically with dataset size\n",
    "\n",
    "**The Need for a Solution**\n",
    "\n",
    "This is where the kernel trick comes in - it allows us to work implicitly in these high-dimensional spaces without ever computing the features explicitly.\n",
    "\n",
    "## 5.2 LMS (Least Mean Squares) with Features\n",
    "\n",
    "### 5.2.1 Review of Standard LMS\n",
    "\n",
    "**The Standard Problem**\n",
    "\n",
    "We will derive the gradient descent algorithm for fitting the model $\\theta^T \\phi(x)$. First recall that for ordinary least square problem where we were to fit $\\theta^T x$, the batch gradient descent update is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha \\sum_{i=1}^n \\left( y^{(i)} - h_\\theta(x^{(i)}) \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    ":= \\theta + \\alpha \\sum_{i=1}^n \\left( y^{(i)} - \\theta^T x^{(i)} \\right) x^{(i)}. \\tag{5.2}\n",
    "$$\n",
    "\n",
    "**Derivation of the Gradient**\n",
    "\n",
    "The gradient of the loss function $J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)})^2$ with respect to $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = -\\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)}) x^{(i)}\n",
    "$$\n",
    "\n",
    "**Step-by-step derivation**:\n",
    "1. $J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)})^2$\n",
    "2. $\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)}) \\cdot (-x_j^{(i)})$\n",
    "3. $\\nabla_\\theta J(\\theta) = -\\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)}) x^{(i)}$\n",
    "\n",
    "### 5.2.2 LMS with Feature Maps\n",
    "\n",
    "**The Extension**\n",
    "\n",
    "Let $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ be a feature map that maps attribute $x$ (in $\\mathbb{R}^d$) to the features $\\phi(x)$ in $\\mathbb{R}^p$. Now our goal is to fit the function $\\theta^T \\phi(x)$, with $\\theta$ being a vector in $\\mathbb{R}^p$ instead of $\\mathbb{R}^d$.\n",
    "\n",
    "**Key Insight**: We can replace all the occurrences of $x^{(i)}$ in the algorithm above by $\\phi(x^{(i)})$ to obtain the new update:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha \\sum_{i=1}^n \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}). \\tag{5.3}\n",
    "$$\n",
    "\n",
    "**Stochastic Version**\n",
    "\n",
    "Similarly, the corresponding stochastic gradient descent update rule is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}). \\tag{5.4}\n",
    "$$\n",
    "\n",
    "**Intuition**: Each update step now works in the feature space, allowing us to learn non-linear patterns while using a linear learning algorithm.\n",
    "\n",
    "### 5.2.3 Computational Complexity Analysis\n",
    "\n",
    "**Complexity Comparison**\n",
    "\n",
    "- **Standard LMS**: $O(d)$ per update\n",
    "- **LMS with Features**: $O(p)$ per update\n",
    "\n",
    "**The Problem**: When $p \\gg d$ (e.g., polynomial features), this becomes computationally expensive.\n",
    "\n",
    "**Example**: For degree-3 polynomial features in 1000 dimensions:\n",
    "- Original space: 1000 operations per update\n",
    "- Feature space: 1,000,000,000 operations per update\n",
    "\n",
    "This is clearly impractical for high-dimensional data.\n",
    "\n",
    "### 5.2.4 Implementation Example\n",
    "\n",
    "*Implementation details are provided in the accompanying Python examples file.*\n",
    "\n",
    "## 5.3 The Kernel Trick: Efficient Computation\n",
    "\n",
    "### 5.3.1 The Computational Challenge\n",
    "\n",
    "**The Problem Statement**\n",
    "\n",
    "The gradient descent update becomes computationally expensive when the features $\\phi(x)$ are high-dimensional. Consider the direct extension of the feature map to high-dimensional input $x$: suppose $x \\in \\mathbb{R}^d$, and let $\\phi(x)$ be the vector that contains all the monomials of $x$ with degree $\\leq 3$:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_1^2 \\\\\n",
    "x_1 x_2 \\\\\n",
    "x_1 x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_2 x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_1^3 \\\\\n",
    "x_1^2 x_2 \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}.\n",
    "\\tag{5.5}\n",
    "$$\n",
    "\n",
    "**The Scale of the Problem**\n",
    "\n",
    "The dimension of the features $\\phi(x)$ is on the order of $d^3$. This is prohibitively expensive — when $d = 1000$, each update requires computing and storing a $1000^3 = 10^9$ dimensional vector.\n",
    "\n",
    "**Why This Happens**: Each monomial $x_1^{a_1} x_2^{a_2} \\cdots x_d^{a_d}$ where $\\sum_{i=1}^d a_i \\leq 3$ becomes a separate feature.\n",
    "\n",
    "### 5.3.2 The Representer Theorem\n",
    "\n",
    "**The Key Insight**\n",
    "\n",
    "At any time, $\\theta$ can be represented as a linear combination of the vectors $\\phi(x^{(1)}), \\ldots, \\phi(x^{(n)})$.\n",
    "\n",
    "**Why This Matters**: This means we don't need to work with $\\theta$ directly in the high-dimensional feature space. Instead, we can work with the coefficients $\\beta_i$ in the dual space.\n",
    "\n",
    "**Proof by Induction**\n",
    "\n",
    "1. **Base Case**: At initialization, $\\theta = 0 = \\sum_{i=1}^n 0 \\cdot \\phi(x^{(i)})$\n",
    "\n",
    "2. **Inductive Step**: Assume at some point, $\\theta$ can be represented as:\n",
    "\n",
    "$$\n",
    "\\theta = \\sum_{i=1}^n \\beta_i \\phi(x^{(i)}) \\tag{5.6}\n",
    "$$\n",
    "\n",
    "   for some $\\beta_1, \\ldots, \\beta_n \\in \\mathbb{R}$.\n",
    "\n",
    "3. **Update Step**: After one gradient update:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta &:= \\theta + \\alpha \\sum_{i=1}^n \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^n \\beta_i \\phi(x^{(i)}) + \\alpha \\sum_{i=1}^n \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^n \\left( \\beta_i + \\alpha \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\right) \\phi(x^{(i)}) \\tag{5.7}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**The Result**: This shows that $\\theta$ remains a linear combination of the training feature vectors.\n",
    "\n",
    "**Implications**: \n",
    "- We can work entirely in terms of the coefficients $\\beta_i$\n",
    "- The dimensionality of our optimization problem is $n$ (number of training points) rather than $p$ (feature space dimension)\n",
    "- This is the foundation of the kernel trick\n",
    "\n",
    "### 5.3.3 The Kernel Function\n",
    "\n",
    "**Definition**: The **Kernel** corresponding to the feature map $\\phi$ is a function $K : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ satisfying:\n",
    "$$\n",
    "K(x, z) \\triangleq \\langle \\phi(x), \\phi(z) \\rangle\n",
    "$$\n",
    "\n",
    "**Key Insight**: We can compute $K(x, z)$ efficiently without explicitly computing $\\phi(x)$ and $\\phi(z)$.\n",
    "\n",
    "**Why This is Powerful**: \n",
    "- We can work in infinite-dimensional feature spaces\n",
    "- We only need to compute inner products between data points\n",
    "- The kernel function encapsulates the feature map implicitly\n",
    "\n",
    "### 5.3.4 The Polynomial Kernel\n",
    "\n",
    "**The Efficient Computation**\n",
    "\n",
    "For the polynomial feature map $\\phi$ defined in (5.5), we can compute the kernel efficiently:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle \\phi(x), \\phi(z) \\rangle &= 1 + \\sum_{i=1}^d x_i z_i + \\sum_{i,j \\in \\{1,\\ldots,d\\}} x_i x_j z_i z_j + \\sum_{i,j,k \\in \\{1,\\ldots,d\\}} x_i x_j x_k z_i z_j z_k \\\\\n",
    "&= 1 + \\sum_{i=1}^d x_i z_i + \\left( \\sum_{i=1}^d x_i z_i \\right)^2 + \\left( \\sum_{i=1}^d x_i z_i \\right)^3 \\\\\n",
    "&= 1 + \\langle x, z \\rangle + \\langle x, z \\rangle^2 + \\langle x, z \\rangle^3\n",
    "\\end{align*}\n",
    "\\tag{5.9}\n",
    "$$\n",
    "\n",
    "**The Magic**: Instead of computing $O(d^3)$ features, we only need to compute the inner product $\\langle x, z \\rangle$ once and then raise it to powers.\n",
    "\n",
    "**Computational Complexity**:\n",
    "- **Explicit feature computation**: $O(d^3)$\n",
    "- **Kernel computation**: $O(d)$\n",
    "\n",
    "**The Speedup**: For $d=1000$, this is a factor of 1,000,000 improvement!\n",
    "\n",
    "### 5.3.5 The Kernelized LMS Algorithm\n",
    "\n",
    "**The Algorithm**\n",
    "\n",
    "**Step 1**: Pre-compute the kernel matrix $K$ where $K_{ij} = K(x^{(i)}, x^{(j)})$\n",
    "\n",
    "**Step 2**: Initialize $\\beta = 0$\n",
    "\n",
    "**Step 3**: Iterative updates:\n",
    "$$\n",
    "\\beta_i := \\beta_i + \\alpha \\left( y^{(i)} - \\sum_{j=1}^n \\beta_j K(x^{(i)}, x^{(j)}) \\right) \\tag{5.11}\n",
    "$$\n",
    "\n",
    "**Vector notation**:\n",
    "$$\n",
    "\\beta := \\beta + \\alpha (\\vec{y} - K \\beta)\n",
    "$$\n",
    "\n",
    "**Prediction for new point $x$**:\n",
    "$$\n",
    "\\theta^T \\phi(x) = \\sum_{i=1}^n \\beta_i \\phi(x^{(i)})^T \\phi(x) = \\sum_{i=1}^n \\beta_i K(x^{(i)}, x) \\tag{5.12}\n",
    "$$\n",
    "\n",
    "**Key Insights**:\n",
    "- We work entirely with the kernel matrix $K$\n",
    "- The algorithm is expressed purely in terms of kernel evaluations\n",
    "- No explicit feature computation is needed\n",
    "\n",
    "### 5.3.6 Implementation of Kernelized LMS\n",
    "\n",
    "*Implementation details are provided in the accompanying Python examples file.*\n",
    "\n",
    "## 5.4 Common Kernel Functions\n",
    "\n",
    "### 5.4.1 Linear Kernel\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "K(x, z) = \\langle x, z \\rangle\n",
    "$$\n",
    "\n",
    "**Feature map**: $\\phi(x) = x$ (identity mapping)\n",
    "\n",
    "**Use case**: Linear models, when data is already linearly separable\n",
    "\n",
    "**Properties**:\n",
    "- **Computational cost**: $O(d)$ - fastest possible\n",
    "- **Memory**: Minimal - no kernel matrix needed\n",
    "- **Interpretability**: High - coefficients directly correspond to features\n",
    "\n",
    "**When to use**: \n",
    "- Data is linearly separable\n",
    "- You want maximum interpretability\n",
    "- Computational efficiency is critical\n",
    "\n",
    "### 5.4.2 Polynomial Kernel\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "K(x, z) = (\\gamma \\langle x, z \\rangle + r)^d\n",
    "$$\n",
    "\n",
    "**Feature map**: All monomials up to degree $d$\n",
    "\n",
    "**Parameters**:\n",
    "- $\\gamma$ (scaling): Controls the influence of higher-order terms\n",
    "- $r$ (bias): Adds a constant term to prevent the kernel from being zero\n",
    "- $d$ (degree): Maximum degree of polynomial terms\n",
    "\n",
    "**Intuition**: \n",
    "- $\\gamma$ controls how much the inner product is \"stretched\" before raising to power\n",
    "- $r$ ensures that even when $\\langle x, z \\rangle = 0$, the kernel is non-zero\n",
    "- $d$ determines the complexity of the polynomial\n",
    "\n",
    "**Use case**: Polynomial regression, when you expect polynomial relationships in the data\n",
    "\n",
    "**Example**: For $d=2, \\gamma=1, r=1$:\n",
    "$$\n",
    "K(x, z) = (1 + \\langle x, z \\rangle)^2 = 1 + 2\\langle x, z \\rangle + \\langle x, z \\rangle^2\n",
    "$$\n",
    "\n",
    "### 5.4.3 Radial Basis Function (RBF) Kernel\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "K(x, z) = \\exp(-\\gamma \\|x - z\\|^2)\n",
    "$$\n",
    "\n",
    "**Feature map**: Infinite-dimensional (Mercer's theorem)\n",
    "\n",
    "**Parameter**: $\\gamma$ (bandwidth) - controls the \"reach\" of each training point\n",
    "\n",
    "**Intuition**: \n",
    "- Points close to each other have high similarity (kernel value close to 1)\n",
    "- Points far apart have low similarity (kernel value close to 0)\n",
    "- $\\gamma$ controls how quickly similarity decays with distance\n",
    "\n",
    "**Properties**:\n",
    "- **Universal**: Can approximate any continuous function\n",
    "- **Local**: Each training point influences only nearby regions\n",
    "- **Smooth**: Provides smooth decision boundaries\n",
    "\n",
    "**Use case**: Non-linear classification/regression, when data has no obvious structure (default choice)\n",
    "\n",
    "**Parameter selection**: \n",
    "- **Small $\\gamma$**: Wide influence, smooth boundaries, may underfit\n",
    "- **Large $\\gamma$**: Narrow influence, complex boundaries, may overfit\n",
    "\n",
    "### 5.4.4 Sigmoid Kernel\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "K(x, z) = \\tanh(\\gamma \\langle x, z \\rangle + r)\n",
    "$$\n",
    "\n",
    "**Feature map**: Neural network-like\n",
    "\n",
    "**Parameters**: \n",
    "- $\\gamma$ (scaling): Controls the steepness of the sigmoid\n",
    "- $r$ (bias): Shifts the sigmoid function\n",
    "\n",
    "**Intuition**: Similar to the activation function in neural networks\n",
    "\n",
    "**Use case**: Neural network approximation, when you want neural network-like behavior\n",
    "\n",
    "**Caution**: Not always positive definite, so not guaranteed to work with all algorithms\n",
    "\n",
    "### 5.4.5 Kernel Selection Guidelines\n",
    "\n",
    "**Decision Tree for Kernel Selection**:\n",
    "\n",
    "1. **Start with RBF Kernel** (default choice)\n",
    "   - Works well for most problems\n",
    "   - Has good theoretical properties\n",
    "   - Only one parameter to tune ($\\gamma$)\n",
    "\n",
    "2. **Try Linear Kernel** if:\n",
    "   - Data is high-dimensional\n",
    "   - You suspect linear separability\n",
    "   - Computational efficiency is important\n",
    "\n",
    "3. **Try Polynomial Kernel** if:\n",
    "   - You have domain knowledge suggesting polynomial relationships\n",
    "   - Data shows polynomial patterns\n",
    "   - You want to capture feature interactions\n",
    "\n",
    "4. **Try Sigmoid Kernel** if:\n",
    "   - You want neural network-like behavior\n",
    "   - Other kernels don't work well\n",
    "\n",
    "**Practical Tips**:\n",
    "- **Cross-validation**: Always use cross-validation to select kernel parameters\n",
    "- **Grid search**: Start with a coarse grid, then refine\n",
    "- **Multiple kernels**: Try different kernels and compare performance\n",
    "- **Domain knowledge**: Use domain knowledge to guide kernel selection\n",
    "\n",
    "## 5.5 Kernel Properties and Mercer's Theorem\n",
    "\n",
    "### 5.5.1 Positive Definite Kernels\n",
    "\n",
    "**Definition**: A kernel function $K$ is **positive definite** if for any finite set of points $x_1, \\ldots, x_n$ and any real numbers $c_1, \\ldots, c_n$:\n",
    "$$\n",
    "\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j K(x_i, x_j) \\geq 0\n",
    "$$\n",
    "\n",
    "**Intuition**: This means that the kernel matrix $K_{ij} = K(x_i, x_j)$ is positive semi-definite for any set of points.\n",
    "\n",
    "**Why this matters**: \n",
    "- Ensures that the kernel corresponds to an inner product in some feature space\n",
    "- Guarantees that optimization problems are well-behaved\n",
    "- Prevents numerical instability\n",
    "\n",
    "**Testing positive definiteness**:\n",
    "1. Compute the kernel matrix $K$\n",
    "2. Check if all eigenvalues are non-negative\n",
    "3. If yes, the kernel is positive definite\n",
    "\n",
    "### 5.5.2 Mercer's Theorem\n",
    "\n",
    "**Mercer's Theorem**: If $K$ is a positive definite kernel, then there exists a feature map $\\phi$ such that:\n",
    "$$\n",
    "K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle\n",
    "$$\n",
    "\n",
    "**Implications**:\n",
    "- Every positive definite kernel corresponds to an inner product in some feature space\n",
    "- We can work implicitly in infinite-dimensional spaces\n",
    "- The kernel trick is theoretically sound\n",
    "\n",
    "**Proof sketch**:\n",
    "1. Use the spectral decomposition of the kernel matrix\n",
    "2. Construct the feature map using the eigenvectors\n",
    "3. Show that the kernel equals the inner product in this space\n",
    "\n",
    "### 5.5.3 Kernel Construction Rules\n",
    "\n",
    "**Building New Kernels from Old Ones**\n",
    "\n",
    "If $K_1$ and $K_2$ are kernels, then the following are also kernels:\n",
    "\n",
    "1. **Scalar multiplication**: $aK_1$ where $a > 0$\n",
    "   - Intuition: Scaling doesn't change the fundamental structure\n",
    "   - Use case: Normalizing kernels\n",
    "\n",
    "2. **Addition**: $K_1 + K_2$\n",
    "   - Intuition: Combining different types of similarity\n",
    "   - Use case: Multiple kernel learning\n",
    "\n",
    "3. **Multiplication**: $K_1 \\cdot K_2$\n",
    "   - Intuition: Both similarities must be high for high kernel value\n",
    "   - Use case: Combining different data sources\n",
    "\n",
    "4. **Composition**: $K_1(f(x), f(z))$ where $f$ is any function\n",
    "   - Intuition: Apply a transformation before computing similarity\n",
    "   - Use case: Preprocessing data\n",
    "\n",
    "**Examples**:\n",
    "- **Sum of RBF and linear**: $K(x, z) = \\exp(-\\gamma \\|x - z\\|^2) + \\langle x, z \\rangle$\n",
    "- **Product of polynomial and RBF**: $K(x, z) = (\\langle x, z \\rangle + 1)^2 \\cdot \\exp(-\\gamma \\|x - z\\|^2)$\n",
    "\n",
    "## 5.6 Practical Considerations\n",
    "\n",
    "### 5.6.1 Computational Complexity\n",
    "\n",
    "**Training Complexity**:\n",
    "- **Kernel matrix computation**: $O(n^2d)$ where $n$ is number of samples, $d$ is input dimension\n",
    "- **Per iteration**: $O(n^2)$ for most algorithms\n",
    "- **Total**: $O(n^2d + Tn^2)$ where $T$ is number of iterations\n",
    "\n",
    "**Prediction Complexity**:\n",
    "- **Per prediction**: $O(nd)$ - need to compute kernel with all training points\n",
    "- **For large datasets**: This becomes the bottleneck\n",
    "\n",
    "**Memory Requirements**:\n",
    "- **Kernel matrix**: $O(n^2)$ storage\n",
    "- **For large datasets**: This becomes prohibitive\n",
    "\n",
    "**Example**: For $n=10,000$:\n",
    "- Kernel matrix: 100,000,000 entries\n",
    "- Memory: ~800 MB (assuming 8 bytes per entry)\n",
    "- Training time: Hours to days\n",
    "\n",
    "### 5.6.2 Scalability Solutions\n",
    "\n",
    "**1. Random Fourier Features**\n",
    "- **Idea**: Approximate RBF kernels using random projections\n",
    "- **Complexity**: $O(Dn)$ where $D$ is number of random features\n",
    "- **Trade-off**: Speed vs. accuracy\n",
    "\n",
    "**2. Nyström Method**\n",
    "- **Idea**: Approximate kernel matrix using subset of training points\n",
    "- **Complexity**: $O(m^2n)$ where $m$ is subset size\n",
    "- **Trade-off**: Memory vs. accuracy\n",
    "\n",
    "**3. Sparse Approximations**\n",
    "- **Idea**: Use only a subset of training points (support vectors)\n",
    "- **Complexity**: $O(sn)$ where $s$ is number of support vectors\n",
    "- **Trade-off**: Accuracy vs. speed\n",
    "\n",
    "### 5.6.3 Hyperparameter Tuning\n",
    "\n",
    "**Cross-validation**: Essential for kernel parameter selection\n",
    "\n",
    "**Grid search**: Common approach for parameter optimization\n",
    "\n",
    "**Example for RBF kernel**:\n",
    "```python\n",
    "# Grid of gamma values to try\n",
    "gamma_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# For each gamma, compute cross-validation score\n",
    "for gamma in gamma_values:\n",
    "    score = cross_validate(K_rbf(gamma), X, y)\n",
    "    print(f\"Gamma: {gamma}, Score: {score}\")\n",
    "```\n",
    "\n",
    "**Advanced techniques**:\n",
    "- **Bayesian optimization**: More efficient than grid search\n",
    "- **Gradient-based optimization**: For differentiable kernels\n",
    "- **Multi-objective optimization**: Balance accuracy and complexity\n",
    "\n",
    "*Implementation details for hyperparameter tuning are provided in the accompanying Python examples file.*\n",
    "\n",
    "## 5.7 Advanced Topics\n",
    "\n",
    "### 5.7.1 Multiple Kernel Learning\n",
    "\n",
    "**Motivation**: Different kernels capture different aspects of the data. Why not combine them?\n",
    "\n",
    "**Formulation**:\n",
    "$$\n",
    "K(x, z) = \\sum_{i=1}^m \\alpha_i K_i(x, z)\n",
    "$$\n",
    "where $\\alpha_i \\geq 0$ and $\\sum_{i=1}^m \\alpha_i = 1$\n",
    "\n",
    "**Example**: Combine linear, polynomial, and RBF kernels:\n",
    "$$\n",
    "K(x, z) = \\alpha_1 \\langle x, z \\rangle + \\alpha_2 (\\langle x, z \\rangle + 1)^2 + \\alpha_3 \\exp(-\\gamma \\|x - z\\|^2)\n",
    "$$\n",
    "\n",
    "**Optimization**: Learn both the kernel weights $\\alpha_i$ and the model parameters simultaneously.\n",
    "\n",
    "**Benefits**:\n",
    "- **Flexibility**: Can adapt to different parts of the data\n",
    "- **Robustness**: Less sensitive to kernel choice\n",
    "- **Performance**: Often better than single kernels\n",
    "\n",
    "### 5.7.2 Kernel PCA\n",
    "\n",
    "**Idea**: Perform Principal Component Analysis in the feature space.\n",
    "\n",
    "**Algorithm**:\n",
    "1. Compute kernel matrix $K$\n",
    "2. Center the kernel matrix: $K_{centered} = K - \\frac{1}{n}1_n K - \\frac{1}{n}K 1_n + \\frac{1}{n^2}1_n K 1_n$\n",
    "3. Find eigenvectors of $K_{centered}$\n",
    "4. Project data onto principal components\n",
    "\n",
    "**Use cases**:\n",
    "- **Dimensionality reduction**: Reduce dimension while preserving non-linear structure\n",
    "- **Feature extraction**: Extract non-linear features\n",
    "- **Visualization**: Visualize high-dimensional data\n",
    "\n",
    "### 5.7.3 Kernel Ridge Regression\n",
    "\n",
    "**Formulation**: Ridge regression with kernels:\n",
    "$$\n",
    "\\beta = (K + \\lambda I)^{-1} y\n",
    "$$\n",
    "\n",
    "**Properties**:\n",
    "- **Regularization**: $\\lambda$ controls complexity\n",
    "- **Closed-form solution**: No iterative optimization needed\n",
    "- **Kernel flexibility**: Can use any positive definite kernel\n",
    "\n",
    "**Comparison with standard ridge regression**:\n",
    "- **Standard**: $\\theta = (X^T X + \\lambda I)^{-1} X^T y$\n",
    "- **Kernel**: $\\beta = (K + \\lambda I)^{-1} y$\n",
    "\n",
    "## 5.8 Summary and Key Insights\n",
    "\n",
    "### 5.8.1 The Kernel Trick\n",
    "\n",
    "**The Three Pillars**:\n",
    "\n",
    "1. **Representer Theorem**: $\\theta$ can be written as linear combination of training features\n",
    "   - Enables dual formulation\n",
    "   - Reduces optimization complexity\n",
    "   - Foundation for kernel methods\n",
    "\n",
    "2. **Kernel Function**: $K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle$ can be computed efficiently\n",
    "   - Avoids explicit feature computation\n",
    "   - Enables infinite-dimensional feature spaces\n",
    "   - Provides computational efficiency\n",
    "\n",
    "3. **Dual Representation**: Work with $\\beta$ coefficients instead of $\\theta$\n",
    "   - Dimensionality is number of training points, not feature space dimension\n",
    "   - Enables kernelization of any inner product-based algorithm\n",
    "   - Provides interpretability through support vectors\n",
    "\n",
    "### 5.8.2 Computational Benefits\n",
    "\n",
    "**Before Kernel Trick**:\n",
    "- **Explicit features**: $O(d^k)$ computation\n",
    "- **Memory**: $O(d^k)$ storage per data point\n",
    "- **Scalability**: Limited to small feature spaces\n",
    "\n",
    "**After Kernel Trick**:\n",
    "- **Kernel computation**: $O(d)$ computation\n",
    "- **Memory**: $O(n^2)$ for kernel matrix\n",
    "- **Scalability**: Can handle infinite-dimensional feature spaces\n",
    "\n",
    "**Example**: For degree-3 polynomial features in 1000 dimensions:\n",
    "- **Explicit**: 1,000,000,000 operations per data point\n",
    "- **Kernel**: 1,000 operations per data point\n",
    "- **Speedup**: 1,000,000x improvement\n",
    "\n",
    "### 5.8.3 When to Use Kernels\n",
    "\n",
    "**Use kernels when**:\n",
    "- **Data is non-linear**: Linear models fail to capture patterns\n",
    "- **Feature space is high-dimensional**: Explicit computation is expensive\n",
    "- **Explicit feature computation is expensive**: Curse of dimensionality\n",
    "- **You want flexibility**: Different kernels for different problems\n",
    "- **Theoretical guarantees matter**: Mercer's theorem provides soundness\n",
    "\n",
    "**Avoid kernels when**:\n",
    "- **Data is linear**: Linear models work well\n",
    "- **Dataset is very large**: Memory and computation become prohibitive\n",
    "- **Interpretability is important**: Kernel methods are less interpretable\n",
    "- **Real-time prediction is needed**: $O(n)$ prediction cost\n",
    "- **Feature engineering is preferred**: You want explicit control over features\n",
    "\n",
    "### 5.8.4 The Broader Impact\n",
    "\n",
    "**The kernel trick is one of the most powerful ideas in machine learning**, allowing us to:\n",
    "\n",
    "- **Work in infinite-dimensional spaces** with finite computation\n",
    "- **Capture complex non-linear patterns** efficiently\n",
    "- **Apply linear algorithms** to non-linear problems\n",
    "- **Unify many algorithms** under a common framework\n",
    "\n",
    "**Historical significance**:\n",
    "- **Revolutionized machine learning** in the 1990s and 2000s\n",
    "- **Led to the development of SVMs** and other kernel methods\n",
    "- **Influenced modern deep learning** (neural tangent kernels)\n",
    "- **Continues to inspire new research** in machine learning\n",
    "\n",
    "**Future directions**:\n",
    "- **Deep kernels**: Combining kernels with deep learning\n",
    "- **Graph kernels**: Kernels for structured data\n",
    "- **Quantum kernels**: Kernels for quantum computing\n",
    "- **Automatic kernel learning**: Learning optimal kernels from data\n",
    "\n",
    "The kernel trick demonstrates the power of mathematical abstraction in machine learning - by working with inner products rather than explicit features, we can achieve remarkable computational efficiency while maintaining theoretical rigor.\n",
    "\n",
    "## From Kernel Methods to Mathematical Foundations\n",
    "\n",
    "We've now explored the **kernel trick** - one of the most powerful ideas in machine learning that allows us to work in high-dimensional feature spaces without explicitly computing the features. We've seen how kernels can capture complex non-linear patterns efficiently, from polynomial features to radial basis functions, and how they enable algorithms to operate in infinite-dimensional spaces with finite computation.\n",
    "\n",
    "However, having this computational tool raises a fundamental question: **What makes a function a valid kernel?** Not every function $K(x, z)$ corresponds to an inner product in some feature space. We need mathematical criteria to distinguish valid kernels from invalid ones.\n",
    "\n",
    "This motivates our exploration of **kernel properties** - the mathematical foundations that tell us which functions can serve as kernels. We'll learn about positive semi-definiteness, Mercer's theorem, and the conditions that guarantee a function corresponds to a valid feature map.\n",
    "\n",
    "The transition from kernel methods to kernel properties represents the bridge from computational techniques to mathematical rigor - understanding not just how to use kernels, but why they work and how to design new ones.\n",
    "\n",
    "In the next section, we'll explore the mathematical properties that make kernels valid and learn how to test whether a given function can serve as a kernel.\n",
    "\n",
    "---\n",
    "\n",
    "**Next: [Kernel Properties](02_kernel_properties.md)** - Understand the mathematical foundations that make kernels valid and learn how to design new kernels.\n",
    "\n",
    "[1]: Here, for simplicity, we include all the monomials with repetitions (so that, e.g., $x_1 x_2 x_3$ and $x_2 x_3 x_1$ both appear in $\\phi(x)$). Therefore, there are totally $1 + d + d^2 + d^3$ entries in $\\phi(x)$.\n",
    "\n",
    "[2]: Recall that $\\mathcal{X}$ is the space of the input $x$. In our running example, $\\mathcal{X} = \\mathbb{R}^d$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
