# 11.5 Variational inference and variational auto-encoder

Loosely speaking, variational auto-encoder (Kingma and Welling [2013]) generally refers to a family of algorithms that extend the EM algorithms to more complex models parameterized by neural networks. It extends the technique of variational inference with the additional "re-parameterization trick" which will be introduced below. Variational auto-encoder may not give the best performance for many datasets, but it contains several central ideas about how to extend EM algorithms to high-dimensional continuous latent variables with non-linear models. Understanding it will likely give you the language and backgrounds to understand various recent papers related to it.

## Neural Network Parameterization and the Generative Model

As a running example, we will consider the following parameterization of $`p(x, z; \theta)`$ by a neural network. Let $`\theta`$ be the collection of the weights of a neural network $`g(z; \theta)`$ that maps $`z \in \mathbb{R}^k`$ to $`\mathbb{R}^d`$.

- **Latent variable prior:**

```math
z \sim \mathcal{N}(0, I_{k \times k})
```

- **Observation model (decoder):**

```math
x|z \sim \mathcal{N}(g(z; \theta), \sigma^2 I_{d \times d})
```

Here $`I_{k \times k}`$ denotes the identity matrix of dimension $`k`$ by $`k`$, and $`\sigma`$ is a scalar that we assume to be known for simplicity.

**Intuition:**
- The latent variable $`z`$ is drawn from a standard normal distribution.
- The observed data $`x`$ is generated by passing $`z`$ through a neural network $`g(z; \theta)`$ (the decoder), and then adding Gaussian noise.
- This setup allows us to model complex, non-linear relationships between $`z`$ and $`x`$.

### Python Implementation: Generative Model

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, Independent
import matplotlib.pyplot as plt

class GenerativeModel:
    """
    Implements the generative model p(x, z; θ) = p(z) * p(x|z; θ)
    where z ~ N(0, I) and x|z ~ N(g(z; θ), σ²I)
    """
    
    def __init__(self, latent_dim=2, data_dim=784, hidden_dim=512, sigma=1.0):
        self.latent_dim = latent_dim
        self.data_dim = data_dim
        self.sigma = sigma
        
        # Decoder network g(z; θ) - maps from latent space to data space
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, data_dim),
            nn.Sigmoid()  # Output in [0,1] for image data
        )
    
    def sample_prior(self, batch_size):
        """Sample z from the prior p(z) = N(0, I)"""
        return torch.randn(batch_size, self.latent_dim)
    
    def decode(self, z):
        """Generate x from z using the decoder network"""
        return self.decoder(z)
    
    def sample_data(self, z):
        """Sample x from p(x|z) = N(g(z; θ), σ²I)"""
        mu = self.decode(z)
        # For continuous data, add Gaussian noise
        # For discrete data (like images), we might use Bernoulli distribution
        return mu + self.sigma * torch.randn_like(mu)
    
    def log_prob_data_given_latent(self, x, z):
        """Compute log p(x|z)"""
        mu = self.decode(z)
        # Assuming Gaussian observation model
        log_prob = -0.5 * ((x - mu) / self.sigma)**2 - np.log(self.sigma * np.sqrt(2 * np.pi))
        return log_prob.sum(dim=-1)
    
    def log_prob_prior(self, z):
        """Compute log p(z)"""
        return -0.5 * (z**2).sum(dim=-1) - 0.5 * self.latent_dim * np.log(2 * np.pi)
    
    def log_prob_joint(self, x, z):
        """Compute log p(x, z) = log p(z) + log p(x|z)"""
        return self.log_prob_prior(z) + self.log_prob_data_given_latent(x, z)

# Example usage
if __name__ == "__main__":
    # Create generative model
    gen_model = GenerativeModel(latent_dim=2, data_dim=4, sigma=0.1)
    
    # Sample from prior and generate data
    z_samples = gen_model.sample_prior(batch_size=5)
    x_samples = gen_model.sample_data(z_samples)
    
    print("Latent samples z:")
    print(z_samples)
    print("\nGenerated data x:")
    print(x_samples)
    print("\nJoint log probability:")
    print(gen_model.log_prob_joint(x_samples, z_samples))
```

## The Challenge of Posterior Inference

For the Gaussian mixture models in Section 11.4, the optimal choice of $`Q(z) = p(z|x; \theta)`$ for each fixed $`\theta`$ (the posterior distribution of $`z`$) can be analytically computed. In many more complex models such as the model (11.19), it's intractable to compute the exact posterior $`p(z|x; \theta)`$.

**Why is this hard?**
- The posterior $`p(z|x; \theta)`$ may not have a closed-form solution when $`g(z; \theta)`$ is a neural network.
- We need to approximate the posterior with a simpler, tractable distribution $`Q(z)`$.

### Python Implementation: Posterior Inference Challenge

```python
def demonstrate_posterior_intractability():
    """
    Demonstrate why posterior inference is intractable for neural network models
    """
    print("=== Posterior Inference Challenge ===")
    
    # For a simple linear model, posterior is tractable
    print("1. Linear model: z ~ N(0,1), x|z ~ N(az + b, σ²)")
    print("   Posterior: z|x ~ N(μ_post, σ²_post) - CLOSED FORM!")
    
    # For neural network model, posterior is intractable
    print("\n2. Neural network model: z ~ N(0,1), x|z ~ N(NN(z), σ²)")
    print("   Posterior: z|x ~ ??? - NO CLOSED FORM!")
    print("   We need to approximate with Q(z) ≈ p(z|x)")
    
    # Monte Carlo estimation (expensive and not practical for training)
    print("\n3. Monte Carlo estimation would require:")
    print("   - Sampling many z values")
    print("   - Computing p(x|z) for each")
    print("   - Normalizing - EXPENSIVE!")
    
    return True

# Demonstrate the challenge
demonstrate_posterior_intractability()
```

## Variational Inference and the ELBO

Recall from equation (11.10), ELBO is always a lower bound for any choice of $`Q`$, and therefore, we can also aim for finding an **approximation** of the true posterior distribution. Often, one has to use some particular form to approximate the true posterior distribution. Let $`\mathcal{Q}`$ be a family of $`Q`$'s that we are considering, and we will aim to find a $`Q`$ within the family of $`\mathcal{Q}`$ that is closest to the true posterior distribution. To formalize, recall the definition of the ELBO lower bound as a function of $`Q`$ and $`\theta`$ defined in equation (11.14):

```math
\mathrm{ELBO}(Q, \theta) = \sum_{i=1}^n \mathrm{ELBO}(x^{(i)}; Q_i, \theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
```

**Intuition:**
- The ELBO is a lower bound on the log-likelihood of the data.
- Maximizing the ELBO with respect to $`Q`$ and $`\theta`$ allows us to learn both the generative model and the approximate posterior.
- In variational auto-encoders, we optimize the ELBO over both $`Q \in \mathcal{Q}`$ and $`\theta`$:

```math
\max_{Q \in \mathcal{Q}} \max_{\theta} \mathrm{ELBO}(Q, \theta)
```

### Python Implementation: ELBO Computation

```python
def compute_elbo_components(x, z, q_z, log_p_xz, log_p_z):
    """
    Compute ELBO components for a single data point
    
    ELBO = E_{z~Q}[log p(x,z) - log Q(z)]
         = E_{z~Q}[log p(x|z) + log p(z) - log Q(z)]
         = E_{z~Q}[log p(x|z)] + E_{z~Q}[log p(z) - log Q(z)]
         = Reconstruction term + KL divergence term
    """
    # Reconstruction term: E_{z~Q}[log p(x|z)]
    reconstruction_term = log_p_xz
    
    # KL divergence term: E_{z~Q}[log p(z) - log Q(z)] = KL(Q||p)
    kl_term = log_p_z - q_z.log_prob(z)
    
    return reconstruction_term, kl_term

def compute_elbo_monte_carlo(x, q_dist, generative_model, n_samples=10):
    """
    Compute ELBO using Monte Carlo estimation
    ELBO = E_{z~Q}[log p(x,z) - log Q(z)]
    """
    batch_size = x.shape[0]
    total_elbo = 0
    
    for _ in range(n_samples):
        # Sample from approximate posterior
        z = q_dist.sample()
        
        # Compute log probabilities
        log_q_z = q_dist.log_prob(z)
        log_p_xz = generative_model.log_prob_data_given_latent(x, z)
        log_p_z = generative_model.log_prob_prior(z)
        
        # Compute ELBO for this sample
        elbo_sample = log_p_xz + log_p_z - log_q_z
        total_elbo += elbo_sample
    
    return total_elbo / n_samples

# Example ELBO computation
def demonstrate_elbo():
    print("\n=== ELBO Computation ===")
    
    # Create simple example
    x = torch.randn(2, 4)  # 2 data points, 4 dimensions
    q_mean = torch.randn(2, 2)  # 2 latent dimensions
    q_std = torch.ones(2, 2) * 0.1
    
    q_dist = Independent(Normal(q_mean, q_std), 1)
    z = q_dist.sample()
    
    # Compute components
    log_q_z = q_dist.log_prob(z)
    log_p_z = -0.5 * (z**2).sum(dim=-1) - np.log(2 * np.pi)
    log_p_xz = -0.5 * ((x - z.mean(dim=-1, keepdim=True))**2).sum(dim=-1)
    
    reconstruction_term, kl_term = compute_elbo_components(x, z, q_dist, log_p_xz, log_p_z)
    elbo = reconstruction_term + kl_term
    
    print(f"Reconstruction term: {reconstruction_term.mean():.4f}")
    print(f"KL divergence term: {kl_term.mean():.4f}")
    print(f"Total ELBO: {elbo.mean():.4f}")
    
    return elbo

demonstrate_elbo()
```

## The Mean Field Assumption and Parameterization of Q

Now the next question is what form of $`Q`$ (or what structural assumptions to make about $`Q`$) allows us to efficiently maximize the objective above. When the latent variable $`z`$ are high-dimensional discrete variables, one popular assumption is the **mean field assumption**, which assumes that $`Q_i(z)`$ gives a distribution with independent coordinates, or in other words, $`Q_i`$ can be decomposed into $`Q_i(z) = Q_i^1(z_1) \cdots Q_i^k(z_k)`$.

- **Why mean field?**
  - It makes the optimization tractable by reducing dependencies between latent variables.
  - Widely used in probabilistic graphical models and variational inference.

There are tremendous applications of mean field assumptions to learning generative models with discrete latent variables, and we refer to Blei et al. [2017] for a survey of these models and their impact to a wide range of applications including computational biology, computational neuroscience, social sciences. We will not get into the details about the discrete latent variable cases, and our main focus is to deal with continuous latent variables, which requires not only mean field assumptions, but additional techniques.

### Python Implementation: Mean Field Assumption

```python
class MeanFieldApproximation:
    """
    Implements mean field approximation for continuous latent variables
    Q(z) = Q_1(z_1) * Q_2(z_2) * ... * Q_k(z_k)
    where each Q_i is a Gaussian distribution
    """
    
    def __init__(self, latent_dim):
        self.latent_dim = latent_dim
    
    def create_mean_field_distribution(self, means, stds):
        """
        Create a mean field distribution where each dimension is independent
        
        Args:
            means: [batch_size, latent_dim] - means for each dimension
            stds: [batch_size, latent_dim] - standard deviations for each dimension
        """
        # Each dimension is independent, so we use Independent distribution
        return Independent(Normal(means, stds), 1)
    
    def sample_from_mean_field(self, means, stds, n_samples=1):
        """Sample from mean field approximation"""
        q_dist = self.create_mean_field_distribution(means, stds)
        return q_dist.sample((n_samples,))
    
    def compute_kl_divergence(self, q_means, q_stds, p_means=None, p_stds=None):
        """
        Compute KL divergence between Q (mean field) and P (prior)
        KL(Q||P) where P is typically N(0,1) for each dimension
        """
        if p_means is None:
            p_means = torch.zeros_like(q_means)
        if p_stds is None:
            p_stds = torch.ones_like(q_stds)
        
        # KL divergence for Gaussian distributions
        kl_div = 0.5 * (
            (q_stds / p_stds)**2 + 
            ((q_means - p_means) / p_stds)**2 - 
            1 - 
            2 * torch.log(q_stds / p_stds)
        )
        
        return kl_div.sum(dim=-1)

# Example mean field approximation
def demonstrate_mean_field():
    print("\n=== Mean Field Approximation ===")
    
    mf = MeanFieldApproximation(latent_dim=3)
    
    # Create mean field distribution
    batch_size = 4
    q_means = torch.randn(batch_size, 3)
    q_stds = torch.ones(batch_size, 3) * 0.5
    
    # Sample from mean field
    samples = mf.sample_from_mean_field(q_means, q_stds, n_samples=10)
    print(f"Mean field samples shape: {samples.shape}")
    
    # Compute KL divergence with prior
    kl_div = mf.compute_kl_divergence(q_means, q_stds)
    print(f"KL divergence with prior: {kl_div}")
    
    return mf

mean_field_example = demonstrate_mean_field()
```

## Gaussian Q and the Encoder/Decoder

When $`z \in \mathbb{R}^k`$ is a continuous latent variable, there are several decisions to make towards successfully optimizing (11.20). First we need to give a succinct representation of the distribution $`Q_i`$ because it is over an infinite number of points. A natural choice is to assume $`Q_i`$ is a Gaussian distribution with some mean and variance. We would also like to have more succinct representation of the means of $`Q_i`$ of all the examples. Note that $`Q_i(z^{(i)})`$ is supposed to approximate $`p(z^{(i)}|x^{(i)}; \theta)`$. It would make sense let all the means of the $`Q_i`$'s be some function of $`x^{(i)}`$.

- **Parameterization:**
  - Let $`q(\cdot; \phi)`$, $`v(\cdot; \psi)`$ be two functions (often neural networks) that map from dimension $`d`$ to $`k`$, parameterized by $`\phi`$ and $`\psi`$.
  - We assume that

```math
Q_i = \mathcal{N}(q(x^{(i)}; \phi), \text{diag}(v(x^{(i)}; \psi))^2)
```

Here $`\text{diag}(w)`$ means the $`k \times k`$ matrix with the entries of $`w \in \mathbb{R}^k`$ on the diagonal. In other words, the distribution $`Q_i`$ is assumed to be a Gaussian distribution with independent coordinates, and the mean and standard deviations are governed by $`q`$ and $`v`$.

- In variational auto-encoder, $`q`$ and $`v`$ are often neural networks.
- In deep learning literature, $`q, v`$ are called the **encoder** (encoding the data into latent code), whereas $`g(z; \theta)`$ is the **decoder** (generating data from latent code).

**Note:** $`Q_i`$ of such form in many cases are very far from a good approximation of the true posterior distribution. However, some approximation is necessary for feasible optimization. The form of $`Q_i`$ needs to satisfy other requirements (which happened to be satisfied by the form (11.21)).

### Python Implementation: Encoder Network

```python
class Encoder(nn.Module):
    """
    Encoder network that parameterizes the approximate posterior Q(z|x)
    Maps x to parameters of Q(z|x) = N(μ(x), σ²(x))
    """
    
    def __init__(self, data_dim, latent_dim, hidden_dim=512):
        super().__init__()
        self.data_dim = data_dim
        self.latent_dim = latent_dim
        
        # Encoder network that outputs both mean and log variance
        self.encoder = nn.Sequential(
            nn.Linear(data_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2 * latent_dim)  # μ and log σ²
        )
    
    def forward(self, x):
        """
        Encode x to parameters of Q(z|x)
        
        Args:
            x: [batch_size, data_dim]
            
        Returns:
            mu: [batch_size, latent_dim] - mean of Q(z|x)
            log_var: [batch_size, latent_dim] - log variance of Q(z|x)
        """
        h = self.encoder(x)
        mu = h[:, :self.latent_dim]
        log_var = h[:, self.latent_dim:]
        return mu, log_var
    
    def create_posterior(self, x):
        """
        Create the approximate posterior distribution Q(z|x)
        
        Args:
            x: [batch_size, data_dim]
            
        Returns:
            q_dist: Independent Normal distribution
        """
        mu, log_var = self.forward(x)
        std = torch.exp(0.5 * log_var)  # Convert log_var to std
        return Independent(Normal(mu, std), 1)

# Example encoder usage
def demonstrate_encoder():
    print("\n=== Encoder Network ===")
    
    data_dim = 784  # MNIST image size
    latent_dim = 10
    batch_size = 4
    
    encoder = Encoder(data_dim, latent_dim)
    x = torch.randn(batch_size, data_dim)
    
    # Get posterior parameters
    mu, log_var = encoder(x)
    print(f"Posterior mean shape: {mu.shape}")
    print(f"Posterior log variance shape: {log_var.shape}")
    
    # Create posterior distribution
    q_dist = encoder.create_posterior(x)
    z_samples = q_dist.sample()
    print(f"Sampled z shape: {z_samples.shape}")
    
    return encoder

encoder_example = demonstrate_encoder()
```

## The ELBO for Continuous Latent Variables

Before optimizing the ELBO, let's first verify whether we can efficiently evaluate the value of the ELBO for fixed $`Q`$ of the form (11.21) and $`\theta`$. We can rewrite the ELBO as a function of $`\phi, \psi, \theta`$ by

```math
\mathrm{ELBO}(\phi, \psi, \theta) = \sum_{i=1}^n \mathbb{E}_{z^{(i)} \sim Q_i} \left[ \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \right]
```

where $`Q_i = \mathcal{N}(q(x^{(i)}; \phi), \text{diag}(v(x^{(i)}; \psi))^2)`$.

- To evaluate $`Q_i(z^{(i)})`$ inside the expectation, we need to be able to **compute the density** of $`Q_i`$.
- To estimate the expectation $`\mathbb{E}_{z^{(i)} \sim Q_i}`$, we need to be able to **sample from** $`Q_i`$ (i.e., draw random samples).
- For Gaussian $`Q_i`$, both are efficient.

### Python Implementation: ELBO for Continuous Variables

```python
def compute_vae_elbo(x, encoder, decoder, n_samples=1):
    """
    Compute VAE ELBO using Monte Carlo estimation
    
    ELBO = E_{z~Q(z|x)}[log p(x|z) + log p(z) - log Q(z|x)]
         = E_{z~Q(z|x)}[log p(x|z)] - KL(Q(z|x) || p(z))
    
    Args:
        x: [batch_size, data_dim] - input data
        encoder: Encoder network
        decoder: Decoder network
        n_samples: number of samples for Monte Carlo estimation
        
    Returns:
        elbo: scalar - ELBO value
        reconstruction_loss: scalar - reconstruction term
        kl_loss: scalar - KL divergence term
    """
    batch_size = x.shape[0]
    
    # Get posterior parameters
    mu, log_var = encoder(x)
    std = torch.exp(0.5 * log_var)
    
    # Create posterior distribution
    q_dist = Independent(Normal(mu, std), 1)
    
    # Sample from posterior
    z = q_dist.sample((n_samples,))  # [n_samples, batch_size, latent_dim]
    z = z.transpose(0, 1)  # [batch_size, n_samples, latent_dim]
    
    # Compute reconstruction term: E[log p(x|z)]
    x_recon = decoder(z.view(-1, z.shape[-1]))  # [batch_size * n_samples, data_dim]
    x_recon = x_recon.view(batch_size, n_samples, -1)  # [batch_size, n_samples, data_dim]
    
    # Assuming Gaussian observation model
    reconstruction_loss = -0.5 * ((x.unsqueeze(1) - x_recon) ** 2).sum(dim=-1)
    reconstruction_loss = reconstruction_loss.mean(dim=1)  # Average over samples
    
    # Compute KL divergence: KL(Q(z|x) || p(z))
    # KL divergence between two Gaussians has closed form
    kl_loss = -0.5 * (1 + log_var - mu**2 - torch.exp(log_var))
    kl_loss = kl_loss.sum(dim=-1)
    
    # Total ELBO
    elbo = reconstruction_loss - kl_loss
    
    return elbo.mean(), reconstruction_loss.mean(), kl_loss.mean()

# Example ELBO computation
def demonstrate_vae_elbo():
    print("\n=== VAE ELBO Computation ===")
    
    data_dim = 784
    latent_dim = 10
    batch_size = 4
    
    encoder = Encoder(data_dim, latent_dim)
    decoder = nn.Sequential(
        nn.Linear(latent_dim, 512),
        nn.ReLU(),
        nn.Linear(512, data_dim),
        nn.Sigmoid()
    )
    
    x = torch.randn(batch_size, data_dim)
    
    elbo, recon_loss, kl_loss = compute_vae_elbo(x, encoder, decoder)
    
    print(f"ELBO: {elbo:.4f}")
    print(f"Reconstruction loss: {recon_loss:.4f}")
    print(f"KL divergence: {kl_loss:.4f}")
    
    return elbo, recon_loss, kl_loss

demonstrate_vae_elbo()
```

## Gradient Ascent and the Reparameterization Trick

Now let's optimize the ELBO. It turns out that we can run gradient ascent over $`\phi, \psi, \theta`$ instead of alternating maximization. There is no strong need to compute the maximum over each variable at a much greater cost. (For Gaussian mixture model in Section 11.4, computing the maximum is analytically feasible and relatively cheap, and therefore we did alternating maximization.)

Mathematically, let $`\eta`$ be the learning rate, the gradient ascent step is

```math
\theta := \theta + \eta \nabla_{\theta} \mathrm{ELBO}(\phi, \psi, \theta)
\phi := \phi + \eta \nabla_{\phi} \mathrm{ELBO}(\phi, \psi, \theta)
\psi := \psi + \eta \nabla_{\psi} \mathrm{ELBO}(\phi, \psi, \theta)
```

- **Gradient over $`\theta`$:**

```math
\nabla_{\theta} \mathrm{ELBO}(\phi, \psi, \theta) = \sum_{i=1}^n \mathbb{E}_{z^{(i)} \sim Q_i} \left[ \nabla_{\theta} \log p(x^{(i)}, z^{(i)}; \theta) \right]
```

- **Gradient over $`\phi, \psi`$:**
  - Trickier, because the sampling distribution $`Q_i`$ depends on $`\phi, \psi`$.
  - In general, $`\nabla_{\phi} \mathbb{E}_{z \sim Q_{\phi}}[f(\phi)] \neq \mathbb{E}_{z \sim Q_{\phi}}[\nabla_{\phi} f(\phi)]`$ because the dependency of $`Q_{\phi}`$ on $`\phi`$ has to be taken into account as well.

### The Reparameterization Trick

The idea that comes to rescue is the so-called **re-parameterization trick**: we rewrite $`z^{(i)} \sim Q_i = \mathcal{N}(q(x^{(i)}; \phi), \text{diag}(v(x^{(i)}; \psi))^2)`$ in an equivalent way:

```math
z^{(i)} = q(x^{(i)}; \phi) + v(x^{(i)}; \psi) \odot \xi^{(i)} \quad \text{where} \quad \xi^{(i)} \sim \mathcal{N}(0, I_{k \times k})
```

Here $`x \odot y`$ denotes the entry-wise product of two vectors of the same dimension. This is just the standard way to sample from a Gaussian: $`x = \mu + \sigma \xi`$ with $`\xi \sim N(0, 1)`$.

With this re-parameterization, we have that

```math
\mathbb{E}_{z^{(i)} \sim Q_i} \left[ \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \right]
= \mathbb{E}_{\xi^{(i)} \sim \mathcal{N}(0, 1)} \left[ \log \frac{p(x^{(i)}, q(x^{(i)}; \phi) + v(x^{(i)}; \psi) \odot \xi^{(i)}; \theta)}{Q_i(q(x^{(i)}; \phi) + v(x^{(i)}; \psi) \odot \xi^{(i)})} \right]
```

- This allows us to sample $`\xi^{(i)}`$ from a standard normal, and then transform it to get samples from $`Q_i`$.
- We can now sample multiple copies of $`\xi^{(i)}`$'s to estimate the expectation in the RHS of the equation above.
- We can estimate the gradient with respect to $`\psi`$ similarly, and with these, we can implement the gradient ascent algorithm to optimize the ELBO over $`\phi, \psi, \theta`$.

**Note:** There are not many high-dimensional distributions with analytically computable density function that are known to be re-parameterizable. We refer to Kingma and Welling (2013) for a few other choices that can replace Gaussian distribution.

### Python Implementation: Reparameterization Trick and Complete VAE

```python
class VAE(nn.Module):
    """
    Complete Variational Auto-Encoder implementation
    """
    
    def __init__(self, data_dim, latent_dim, hidden_dim=512):
        super().__init__()
        self.data_dim = data_dim
        self.latent_dim = latent_dim
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(data_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2 * latent_dim)  # μ and log σ²
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, data_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        """Encode x to posterior parameters"""
        h = self.encoder(x)
        mu = h[:, :self.latent_dim]
        log_var = h[:, self.latent_dim:]
        return mu, log_var
    
    def reparameterize(self, mu, log_var):
        """
        Reparameterization trick: z = μ + σ * ε, where ε ~ N(0,1)
        This allows gradients to flow through the sampling process
        """
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)  # Sample from N(0,1)
        z = mu + eps * std  # Reparameterization trick
        return z
    
    def decode(self, z):
        """Decode z to reconstructed data"""
        return self.decoder(z)
    
    def forward(self, x):
        """Forward pass through VAE"""
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        x_recon = self.decode(z)
        return x_recon, mu, log_var
    
    def loss_function(self, x, x_recon, mu, log_var, beta=1.0):
        """
        Compute VAE loss: reconstruction loss + β * KL divergence
        
        Args:
            x: original data
            x_recon: reconstructed data
            mu: posterior mean
            log_var: posterior log variance
            beta: weight for KL divergence (β-VAE)
        """
        # Reconstruction loss (assuming Bernoulli for binary data)
        recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
        
        # KL divergence: KL(N(μ,σ²) || N(0,1))
        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        
        # Total loss
        total_loss = recon_loss + beta * kl_loss
        
        return total_loss, recon_loss, kl_loss
    
    def sample(self, n_samples=1):
        """Generate samples from the learned model"""
        z = torch.randn(n_samples, self.latent_dim)
        with torch.no_grad():
            samples = self.decode(z)
        return samples

# Training function
def train_vae(vae, dataloader, optimizer, device, epochs=10):
    """Train VAE model"""
    vae.train()
    
    for epoch in range(epochs):
        total_loss = 0
        total_recon_loss = 0
        total_kl_loss = 0
        num_batches = 0
        
        for batch_idx, (data, _) in enumerate(dataloader):
            data = data.to(device)
            data = data.view(data.size(0), -1)  # Flatten
            
            optimizer.zero_grad()
            
            # Forward pass
            x_recon, mu, log_var = vae(data)
            
            # Compute loss
            loss, recon_loss, kl_loss = vae.loss_function(data, x_recon, mu, log_var)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_kl_loss += kl_loss.item()
            num_batches += 1
        
        # Print progress
        avg_loss = total_loss / num_batches
        avg_recon = total_recon_loss / num_batches
        avg_kl = total_kl_loss / num_batches
        
        print(f'Epoch [{epoch+1}/{epochs}], '
              f'Loss: {avg_loss:.4f}, '
              f'Recon: {avg_recon:.4f}, '
              f'KL: {avg_kl:.4f}')

# Example usage and demonstration
def demonstrate_complete_vae():
    print("\n=== Complete VAE Implementation ===")
    
    # Create VAE
    data_dim = 784
    latent_dim = 20
    vae = VAE(data_dim, latent_dim)
    
    # Create dummy data
    batch_size = 32
    x = torch.rand(batch_size, data_dim)
    
    # Forward pass
    x_recon, mu, log_var = vae(x)
    
    # Compute loss
    loss, recon_loss, kl_loss = vae.loss_function(x, x_recon, mu, log_var)
    
    print(f"VAE loss: {loss:.4f}")
    print(f"Reconstruction loss: {recon_loss:.4f}")
    print(f"KL divergence: {kl_loss:.4f}")
    
    # Generate samples
    samples = vae.sample(n_samples=5)
    print(f"Generated samples shape: {samples.shape}")
    
    # Demonstrate reparameterization trick
    print("\n--- Reparameterization Trick ---")
    mu = torch.randn(2, 3)
    log_var = torch.randn(2, 3)
    
    # Without reparameterization (no gradients)
    std = torch.exp(0.5 * log_var)
    z_no_grad = mu + torch.randn_like(std) * std
    print(f"z without reparameterization: {z_no_grad}")
    
    # With reparameterization (gradients flow)
    z_with_grad = vae.reparameterize(mu, log_var)
    print(f"z with reparameterization: {z_with_grad}")
    
    return vae

# Run complete demonstration
complete_vae = demonstrate_complete_vae()

# Save the complete implementation
if __name__ == "__main__":
    print("\n=== Summary ===")
    print("This implementation includes:")
    print("1. Generative model with neural network decoder")
    print("2. Encoder network for approximate posterior")
    print("3. Mean field approximation")
    print("4. ELBO computation with Monte Carlo estimation")
    print("5. Reparameterization trick for gradient flow")
    print("6. Complete VAE training framework")
    print("7. Sample generation from learned model")
    
    print("\nKey concepts demonstrated:")
    print("- Posterior intractability and variational approximation")
    print("- ELBO as a lower bound on log-likelihood")
    print("- Mean field assumption for tractable inference")
    print("- Reparameterization trick for gradient-based optimization")
    print("- Encoder-decoder architecture")
    print("- Trade-off between reconstruction and KL divergence")
```

This comprehensive implementation covers all the key concepts from the variational auto-encoder section:

1. **Generative Model**: Neural network parameterization with prior and observation models
2. **Posterior Inference Challenge**: Why exact inference is intractable
3. **ELBO Computation**: Evidence Lower BOund with Monte Carlo estimation
4. **Mean Field Approximation**: Independent latent variables for tractability
5. **Encoder Network**: Neural network to parameterize approximate posterior
6. **Reparameterization Trick**: Enabling gradient flow through sampling
7. **Complete VAE**: Full implementation with training and sampling

The code includes detailed comments, examples, and demonstrations of each concept, making it easier for readers to understand and implement variational auto-encoders.

