# Expectation-Maximization (EM) Algorithms: Mixture of Gaussians

## Motivation: Why Mixture Models? The Story of Hidden Patterns

Imagine you're a detective trying to solve a mystery. You have a collection of footprints in the snow, but you don't know which footprints belong to which person. Some footprints are large and deep (probably from a heavy person), while others are small and shallow (probably from a lighter person). You want to figure out how many different people walked through the snow and which footprints belong to whom.

This is exactly what **mixture models** do with data. They help us discover hidden patterns by modeling our data as coming from a mix of several underlying distributions (clusters), each with its own characteristics.

### The Core Problem: Latent Variables
- **Latent variables** are hidden variables that we don't observe directly, but which explain the structure in our data
- In our footprint example, the latent variable is "which person made this footprint?"
- In clustering, the latent variable is "which cluster does this data point belong to?"

### Why Traditional Methods Fall Short
**k-means limitations:**
- Makes **hard assignments** (each point belongs to exactly one cluster)
- Assumes **spherical clusters** of equal size
- No **uncertainty estimates** about cluster assignments
- Can't handle **overlapping clusters**

**Real-world data is messy:**
- Clusters often overlap
- Different clusters have different shapes and sizes
- We need to quantify our uncertainty about assignments

## From Hard Clustering to Probabilistic Models: The Evolution

We've now explored **k-means clustering** - a simple but effective algorithm for partitioning data into groups based on similarity. We've seen how k-means works by alternating between assigning points to the nearest centroid and updating centroids to the mean of their assigned points, minimizing the total squared distance within clusters.

However, while k-means provides a straightforward approach to clustering, it has limitations: it makes **hard assignments** (each point belongs to exactly one cluster), assumes **spherical clusters** of equal size, and doesn't provide uncertainty estimates about cluster assignments. Many real-world clustering problems require more flexibility and probabilistic reasoning.

This motivates our exploration of **Gaussian Mixture Models (GMM)** and the **Expectation-Maximization (EM) algorithm** - a probabilistic approach to clustering that provides soft assignments, can model clusters of different shapes and sizes, and gives us uncertainty estimates about cluster membership.

The transition from k-means to GMM represents the bridge from deterministic to probabilistic clustering - taking our understanding of how to group similar data points and extending it to handle uncertainty and more complex cluster structures.

In this section, we'll explore how EM works with mixture models, how to compute soft assignments, and how this probabilistic framework provides more flexibility than hard clustering methods.

## The Mixture of Gaussians Model: A Probabilistic Framework

### The Generative Story: How Data is Created
Think of a **generative model** as a recipe for creating data. Here's how a Gaussian Mixture Model generates data:

1. **Choose a cluster**: First, pick which cluster this data point will come from (like rolling a weighted die)
2. **Generate the point**: Then, sample a point from the Gaussian distribution for that cluster
3. **Repeat**: Do this for every data point

**Mathematical formulation:**
- Each data point $`x^{(i)}`$ is generated by first picking a cluster $`z^{(i)}`$ (which is hidden), then sampling $`x^{(i)}`$ from a Gaussian specific to that cluster
- $`z^{(i)}`$ is a categorical variable (e.g., 1 for cluster 1, 2 for cluster 2, etc.)
- $`x^{(i)}|z^{(i)} = j \sim \mathcal{N}(\mu_j, \Sigma_j)`$ (Gaussian with mean $`\mu_j`$ and covariance $`\Sigma_j`$)
- $`z^{(i)} \sim \text{Multinomial}(\phi)`$ (probability $`\phi_j`$ for each cluster $`j`$)

### The Parameters: What We Need to Learn

**Mixing proportions ($`\phi`$):**
- $`\phi_j`$ is the probability that a randomly chosen point comes from cluster $`j`$
- Must satisfy: $`\sum_{j=1}^k \phi_j = 1`$ and $`\phi_j \geq 0`$ for all $`j`$
- Example: $`\phi = [0.3, 0.7]`$ means 30% of points come from cluster 1, 70% from cluster 2

**Cluster means ($`\mu`$):**
- $`\mu_j`$ is the center of cluster $`j`$
- In 2D: $`\mu_j = [\mu_{j1}, \mu_{j2}]`$
- Example: $`\mu_1 = [1, 2]`$, $`\mu_2 = [5, 3]`$

**Cluster covariances ($`\Sigma`$):**
- $`\Sigma_j`$ describes the shape and spread of cluster $`j`$
- Controls how "stretched out" or "round" each cluster is
- Example: $`\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}`$ (circular), $`\Sigma_2 = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 0.5 \end{bmatrix}`$ (elliptical)

### Visual Intuition: What Each Parameter Does

**Mixing proportions:** Control the relative sizes of clusters
- $`\phi = [0.5, 0.5]`$: Equal-sized clusters
- $`\phi = [0.8, 0.2]`$: One large cluster, one small cluster

**Means:** Control where clusters are located
- Far apart means = well-separated clusters
- Close means = overlapping clusters

**Covariances:** Control the shape and spread
- Diagonal covariance = elliptical clusters aligned with axes
- Non-diagonal covariance = rotated elliptical clusters
- Large eigenvalues = stretched clusters
- Small eigenvalues = compact clusters

## Likelihood and Why Direct Optimization is Hard: The Mathematical Challenge

### The Likelihood Function: Measuring Model Fit
The likelihood measures how well our model explains the observed data:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}; \phi, \mu, \Sigma)
```

**What this means:**
- For each data point, compute the probability of observing it under our model
- Take the log (for numerical stability and mathematical convenience)
- Sum over all data points
- Higher likelihood = better model fit

### The Marginal Likelihood: Summing Over Hidden Variables
Since we don't know which cluster each point came from, we need to sum over all possibilities:

```math
p(x^{(i)}; \phi, \mu, \Sigma) = \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)}; \mu, \Sigma) p(z^{(i)}; \phi)
```

**Breaking this down:**
- $`p(z^{(i)}; \phi)`$: Probability of choosing cluster $`z^{(i)}`$
- $`p(x^{(i)}|z^{(i)}; \mu, \Sigma)`$: Probability of $`x^{(i)}`$ given it came from cluster $`z^{(i)}`$
- The sum accounts for all possible cluster assignments

### Why Direct Optimization is Hard: The Log-Sum Problem
The full likelihood becomes:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)}; \mu, \Sigma) p(z^{(i)}; \phi)
```

**The problem:** The sum is inside the log, making direct optimization extremely difficult.

**Why is this hard?**
- We can't take derivatives easily because of the log-sum structure
- No closed-form solution exists
- Gradient-based methods struggle with this form

**Analogy:** It's like trying to optimize a recipe where you have to mix ingredients first, then taste the result, but you can't adjust individual ingredients based on the final taste.

## If We Knew the Cluster Assignments: The "Oracle" Scenario

### The Complete Data Likelihood: When We Have All Information
If we magically knew which cluster each point came from, the likelihood would be much simpler:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}|z^{(i)}; \mu, \Sigma) + \log p(z^{(i)}; \phi)
```

**Why this is easier:**
- No sums inside logs
- Each term can be optimized independently
- We get closed-form solutions

### The Optimal Parameters: Closed-Form Solutions

**Mixing proportions:**
```math
\phi_j = \frac{1}{n} \sum_{i=1}^n 1\{z^{(i)} = j\}
```
- Simply the fraction of points assigned to cluster $`j`$

**Cluster means:**
```math
\mu_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} x^{(i)}}{\sum_{i=1}^n 1\{z^{(i)} = j\}}
```
- The average of all points in cluster $`j`$

**Cluster covariances:**
```math
\Sigma_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n 1\{z^{(i)} = j\}}
```
- The sample covariance of all points in cluster $`j`$

**Intuition:** These are just the standard formulas for estimating Gaussian parameters, applied to each cluster separately.

## The EM Algorithm: Handling Hidden Variables with Elegance

### The Key Insight: Alternating Optimization
Since we don't know the cluster assignments, we use the **Expectation-Maximization (EM)** algorithm. The key idea is to alternate between:

1. **E-step (Expectation):** Estimate the probability that each point belongs to each cluster (soft assignments)
2. **M-step (Maximization):** Update the parameters as if these probabilities were the true assignments

**Why this works:**
- E-step: We "guess" the hidden variables based on current parameters
- M-step: We optimize parameters assuming our guesses are correct
- Each step improves our model, and the process converges

### The EM Algorithm for Mixture of Gaussians: Step-by-Step

**Initialize:** Choose starting values for $`\phi`$, $`\mu`$, and $`\Sigma`$

**Repeat until convergence:**

#### E-step (Expectation): Computing Soft Assignments
For each data point $`i`$ and cluster $`j`$, compute:

```math
w_j^{(i)} := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma)
```

**What is $`w_j^{(i)}`$?**
- The probability (under current parameters) that $`x^{(i)}`$ came from cluster $`j``
- Also called the **responsibility** or **posterior probability**
- Must satisfy: $`\sum_{j=1}^k w_j^{(i)} = 1`$ for each $`i`$

**How do we compute this?**
Using Bayes' rule:

```math
p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma) = \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)} = l; \phi)}
```

**Breaking down the formula:**
- **Numerator:** $`p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)`$ × $`p(z^{(i)} = j; \phi)`$
  - Likelihood of $`x^{(i)}`$ under cluster $`j`$ × prior probability of cluster $`j`$
- **Denominator:** Sum over all clusters (normalization constant)
  - Ensures probabilities sum to 1

**Intuition:** This is like asking "Given this data point and our current model, how likely is it that this point came from each cluster?"

#### M-step (Maximization): Updating Parameters
Update parameters using the soft assignments:

**Mixing proportions:**
```math
\phi_j := \frac{1}{n} \sum_{i=1}^n w_j^{(i)}
```
- Average responsibility of cluster $`j`$ across all points

**Cluster means:**
```math
\mu_j := \frac{\sum_{i=1}^n w_j^{(i)} x^{(i)}}{\sum_{i=1}^n w_j^{(i)}}
```
- Weighted average of all points, where weights are the responsibilities

**Cluster covariances:**
```math
\Sigma_j := \frac{\sum_{i=1}^n w_j^{(i)} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n w_j^{(i)}}
```
- Weighted sample covariance, where weights are the responsibilities

**Key insight:** These are the same formulas as before, but with soft assignments ($`w_j^{(i)}`$) instead of hard assignments ($`1\{z^{(i)} = j\}`$).

### Convergence and Stopping Criteria
**When to stop:**
- **Parameter convergence:** Parameters change less than a threshold
- **Likelihood convergence:** Log-likelihood changes less than a threshold
- **Assignment convergence:** Soft assignments change less than a threshold

**Why EM converges:**
- Each E-step and M-step increases (or maintains) the likelihood
- The likelihood is bounded above
- Therefore, the algorithm must converge to a local maximum

## Detailed Worked Example: From Theory to Practice

### The Setup: A Simple 1D Example
Suppose you have 6 points: [1, 2, 3, 10, 11, 12], and you want $`k=2`$ clusters.

**Initialization:**
- $`\phi = [0.5, 0.5]`$ (equal mixing)
- $`\mu_1 = 2`$, $`\mu_2 = 11`$ (centroids from k-means)
- $`\Sigma_1 = \Sigma_2 = 1`$ (unit variance)

### Iteration 1: E-step
For point $`x^{(1)} = 1`$:

**Compute likelihoods:**
- $`p(x^{(1)}|z^{(1)} = 1) = \mathcal{N}(1; 2, 1) = 0.242`$
- $`p(x^{(1)}|z^{(1)} = 2) = \mathcal{N}(1; 11, 1) = 1.27 \times 10^{-22}`$

**Compute responsibilities:**
- $`w_1^{(1)} = \frac{0.242 \times 0.5}{0.242 \times 0.5 + 1.27 \times 10^{-22} \times 0.5} = 0.999`$
- $`w_2^{(1)} = 1 - 0.999 = 0.001`$

**Result:** Point 1 is almost certainly in cluster 1.

### Iteration 1: M-step
**Update mixing proportions:**
- $`\phi_1 = \frac{0.999 + 0.999 + 0.999 + 0.001 + 0.001 + 0.001}{6} = 0.5`$
- $`\phi_2 = 0.5`$

**Update means:**
- $`\mu_1 = \frac{0.999 \times 1 + 0.999 \times 2 + 0.999 \times 3 + 0.001 \times 10 + 0.001 \times 11 + 0.001 \times 12}{0.999 + 0.999 + 0.999 + 0.001 + 0.001 + 0.001} = 2.0`$
- $`\mu_2 = 11.0`$

**Update covariances:**
- $`\Sigma_1 = 1.0`$ (approximately)
- $`\Sigma_2 = 1.0`$ (approximately)

### Convergence
After a few iterations, the responsibilities become very close to 0 or 1, and the parameters stabilize.

**Final responsibilities:**
- Points 1, 2, 3: $`w_1^{(i)} \approx 1`$, $`w_2^{(i)} \approx 0`$
- Points 10, 11, 12: $`w_1^{(i)} \approx 0`$, $`w_2^{(i)} \approx 1`$

### Why This Worked So Well
- The data naturally formed two well-separated groups
- The initial parameters were reasonable
- The clusters are roughly Gaussian-shaped

## Intuition: EM vs. k-means - The Key Differences

### Assignment Strategy
**k-means (Hard Assignment):**
- Each point belongs to exactly one cluster
- Assignment rule: "Join the nearest centroid"
- Like choosing one team to join

**EM (Soft Assignment):**
- Each point has a probability for each cluster
- Assignment rule: "Compute probability for each cluster"
- Like having partial membership in multiple teams

### Cluster Shape Assumptions
**k-means:**
- Assumes spherical clusters of equal size
- Only uses centroids (means)
- Like drawing circles of equal radius around each center

**EM with Gaussians:**
- Can model clusters of different shapes and sizes
- Uses means and covariances
- Like drawing ellipses of different sizes and orientations

### Uncertainty Quantification
**k-means:**
- No uncertainty about assignments
- Each point is definitively in one cluster

**EM:**
- Provides uncertainty estimates
- Can identify points that are "between" clusters

### Mathematical Foundation
**k-means:**
- Minimizes squared distance
- Deterministic algorithm

**EM:**
- Maximizes likelihood
- Probabilistic framework

## Jensen's Inequality: The Mathematical Foundation of EM

### What is Jensen's Inequality?
Jensen's inequality states that for a convex function $`f`$:

```math
\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])
```

**In words:** The average of the function is at least the function of the average.

### Simple Example: Building Intuition
Suppose $`X`$ is 0 or 2 with equal probability, and $`f(x) = x^2`$ (convex).

**Calculations:**
- $`\mathbb{E}[X] = (0 + 2)/2 = 1`$
- $`f(\mathbb{E}[X]) = f(1) = 1^2 = 1`$
- $`\mathbb{E}[f(X)] = (0^2 + 2^2)/2 = 2`$
- So $`2 \geq 1`$, confirming the inequality

**Visual intuition:** The square function is convex (curves upward), so the average of squares is greater than the square of the average.

### How EM Uses Jensen's Inequality
EM constructs a **lower bound** on the log-likelihood using Jensen's inequality:

```math
\log \sum_{z} p(x, z) = \log \sum_{z} q(z) \frac{p(x, z)}{q(z)} \geq \sum_{z} q(z) \log \frac{p(x, z)}{q(z)}
```

**The ELBO (Evidence Lower BOund):**
```math
\mathcal{L}(q, \theta) = \sum_{z} q(z) \log \frac{p(x, z; \theta)}{q(z)}
```

**EM alternates between:**
1. **E-step:** Choose $`q(z)`$ to make the bound tight
2. **M-step:** Maximize the bound with respect to $`\theta``

### Why This Works
- Each step increases the lower bound
- The lower bound approaches the true likelihood
- Therefore, EM increases the likelihood at each step

## Practical Tips and Pitfalls: Real-World Considerations

### Initialization Strategies
**Random initialization:**
- Start with random parameters
- Run multiple times and pick the best result
- Simple but can be slow to converge

**k-means initialization:**
- Use k-means to get initial means
- Set covariances to identity matrix
- Often works well in practice

**k-means++ initialization:**
- Use k-means++ for better initial means
- More robust than random initialization

### Model Selection: Choosing the Number of Clusters

**Information criteria:**
- **BIC (Bayesian Information Criterion):** $`\text{BIC} = \log p(x|\hat{\theta}) - \frac{d}{2} \log n`$
- **AIC (Akaike Information Criterion):** $`\text{AIC} = \log p(x|\hat{\theta}) - d`$
- Choose $`k`$ that maximizes the criterion

**Cross-validation:**
- Split data into training and validation sets
- Train models with different $`k`$ on training data
- Evaluate likelihood on validation data
- Choose $`k`$ with highest validation likelihood

**Domain knowledge:**
- Use your understanding of the problem
- Consider interpretability and computational cost

### Numerical Stability Issues

**Singularities:**
- If a cluster collapses to a single point, covariance becomes singular
- **Solution:** Add regularization: $`\Sigma_j + \epsilon I`$

**Overflow/underflow:**
- Very small or large probabilities can cause numerical issues
- **Solution:** Work in log-space when possible

**Convergence issues:**
- EM can converge slowly or to poor local optima
- **Solution:** Multiple runs with different initializations

### Interpreting Results

**Soft assignments:**
- $`w_j^{(i)}`$ is the probability that point $`i`$ belongs to cluster $`j``
- Values close to 0 or 1 indicate confident assignments
- Values around 0.5 indicate uncertainty

**Cluster parameters:**
- **Means:** Centers of clusters
- **Covariances:** Shape and orientation of clusters
- **Mixing proportions:** Relative sizes of clusters

**Model diagnostics:**
- Check if clusters are well-separated
- Verify that the model assumptions are reasonable
- Look for outliers or unusual patterns

## Frequently Asked Questions (FAQ)

**Q: How is EM different from k-means?**
A: EM uses soft assignments and can model clusters with different shapes and sizes via covariance matrices. k-means uses hard assignments and assumes spherical clusters.

**Q: What if a cluster gets no points?**
A: Reinitialize its parameters randomly or use regularization to prevent collapse.

**Q: Is EM guaranteed to find the best solution?**
A: No, it can get stuck in local optima. Multiple runs with different initializations help.

**Q: Can I use EM for non-Gaussian mixtures?**
A: Yes! EM is a general framework for models with hidden variables. You can use any distribution family.

**Q: How do I choose the number of clusters?**
A: Use information criteria (BIC/AIC), cross-validation, or domain knowledge.

**Q: What's the computational complexity of EM?**
A: O(nkd²) per iteration, where n=points, k=clusters, d=dimensions. More expensive than k-means but more flexible.

**Q: How do I handle categorical data with EM?**
A: Use appropriate distributions (e.g., multinomial for categorical variables) or encode categorical variables.

**Q: What's the relationship between EM and variational inference?**
A: EM is a special case of variational inference where we use exact inference in the E-step.

## Summary: The Big Picture

Gaussian Mixture Models and the EM algorithm provide a powerful framework for probabilistic clustering. Here's what we've learned:

### Key Concepts:
- **Mixture models:** Model data as coming from multiple underlying distributions
- **Latent variables:** Hidden variables that explain data structure
- **Soft assignments:** Probabilistic cluster membership
- **EM algorithm:** Alternating optimization for latent variable models

### The EM Algorithm:
1. **E-step:** Compute soft assignments (responsibilities)
2. **M-step:** Update parameters using soft assignments
3. **Repeat** until convergence

### Advantages over k-means:
- **Flexibility:** Can model clusters of different shapes and sizes
- **Uncertainty:** Provides confidence estimates for assignments
- **Probabilistic:** Full probabilistic framework
- **Overlapping clusters:** Can handle overlapping cluster structures

### Best Practices:
- Use multiple initializations to avoid local optima
- Choose number of clusters carefully using model selection
- Handle numerical stability issues with regularization
- Interpret soft assignments appropriately

### Limitations:
- Assumes Gaussian clusters (may not fit all data)
- Can be sensitive to initialization
- More computationally expensive than k-means
- Requires choosing number of clusters

## From Specific Models to General Framework: The Next Step

We've now explored **Gaussian Mixture Models** and the **Expectation-Maximization algorithm** - a powerful approach to probabilistic clustering that provides soft assignments and can model clusters of different shapes and sizes. We've seen how EM alternates between estimating cluster probabilities (E-step) and updating parameters (M-step), using the ELBO to maximize a lower bound on the likelihood.

However, while GMM provides a specific application of EM, the **Expectation-Maximization algorithm** is actually a general framework that can be applied to any model with latent variables. The principles we've learned - alternating between expectation and maximization steps, using the ELBO, and handling hidden variables - extend far beyond mixture models.

This motivates our exploration of the **general EM framework** - a flexible approach for learning in any latent variable model. We'll see how the ELBO provides a unified framework for variational inference, how to apply EM to different types of models, and how this general approach enables us to tackle a wide range of unsupervised learning problems.

The transition from GMM to general EM represents the bridge from specific application to universal framework - taking our understanding of how EM works with mixture models and extending it to any model with hidden variables.

In the next section, we'll explore the mathematical foundations of the general EM framework, understand the ELBO in its most general form, and see how this framework applies to various latent variable models.

---

**Previous: [K-Means Clustering](01_clustering.md)** - Understand the fundamental clustering algorithm for partitioning data.

**Next: [General EM Framework](03_general_em.md)** - Learn the universal framework for latent variable models and variational inference.

