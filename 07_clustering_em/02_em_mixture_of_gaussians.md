# Expectation-Maximization (EM) Algorithms: Mixture of Gaussians

## Motivation: Why Mixture Models?

Imagine you have a dataset of animal heights, but you don't know which animal is which. If you plot the data, you might see two peaksâ€”one for cats and one for dogs. You want to model the data as coming from two different groups, but you don't know which point belongs to which group. This is the motivation for **mixture models**: modeling data as coming from a mix of several underlying distributions (clusters), each with its own parameters.

- **Latent variables:** These are hidden variables (like "cat" or "dog") that we don't observe directly, but which explain the structure in the data.

## The Mixture of Gaussians Model (Step-by-Step)

We model the data as follows:
- Each data point $`x^{(i)}`$ is generated by first picking a cluster $`z^{(i)}`$ (which is hidden), then sampling $`x^{(i)}`$ from a Gaussian specific to that cluster.
- $`z^{(i)}`$ is a categorical variable (e.g., 1 for cat, 2 for dog).
- $`x^{(i)}|z^{(i)} = j \sim \mathcal{N}(\mu_j, \Sigma_j)`$ (Gaussian with mean $`\mu_j`$ and covariance $`\Sigma_j`$).
- $`z^{(i)} \sim \text{Multinomial}(\phi)`$ (probability $`\phi_j`$ for each cluster $`j`$).

**Parameters:**
- $`\phi`$: Mixing proportions (how common each cluster is)
- $`\mu`$: Means of each Gaussian
- $`\Sigma`$: Covariances of each Gaussian

## Likelihood and Why Direct Optimization is Hard

The likelihood of the data is:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}; \phi, \mu, \Sigma)
```

But each $`x^{(i)}`$ could have come from any cluster, so we sum over all possible $`z^{(i)}`$:

```math
= \sum_{i=1}^n \log \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)}; \mu, \Sigma) p(z^{(i)}; \phi)
```

- **Why is this hard?**
  - The sum is inside the log, making the math tricky. We can't just maximize with respect to the parameters directly.

## If We Knew the Cluster Assignments (Latent Variables)

If we magically knew which cluster each point came from, the likelihood would be much simpler:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}|z^{(i)}; \mu, \Sigma) + \log p(z^{(i)}; \phi)
```

Maximizing this gives closed-form solutions:
- $`\phi_j = \frac{1}{n} \sum_{i=1}^n 1\{z^{(i)} = j\}`$ (fraction of points in cluster $`j`$)
- $`\mu_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} x^{(i)}}{\sum_{i=1}^n 1\{z^{(i)} = j\}}`$ (mean of cluster $`j`$)
- $`\Sigma_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n 1\{z^{(i)} = j\}}`$ (covariance of cluster $`j`$)

## The EM Algorithm: Handling Hidden Variables

Since we don't know the $`z^{(i)}`$'s, we use the **Expectation-Maximization (EM)** algorithm. The key idea is to alternate between:
- **E-step (Expectation):** Estimate the probability that each point belongs to each cluster (soft assignments).
- **M-step (Maximization):** Update the parameters as if these probabilities were the true assignments.

### EM Algorithm for Mixture of Gaussians (Step-by-Step)

Repeat until convergence:

**E-step:** For each $`i, j`$, compute

```math
w_j^{(i)} := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma)
```
- $`w_j^{(i)}`$ is the probability (under current parameters) that $`x^{(i)}`$ came from cluster $`j`$.
- **How do we compute this?**
  - Use Bayes' rule:

```math
p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma) = \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)} = l; \phi)}
```
- Numerator: Probability of $`x^{(i)}`$ under cluster $`j`$ times the prior probability of cluster $`j`$.
- Denominator: Normalizes so probabilities sum to 1.

**M-step:** Update parameters using the soft assignments:

```math
\phi_j := \frac{1}{n} \sum_{i=1}^n w_j^{(i)}
```

```math
\mu_j := \frac{\sum_{i=1}^n w_j^{(i)} x^{(i)}}{\sum_{i=1}^n w_j^{(i)}}
```

```math
\Sigma_j := \frac{\sum_{i=1}^n w_j^{(i)} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n w_j^{(i)}}
```
- These are just the formulas from before, but with soft assignments instead of hard ones.

### Worked Example (1D, Small Numbers)

Suppose you have 6 points: 1, 2, 3, 10, 11, 12, and want $`k=2`$ clusters. After a few EM steps, you might get:
- $`w_1^{(1)} = 0.99`$, $`w_2^{(1)} = 0.01`$ (point 1 is almost certainly in cluster 1)
- $`w_1^{(6)} = 0.02`$, $`w_2^{(6)} = 0.98`$ (point 12 is almost certainly in cluster 2)
- The means and variances are updated using these weights.

### Intuition: EM vs. k-means

- **k-means:** Each point belongs to exactly one cluster (hard assignment).
- **EM for Gaussians:** Each point has a probability for each cluster (soft assignment). This is more flexible and can model overlapping clusters.

## Jensen's Inequality: The Math Behind EM

EM works by maximizing a lower bound on the likelihood, using **Jensen's inequality**:

```math
\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])
```
- For convex $`f`$, the average of the function is at least the function of the average.
- In EM, this lets us swap the log and the sum in a way that makes the math tractable.

**Simple Example:**
- Suppose $`X`$ is 0 or 2 with equal probability, and $`f(x) = x^2`$ (convex).
- $`\mathbb{E}[X] = 1`$, $`f(\mathbb{E}[X]) = 1`$.
- $`\mathbb{E}[f(X)] = (0^2 + 2^2)/2 = 2`$.
- So $`2 \geq 1`$.

## Practical Tips and Pitfalls

- **Initialization matters:** EM can get stuck in local optima. Try multiple runs with different starting points.
- **Number of clusters:** Use model selection (e.g., BIC/AIC) or cross-validation to choose $`k`$.
- **Singularities:** If a cluster collapses to a single point, the covariance can go to zero. Add a small value to the diagonal for stability.
- **Interpretation:** The soft assignments $`w_j^{(i)}`$ can be interpreted as the probability that $`x^{(i)}`$ belongs to cluster $`j`$.

## Frequently Asked Questions (FAQ)

**Q: How is EM different from k-means?**
- EM uses soft assignments and can model clusters with different shapes and sizes (via covariance matrices). k-means uses hard assignments and assumes spherical clusters.

**Q: What if a cluster gets no points?**
- Reinitialize its parameters randomly.

**Q: Is EM guaranteed to find the best solution?**
- No, it can get stuck in local optima. Multiple runs help.

**Q: Can I use EM for non-Gaussian mixtures?**
- Yes! EM is a general framework for models with hidden variables.

## Summary

- Mixture of Gaussians models data as coming from several Gaussian clusters, each with its own mean and covariance.
- The EM algorithm alternates between estimating cluster probabilities (E-step) and updating parameters (M-step).
- EM is more flexible than k-means and can model overlapping, non-spherical clusters.
- Understanding the math and intuition behind EM helps you use it effectively in practice.

