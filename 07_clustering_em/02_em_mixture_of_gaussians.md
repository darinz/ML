# Expectation-Maximization (EM) Algorithms: Mixture of Gaussians

## Motivation: Why Mixture Models?

Imagine you have a dataset of animal heights, but you don't know which animal is which. If you plot the data, you might see two peaksâ€”one for cats and one for dogs. You want to model the data as coming from two different groups, but you don't know which point belongs to which group. This is the motivation for **mixture models**: modeling data as coming from a mix of several underlying distributions (clusters), each with its own parameters.

- **Latent variables:** These are hidden variables (like "cat" or "dog") that we don't observe directly, but which explain the structure in the data.

## From Hard Clustering to Probabilistic Models

We've now explored **k-means clustering** - a simple but effective algorithm for partitioning data into groups based on similarity. We've seen how k-means works by alternating between assigning points to the nearest centroid and updating centroids to the mean of their assigned points, minimizing the total squared distance within clusters.

However, while k-means provides a straightforward approach to clustering, it has limitations: it makes **hard assignments** (each point belongs to exactly one cluster), assumes **spherical clusters** of equal size, and doesn't provide uncertainty estimates about cluster assignments. Many real-world clustering problems require more flexibility and probabilistic reasoning.

This motivates our exploration of **Gaussian Mixture Models (GMM)** and the **Expectation-Maximization (EM) algorithm** - a probabilistic approach to clustering that provides soft assignments, can model clusters of different shapes and sizes, and gives us uncertainty estimates about cluster membership.

The transition from k-means to GMM represents the bridge from deterministic to probabilistic clustering - taking our understanding of how to group similar data points and extending it to handle uncertainty and more complex cluster structures.

In this section, we'll explore how EM works with mixture models, how to compute soft assignments, and how this probabilistic framework provides more flexibility than hard clustering methods.

## The Mixture of Gaussians Model (Step-by-Step)

We model the data as follows:
- Each data point $`x^{(i)}`$ is generated by first picking a cluster $`z^{(i)}`$ (which is hidden), then sampling $`x^{(i)}`$ from a Gaussian specific to that cluster.
- $`z^{(i)}`$ is a categorical variable (e.g., 1 for cat, 2 for dog).
- $`x^{(i)}|z^{(i)} = j \sim \mathcal{N}(\mu_j, \Sigma_j)`$ (Gaussian with mean $`\mu_j`$ and covariance $`\Sigma_j`$).
- $`z^{(i)} \sim \text{Multinomial}(\phi)`$ (probability $`\phi_j`$ for each cluster $`j`$).

**Parameters:**
- $`\phi`$: Mixing proportions (how common each cluster is)
- $`\mu`$: Means of each Gaussian
- $`\Sigma`$: Covariances of each Gaussian

## Likelihood and Why Direct Optimization is Hard

The likelihood of the data is:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}; \phi, \mu, \Sigma)
```

But each $`x^{(i)}`$ could have come from any cluster, so we sum over all possible $`z^{(i)}`$:

```math
= \sum_{i=1}^n \log \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)}; \mu, \Sigma) p(z^{(i)}; \phi)
```

- **Why is this hard?**
  - The sum is inside the log, making the math tricky. We can't just maximize with respect to the parameters directly.

## If We Knew the Cluster Assignments (Latent Variables)

If we magically knew which cluster each point came from, the likelihood would be much simpler:

```math
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^n \log p(x^{(i)}|z^{(i)}; \mu, \Sigma) + \log p(z^{(i)}; \phi)
```

Maximizing this gives closed-form solutions:
- $`\phi_j = \frac{1}{n} \sum_{i=1}^n 1\{z^{(i)} = j\}`$ (fraction of points in cluster $`j`$)
- $`\mu_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} x^{(i)}}{\sum_{i=1}^n 1\{z^{(i)} = j\}}`$ (mean of cluster $`j`$)
- $`\Sigma_j = \frac{\sum_{i=1}^n 1\{z^{(i)} = j\} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n 1\{z^{(i)} = j\}}`$ (covariance of cluster $`j`$)

## The EM Algorithm: Handling Hidden Variables

Since we don't know the $`z^{(i)}`$'s, we use the **Expectation-Maximization (EM)** algorithm. The key idea is to alternate between:
- **E-step (Expectation):** Estimate the probability that each point belongs to each cluster (soft assignments).
- **M-step (Maximization):** Update the parameters as if these probabilities were the true assignments.

### EM Algorithm for Mixture of Gaussians (Step-by-Step)

Repeat until convergence:

**E-step:** For each $`i, j`$, compute

```math
w_j^{(i)} := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma)
```
- $`w_j^{(i)}`$ is the probability (under current parameters) that $`x^{(i)}`$ came from cluster $`j`$.
- **How do we compute this?**
  - Use Bayes' rule:

```math
p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma) = \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)} = l; \phi)}
```
- Numerator: Probability of $`x^{(i)}`$ under cluster $`j`$ times the prior probability of cluster $`j`$.
- Denominator: Normalizes so probabilities sum to 1.

**M-step:** Update parameters using the soft assignments:

```math
\phi_j := \frac{1}{n} \sum_{i=1}^n w_j^{(i)}
```

```math
\mu_j := \frac{\sum_{i=1}^n w_j^{(i)} x^{(i)}}{\sum_{i=1}^n w_j^{(i)}}
```

```math
\Sigma_j := \frac{\sum_{i=1}^n w_j^{(i)} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n w_j^{(i)}}
```
- These are just the formulas from before, but with soft assignments instead of hard ones.

### Worked Example (1D, Small Numbers)

Suppose you have 6 points: 1, 2, 3, 10, 11, 12, and want $`k=2`$ clusters. After a few EM steps, you might get:
- $`w_1^{(1)} = 0.99`$, $`w_2^{(1)} = 0.01`$ (point 1 is almost certainly in cluster 1)
- $`w_1^{(6)} = 0.02`$, $`w_2^{(6)} = 0.98`$ (point 12 is almost certainly in cluster 2)
- The means and variances are updated using these weights.

### Intuition: EM vs. k-means

- **k-means:** Each point belongs to exactly one cluster (hard assignment).
- **EM for Gaussians:** Each point has a probability for each cluster (soft assignment). This is more flexible and can model overlapping clusters.

## Jensen's Inequality: The Math Behind EM

EM works by maximizing a lower bound on the likelihood, using **Jensen's inequality**:

```math
\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])
```
- For convex $`f`$, the average of the function is at least the function of the average.
- In EM, this lets us swap the log and the sum in a way that makes the math tractable.

**Simple Example:**
- Suppose $`X`$ is 0 or 2 with equal probability, and $`f(x) = x^2`$ (convex).
- $`\mathbb{E}[X] = 1`$, $`f(\mathbb{E}[X]) = 1`$.
- $`\mathbb{E}[f(X)] = (0^2 + 2^2)/2 = 2`$.
- So $`2 \geq 1`$.

## Practical Tips and Pitfalls

- **Initialization matters:** EM can get stuck in local optima. Try multiple runs with different starting points.
- **Number of clusters:** Use model selection (e.g., BIC/AIC) or cross-validation to choose $`k`$.
- **Singularities:** If a cluster collapses to a single point, the covariance can go to zero. Add a small value to the diagonal for stability.
- **Interpretation:** The soft assignments $`w_j^{(i)}`$ can be interpreted as the probability that $`x^{(i)}`$ belongs to cluster $`j`$.

## Frequently Asked Questions (FAQ)

**Q: How is EM different from k-means?**
- EM uses soft assignments and can model clusters with different shapes and sizes (via covariance matrices). k-means uses hard assignments and assumes spherical clusters.

**Q: What if a cluster gets no points?**
- Reinitialize its parameters randomly.

**Q: Is EM guaranteed to find the best solution?**
- No, it can get stuck in local optima. Multiple runs help.

**Q: Can I use EM for non-Gaussian mixtures?**
- Yes! EM is a general framework for models with hidden variables.

## Summary

- Mixture of Gaussians models data as coming from several Gaussian clusters, each with its own mean and covariance.
- The EM algorithm alternates between estimating cluster probabilities (E-step) and updating parameters (M-step).
- EM is more flexible than k-means and can model overlapping, non-spherical clusters.
- Understanding the math and intuition behind EM helps you use it effectively in practice.

## From Specific Models to General Framework

We've now explored **Gaussian Mixture Models** and the **Expectation-Maximization algorithm** - a powerful approach to probabilistic clustering that provides soft assignments and can model clusters of different shapes and sizes. We've seen how EM alternates between estimating cluster probabilities (E-step) and updating parameters (M-step), using the ELBO to maximize a lower bound on the likelihood.

However, while GMM provides a specific application of EM, the **Expectation-Maximization algorithm** is actually a general framework that can be applied to any model with latent variables. The principles we've learned - alternating between expectation and maximization steps, using the ELBO, and handling hidden variables - extend far beyond mixture models.

This motivates our exploration of the **general EM framework** - a flexible approach for learning in any latent variable model. We'll see how the ELBO provides a unified framework for variational inference, how to apply EM to different types of models, and how this general approach enables us to tackle a wide range of unsupervised learning problems.

The transition from GMM to general EM represents the bridge from specific application to universal framework - taking our understanding of how EM works with mixture models and extending it to any model with hidden variables.

In the next section, we'll explore the mathematical foundations of the general EM framework, understand the ELBO in its most general form, and see how this framework applies to various latent variable models.

---

**Previous: [K-Means Clustering](01_clustering.md)** - Understand the fundamental clustering algorithm for partitioning data.

**Next: [General EM Framework](03_general_em.md)** - Learn the universal framework for latent variable models and variational inference.

