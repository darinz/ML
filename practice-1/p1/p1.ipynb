{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5a6745-45c3-4f09-a9f0-e38887a665bd",
   "metadata": {},
   "source": [
    "# Practice 1\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Scientists tell us that there is a 10% probability that a person will have the flu this winter. Meanwhile, the CDC reports that 30% of the population will experience flu-like symptoms and that 60% of people with the flu will be symptomatic. What's the probability that Sarah has the flu given that she has flu-like symptoms?\n",
    "\n",
    "(A) 60%\n",
    "\n",
    "(B) 20%\n",
    "\n",
    "(C) 30%\n",
    "\n",
    "(D) 10%\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "True/False: The variance of a model typically decreases as the number of features increases.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Assume you're given two independent random variables X and Y. X is uniformly distributed on the interval [1, 3], whereas Y follows a normal distribution with mean 3 and standard deviation 1. What is $(E[XY])^2 â€“ E[X]E[Y]$?\n",
    "\n",
    "(A) 3\n",
    "\n",
    "(B) 30\n",
    "\n",
    "(C) 6\n",
    "\n",
    "(D) 0\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "True/False: If the columns of A are orthogonal, then $A^TA$ is diagonal.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "True/False: Assume we train a model on a given dataset. If we were to remove 50% of samples from the dataset and re-train the model from scratch, the new model will be more likely to overfit to its training data than the old one.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "True/False: If $\\{v_1, v_2, \\dots, v_n\\}$ and $\\{w_1, w_2, \\dots, w_n\\}$ are linearly independent, then $\\{v_1 + w_1, v_2 + w_2, \\dots, v_n + w_n\\}$ are linearly independent.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "True/False: $E[\\epsilon\\epsilon^T] = I$ where $\\epsilon_i \\sim N(0, \\sigma^2)$ such that $\\epsilon$ is a column vector: $\\epsilon \\in \\mathbb{R}^d$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "<img src=\"./figure_1.png\">\n",
    "\n",
    "Figure 1: The following graphic will be used as a representation of bias and variance. Imagine that a true/correct model is one that always predicts a location at the center of each target (being farther away from the center of the target indicates that a model's predictions are worse). We retrain a model multiple times, and make a prediction with each trained model. For each of the targets, determine whether the bias and variance is low or high with respect to the true model.\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "In Figure 1, subplot I, how are bias and variance related to the true model?\n",
    "\n",
    "(A) High bias, High variance\n",
    "\n",
    "(B) High bias, Low variance\n",
    "\n",
    "(C) Low bias, High variance\n",
    "\n",
    "(D) Low bias, Low variance\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "In Figure 1, subplot II, how are bias and variance related to the true model?\n",
    "\n",
    "(A) High bias, High variance\n",
    "\n",
    "(B) High bias, Low variance\n",
    "\n",
    "(C) Low bias, High variance\n",
    "\n",
    "(D) Low bias, Low variance\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "In Figure 1, subplot III, how are bias and variance related to the true model?\n",
    "\n",
    "(A) High bias, High variance\n",
    "\n",
    "(B) High bias, Low variance\n",
    "\n",
    "(C) Low bias, High variance\n",
    "\n",
    "(D) Low bias, Low variance\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "In Figure 1, subplot IV, how are bias and variance related to the true model?\n",
    "\n",
    "(A) High bias, High variance\n",
    "\n",
    "(B) High bias, Low variance\n",
    "\n",
    "(C) Low bias, High variance\n",
    "\n",
    "(D) Low bias, Low variance\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "Let $x_1, x_2 \\in \\mathbb{R}$ be sampled from the distribution $N(\\mu, 1)$, where $\\mu \\in \\mathbb{R}$ is an unknown variable. Remember that the PDF of the normal distribution is $f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2}}$. Using the log-likelihood, find the maximum likelihood estimation of $\\mu$ in terms of $x_1, x_2$.\n",
    "\n",
    "(A) $\\frac{2}{x_1+x_2}$\n",
    "\n",
    "(B) $\\log \\left(\\frac{e^{x_1}+e^{x_2}}{2}\\right)$\n",
    "\n",
    "(C) $\\frac{\\log(x_1)+\\log(x_2)}{2}$\n",
    "\n",
    "(D) $\\frac{x_1+x_2}{2}$\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "Suppose our data distribution has the property that $y_i = \\beta x_i + c + \\epsilon_i$ for $x_i, \\beta \\in \\mathbb{R}^d$, $c \\in \\mathbb{R}$, $\\epsilon_i \\sim N(0, \\sigma^2)$. Suppose we learn a model $\\hat{\\beta} = \\operatorname{argmin}_{\\gamma} ||\\gamma X - y||_2^2$. True/False: $\\hat{\\beta}$ is an unbiased estimate of $\\beta$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "How will regularizing the weights in a linear regression model change the bias and variance (relative to the same model with no regularization)?\n",
    "\n",
    "(A) Increase bias, increase variance\n",
    "\n",
    "(B) Increase bias, decrease variance\n",
    "\n",
    "(C) Decrease bias, increase variance\n",
    "\n",
    "(D) Decrease bias, decrease variance\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "True/False: Given a fixed training set, the training loss is never larger in a polynomial regression of degree $d + 1$ than in one of degree $d$, where ($d \\ge 1$).\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "True/False: Given both a train and test set, the test loss is always lower in a polynomial regression of degree $d + 1$ than in one of degree $d$, where ($d \\ge 1$).\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "On which factor does the value of irreducible error depend in linear regression?\n",
    "\n",
    "(A) n, the number of observations in the training set\n",
    "\n",
    "(B) m, the dimension of features in the training set\n",
    "\n",
    "(C) $\\sigma^2$, the variance of the noise\n",
    "\n",
    "(D) None of those\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "How does the irreducible error change if we increase the regularization coefficient $\\lambda$ in ridge regression?\n",
    "\n",
    "(A) Increase\n",
    "\n",
    "(B) Decrease\n",
    "\n",
    "(C) Not change\n",
    "\n",
    "(D) The answer depends on the dataset $X$ and true weights $w^*$.\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "True/False: k-fold cross-validation with $k = 100$ is computationally more expensive (slower) than \"leave-one-out\" cross validation. (Assume that there are enough data points to divide the dataset evenly by $k$.)\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "Assume we have a data matrix $X$. Which of the following is a true statement when comparing leave-one-out cross validation (LOOCV) error with the true error?\n",
    "\n",
    "(A) LOOCV error is typically a slight underestimation of the true error of a model trained on $X$.\n",
    "\n",
    "(B) LOOCV error is typically a slight overestimation of the true error of a model trained on $X$.\n",
    "\n",
    "(C) LOOCV error is an unbiased estimator of the true error of a model trained on $X$.\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "True/False: LASSO is a convex optimization problem.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "In LASSO regression, if the regularization parameter $\\lambda = 0$, then which of the following is true?\n",
    "\n",
    "(A) This LASSO model can be used for feature selection.\n",
    "\n",
    "(B) The loss function is as same as the ridge regression loss function.\n",
    "\n",
    "(C) The loss function is as same as the ordinary least square loss function.\n",
    "\n",
    "(D) Large coefficients are penalized.\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "In a LASSO Regression, if the regularization parameter $\\lambda$ is very high, which of the following is true?\n",
    "\n",
    "(A) The model can shrink the coefficients of uninformative features to exactly 0\n",
    "\n",
    "(B) The loss function is as same as the ordinary least square loss function.\n",
    "\n",
    "(C) The loss function is as same as the ridge regression loss function\n",
    "\n",
    "(D) The bias of the model is no lower than the bias of the model with a smaller $\\lambda$.\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "True/False: In LASSO regression, if the regularization parameter $\\lambda$ is very large and two informative features are highly collinear (i.e., that there exists an $\\alpha$ such that $x_{ij} \\approx \\alpha x_{ij'}$ for all $i \\in [n]$), then LASSO will assign one of those coefficients to zero while ridge regression never will.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 25\n",
    "\n",
    "For ridge regression, if the regularization parameter is too large, which of the following is true?\n",
    "\n",
    "(A) Large coefficients will not be penalized\n",
    "\n",
    "(B) The model will underfit the data\n",
    "\n",
    "(C) The loss function will be the as same as the ordinary least square loss function\n",
    "\n",
    "(D) The model will overfit the data\n",
    "\n",
    "## Problem 26\n",
    "\n",
    "True/False: For any convex function $f: \\mathbb{R} \\to \\mathbb{R}$, for any $x \\in \\mathbb{R}$, any $\\lambda \\in (0, 1)$ we have that:\n",
    "\n",
    "$f(\\lambda x_1 + (1 - \\lambda)x_2) \\le (1-\\lambda)f(x_1) + \\lambda f(x_2)$\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 27\n",
    "\n",
    "True/False: All local minimizers for a convex function $f$ are global minimizers for $f$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 28\n",
    "\n",
    "Which function is not a convex function?\n",
    "\n",
    "(A) Sigmoid/Logistic function: $f(x) = 1/(1+e^{-x})$\n",
    "\n",
    "(B) Linear function: $f(x) = 3x$\n",
    "\n",
    "(C) Square function: $f(x) = x^2$\n",
    "\n",
    "(D) ReLU function: $f(x) = \\max\\{x, 0\\}$\n",
    "\n",
    "## Problem 29\n",
    "\n",
    "Which of the following is not a convex set?\n",
    "\n",
    "(A) Unit ball: $\\{x \\in \\mathbb{R}^2 | ||x||_2 \\le 1\\}$\n",
    "\n",
    "(B) Unit sphere: $\\{x \\in \\mathbb{R}^2 | ||x||_2 = 1\\}$\n",
    "\n",
    "(C) Unit cube: $\\{x \\in \\mathbb{R}^2 | 0 \\le x_1 \\le 1, 0 \\le x_2 \\le 1\\}$\n",
    "\n",
    "(D) Line: $\\{x \\in \\mathbb{R}^2 | x_1 + x_2 = 1\\}$\n",
    "\n",
    "## Problem 30\n",
    "\n",
    "True/False: We use stochastic gradient descent instead of gradient descent in order to speed up per-iteration computation at the expense of more variance.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 31\n",
    "\n",
    "<img src=\"./figure_2.png\" width=\"350px\">\n",
    "\n",
    "Figure 2\n",
    "\n",
    "Which of the shapes shown in Figure 2 is a convex shape?\n",
    "\n",
    "(A) Heart\n",
    "\n",
    "(B) Diamonds\n",
    "\n",
    "(C) Club\n",
    "\n",
    "(D) Spade\n",
    "\n",
    "## Problem 32\n",
    "\n",
    "Which of the following is *not* a true statement about gradient descent (GD) vs. stochastic gradient descent (SGD)?\n",
    "\n",
    "(A) Both provide unbiased estimates of the true gradient at each step.\n",
    "\n",
    "(B) The memory and compute requirements of a single update step for both methods scales linearly with the number of features.\n",
    "\n",
    "(C) The memory and compute requirements of a single update step for both methods scales linearly with the number of data points.\n",
    "\n",
    "(D) GD is likely to converge in fewer updates/iterations than SGD, with a properly selected learning rate.\n",
    "\n",
    "## Problem 33\n",
    "\n",
    "Which of the following is a true statement about gradient descent (GD)?\n",
    "\n",
    "(A) When training, we should not update the bias (aka offset, or intercept) term using GD.\n",
    "\n",
    "(B) Decreasing the learning rate, keeping all other hyperparameters fixed, guarantees that the error of our estimated parameters will decrease.\n",
    "\n",
    "(C) GD can be expensive to run on datasets with a large number of samples.\n",
    "\n",
    "(D) An advantage of GD over SGD is that GD requires only a single update step to converge.\n",
    "\n",
    "## Problem 34\n",
    "\n",
    "True/False: The bias of a model is defined as the expected difference between the prediction $\\hat{y}$ and the true value $y$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 35\n",
    "\n",
    "True/False: Consider the sets of features $S \\subseteq S'$. True or false: the bias of the model trained on features in $S'$ is no larger than the bias of the model trained on features in $S$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 36\n",
    "\n",
    "True/False: The cross-validation error is a better estimate of the true error than the training error.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 37\n",
    "\n",
    "True/False: If a model is trained with \"leave-one-out\" cross validation, then the expected error of the model on unseen data is equal to the training error of the model.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n",
    "## Problem 38\n",
    "\n",
    "Write down a closed form solution for the optimal parameters $w$ that minimize the loss function\n",
    "\n",
    "$L(w) = \\sum_{i=1}^{n}(y_i - x_i^T w)^2$\n",
    "\n",
    "in terms of the $n \\times d$ matrix $X$ whose $i$-th row is $x_i^T$ and the $n$ by $1$ vector $y$ whose $i$-th entry is $y_i$. (You may assume that any relevant matrix is invertible.)\n",
    "\n",
    "(A) $\\hat{w} = 2(X^T X)^{-1}X^T y$\n",
    "\n",
    "(B) $\\hat{w} = (X^T X)^{-1}X^T y$\n",
    "\n",
    "(C) $\\hat{w} = (X^T X)^{-1}X y$\n",
    "\n",
    "(D) $\\hat{w} = (X X^T)^{-1}X^T y$\n",
    "\n",
    "## Problem 39\n",
    "\n",
    "True/False: Let $x_1, \\dots, x_n \\in \\mathbb{R}^+$ be sampled i.i.d. from the distribution $\\text{Exp}(\\theta) = \\theta e^{-\\theta x}$, where $\\theta \\in \\mathbb{R}^+$ is an unknown variable. By analyzing the log-likelihood, what is the maximum likelihood estimation of $\\theta$ (in terms of the samples)?\n",
    "\n",
    "(A) $\\frac{1}{n} \\prod_{i=1}^{n} x_i$\n",
    "\n",
    "(B) $\\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "\n",
    "(C) $n / (\\sum_{i=1}^{n} x_i)$\n",
    "\n",
    "(D) $-n / (\\sum_{i=1}^{n} x_i)$\n",
    "\n",
    "## Problem 40\n",
    "\n",
    "True/False: In the least-squares linear regression setting, if we double the data matrix $X$, we double the resulting least squares solution $\\hat{w}$.\n",
    "\n",
    "(A) True\n",
    "\n",
    "(B) False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
