{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 1 Solutions\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Scientists tell us that there is a 10% probability that a person will\n",
    "have the flu this winter. Meanwhile, the CDC reports that 30% of the\n",
    "population will experience flu-like symptoms and that 60% of people with\n",
    "the flu will be symptomatic. What’s the probability that Sarah has the\n",
    "flu given that she has flu-like symptoms?\n",
    "\n",
    "1.  60%\n",
    "\n",
    "2.  20%\n",
    "\n",
    "3.  30%\n",
    "\n",
    "4.  10%\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "True/False: The variance of a model typically decreases as the number of\n",
    "features increases.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Assume you’re given two independent random variables X and Y. X is\n",
    "uniformly distributed on the interval \\[1, 3\\], whereas Y follows a\n",
    "normal distribution with mean 3 and standard deviation 1. What is\n",
    "$(E[XY])^2 – E[X]E[Y]$?\n",
    "\n",
    "1.  3\n",
    "\n",
    "2.  30\n",
    "\n",
    "3.  6\n",
    "\n",
    "4.  0\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "True/False: If the columns of A are orthogonal, then $A^TA$ is diagonal.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "True/False: Assume we train a model on a given dataset. If we were to\n",
    "remove 50% of samples from the dataset and re-train the model from\n",
    "scratch, the new model will be more likely to overfit to its training\n",
    "data than the old one.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "True/False: If $\\{v_1, v_2, \\dots, v_n\\}$ and $\\{w_1, w_2, \\dots, w_n\\}$\n",
    "are linearly independent, then\n",
    "$\\{v_1 + w_1, v_2 + w_2, \\dots, v_n + w_n\\}$ are linearly independent.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "## Problem 7\n",
    "\n",
    "True/False: $E[\\epsilon\\epsilon^T] = I$ where\n",
    "$\\epsilon_i \\sim N(0, \\sigma^2)$ such that $\\epsilon$ is a column\n",
    "vector: $\\epsilon \\in \\mathbb{R}^d$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "<img src=\"./figure_1.png\">\n",
    "\n",
    "Figure 1: The following graphic will be used as a representation of bias\n",
    "and variance. Imagine that a true/correct model is one that always\n",
    "predicts a location at the center of each target (being farther away\n",
    "from the center of the target indicates that a model’s predictions are\n",
    "worse). We retrain a model multiple times, and make a prediction with\n",
    "each trained model. For each of the targets, determine whether the bias\n",
    "and variance is low or high with respect to the true model.\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "In Figure 1, subplot I, how are bias and variance related to the true\n",
    "model?\n",
    "\n",
    "1.  High bias, High variance\n",
    "\n",
    "2.  High bias, Low variance\n",
    "\n",
    "3.  Low bias, High variance\n",
    "\n",
    "4.  Low bias, Low variance\n",
    "\n",
    "**Solution:** The solution is (D).\n",
    "\n",
    "## Problem 9\n",
    "\n",
    "In Figure 1, subplot II, how are bias and variance related to the true\n",
    "model?\n",
    "\n",
    "1.  High bias, High variance\n",
    "\n",
    "2.  High bias, Low variance\n",
    "\n",
    "3.  Low bias, High variance\n",
    "\n",
    "4.  Low bias, Low variance\n",
    "\n",
    "**Solution:** The solution is (C).\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "In Figure 1, subplot III, how are bias and variance related to the true\n",
    "model?\n",
    "\n",
    "1.  High bias, High variance\n",
    "\n",
    "2.  High bias, Low variance\n",
    "\n",
    "3.  Low bias, High variance\n",
    "\n",
    "4.  Low bias, Low variance\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 11\n",
    "\n",
    "In Figure 1, subplot IV, how are bias and variance related to the true\n",
    "model?\n",
    "\n",
    "1.  High bias, High variance\n",
    "\n",
    "2.  High bias, Low variance\n",
    "\n",
    "3.  Low bias, High variance\n",
    "\n",
    "4.  Low bias, Low variance\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 12\n",
    "\n",
    "Let $x_1, x_2 \\in \\mathbb{R}$ be sampled from the distribution\n",
    "$N(\\mu, 1)$, where $\\mu \\in \\mathbb{R}$ is an unknown variable. Remember\n",
    "that the PDF of the normal distribution is\n",
    "$f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2}}$. Using the\n",
    "log-likelihood, find the maximum likelihood estimation of $\\mu$ in terms\n",
    "of $x_1, x_2$.\n",
    "\n",
    "1.  $\\frac{2}{x_1+x_2}$\n",
    "\n",
    "2.  $\\log \\left(\\frac{e^{x_1}+e^{x_2}}{2}\\right)$\n",
    "\n",
    "3.  $\\frac{\\log(x_1)+\\log(x_2)}{2}$\n",
    "\n",
    "4.  $\\frac{x_1+x_2}{2}$\n",
    "\n",
    "**Solution:** The answer is (D).\n",
    "\n",
    "## Problem 13\n",
    "\n",
    "Suppose our data distribution has the property that\n",
    "$y_i = \\beta x_i + c + \\epsilon_i$ for $x_i, \\beta \\in \\mathbb{R}^d$,\n",
    "$c \\in \\mathbb{R}$, $\\epsilon_i \\sim N(0, \\sigma^2)$. Suppose we learn a\n",
    "model\n",
    "$\\hat{\\beta} = \\operatorname{argmin}_{\\gamma} ||\\gamma X - y||_2^2$.\n",
    "True/False: $\\hat{\\beta}$ is an unbiased estimate of $\\beta$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "## Problem 14\n",
    "\n",
    "How will regularizing the weights in a linear regression model change\n",
    "the bias and variance (relative to the same model with no\n",
    "regularization)?\n",
    "\n",
    "1.  Increase bias, increase variance\n",
    "\n",
    "2.  Increase bias, decrease variance\n",
    "\n",
    "3.  Decrease bias, increase variance\n",
    "\n",
    "4.  Decrease bias, decrease variance\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 15\n",
    "\n",
    "True/False: Given a fixed training set, the training loss is never\n",
    "larger in a polynomial regression of degree $d + 1$ than in one of\n",
    "degree $d$, where ($d \\ge 1$).\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 16\n",
    "\n",
    "True/False: Given both a train and test set, the test loss is always\n",
    "lower in a polynomial regression of degree $d + 1$ than in one of degree\n",
    "$d$, where ($d \\ge 1$).\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 17\n",
    "\n",
    "On which factor does the value of irreducible error depend in linear\n",
    "regression?\n",
    "\n",
    "1.  n, the number of observations in the training set\n",
    "\n",
    "2.  m, the dimension of features in the training set\n",
    "\n",
    "3.  $\\sigma^2$, the variance of the noise\n",
    "\n",
    "4.  None of those\n",
    "\n",
    "**Solution:** The solution is (C).\n",
    "\n",
    "## Problem 18\n",
    "\n",
    "How does the irreducible error change if we increase the regularization\n",
    "coefficient $\\lambda$ in ridge regression?\n",
    "\n",
    "1.  Increase\n",
    "\n",
    "2.  Decrease\n",
    "\n",
    "3.  Not change\n",
    "\n",
    "4.  The answer depends on the dataset $X$ and true weights $w^*$.\n",
    "\n",
    "**Solution:** The solution is (C).\n",
    "\n",
    "## Problem 19\n",
    "\n",
    "True/False: k-fold cross-validation with $k = 100$ is computationally\n",
    "more expensive (slower) than “leave-one-out” cross validation. (Assume\n",
    "that there are enough data points to divide the dataset evenly by $k$.)\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 20\n",
    "\n",
    "Assume we have a data matrix $X$. Which of the following is a true\n",
    "statement when comparing leave-one-out cross validation (LOOCV) error\n",
    "with the true error?\n",
    "\n",
    "1.  LOOCV error is typically a slight underestimation of the true error\n",
    "    of a model trained on $X$.\n",
    "\n",
    "2.  LOOCV error is typically a slight overestimation of the true error\n",
    "    of a model trained on $X$.\n",
    "\n",
    "3.  LOOCV error is an unbiased estimator of the true error of a model\n",
    "    trained on $X$.\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 21\n",
    "\n",
    "True/False: LASSO is a convex optimization problem.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "## Problem 22\n",
    "\n",
    "In LASSO regression, if the regularization parameter $\\lambda = 0$, then\n",
    "which of the following is true?\n",
    "\n",
    "1.  This LASSO model can be used for feature selection.\n",
    "\n",
    "2.  The loss function is as same as the ridge regression loss function.\n",
    "\n",
    "3.  The loss function is as same as the ordinary least square loss\n",
    "    function.\n",
    "\n",
    "4.  Large coefficients are penalized.\n",
    "\n",
    "**Solution:** No solution is provided for this question.\n",
    "\n",
    "## Problem 23\n",
    "\n",
    "In a LASSO Regression, if the regularization parameter $\\lambda$ is very\n",
    "high, which of the following is true?\n",
    "\n",
    "1.  The model can shrink the coefficients of uninformative features to\n",
    "    exactly 0\n",
    "\n",
    "2.  The loss function is as same as the ordinary least square loss\n",
    "    function.\n",
    "\n",
    "3.  The loss function is as same as the ridge regression loss function\n",
    "\n",
    "4.  The bias of the model is no lower than the bias of the model with a\n",
    "    smaller $\\lambda$.\n",
    "\n",
    "**Solution:** The intended solution was (A), but (D) also accepted.\n",
    "\n",
    "## Problem 24\n",
    "\n",
    "True/False: In LASSO regression, if the regularization parameter\n",
    "$\\lambda$ is very large and two informative features are highly\n",
    "collinear (i.e., that there exists an $\\alpha$ such that\n",
    "$x_{ij} \\approx \\alpha x_{ij'}$ for all $i \\in [n]$), then LASSO will\n",
    "assign one of those coefficients to zero while ridge regression never\n",
    "will.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 25\n",
    "\n",
    "For ridge regression, if the regularization parameter is too large,\n",
    "which of the following is true?\n",
    "\n",
    "1.  Large coefficients will not be penalized\n",
    "2.  The model will underfit the data\n",
    "3.  The loss function will be the as same as the ordinary least square\n",
    "    loss function\n",
    "4.  The model will overfit the data\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 26\n",
    "\n",
    "True/False: For any convex function $f: \\mathbb{R} \\to \\mathbb{R}$, for\n",
    "any $x \\in \\mathbb{R}$, any $\\lambda \\in (0, 1)$ we have that:\n",
    "\n",
    "$f(\\lambda x_1 + (1 - \\lambda)x_2) \\le (1-\\lambda)f(x_1) + \\lambda f(x_2)$\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "## Problem 27\n",
    "\n",
    "True/False: All local minimizers for a convex function $f$ are global\n",
    "minimizers for $f$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "## Problem 28\n",
    "\n",
    "Which function is not a convex function?\n",
    "\n",
    "1.  Sigmoid/Logistic function: $f(x) = 1/(1+e^{-x})$\n",
    "2.  Linear function: $f(x) = 3x$\n",
    "3.  Square function: $f(x) = x^2$\n",
    "4.  ReLU function: $f(x) = \\max\\{x, 0\\}$\n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "## Problem 29\n",
    "\n",
    "Which of the following is not a convex set?\n",
    "\n",
    "1.  Unit ball: $\\{x \\in \\mathbb{R}^2 | ||x||_2 \\le 1\\}$\n",
    "2.  Unit sphere: $\\{x \\in \\mathbb{R}^2 | ||x||_2 = 1\\}$\n",
    "3.  Unit cube:\n",
    "    $\\{x \\in \\mathbb{R}^2 | 0 \\le x_1 \\le 1, 0 \\le x_2 \\le 1\\}$\n",
    "4.  Line: $\\{x \\in \\mathbb{R}^2 | x_1 + x_2 = 1\\}$\n",
    "\n",
    "**Solution:** No solution is provided for this question.\n",
    "\n",
    "## Problem 30\n",
    "\n",
    "True/False: We use stochastic gradient descent instead of gradient\n",
    "descent in order to speed up per-iteration computation at the expense of\n",
    "more variance.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (A).\n",
    "\n",
    "## Problem 31\n",
    "\n",
    "<img src=\"./figure_2.png\" width=\"350px\">\n",
    "\n",
    "Figure 2\n",
    "\n",
    "Which of the shapes shown in Figure 2 is a convex shape?\n",
    "\n",
    "1.  Heart\n",
    "2.  Diamonds\n",
    "3.  Club\n",
    "4.  Spade\n",
    "\n",
    "**Solution:** The answer is (B).\n",
    "\n",
    "## Problem 32\n",
    "\n",
    "Which of the following is *not* a true statement about gradient descent\n",
    "(GD) vs. stochastic gradient descent (SGD)?\n",
    "\n",
    "1.  Both provide unbiased estimates of the true gradient at each step.\n",
    "2.  The memory and compute requirements of a single update step for both\n",
    "    methods scales linearly with the number of features.\n",
    "3.  The memory and compute requirements of a single update step for both\n",
    "    methods scales linearly with the number of data points.\n",
    "4.  GD is likely to converge in fewer updates/iterations than SGD, with\n",
    "    a properly selected learning rate.\n",
    "\n",
    "**Solution:** The solution is (C).\n",
    "\n",
    "## Problem 33\n",
    "\n",
    "Which of the following is a true statement about gradient descent (GD)?\n",
    "\n",
    "1.  When training, we should not update the bias (aka offset, or\n",
    "    intercept) term using GD.\n",
    "2.  Decreasing the learning rate, keeping all other hyperparameters\n",
    "    fixed, guarantees that the error of our estimated parameters will\n",
    "    decrease.\n",
    "3.  GD can be expensive to run on datasets with a large number of\n",
    "    samples.\n",
    "4.  An advantage of GD over SGD is that GD requires only a single update\n",
    "    step to converge.\n",
    "\n",
    "**Solution:** The solution is (C).\n",
    "\n",
    "## Problem 34\n",
    "\n",
    "True/False: The bias of a model is defined as the expected difference\n",
    "between the prediction $\\hat{y}$ and the true value $y$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** In Autumn 2022, we accepted either answer due to ambiguity\n",
    "in the question.\n",
    "\n",
    "## Problem 35\n",
    "\n",
    "True/False: Consider the sets of features $S \\subseteq S'$. True or\n",
    "false: the bias of the model trained on features in $S'$ is no larger\n",
    "than the bias of the model trained on features in $S$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 36\n",
    "\n",
    "True/False: The cross-validation error is a better estimate of the true\n",
    "error than the training error.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (A).\n",
    "\n",
    "## Problem 37\n",
    "\n",
    "True/False: If a model is trained with “leave-one-out” cross validation,\n",
    "then the expected error of the model on unseen data is equal to the\n",
    "training error of the model.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 38\n",
    "\n",
    "Write down a closed form solution for the optimal parameters $w$ that\n",
    "minimize the loss function\n",
    "\n",
    "$L(w) = \\sum_{i=1}^{n}(y_i - x_i^T w)^2$\n",
    "\n",
    "in terms of the $n \\times d$ matrix $X$ whose $i$-th row is $x_i^T$ and\n",
    "the $n$ by $1$ vector $y$ whose $i$-th entry is $y_i$. (You may assume\n",
    "that any relevant matrix is invertible.)\n",
    "\n",
    "1.  $\\hat{w} = 2(X^T X)^{-1}X^T y$\n",
    "2.  $\\hat{w} = (X^T X)^{-1}X^T y$\n",
    "3.  $\\hat{w} = (X^T X)^{-1}X y$\n",
    "4.  $\\hat{w} = (X X^T)^{-1}X^T y$\n",
    "\n",
    "**Solution:** The solution is (B).\n",
    "\n",
    "## Problem 39\n",
    "\n",
    "True/False: Let $x_1, \\dots, x_n \\in \\mathbb{R}^+$ be sampled i.i.d.\n",
    "from the distribution $\\text{Exp}(\\theta) = \\theta e^{-\\theta x}$, where\n",
    "$\\theta \\in \\mathbb{R}^+$ is an unknown variable. By analyzing the\n",
    "log-likelihood, what is the maximum likelihood estimation of $\\theta$\n",
    "(in terms of the samples)?\n",
    "\n",
    "1.  $\\frac{1}{n} \\prod_{i=1}^{n} x_i$\n",
    "2.  $\\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "3.  $n / (\\sum_{i=1}^{n} x_i)$\n",
    "4.  $-n / (\\sum_{i=1}^{n} x_i)$\n",
    "\n",
    "**Solution:** The answer is (C).\n",
    "\n",
    "## Problem 40\n",
    "\n",
    "True/False: In the least-squares linear regression setting, if we double\n",
    "the data matrix $X$, we double the resulting least squares solution\n",
    "$\\hat{w}$.\n",
    "\n",
    "1.  True\n",
    "\n",
    "2.  False\n",
    "\n",
    "**Solution:** The answer is (B)."
   ],
   "id": "ecea0c59-5769-403a-849c-6bb9736d533f"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
