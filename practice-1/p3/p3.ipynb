{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63013859-8824-4df3-ab3e-76a2202afaf4",
   "metadata": {},
   "source": [
    "# Practice 3\n",
    "\n",
    "**1. We need to fit a function to our dataset $\\{(x_i, y_i)\\}_{i=1}^n$. Suppose our dataset looks like the following:**\n",
    "\n",
    "<img src=\"./dataset_plot.png\" width=\"450px\">\n",
    "\n",
    "**We decide to expand our features with general basis functions to improve our estimator:**\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "\\rightarrow\n",
    "\\begin{pmatrix}\n",
    "x_1 & g(x_1) & h(x_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_n & g(x_n) & h(x_n)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**Which of the following choices of $g$ and $h$ are most likely to produce the best estimator function?**\n",
    "\n",
    "*   (a) $g(x) = \\log(x)$, $h(x) = x^2$\n",
    "*   (b) $g(x) = x^4$, $h(x) = x^2$\n",
    "*   (c) $g(x) = \\sin(x)$, $h(x) = x^2$\n",
    "*   (d) $g(x) = \\cos(x)$, $h(x) = x$\n",
    "\n",
    "**2. Irreducible error can be completely eliminated by:**\n",
    "\n",
    "*   (a) Collecting more training data\n",
    "*   (b) Tuning hyperparameters of the model\n",
    "*   (c) Regularizing the model\n",
    "*   (d) None of the above\n",
    "\n",
    "**3. Increasing the regularization of a model would typically:**\n",
    "\n",
    "*   (a) Increase its bias and increase its variance\n",
    "*   (b) Increase its bias and decrease its variance\n",
    "*   (c) Decrease its bias and increase its variance\n",
    "*   (d) Decrease its bias and decrease its variance\n",
    "\n",
    "**4. In a binary classification problem with balanced classes (exactly the same number of positive examples as negative examples), a machine learning model has an accuracy of 85% and misclassifies 10% of positive examples as negative. What is the probability that the model will correctly classify a negative sample?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**5. The below figures are graphs of some loss functions with Loss on the Vertical axis and weight variables on the horizontal axes.**\n",
    "\n",
    "<img src=\"./loss_function_a.png\" width=\"350px\">\n",
    "\n",
    "<img src=\"./loss_function_b.png\" width=\"350px\">\n",
    "\n",
    "**Which graph represents a Ridge Regression Loss function?**\n",
    "\n",
    "*   (a) Graph A\n",
    "*   (b) Graph B\n",
    "\n",
    "**6. Irreducible error in machine learning is caused by:**\n",
    "\n",
    "*   (a) Noise in the data\n",
    "*   (b) Bias in the model\n",
    "*   (c) Variance in the model\n",
    "*   (d) Overfitting of the model\n",
    "\n",
    "**7. Suppose that we are given train, validation, and test sets. Which set(s) should be used to standardize the test data when generating a prediction?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**8. Suppose we are performing leave-one-out (LOO) validation and 10-fold cross validation on a dataset of size 100,000 to pick between 4 different values of a single hyperparameter. How many times greater is the number of models that need to be trained for LOO validation versus 10-fold cross validation?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**9. What are two possible ways to reduce the variance of a model?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**10. Below are a list of potential advantages and disadvantages of stochastic gradient descent(SGD). Select all that are true regarding SGD.**\n",
    "\n",
    "**Advantages:**\n",
    "*   (a) SGD is more memory-efficient because it takes a smaller number of samples at a time compared to classical gradient descent which takes the entire dataset into weight update\n",
    "*   (b) In SGD, the update on weight $w_{t+1}$ has lower variance because it doesn't take many samples into account at a time\n",
    "\n",
    "**Disadvantages:**\n",
    "*   (c) The noise in the dataset has higher impact on the stability of SGD than on that of the classical gradient descent.\n",
    "*   (d) SGD is more sensitive to learning rate compared to classical gradient descent\n",
    "*   (e) It's more computationally inefficient to use SGD for a large dataset than to use classical gradient descent because it requires more resources to randomly sample a data point for the weight update\n",
    "\n",
    "**11. Which of the following is not a convex function?**\n",
    "*   (a) $f(x) = x$\n",
    "*   (b) $f(x) = x^2$\n",
    "*   (c) $f(x) = e^x$\n",
    "*   (d) $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "**12. Recall the loss function used in ridge regression,**\n",
    "\n",
    "$$f(w) = \\sum_{i=1}^{n} (y_i - x_i^T w)^2 + \\lambda ||w||_2^2$$\n",
    "\n",
    "**What happens to the weights as $\\lambda \\rightarrow \\infty$?**\n",
    "*   (a) Weights approach positive infinity.\n",
    "*   (b) Weights approach 0.\n",
    "*   (c) Weights approach negative infinity.\n",
    "*   (d) Not enough information.\n",
    "\n",
    "**13. Why is it important to use a different test set to evaluate the final performance of the model, rather than the validation set used during model selection?**\n",
    "*   (a) The model may have overfit to the validation set\n",
    "*   (b) The test set is a better representation of new, unseen data\n",
    "*   (c) Both a and b\n",
    "*   (d) None of the above\n",
    "\n",
    "**14. What is cross-validation not used for?**\n",
    "*   (a) To evaluate the performance of a machine learning model on unseen data.\n",
    "*   (b) To select a model's hyperparameters.\n",
    "*   (c) To determine the generalization of a machine learning model.\n",
    "*   (d) To train multiple machine learning models on different datasets.\n",
    "\n",
    "**15. The plots below show linear regression results on the basis of only three data points.**\n",
    "\n",
    "<img src=\"./linear_regression_plots.png\" width=\"600px\">\n",
    "\n",
    "**Which plot would result from using the following objective, where $\\lambda = 10$?**\n",
    "\n",
    "$$f(w) = \\sum_{i=1}^{3} (y_i - wx_i - b)^2 + \\lambda w^2$$\n",
    "\n",
    "*   (a) Plot A\n",
    "*   (b) Plot B\n",
    "*   (c) Plot C\n",
    "*   (d) Plot D\n",
    "\n",
    "**16. Let $a \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}^m$. Let $C = ab^T$. What are the dimensions of the matrix $C$, what is the range of $C$, and what is the null space of $C$?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**17. What is the objective of least squares regression?**\n",
    "*   (a) To minimize the sum of the absolute differences between predicted and actual values.\n",
    "*   (b) To minimize the sum of the squared differences between predicted and actual values.\n",
    "*   (c) To maximize the number of points on the line of best fit.\n",
    "\n",
    "**18. An unbiased machine learning model is trained on a dataset with noisy features and achieves a prediction accuracy of 75%. If the irreducible error due to noise in the features is estimated to be 10%, what is the estimated variance of the model?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**19. Convexity is a desirable property in machine learning because it:**\n",
    "*   (a) guarantees gradient descent finds a global minimum in optimization problems for functions that have a global minimum\n",
    "*   (b) helps to avoid the model overfitting\n",
    "*   (c) speeds up model training\n",
    "*   (d) reduces model complexity\n",
    "\n",
    "**20. True/False: Stochastic gradient descent typically results in a smoother convergence plot (loss vs. epochs) as compared to gradient descent.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**21. Consider the univariate function $f(x) = x^2$. This function has a unique minimum at $x^*= 0$. We're using gradient descent (GD) to find this minimum and at time $t$ we arrive at the point $x_t = 2$. What is the step size that would bring us to $x^*$ at time $t + 1$?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**22. Let $X \\in \\mathbb{R}^{m \\times n}$, $w \\in \\mathbb{R}^n$, $Y \\in \\mathbb{R}^m$, and $R(w)$ be some regularization function from $\\mathbb{R}^n \\to \\mathbb{R}$. Consider mean squared error with regularization $L(w) = \\|Xw - Y\\|_2^2 + \\lambda R(w)$. What is $\\nabla_w L(w)$?**\n",
    "*   (a) $2Y^T (Xw - Y) + \\lambda$\n",
    "*   (b) $2X^T (X^T Xw - Y) + \\lambda \\nabla_w R(w)$\n",
    "*   (c) $2X^T (Xw - Y) + \\lambda \\nabla_w R(w)$\n",
    "*   (d) $2Y^T (X^T Xw - Y) + \\lambda R(w)$\n",
    "\n",
    "**23. Suppose we have $n$ Gaussian distributions $\\mathcal{D}_1, \\dots, \\mathcal{D}_n$, where each $\\mathcal{D}_i = \\mathcal{N}(\\mu_i, \\sigma^2)$. In other words, each Gaussian distribution shares the same variance $\\sigma^2$, but may have different means $\\mu_i$. For each distribution, we draw a single data point $X_i \\sim \\mathcal{D}_i$. Given**\n",
    "$$\\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{pmatrix}, \\text{ we want to predict } \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}$$\n",
    "**Solution 1 is to predict $X$, and Solution 2 is to predict $\\frac{7}{8}X$. Why might Solution 2 produce lower mean squared error than Solution 1?**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
