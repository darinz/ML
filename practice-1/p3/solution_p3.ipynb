{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2542518-7f10-4b12-82fd-e73a123546b8",
   "metadata": {},
   "source": [
    "# Practice 3 Solutions\n",
    "\n",
    "**Problem 1. We need to fit a function to our dataset $\\{(x_i, y_i)\\}_{i=1}^n$. Suppose our dataset looks like the following:**\n",
    "\n",
    "<img src=\"./dataset_plot.png\" width=\"450px\">\n",
    "\n",
    "**We decide to expand our features with general basis functions to improve our estimator:**\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "\\rightarrow\n",
    "\\begin{pmatrix}\n",
    "x_1 & g(x_1) & h(x_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_n & g(x_n) & h(x_n)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**Which of the following choices of $g$ and $h$ are most likely to produce the best estimator function?**\n",
    "\n",
    "*   (a) $g(x) = \\log(x)$, $h(x) = x^2$\n",
    "*   (b) $g(x) = x^4$, $h(x) = x^2$\n",
    "*   (c) $g(x) = \\sin(x)$, $h(x) = x^2$\n",
    "*   (d) $g(x) = \\cos(x)$, $h(x) = x$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Looking at the dataset plot, we can see a **periodic wave-like pattern** that suggests **sinusoidal functions** would be most appropriate.\n",
    "\n",
    "**Why (c) is correct:**\n",
    "\n",
    "**1. Sinusoidal pattern recognition:**\n",
    "- The data shows **periodic oscillations** around a curved trend\n",
    "- **g(x) = sin(x)** captures the periodic component perfectly\n",
    "- **h(x) = x²** captures the underlying quadratic trend\n",
    "\n",
    "**2. Why other options fail:**\n",
    "\n",
    "**Option (a): g(x) = log(x), h(x) = x²**\n",
    "- **log(x)** is undefined for x < 0, making it unsuitable for general data\n",
    "- The periodic pattern is not captured\n",
    "\n",
    "**Option (b): g(x) = x⁴, h(x) = x²**\n",
    "- **Polynomial functions** cannot capture periodic behavior\n",
    "- The data shows clear oscillations, not just polynomial trends\n",
    "\n",
    "**Option (d): g(x) = cos(x), h(x) = x**\n",
    "- **cos(x)** could work, but **h(x) = x** doesn't capture the quadratic trend\n",
    "- **x²** is better than **x** for the underlying curvature\n",
    "\n",
    "**Key insight:** Choose basis functions that **match the data structure** - periodic data needs periodic functions.\n",
    "\n",
    "**Problem 2. Irreducible error can be completely eliminated by:**\n",
    "\n",
    "*   (a) Collecting more training data\n",
    "*   (b) Tuning hyperparameters of the model\n",
    "*   (c) Regularizing the model\n",
    "*   (d) None of the above\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Irreducible error cannot be completely eliminated** by any of the listed methods. This is because irreducible error represents the **fundamental uncertainty** in the data generation process.\n",
    "\n",
    "**Why each option fails:**\n",
    "\n",
    "**Option (a): Collecting more training data**\n",
    "- More data can reduce **reducible error** (bias and variance)\n",
    "- But **irreducible error** is inherent to the data itself\n",
    "- No amount of data can eliminate measurement noise or natural variability\n",
    "\n",
    "**Option (b): Tuning hyperparameters**\n",
    "- Hyperparameter tuning optimizes **model performance**\n",
    "- It can reduce bias and variance, but not irreducible error\n",
    "- The fundamental uncertainty remains regardless of model choice\n",
    "\n",
    "**Option (c): Regularizing the model**\n",
    "- Regularization affects **model complexity** and generalization\n",
    "- It trades bias for variance, but cannot touch irreducible error\n",
    "- The inherent noise in the data persists\n",
    "\n",
    "**Key insight:** Irreducible error is a **lower bound** on model performance that cannot be overcome by any modeling technique.\n",
    "\n",
    "**Problem 3. Increasing the regularization of a model would typically:**\n",
    "\n",
    "*   (a) Increase its bias and increase its variance\n",
    "*   (b) Increase its bias and decrease its variance\n",
    "*   (c) Decrease its bias and increase its variance\n",
    "*   (d) Decrease its bias and decrease its variance\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Regularization increases bias and decreases variance** - this is the fundamental **bias-variance tradeoff**.\n",
    "\n",
    "**Why this happens:**\n",
    "\n",
    "**1. Increased Bias:**\n",
    "- **Constrained parameters** - regularization prevents the model from fitting training data too closely\n",
    "- **Simpler model** - the model becomes less flexible and may miss true patterns\n",
    "- **Underfitting risk** - excessive regularization can lead to underfitting\n",
    "\n",
    "**2. Decreased Variance:**\n",
    "- **Reduced overfitting** - regularization makes the model less sensitive to training data noise\n",
    "- **More stable predictions** - the model becomes more consistent across different datasets\n",
    "- **Better generalization** - the model generalizes better to unseen data\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **L2 regularization:** $\\min \\|y - Xw\\|^2 + \\lambda\\|w\\|^2$\n",
    "- **L1 regularization:** $\\min \\|y - Xw\\|^2 + \\lambda\\|w\\|_1$\n",
    "- Both penalize large coefficients, forcing them toward zero\n",
    "\n",
    "**Key insight:** The optimal regularization strength **balances** this tradeoff for best generalization performance.\n",
    "\n",
    "**Problem 4. In a binary classification problem with balanced classes (exactly the same number of positive examples as negative examples), a machine learning model has an accuracy of 85% and misclassifies 10% of positive examples as negative. What is the probability that the model will correctly classify a negative sample?**\n",
    "\n",
    "**Answer:** 80%\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **confusion matrix** problem with balanced classes. Let's solve it step by step.\n",
    "\n",
    "**Given information:**\n",
    "- **Balanced classes:** 50% positive, 50% negative\n",
    "- **Overall accuracy:** 85%\n",
    "- **False negative rate:** 10% (misclassifies 10% of positives as negative)\n",
    "\n",
    "**Let's assume 100 total samples:**\n",
    "- **50 positive** samples\n",
    "- **50 negative** samples\n",
    "\n",
    "**From the false negative rate:**\n",
    "- **False negatives:** 10% of 50 = 5 positive samples misclassified as negative\n",
    "- **True positives:** 50 - 5 = 45 positive samples correctly classified\n",
    "\n",
    "**From overall accuracy:**\n",
    "- **Total correct predictions:** 85% of 100 = 85 samples\n",
    "- **True positives:** 45 (from above)\n",
    "- **True negatives:** 85 - 45 = 40 negative samples correctly classified\n",
    "\n",
    "**Probability of correctly classifying a negative sample:**\n",
    "\n",
    "$\\text{True negative rate} = \\frac{\\text{True negatives}}{\\text{Total negatives}} = \\frac{40}{50} = 0.8 = 80\\%$\n",
    "\n",
    "**Key insight:** Use the **confusion matrix** framework to break down the problem systematically.\n",
    "\n",
    "**Problem 5. The below figures are graphs of some loss functions with Loss on the Vertical axis and weight variables on the horizontal axes.**\n",
    "\n",
    "<img src=\"./loss_function_a.png\" width=\"350px\">\n",
    "\n",
    "<img src=\"./loss_function_b.png\" width=\"350px\">\n",
    "\n",
    "**Which graph represents a Ridge Regression Loss function?**\n",
    "\n",
    "*   (a) Graph A\n",
    "*   (b) Graph B\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Graph B** represents the Ridge Regression loss function because it shows a **smooth, bowl-shaped surface** typical of L2 regularization.\n",
    "\n",
    "**Why Graph B is Ridge Regression:**\n",
    "\n",
    "**1. Mathematical form:**\n",
    "\n",
    "$L(w) = \\|y - Xw\\|^2 + \\lambda\\|w\\|^2$\n",
    "\n",
    "**2. Visual characteristics:**\n",
    "- **Smooth contours** - L2 penalty creates smooth, differentiable surface\n",
    "- **Circular/elliptical contours** - L2 norm creates circular constraint regions\n",
    "- **No sharp corners** - unlike L1 regularization which creates diamond-shaped contours\n",
    "\n",
    "**3. Why Graph A is not Ridge:**\n",
    "- **Sharp edges/corners** - suggests L1 regularization (LASSO)\n",
    "- **Diamond-shaped contours** - characteristic of L1 penalty\n",
    "- **Non-differentiable points** - L1 creates non-differentiable corners\n",
    "\n",
    "**Key insight:** **L2 regularization** creates **smooth, circular contours**, while **L1 regularization** creates **sharp, diamond-shaped contours**.\n",
    "\n",
    "**Problem 6. Irreducible error in machine learning is caused by:**\n",
    "\n",
    "*   (a) Noise in the data\n",
    "*   (b) Bias in the model\n",
    "*   (c) Variance in the model\n",
    "*   (d) Overfitting of the model\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Irreducible error is caused by noise in the data** - this represents the fundamental uncertainty that cannot be eliminated by any model.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. Definition of irreducible error:**\n",
    "- **Inherent uncertainty** in the data generation process\n",
    "- **Measurement noise** from sensors or instruments\n",
    "- **Natural variability** in the underlying phenomenon\n",
    "- **Missing information** that affects the outcome\n",
    "\n",
    "**2. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Bias in the model**\n",
    "- This is **reducible error** that can be reduced with better model selection\n",
    "- Model bias can be addressed through more complex models or better features\n",
    "\n",
    "**Option (c): Variance in the model**\n",
    "- This is also **reducible error** that can be reduced with regularization or more data\n",
    "- Model variance can be addressed through ensemble methods or simpler models\n",
    "\n",
    "**Option (d): Overfitting of the model**\n",
    "- This is a **modeling issue** that can be fixed with proper regularization\n",
    "- Overfitting is a reducible source of error\n",
    "\n",
    "**Key insight:** **Irreducible error** comes from the **data itself**, while **reducible error** comes from **modeling choices**.\n",
    "\n",
    "**Problem 7. Suppose that we are given train, validation, and test sets. Which set(s) should be used to standardize the test data when generating a prediction?**\n",
    "\n",
    "**Answer:** Use the mean and standard deviation from the training data\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Data leakage** occurs when we use information from the test set during preprocessing, which violates the principle of keeping the test set completely unseen.\n",
    "\n",
    "**Why use training data statistics:**\n",
    "\n",
    "**1. Preventing data leakage:**\n",
    "- **Test set should be completely unseen** during model development\n",
    "- Using test set statistics gives the model **unfair advantage**\n",
    "- This would make performance estimates **overly optimistic**\n",
    "\n",
    "**2. Proper workflow:**\n",
    "\n",
    "1. Calculate $\\mu_{\\text{train}}$, $\\sigma_{\\text{train}}$ from training data\n",
    "2. Standardize training data: $(x - \\mu_{\\text{train}}) / \\sigma_{\\text{train}}$\n",
    "3. Standardize validation data: $(x - \\mu_{\\text{train}}) / \\sigma_{\\text{train}}$\n",
    "4. Standardize test data: $(x - \\mu_{\\text{train}}) / \\sigma_{\\text{train}}$\n",
    "\n",
    "**3. Consequences of using test statistics:**\n",
    "- **Different predictions** depending on test set size\n",
    "- **Unrealistic performance estimates**\n",
    "- **Violation of generalization principle**\n",
    "\n",
    "**Alternative:** Using statistics from **train + validation** combined is also acceptable, as validation data is used for model selection.\n",
    "\n",
    "**Key insight:** **Never use test set information** during model development to maintain honest performance estimates.\n",
    "\n",
    "**Problem 8. Suppose we are performing leave-one-out (LOO) validation and 10-fold cross validation on a dataset of size 100,000 to pick between 4 different values of a single hyperparameter. How many times greater is the number of models that need to be trained for LOO validation versus 10-fold cross validation?**\n",
    "\n",
    "**Answer:** 10,000\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **computational complexity** comparison between LOO and k-fold cross-validation.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "**LOO validation:**\n",
    "- **Number of models per hyperparameter:** n = 100,000\n",
    "- **Total models:** 4 × 100,000 = 400,000 models\n",
    "\n",
    "**10-fold cross-validation:**\n",
    "- **Number of models per hyperparameter:** k = 10\n",
    "- **Total models:** 4 × 10 = 40 models\n",
    "\n",
    "**Ratio:**\n",
    "\n",
    "$\\text{LOO models} / \\text{10-fold models} = 400,000 / 40 = 10,000$\n",
    "\n",
    "**Why this happens:**\n",
    "\n",
    "**1. LOO validation:**\n",
    "- Trains **n models** (one for each data point)\n",
    "- Each model uses **n-1 training points**\n",
    "- Very computationally expensive for large datasets\n",
    "\n",
    "**2. 10-fold cross-validation:**\n",
    "- Trains **k models** (k = 10)\n",
    "- Each model uses **9n/10 training points**\n",
    "- Much more computationally efficient\n",
    "\n",
    "**Key insight:** LOO becomes **prohibitively expensive** for large datasets, making k-fold CV the practical choice.\n",
    "\n",
    "**Problem 9. What are two possible ways to reduce the variance of a model?**\n",
    "\n",
    "**Answer:** \n",
    "1. Use more training data\n",
    "2. Use a less complex model\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Variance reduction** is crucial for improving model generalization. Here are the two main approaches:\n",
    "\n",
    "**1. Use more training data:**\n",
    "- **Larger sample size** reduces parameter estimation uncertainty\n",
    "- **More stable estimates** of model parameters\n",
    "- **Better generalization** to unseen data\n",
    "- **Natural regularization** effect from averaging over more examples\n",
    "\n",
    "**2. Use a less complex model:**\n",
    "- **Fewer parameters** to estimate from the same amount of data\n",
    "- **Reduced overfitting** risk\n",
    "- **More stable predictions** across different datasets\n",
    "- **Better bias-variance tradeoff**\n",
    "\n",
    "**Additional methods (not required but worth mentioning):**\n",
    "- **Regularization** (L1/L2 penalties)\n",
    "- **Ensemble methods** (bagging, random forests)\n",
    "- **Early stopping** during training\n",
    "- **Cross-validation** for model selection\n",
    "\n",
    "**Key insight:** **More data** and **simpler models** are the most effective ways to reduce variance and improve generalization.\n",
    "\n",
    "**Problem 10. Below are a list of potential advantages and disadvantages of stochastic gradient descent(SGD). Select all that are true regarding SGD.**\n",
    "\n",
    "**Advantages:**\n",
    "*   (a) SGD is more memory-efficient because it takes a smaller number of samples at a time compared to classical gradient descent which takes the entire dataset into weight update\n",
    "*   (b) In SGD, the update on weight $w_{t+1}$ has lower variance because it doesn't take many samples into account at a time\n",
    "\n",
    "**Disadvantages:**\n",
    "*   (c) The noise in the dataset has higher impact on the stability of SGD than on that of the classical gradient descent.\n",
    "*   (d) SGD is more sensitive to learning rate compared to classical gradient descent\n",
    "*   (e) It's more computationally inefficient to use SGD for a large dataset than to use classical gradient descent because it requires more resources to randomly sample a data point for the weight update\n",
    "\n",
    "**Correct answers:** (a), (c), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**SGD advantages and disadvantages** compared to full gradient descent:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "**Option (a): Memory efficiency ✓**\n",
    "- **Smaller batches** require less memory per iteration\n",
    "- **Scalable** to large datasets that don't fit in memory\n",
    "- **Practical** for big data applications\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "**Option (c): Noise sensitivity ✓**\n",
    "- **Stochastic noise** from mini-batches affects stability\n",
    "- **Higher variance** in gradient estimates\n",
    "- **Less stable** convergence compared to full gradient descent\n",
    "\n",
    "**Option (d): Learning rate sensitivity ✓**\n",
    "- **More sensitive** to learning rate choice due to noise\n",
    "- **Requires careful tuning** of learning rate schedule\n",
    "- **Adaptive methods** (Adam, RMSprop) often needed\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Lower variance ✗**\n",
    "- SGD actually has **higher variance** due to stochastic sampling\n",
    "- Full gradient descent has lower variance but higher computational cost\n",
    "\n",
    "**Option (e): Computational inefficiency ✗**\n",
    "- SGD is **more computationally efficient** per iteration\n",
    "- The sampling overhead is negligible compared to full gradient computation\n",
    "\n",
    "**Key insight:** SGD trades **stability for efficiency** - faster per iteration but requires more careful hyperparameter tuning.\n",
    "\n",
    "**Problem 11. Which of the following is not a convex function?**\n",
    "*   (a) $f(x) = x$\n",
    "*   (b) $f(x) = x^2$\n",
    "*   (c) $f(x) = e^x$\n",
    "*   (d) $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The sigmoid function f(x) = 1/(1+e^(-x)) is not convex** - it's actually concave in some regions.\n",
    "\n",
    "**Why (d) is not convex:**\n",
    "\n",
    "**1. Second derivative test:**\n",
    "\n",
    "$f(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$\n",
    "\n",
    "$f''(x) = \\frac{e^{-x}(e^{-x}-1)}{(1+e^{-x})^3}$\n",
    "\n",
    "**2. Analysis:**\n",
    "- **f''(x) < 0** for x > 0 (concave)\n",
    "- **f''(x) > 0** for x < 0 (convex)\n",
    "- **f''(0) = 0** (inflection point)\n",
    "\n",
    "**3. Why other functions are convex:**\n",
    "\n",
    "**Option (a): f(x) = x**\n",
    "- **f''(x) = 0** (linear functions are convex)\n",
    "\n",
    "**Option (b): f(x) = x²**\n",
    "- **f''(x) = 2 > 0** (strictly convex)\n",
    "\n",
    "**Option (c): f(x) = e^x**\n",
    "- **f''(x) = e^x > 0** (strictly convex)\n",
    "\n",
    "**Key insight:** The **sigmoid function** has an **S-shape** that makes it **non-convex** overall, despite being convex in some regions.\n",
    "\n",
    "**Problem 12. Recall the loss function used in ridge regression,**\n",
    "\n",
    "$$f(w) = \\sum_{i=1}^{n} (y_i - x_i^T w)^2 + \\lambda ||w||_2^2$$\n",
    "\n",
    "**What happens to the weights as $\\lambda \\rightarrow \\infty$?**\n",
    "*   (a) Weights approach positive infinity.\n",
    "*   (b) Weights approach 0.\n",
    "*   (c) Weights approach negative infinity.\n",
    "*   (d) Not enough information.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**As λ → ∞, the weights approach 0** due to the overwhelming influence of the L2 regularization term.\n",
    "\n",
    "**Mathematical intuition:**\n",
    "\n",
    "**1. Ridge regression objective:**\n",
    "\n",
    "$\\min f(w) = \\|y - Xw\\|^2 + \\lambda\\|w\\|^2$\n",
    "\n",
    "**2. As λ → ∞:**\n",
    "- The **regularization term $\\lambda\\|w\\|^2$** dominates\n",
    "- The **data term $\\|y - Xw\\|^2$** becomes negligible\n",
    "- The objective becomes approximately: $\\min \\lambda\\|w\\|^2$\n",
    "\n",
    "**3. Solution:**\n",
    "- $\\min \\|w\\|^2$ subject to no constraints\n",
    "- The minimum occurs at $w = 0$\n",
    "- All weights shrink toward zero\n",
    "\n",
    "**4. Intuitive understanding:**\n",
    "- **Extreme regularization** forces the model to be as simple as possible\n",
    "- **Zero weights** represent the simplest possible model (constant predictor)\n",
    "- This is the **maximum bias, minimum variance** scenario\n",
    "\n",
    "**Key insight:** **Infinite regularization** creates the **simplest possible model** with all weights equal to zero.\n",
    "\n",
    "**Problem 13. Why is it important to use a different test set to evaluate the final performance of the model, rather than the validation set used during model selection?**\n",
    "*   (a) The model may have overfit to the validation set\n",
    "*   (b) The test set is a better representation of new, unseen data\n",
    "*   (c) Both a and b\n",
    "*   (d) None of the above\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Both (a) and (b) are correct** - using a separate test set is crucial for honest model evaluation.\n",
    "\n",
    "**Why (a) is correct - Overfitting to validation set:**\n",
    "\n",
    "**1. Model selection process:**\n",
    "- **Multiple models** are evaluated on the validation set\n",
    "- **Hyperparameter tuning** is based on validation performance\n",
    "- **Model architecture** choices are influenced by validation results\n",
    "\n",
    "**2. Overfitting risk:**\n",
    "- The **best model** is selected based on validation performance\n",
    "- This creates **selection bias** toward the validation set\n",
    "- The model may **memorize** validation set patterns\n",
    "\n",
    "**Why (b) is correct - Better representation:**\n",
    "\n",
    "**1. Unseen data principle:**\n",
    "- **Test set** should be completely unseen during model development\n",
    "- **Validation set** has been \"seen\" through the selection process\n",
    "- **Test set** provides unbiased estimate of true performance\n",
    "\n",
    "**2. Generalization assessment:**\n",
    "- **Test set** represents truly new, unseen data\n",
    "- **Validation set** performance may be optimistic\n",
    "- **Test set** gives realistic performance expectations\n",
    "\n",
    "**Key insight:** **Three-way split** (train/validation/test) ensures honest evaluation by keeping the test set completely isolated.\n",
    "\n",
    "**Problem 14. What is cross-validation not used for?**\n",
    "*   (a) To evaluate the performance of a machine learning model on unseen data.\n",
    "*   (b) To select a model's hyperparameters.\n",
    "*   (c) To determine the generalization of a machine learning model.\n",
    "*   (d) To train multiple machine learning models on different datasets.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Cross-validation is NOT used to train multiple ML models on different datasets** - this is a fundamental misunderstanding of what CV does.\n",
    "\n",
    "**What cross-validation actually does:**\n",
    "\n",
    "**1. Same model, different partitions:**\n",
    "- **Single model architecture** trained multiple times\n",
    "- **Same dataset** split into different train/validation partitions\n",
    "- **Same hyperparameters** evaluated across different folds\n",
    "\n",
    "**2. What CV is used for:**\n",
    "\n",
    "**Option (a): Performance evaluation ✓**\n",
    "- Estimates how well the model generalizes to unseen data\n",
    "- Provides confidence intervals for performance metrics\n",
    "\n",
    "**Option (b): Hyperparameter selection ✓**\n",
    "- Compares different hyperparameter settings\n",
    "- Finds optimal hyperparameters for the given model\n",
    "\n",
    "**Option (c): Generalization assessment ✓**\n",
    "- Measures model's ability to generalize\n",
    "- Reduces overfitting risk through multiple evaluations\n",
    "\n",
    "**3. Why (d) is incorrect:**\n",
    "- CV uses **one dataset** split multiple ways\n",
    "- CV trains the **same model** multiple times\n",
    "- CV does **not** use different datasets\n",
    "\n",
    "**Key insight:** Cross-validation is a **resampling technique** for the same dataset, not a method for training different models on different datasets.\n",
    "\n",
    "**Problem 15. The plots below show linear regression results on the basis of only three data points.**\n",
    "\n",
    "<img src=\"./linear_regression_plots.png\" width=\"600px\">\n",
    "\n",
    "**Which plot would result from using the following objective, where $\\lambda = 10$?**\n",
    "\n",
    "$$f(w) = \\sum_{i=1}^{3} (y_i - wx_i - b)^2 + \\lambda w^2$$\n",
    "\n",
    "*   (a) Plot A\n",
    "*   (b) Plot B\n",
    "*   (c) Plot C\n",
    "*   (d) Plot D\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Plot B** shows the result of strong L2 regularization (λ = 10) on linear regression with 3 data points.\n",
    "\n",
    "**Why Plot B is correct:**\n",
    "\n",
    "**1. Strong regularization effect:**\n",
    "- **λ = 10** is a very high regularization strength\n",
    "- The **slope w** is heavily penalized and approaches zero\n",
    "- The **intercept b** is not regularized, so it can still adjust\n",
    "\n",
    "**2. Visual characteristics:**\n",
    "- **Nearly horizontal line** - slope is close to zero\n",
    "- **Line passes through middle** - intercept adjusts to minimize error\n",
    "- **Flat regression** - strong regularization prevents overfitting\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "\n",
    "$\\min f(w,b) = \\sum(y_i - wx_i - b)^2 + \\lambda w^2$\n",
    "\n",
    "- **w term** is penalized by $\\lambda w^2$\n",
    "- **b term** has no penalty\n",
    "- **Optimal w ≈ 0** for large λ\n",
    "- **Optimal b** minimizes the sum of squared errors\n",
    "\n",
    "**4. Why other plots are incorrect:**\n",
    "- **Plot A:** Shows overfitting (line passes through all points)\n",
    "- **Plot C:** Shows underfitting without proper intercept adjustment\n",
    "- **Plot D:** Shows incorrect slope direction\n",
    "\n",
    "**Key insight:** **Strong regularization** creates a **simple, flat model** that generalizes better than complex fits.\n",
    "\n",
    "**Problem 16. Let $a \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}^m$. Let $C = ab^T$. What are the dimensions of the matrix $C$, what is the range of $C$, and what is the null space of $C$?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Dimensions:** $C \\in \\mathbb{R}^{n \\times m}$\n",
    "**Range:** $\\text{range}(C) = \\text{span}(\\{a\\})$ (if $b \\neq 0$, otherwise $\\{0\\}$)\n",
    "**Null space:** $\\text{null}(C) = \\{v \\in \\mathbb{R}^m \\mid v^T b = 0\\}$ (if $a \\neq 0$, otherwise $\\mathbb{R}^m$)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **rank-1 matrix** problem. Let's analyze each component:\n",
    "\n",
    "**1. Dimensions:**\n",
    "- **$a \\in \\mathbb{R}^n$** (column vector)\n",
    "- **$b \\in \\mathbb{R}^m$** (column vector)  \n",
    "- **$C = ab^T$** (outer product)\n",
    "- **$C \\in \\mathbb{R}^{n \\times m}$** (n rows, m columns)\n",
    "\n",
    "**2. Range analysis:**\n",
    "- **$C = ab^T$** is a rank-1 matrix\n",
    "- **$\\text{range}(C) = \\text{span}(\\{a\\})$** - all columns are scalar multiples of a\n",
    "- **If $b = 0$:** $C = 0$, so $\\text{range}(C) = \\{0\\}$\n",
    "- **If $b \\neq 0$:** $\\text{range}(C) = \\text{span}(\\{a\\})$\n",
    "\n",
    "**3. Null space analysis:**\n",
    "- **$\\text{null}(C) = \\{v \\in \\mathbb{R}^m \\mid Cv = 0\\}$**\n",
    "- **$Cv = ab^T v = a(b^T v) = 0$**\n",
    "- This requires **$b^T v = 0$** (since $a \\neq 0$)\n",
    "- **$\\text{null}(C) = \\{v \\in \\mathbb{R}^m \\mid v^T b = 0\\}$**\n",
    "- **If $a = 0$:** $C = 0$, so $\\text{null}(C) = \\mathbb{R}^m$\n",
    "\n",
    "**Key insight:** **Rank-1 matrices** have **1-dimensional range** and **(m-1)-dimensional null space**.\n",
    "\n",
    "**Problem 17. What is the objective of least squares regression?**\n",
    "*   (a) To minimize the sum of the absolute differences between predicted and actual values.\n",
    "*   (b) To minimize the sum of the squared differences between predicted and actual values.\n",
    "*   (c) To maximize the number of points on the line of best fit.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Least squares regression minimizes the sum of squared differences** between predicted and actual values.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "**1. Objective function:**\n",
    "\n",
    "$\\min \\sum(y_i - \\hat{y}_i)^2 = \\min \\sum(y_i - x_i^T w)^2$\n",
    "\n",
    "**2. Why squared differences:**\n",
    "\n",
    "**Mathematical advantages:**\n",
    "- **Differentiable** everywhere (unlike absolute differences)\n",
    "- **Convex function** - guarantees unique global minimum\n",
    "- **Analytical solution** exists (normal equations)\n",
    "- **Statistical interpretation** - maximum likelihood under Gaussian noise\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Absolute differences**\n",
    "- **L1 loss** (least absolute deviations)\n",
    "- **Non-differentiable** at zero\n",
    "- **No closed-form solution**\n",
    "- **Robust to outliers** but harder to optimize\n",
    "\n",
    "**Option (c): Maximize points on line**\n",
    "- **Not a mathematical objective**\n",
    "- **Vague and ill-defined**\n",
    "- **No optimization formulation**\n",
    "\n",
    "**Key insight:** **Squared loss** provides **mathematical tractability** and **statistical optimality** under Gaussian assumptions.\n",
    "\n",
    "**Problem 18. An unbiased machine learning model is trained on a dataset with noisy features and achieves a prediction accuracy of 75%. If the irreducible error due to noise in the features is estimated to be 10%, what is the estimated variance of the model?**\n",
    "\n",
    "**Answer:** 15%\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **bias-variance decomposition** problem with irreducible error.\n",
    "\n",
    "**Given information:**\n",
    "- **Model accuracy:** 75% (this is 1 - total error)\n",
    "- **Total error:** 100% - 75% = 25%\n",
    "- **Irreducible error:** 10%\n",
    "- **Model is unbiased:** bias = 0\n",
    "\n",
    "**Bias-variance decomposition:**\n",
    "\n",
    "$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$\n",
    "\n",
    "$25\\% = 0^2 + \\text{Variance} + 10\\%$\n",
    "\n",
    "$25\\% = \\text{Variance} + 10\\%$\n",
    "\n",
    "$\\text{Variance} = 25\\% - 10\\% = 15\\%$\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "**1. Unbiased model assumption:**\n",
    "- **Bias = 0** means the model is unbiased\n",
    "- **Bias² = 0** in the decomposition\n",
    "\n",
    "**2. Error breakdown:**\n",
    "- **Total error:** 25% (from accuracy)\n",
    "- **Irreducible error:** 10% (given)\n",
    "- **Reducible error:** 25% - 10% = 15%\n",
    "- **Since bias = 0:** All reducible error is variance\n",
    "\n",
    "**3. Practical interpretation:**\n",
    "- **15% variance** represents model instability\n",
    "- This could be reduced with more data or regularization\n",
    "- **10% irreducible error** cannot be eliminated\n",
    "\n",
    "**Key insight:** **Variance = Total Error - Irreducible Error** when bias is zero.\n",
    "\n",
    "**Problem 19. Convexity is a desirable property in machine learning because it:**\n",
    "*   (a) guarantees gradient descent finds a global minimum in optimization problems for functions that have a global minimum\n",
    "*   (b) helps to avoid the model overfitting\n",
    "*   (c) speeds up model training\n",
    "*   (d) reduces model complexity\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Convexity guarantees that gradient descent finds the global minimum** - this is the primary advantage of convex optimization.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. Global optimality guarantee:**\n",
    "- **Convex functions** have no local minima other than the global minimum\n",
    "- **Gradient descent** converges to the global minimum\n",
    "- **No initialization sensitivity** - any starting point works\n",
    "- **Deterministic convergence** to the optimal solution\n",
    "\n",
    "**2. Mathematical properties:**\n",
    "- **First-order condition:** ∇f(x*) = 0 implies global minimum\n",
    "- **Second-order condition:** ∇²f(x) ≥ 0 for all x\n",
    "- **Uniqueness:** Global minimum is unique (if strictly convex)\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Avoid overfitting**\n",
    "- **Convexity** doesn't prevent overfitting\n",
    "- **Regularization** prevents overfitting\n",
    "- **Model complexity** controls overfitting\n",
    "\n",
    "**Option (c): Speed up training**\n",
    "- **Convexity** doesn't guarantee faster convergence\n",
    "- **Condition number** affects convergence speed\n",
    "- **Learning rate** is the main factor\n",
    "\n",
    "**Option (d): Reduce complexity**\n",
    "- **Convexity** is about function shape, not model complexity\n",
    "- **Model complexity** is determined by architecture\n",
    "- **Regularization** reduces effective complexity\n",
    "\n",
    "**Key insight:** **Convexity** provides **optimization guarantees**, not **generalization guarantees**.\n",
    "\n",
    "**Problem 20. True/False: Stochastic gradient descent typically results in a smoother convergence plot (loss vs. epochs) as compared to gradient descent.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**SGD typically results in NOISIER convergence plots** compared to full gradient descent, not smoother ones.\n",
    "\n",
    "**Why SGD is noisier:**\n",
    "\n",
    "**1. Stochastic nature:**\n",
    "- **Mini-batch sampling** introduces random noise\n",
    "- **Gradient estimates** are noisy approximations\n",
    "- **Update directions** vary randomly around true gradient\n",
    "\n",
    "**2. Convergence characteristics:**\n",
    "- **SGD:** Jagged, noisy convergence with oscillations\n",
    "- **Full GD:** Smooth, monotonic convergence\n",
    "- **SGD:** May bounce around the minimum due to noise\n",
    "\n",
    "**3. Visual comparison:**\n",
    "```\n",
    "SGD:     /\\/\\/\\/\\/\\  (noisy, oscillating)\n",
    "Full GD: \\________/  (smooth, steady)\n",
    "```\n",
    "\n",
    "**4. Why this happens:**\n",
    "- **SGD** uses only a subset of data per update\n",
    "- **Full GD** uses the exact gradient from all data\n",
    "- **Stochastic noise** creates variance in the optimization path\n",
    "\n",
    "**5. Trade-offs:**\n",
    "- **SGD:** Faster per iteration, but noisier convergence\n",
    "- **Full GD:** Slower per iteration, but smoother convergence\n",
    "- **SGD:** Better for large datasets despite noise\n",
    "\n",
    "**Key insight:** **SGD trades smoothness for speed** - the noise is the price paid for computational efficiency.\n",
    "\n",
    "**Problem 21. Consider the univariate function $f(x) = x^2$. This function has a unique minimum at $x^*= 0$. We're using gradient descent (GD) to find this minimum and at time $t$ we arrive at the point $x_t = 2$. What is the step size that would bring us to $x^*$ at time $t + 1$?**\n",
    "\n",
    "**Answer:** η = 1/2\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is a **gradient descent step size calculation** for the function f(x) = x².\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**1. Given information:**\n",
    "- **f(x) = x²**\n",
    "- **x* = 0** (global minimum)\n",
    "- **x_t = 2** (current position)\n",
    "- **x_{t+1} = x* = 0** (desired next position)\n",
    "\n",
    "**2. Gradient descent update rule:**\n",
    "\n",
    "$x_{t+1} = x_t - \\eta \\nabla f(x_t)$\n",
    "\n",
    "**3. Calculate the gradient:**\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "$\\nabla f(x) = 2x$\n",
    "\n",
    "$\\nabla f(2) = 2 \\times 2 = 4$\n",
    "\n",
    "**4. Set up the equation:**\n",
    "\n",
    "$0 = 2 - \\eta \\times 4$\n",
    "\n",
    "$4\\eta = 2$\n",
    "\n",
    "$\\eta = 2/4 = 1/2$\n",
    "\n",
    "**5. Verification:**\n",
    "\n",
    "$x_{t+1} = 2 - (1/2) \\times 4 = 2 - 2 = 0$ ✓\n",
    "\n",
    "**Key insight:** For **quadratic functions**, the **optimal step size** is η = 1/L where L is the Lipschitz constant of the gradient (L = 2 for f(x) = x²).\n",
    "\n",
    "**Problem 22. Let $X \\in \\mathbb{R}^{m \\times n}$, $w \\in \\mathbb{R}^n$, $Y \\in \\mathbb{R}^m$, and $R(w)$ be some regularization function from $\\mathbb{R}^n \\to \\mathbb{R}$. Consider mean squared error with regularization $L(w) = \\|Xw - Y\\|_2^2 + \\lambda R(w)$. What is $\\nabla_w L(w)$?**\n",
    "*   (a) $2Y^T (Xw - Y) + \\lambda$\n",
    "*   (b) $2X^T (X^T Xw - Y) + \\lambda \\nabla_w R(w)$\n",
    "*   (c) $2X^T (Xw - Y) + \\lambda \\nabla_w R(w)$\n",
    "*   (d) $2Y^T (X^T Xw - Y) + \\lambda R(w)$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (c) is correct:** ∇_w L(w) = 2X^T (Xw - Y) + λ∇_w R(w)\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Loss function:**\n",
    "\n",
    "$L(w) = \\|Xw - Y\\|^2 + \\lambda R(w)$\n",
    "\n",
    "**2. Gradient of squared error term:**\n",
    "\n",
    "$\\nabla_w \\|Xw - Y\\|^2 = \\nabla_w (Xw - Y)^T (Xw - Y)$\n",
    "\n",
    "$= \\nabla_w (w^T X^T Xw - 2Y^T Xw + Y^T Y)$\n",
    "\n",
    "$= 2X^T Xw - 2X^T Y$\n",
    "\n",
    "$= 2X^T (Xw - Y)$\n",
    "\n",
    "**3. Gradient of regularization term:**\n",
    "\n",
    "$\\nabla_w [\\lambda R(w)] = \\lambda \\nabla_w R(w)$\n",
    "\n",
    "**4. Total gradient:**\n",
    "\n",
    "$\\nabla_w L(w) = 2X^T (Xw - Y) + \\lambda \\nabla_w R(w)$\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Missing X^T and incorrect regularization term\n",
    "**Option (b):** Extra X^T in the first term\n",
    "**Option (d):** Wrong matrix dimensions and missing ∇_w\n",
    "\n",
    "**Key insight:** **Chain rule** and **matrix calculus** give us the correct gradient for regularized least squares.\n",
    "\n",
    "**Problem 23. Suppose we have $n$ Gaussian distributions $\\mathcal{D}_1, \\dots, \\mathcal{D}_n$, where each $\\mathcal{D}_i = \\mathcal{N}(\\mu_i, \\sigma^2)$. In other words, each Gaussian distribution shares the same variance $\\sigma^2$, but may have different means $\\mu_i$. For each distribution, we draw a single data point $X_i \\sim \\mathcal{D}_i$. Given**\n",
    "$$\\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{pmatrix}, \\text{ we want to predict } \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}$$\n",
    "**Solution 1 is to predict $X$, and Solution 2 is to predict $\\frac{7}{8}X$. Why might Solution 2 produce lower mean squared error than Solution 1?**\n",
    "\n",
    "**Answer:** This is Stein's Paradox - introducing a small amount of bias can reduce overall error\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This is **Stein's Paradox**, which demonstrates that **biased estimators can outperform unbiased ones** in high-dimensional settings.\n",
    "\n",
    "**Why Solution 2 (7/8 × X) can be better:**\n",
    "\n",
    "**1. Bias-variance tradeoff:**\n",
    "- **Solution 1:** Unbiased estimator (X_i estimates μ_i directly)\n",
    "- **Solution 2:** Biased estimator (7/8 × X_i shrinks toward zero)\n",
    "- **Shrinkage** reduces variance at the cost of introducing bias\n",
    "\n",
    "**2. Mathematical intuition:**\n",
    "- **Solution 1:** $E[X_i] = \\mu_i$, $\\text{Var}(X_i) = \\sigma^2$\n",
    "- **Solution 2:** $E[7/8 \\times X_i] = 7/8 \\times \\mu_i$, $\\text{Var}(7/8 \\times X_i) = (7/8)^2\\sigma^2$\n",
    "- **Bias:** $(7/8 \\times \\mu_i - \\mu_i) = -\\mu_i/8$\n",
    "- **Variance reduction:** $\\sigma^2 - (7/8)^2\\sigma^2 = \\sigma^2(1 - 49/64) = 15\\sigma^2/64$\n",
    "\n",
    "**3. When this helps:**\n",
    "- **High noise** (large σ²) makes variance reduction valuable\n",
    "- **Small means** (μ_i close to 0) make bias penalty small\n",
    "- **Many dimensions** (large n) amplifies the effect\n",
    "\n",
    "**4. Stein's insight:**\n",
    "- **Individual unbiased estimators** can be improved\n",
    "- **Shrinkage estimators** often perform better\n",
    "- **James-Stein estimator** is the optimal shrinkage\n",
    "\n",
    "**Key insight:** **Bias can be beneficial** when it reduces variance more than it increases squared bias.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
