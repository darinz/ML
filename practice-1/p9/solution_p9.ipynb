{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 9 Solutions\n",
    "\n",
    "**1. One Answer**\n",
    "\n",
    "In a popular gacha game, the probability of pulling an SSR character on\n",
    "a single pull is 0.6% (P = 0.006). Assume that each pull is independent\n",
    "and follows a Bernoulli distribution. In such games, players often\n",
    "perform a 10-pull, which means making 10 pulls. Each of these 10 pulls\n",
    "is still independent, meaning the probability of getting an SSR in each\n",
    "pull remains 0.006. If you perform a 10-pull, what is the probability of\n",
    "pulling exactly 2 SSRs? You do not need to know what gacha game is to\n",
    "solve this problem.\n",
    "\n",
    "- 1.  0\n",
    "- 1.  $1 - (1 - P)^{10} = 5.84\\%$\n",
    "- 1.  $(1 - P)^9 \\times P \\times 10 = 5.68\\%$\n",
    "- 1.  $(1 - P)^8 \\times P^2 \\times (10 \\times 9 / 2) = 0.15\\%$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** We model each pull as a Bernoulli trial, where the\n",
    "probability of success (pulling an SSR) is P = 0.006. Since a 10-pull\n",
    "consists of 10 independent trials, the number of SSRs obtained follows a\n",
    "Binomial distribution:\n",
    "\n",
    "$X \\sim \\text{Binomial}(n = 10, p = 0.006)$\n",
    "\n",
    "We want to find the probability of pulling exactly 2 SSRs, which is\n",
    "given by the binomial probability mass function (PMF):\n",
    "\n",
    "$P(X = 2) = \\binom{10}{2} P^2 (1 - P)^8$\n",
    "\n",
    "Computing the combination:\n",
    "$\\binom{10}{2} = \\frac{10!}{2! \\times (10 - 2)!} = \\frac{10 \\times 9}{2} = 45$\n",
    "\n",
    "Thus, the probability is:\n",
    "$P(X = 2) = (1 - P)^8 \\times P^2 \\times 45 = 0.15\\%$\n",
    "\n",
    "Therefore, the correct answer is D.\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**A: 0** - This option would be correct if it were impossible to pull an\n",
    "SSR. However, since the probability of obtaining an SSR is nonzero, this\n",
    "option is incorrect.\n",
    "\n",
    "**B: $1 - (1 - P)^{10} = 5.84\\%$** - This formula calculates the\n",
    "probability of pulling at least 1 SSR, computed as:\n",
    "$P(\\text{at least 1 SSR}) = 1 - P(0 \\text{ SSRs}) = 1 - (1 - P)^{10}$.\n",
    "This probability includes cases where the player gets 1, 2, 3, …, or\n",
    "even 10 SSRs.\n",
    "\n",
    "**C: $(1 - P)^9 \\times P \\times 10 = 5.68\\%$** - This formula calculates\n",
    "the probability of pulling exactly 1 SSR, given by:\n",
    "$P(X = 1) = \\binom{10}{1} P^1(1 - P)^9 = 10 \\times P \\times (1 - P)^9$.\n",
    "Hence, the correct answer remains D.\n",
    "\n",
    "**2. Select All That Apply**\n",
    "\n",
    "Below are several statements about Gradient Descent (GD) and Stochastic\n",
    "Gradient Descent (SGD). Which of the following are correct?\n",
    "\n",
    "- 1.  For GD, each step aims to move along the gradient descent\n",
    "      direction at the current point to reduce the value of the\n",
    "      objective function.\n",
    "- 1.  In SGD, each step computes an estimated gradient based on a single\n",
    "      sample, introducing randomness, which may not guarantee that the\n",
    "      objective function decreases in every step.\n",
    "- 1.  Suppose you have model $w_t$ at the $t$-th iteration of SGD. The\n",
    "      expectation of the direction of the model update for SGD at step\n",
    "      $t$ is different from the negative direction of the gradient\n",
    "      $-\\nabla_w f(w)|_{w=w_t}$.\n",
    "- 1.  GD requires the full gradient information of the objective\n",
    "      function, while SGD only needs the gradient information on a\n",
    "      single sample at each step.\n",
    "\n",
    "**Correct answers:** (a), (b), (d)\n",
    "\n",
    "**Explanation:** The correct answers are (A), (B), and (D). Below is an\n",
    "explanation of each option.\n",
    "\n",
    "**(A) Correct:** In Gradient Descent (GD), each step moves in the\n",
    "direction of the negative gradient of the objective function at the\n",
    "current point. This ensures that the function value decreases (assuming\n",
    "an appropriate step size). Mathematically, the update rule for GD is:\n",
    "\n",
    "$w_{t+1} = w_t - \\eta \\nabla_w f(w_t)$\n",
    "\n",
    "where $\\eta$ is the learning rate, and $\\nabla_w f(w_t)$ is the gradient\n",
    "of the objective function over the entire dataset at iteration $t$. This\n",
    "step guarantees movement in the direction that minimizes the objective\n",
    "function.\n",
    "\n",
    "**(B) Correct:** In Stochastic Gradient Descent (SGD), instead of\n",
    "computing the exact gradient using the full dataset, an estimated\n",
    "gradient is computed based on a single sample (or a mini-batch). This\n",
    "introduces randomness in the updates, meaning that the objective\n",
    "function might not necessarily decrease in every step. The update rule\n",
    "for SGD can be rewritten as:\n",
    "\n",
    "$w_{t+1} = w_t - \\eta(\\nabla_w f(w_t) + \\xi_t)$\n",
    "\n",
    "where $\\nabla_w f(w_t)$ is the true full-batch gradient, and $\\xi_t$ is\n",
    "a noise term introduced due to the stochastic nature of SGD,\n",
    "representing the deviation from the full gradient when using only one\n",
    "sample. This noise term makes each individual update potentially\n",
    "non-optimal, but in expectation, the updates still align with the true\n",
    "gradient direction over multiple iterations.\n",
    "\n",
    "**(C) Incorrect:** The expectation of the SGD update direction is equal\n",
    "to the negative full gradient:\n",
    "\n",
    "$E[\\nabla_w f(w_t) + \\xi_t] = \\nabla_w f(w_t)$\n",
    "\n",
    "Since SGD approximates the full gradient using randomly sampled data\n",
    "points, its update direction is an unbiased estimate of the true\n",
    "gradient. That is, while each step introduces noise, on average, the\n",
    "update follows the same direction as GD. Therefore, the claim that the\n",
    "expectation is different from $-\\nabla_w f(w_t)$ is false.\n",
    "\n",
    "**(D) Correct:** GD requires the full gradient of the objective\n",
    "function, meaning it computes $\\nabla_w f(w)$ over the entire dataset at\n",
    "each step. In contrast, SGD only uses the gradient of a single sample\n",
    "(or a mini-batch), significantly reducing the computational cost per\n",
    "step, especially in large-scale datasets.\n",
    "\n",
    "Therefore, the correct answers are (A), (B), and (D).\n",
    "\n",
    "**3.**\n",
    "\n",
    "In a gacha game, the probability of obtaining an SSR character per pull\n",
    "is $p$, but $p$ is unknown. To estimate $p$, Bob performed 100 pulls and\n",
    "obtained SSRs $k = 3$ times (i.e., 3 successes). Assume that each pull\n",
    "is independent and follows a Bernoulli distribution.\n",
    "\n",
    "What is the likelihood of this scenario occurring?\n",
    "\n",
    "Likelihood function:\n",
    "$L(p) = \\frac{100 \\times 99 \\times 98}{6} p^3 (1-p)^{97}$\n",
    "\n",
    "What is the Maximum Likelihood Estimate (MLE) of $p$ as a fractional\n",
    "number?\n",
    "\n",
    "MLE: $\\hat{p} = \\frac{3}{100}$\n",
    "\n",
    "**Explanation:** Since each pull is independent and follows a Bernoulli\n",
    "distribution, the total number of SSRs obtained follows a Binomial\n",
    "distribution:\n",
    "\n",
    "$X \\sim \\text{Binomial}(n = 100, p)$\n",
    "\n",
    "The likelihood function is given by the binomial probability mass\n",
    "function:\n",
    "\n",
    "$L(p) = \\binom{100}{3} p^3 (1-p)^{97} = \\frac{100 \\times 99 \\times 98}{6} p^3 (1-p)^{97}$\n",
    "\n",
    "To find the MLE $\\hat{p}$, we maximize $\\ln L(p)$:\n",
    "\n",
    "$\\ln L(p) = \\text{constant} + 3 \\ln p + 97 \\ln(1-p)$\n",
    "\n",
    "Taking the derivative and setting it to zero:\n",
    "\n",
    "$\\frac{3}{p} - \\frac{97}{1-p} = 0$\n",
    "\n",
    "Solving for p:\n",
    "\n",
    "$\\hat{p} = \\frac{3}{100}$\n",
    "\n",
    "**4. One Answer**\n",
    "\n",
    "Suppose you train a linear regression model (without doing feature\n",
    "expansion), i.e., $f_w(x) = wx + b$, to approximate the cubic function\n",
    "$g(x) = 2x^3 + 7x^2 + 4x + 3$. What’s the most likely outcome?\n",
    "\n",
    "- 1.  The model will have low bias and low variance\n",
    "- 1.  The model will have low bias and high variance\n",
    "- 1.  The model will have high bias and low variance\n",
    "- 1.  The model will have high bias and high variance\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** The model being linear means the variance is likely to\n",
    "be low. The linear model trying to approximate a cubic function, which\n",
    "is of a higher degree, means that the bias is likely to be high.\n",
    "\n",
    "**5. One Answer**\n",
    "\n",
    "Adding more basis functions to a linear regression model always leads to\n",
    "better prediction accuracy on new, unseen data.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** As the complexity of the model increases, the\n",
    "prediction accuracy on new, unseen data (test data) doesn’t always get\n",
    "better as the model may overfit.\n",
    "\n",
    "**6. One Answer**\n",
    "\n",
    "What datasets from the training/validation/test data split should you\n",
    "utilize during hyperparameter tuning?\n",
    "\n",
    "- 1.  Training Data\n",
    "- 1.  Training Data, Validation Data\n",
    "- 1.  Training Data, Validation Data, Test Data\n",
    "- 1.  Training Data, Test Data\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** You never want to use your Test Data for hyperparameter\n",
    "tuning, as it will bias the model on the test data. The test data should\n",
    "only be used for final evaluation of the model. Instead, for\n",
    "hyperparameter tuning, you want to train your model with different\n",
    "hyperparameters using the Training Data, then evaluate those models on\n",
    "the Validation Data to select the best hyperparameters for the model.\n",
    "(Methods like K-fold Cross Validation)\n",
    "\n",
    "**7. One Answer**\n",
    "\n",
    "Consider $u = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}$,\n",
    "$v = \\begin{bmatrix} -4 \\\\ 5 \\\\ 1 \\end{bmatrix}$,\n",
    "$w = \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix}$. Let\n",
    "$x \\in \\mathbb{R}^3$. Does there exist unique $a, b, c \\in \\mathbb{R}$\n",
    "such that $a \\cdot u + b \\cdot v + c \\cdot w = x$?\n",
    "\n",
    "- 1.  Yes\n",
    "- 1.  No\n",
    "- 1.  Not enough information to determine\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** $u, v, w$ are linearly independent, so the function\n",
    "$f(a, b, c) = a \\cdot u + b \\cdot v + c \\cdot w$ is onto $\\mathbb{R}^3$\n",
    "as well as one-to-one. This can be verified by using row reduction on\n",
    "$\\begin{bmatrix} u & v & w \\end{bmatrix}$ or by noticing the three\n",
    "vectors are orthogonal.\n",
    "\n",
    "**8.**\n",
    "\n",
    "Consider data matrix $X \\in \\mathbb{R}^{n \\times d}$, label vector\n",
    "$y \\in \\mathbb{R}^n$, and regularization parameter $\\lambda > 0$. Write\n",
    "the closed form solution for ridge regression.\n",
    "\n",
    "**Answer:** $(X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "**Explanation:** $(X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "**9. One Answer**\n",
    "\n",
    "Consider a dataset containing three observations for a simple linear\n",
    "regression problem, where $y$ is the dependent variable and $x$ is the\n",
    "independent variable. The dataset is given as follows:\n",
    "\n",
    "| x   | y   |\n",
    "|-----|-----|\n",
    "| 1   | 7   |\n",
    "| 2   | 8   |\n",
    "| 3   | 9   |\n",
    "\n",
    "Find the coefficient $\\beta_1$ of the linear regression (without bias)\n",
    "$y = \\beta_1 x$ using the least squares as loss.\n",
    "\n",
    "- 1.  $\\frac{46}{14}$\n",
    "- 1.  $\\frac{14}{46}$\n",
    "- 1.  $\\frac{50}{14}$\n",
    "- 1.  $\\frac{14}{50}$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** $\\beta_1 = \\frac{50}{14}$\n",
    "\n",
    "$\\beta_1 = (X^T X)^{-1} (X^T Y) = \\frac{1}{14} \\times 50$\n",
    "\n",
    "**10. One Answer**\n",
    "\n",
    "We can find the solution for LASSO by setting the gradient of the loss\n",
    "to 0 and solving for weight parameter $w$.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** LASSO has no closed form solution, which is why we use\n",
    "gradient descent.\n",
    "\n",
    "**11. One Answer**\n",
    "\n",
    "You are building a model to detect fraudulent transactions from a\n",
    "dataset of 100K samples. What would be the most effective way to split\n",
    "and utilize your data?\n",
    "\n",
    "- 1.  Randomly take an 80-20 data split. Use 80% of the data for\n",
    "      training, and 20% for both validation and evaluation.\n",
    "- 1.  Use the first 80% of the data for training, the next 10% for\n",
    "      validation, and the last 10% for evaluation.\n",
    "- 1.  Randomly make a 80-10-10 data split. Use 80% of the data for\n",
    "      training, 10% for validation, and 10% for evaluation.\n",
    "- 1.  Select a random 80% of the data for training, use the remaining\n",
    "      20% for validation. Evaluate on the training set.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** A is incorrect since the validation and test set should\n",
    "be separate. B is incorrect since the data splits should be randomized.\n",
    "C is the correct as it is the standard data split method. D is incorrect\n",
    "since evaluating on the known train set induces bias.\n",
    "\n",
    "**12. One Answer**\n",
    "\n",
    "You are implementing a model to predict house prices. Your dataset\n",
    "contains 15 features (e.g., location, acres, proximity to city, etc.).\n",
    "However, you believe that many of these features are irrelevant to the\n",
    "house prices. Which method would be most suitable for your model?\n",
    "\n",
    "- 1.  Logistic regression with L1 regularization.\n",
    "- 1.  Logistic regression with L2 regularization.\n",
    "- 1.  Linear regression with L1 regularization.\n",
    "- 1.  Linear regression with L2 regularization.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Since this is a regression not classification task, we\n",
    "use linear regression. Additionally, since we are interested in some\n",
    "kind of feature selection, L1 regularization would be more effective in\n",
    "setting unimportant features to 0.\n",
    "\n",
    "**13. Select All That Apply**\n",
    "\n",
    "While training a model, you notice that it has a small bias but a high\n",
    "variance on the training data. Which of the following are valid\n",
    "strategies to address the high variance?\n",
    "\n",
    "- 1.  Increase regularization constant.\n",
    "- 1.  Train on a model class that is simpler.\n",
    "- 1.  Increase the size of the training dataset.\n",
    "- 1.  Use higher-degree features to capture more complex patterns in the\n",
    "      data.\n",
    "\n",
    "**Correct answers:** (a), (b), (c)\n",
    "\n",
    "**Explanation:** A and B are correct because increasing regularization\n",
    "and simplifying model complexity help decrease the impact of less\n",
    "important features, improving generalization and reducing variance. C is\n",
    "also correct because increasing the training set size allows the model\n",
    "to generalize better, which can reduce variance. D is incorrect since\n",
    "using higher-degree features increases model complexity and often leads\n",
    "to overfitting, increasing variance.\n",
    "\n",
    "**14. Select All That Apply**\n",
    "\n",
    "After a student trains and evaluates a Logistic Regression model, you\n",
    "notice their test accuracy is 99.99%. You know that this was supposed to\n",
    "be a difficult dataset to model, so you investigate. Which of the\n",
    "following are reasonable explanations for this excessively high\n",
    "accuracy? Note that if you select multiple answers, not all of them have\n",
    "to be true at the same time.\n",
    "\n",
    "- 1.  There was some form of train/test leakage, resulting in the model\n",
    "      over-performing on the test set\n",
    "- 1.  The data was not linearly separable, making it very easy for the\n",
    "      model to classify things correctly\n",
    "- 1.  The dataset was incredibly imbalanced, with most of the data\n",
    "      points being labeled as positives\n",
    "- 1.  The dataset was incredibly imbalanced, with most of the data\n",
    "      points being labeled as negatives\n",
    "\n",
    "**Correct answers:** (a), (c), (d)\n",
    "\n",
    "**Explanation:** A is true. Train/test leakage can result in incredibly\n",
    "high performance on the evaluation data. C and D are also true. A very\n",
    "imbalanced dataset can make it so that the model only predicts the\n",
    "majority class yet scores very high. B is false as having data that is\n",
    "not linearly separable does not make it easier for a linear model to\n",
    "separate the classes.\n",
    "\n",
    "**15. One Answer**\n",
    "\n",
    "Given\n",
    "$W \\in \\mathbb{R}^{m \\times n}, X \\in \\mathbb{R}^{n \\times n}, Y \\in \\mathbb{R}^{p \\times m}, Z \\in \\mathbb{R}^{m \\times m}$,\n",
    "and $a \\in \\mathbb{R}^n$. If $m, n, p$ are distinct, which one of the\n",
    "following expressions is valid?\n",
    "\n",
    "- 1.  $(X^{-1}aa^T W^T)^{-1}(X^T a)$\n",
    "- 1.  $Xa^T aW^T (Z^{-1}Y^T)$\n",
    "- 1.  $WXaa^T XZY^T$\n",
    "- 1.  None of the above\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** A is incorrect because\n",
    "$X^{-1}aa^T W^T \\in \\mathbb{R}^{n \\times m}$, and you cannot take the\n",
    "inverse of a non-square matrix. B is correct because even though $Xa^T$\n",
    "is not possible ($n \\times n, 1 \\times n$), $a^T a$ becomes a scalar and\n",
    "allows for the expression to be valid. C is incorrect because $XZ$ is\n",
    "not possible ($n \\times n, m \\times m$). D is incorrect because B is\n",
    "correct.\n",
    "\n",
    "**16.**\n",
    "\n",
    "For what value of $k$ will $k$-fold cross-validation create\n",
    "cross-validation splits equivalent to Leave-one-out cross-validation\n",
    "(LOOCV)? Assume you have $n$ data points.\n",
    "\n",
    "**Answer:** $k = n$\n",
    "\n",
    "**Explanation:** If $k = n$, then there will be $n$ folds, each one only\n",
    "leaving 1 datapoint out. This is equivalent to LOOCV.\n",
    "\n",
    "**17. One Answer**\n",
    "\n",
    "We can decrease the variance of a model by increasing the model\n",
    "complexity.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** As model complexity increases, this increases the\n",
    "variance error due to higher degree of freedom.\n",
    "\n",
    "**18. Select All That Apply**\n",
    "\n",
    "Which of the following statements are true about logistic regression?\n",
    "Recall that the sigmoid function is defined as\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$ for $x \\in \\mathbb{R}$\n",
    "\n",
    "- 1.  L2 regularization is often used to reduce overfitting in logistic\n",
    "      regression by adding a penalty for large coefficient values\n",
    "- 1.  The logistic sigmoid function is used to model the probability of\n",
    "      the positive class in binary logistic regression\n",
    "- 1.  The maximum likelihood estimates for the logistic regression\n",
    "      coefficients can be found in closed-form\n",
    "- 1.  For any finite input $x \\in \\mathbb{R}$, $\\sigma(x)$ is strictly\n",
    "      greater than 0 and strictly less than 1. Thus, a binary logistic\n",
    "      regression model with finite input and weights can never output a\n",
    "      probability of exactly 0 or 1, and can never achieve a training\n",
    "      loss of exactly 0.\n",
    "\n",
    "**Correct answers:** (a), (b), (d)\n",
    "\n",
    "**Explanation:** The MLE for logistic regression coefficients cannot be\n",
    "found in closed-form which is why an iterative approach (eg. SGD) is\n",
    "used to find the coefficients.\n",
    "\n",
    "1.  True. $\\sigma(x)$ has horizontal asymptotes at 0 and 1 and therefore\n",
    "    is strictly bounded between those values. Because the output\n",
    "    probability is the output of, this implies that the output\n",
    "    probability is also strictly contained in (0,1). As it cannot output\n",
    "    positive or negative labels with probability 1, it is therefore\n",
    "    unable to reduce the training loss to exactly 0, though it can get\n",
    "    arbitrarily close.\n",
    "\n",
    "**19. Select All That Apply**\n",
    "\n",
    "Below are several statements about the train/test/validation sets and\n",
    "cross-validation. Which of the following are correct?\n",
    "\n",
    "- 1.  k-fold cross validation (where $k > 1$) is faster but more biased\n",
    "      than leave-one-out (LOO) cross validation.\n",
    "- 1.  k-fold cross validation (where $k > 1$) is faster and more\n",
    "      accurate than leave-one-out (LOO) cross validation.\n",
    "- 1.  The test set can be used to evaluate models during training and\n",
    "      for hyperparameter tuning.\n",
    "- 1.  The test error gives us an assessment of how our model does on\n",
    "      unseen data.\n",
    "\n",
    "**Correct answers:** (a), (d)\n",
    "\n",
    "**Explanation:** A is correct since k-fold is faster but generally more\n",
    "biased. D is correct since the purpose of the test set is to test the\n",
    "model on unseen data and assess its performance.\n",
    "\n",
    "**20. Select All That Apply**\n",
    "\n",
    "Consider the principle of Maximum Likelihood Estimation (MLE), which is\n",
    "a method to estimate the parameters of a statistical model. Which of the\n",
    "following statements is correct?\n",
    "\n",
    "- 1.  For MLE, samples must be drawn i.i.d. (independent and identically\n",
    "      distributed).\n",
    "- 1.  Once we have a log-likelihood function, we maximize it with\n",
    "      respect to the parameter $\\theta$ to find the parameter estimate\n",
    "      $\\hat{\\theta}_{MLE}$.\n",
    "- 1.  MLE always provides an unbiased estimator of the true parameter.\n",
    "- 1.  MLE identifies the model parameters that maximize the likelihood\n",
    "      of the observed data.\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:** While i.i.d. is commonly assumed when doing MLE, it is\n",
    "not strictly necessary. Additionally, although it can sometimes be\n",
    "unbiased, MLE is generally a biased estimator.\n",
    "\n",
    "**21. One Answer**\n",
    "\n",
    "If we run gradient descent on $f(x)$, gradient descent guarantees that\n",
    "we will converge to the global minimum even if $\\nabla^2f(x) \\ge 0$ does\n",
    "not hold some $x$, i.e., the Hessian of the objective function is not\n",
    "positive semi-definite for some $x$.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** Gradient descent only guarantees a global minimum if\n",
    "$f(x)$ is convex.\n",
    "\n",
    "**22. One Answer**\n",
    "\n",
    "Let $A_1, A_2,..., A_n \\sim N(\\mu, \\sigma^2)$. What is\n",
    "$E[A_1 + A_2 + A_3]$?\n",
    "\n",
    "- 1.  $3\\mu$\n",
    "- 1.  $6\\mu$\n",
    "- 1.  $9\\mu$\n",
    "- 1.  Cannot be determined from the given information.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** By linearity of expectation,\n",
    "$E[A_1 + A_2 + A_3] = E[A_1] + E[A_2] + E[A_3]$. Since\n",
    "$A_1, A_2,..., A_n \\sim N(\\mu, \\sigma^2)$,\n",
    "$E[A_1] = E[A_2] = E[A_3] = \\mu$. Thus,\n",
    "$E[A_1] + E[A_2] + E[A_3] = \\mu + \\mu + \\mu = 3\\mu$.\n",
    "\n",
    "**23.**\n",
    "\n",
    "Consider a function $f(x,y)$ representing a loss function in a\n",
    "2-dimensional space, where gradient descent is used to minimize $f$.\n",
    "Given the function: $f(x, y) = x^2 + 2y^2 + 4xy$ where the initial point\n",
    "is $(x_0, y_0) = (1, 1)$ and the learning rate is 0.1, write down the\n",
    "$(x_1, y_1)$ you get after one step of gradient descent.\n",
    "\n",
    "**Answer:** $(x_1, y_1) = (0.4, 0.2)$\n",
    "\n",
    "**Explanation:** From the gradient descent algorithm:\n",
    "$x_1 = x_0 - \\eta \\cdot \\frac{\\partial f(x_0, y_0)}{\\partial x}$ and\n",
    "$y_1 = y_0 - \\eta \\cdot \\frac{\\partial f(x_0, y_0)}{\\partial y}$. It is\n",
    "given that $\\eta = 0.1$. $\\frac{\\partial f(x,y)}{\\partial x} = 2x + 4y$.\n",
    "$\\frac{\\partial f(x,y)}{\\partial y} = 4y + 4x$. So,\n",
    "$x_1 = 1 - 0.1 \\cdot 6 = 0.4$ and $y_1 = 1 - 0.1 \\cdot 8 = 0.2$."
   ],
   "id": "10c261c6-6cec-4cb5-9ac4-a554dc3e8d6a"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
