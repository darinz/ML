{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 4\n",
    "\n",
    "**1. True/False: Leave-one-out (LOO) and $k$-fold cross-validation can\n",
    "be used for hyperparameter tuning.** \\* (a) True \\* (b) False\n",
    "\n",
    "**2. Which of the following is most indicative of a model overfitting?**\n",
    "\\* (a) High bias, low variance \\* (b) Low bias, high variance \\* (c) Low\n",
    "bias, low variance\n",
    "\n",
    "**3. Which of the following statements about LASSO is true?** \\* (a)\n",
    "LASSO’s objective function has a closed-form solution. \\* (b) LASSO has\n",
    "lower bias than ordinary least squares. \\* (c) LASSO can be interpreted\n",
    "as least squares regression when the model’s weights are regularized\n",
    "with the $l_1$ norm. \\* (d) LASSO can be interpreted as least squares\n",
    "regression when the model’s weights are regularized with the $l_2$ norm.\n",
    "\n",
    "**4. Which of the following is not a convex set?** \\* (a) The hyperplane\n",
    "given by\n",
    "$H = \\{\\mathbf{x} \\in \\mathbb{R}^n : \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i = \\beta_i\\}$\n",
    "\\* (b) The interval $[a, b]$ where $a, b \\in \\mathbb{R}$ \\* (c) The\n",
    "“unit square” $\\{\\mathbf{x} \\in \\mathbb{R}^2: ||\\mathbf{x}||_1 = 1\\}$ \\*\n",
    "(d) The unit ball\n",
    "$\\{\\mathbf{x} \\in \\mathbb{R}^2: ||\\mathbf{x}||_2 \\leq 1\\}$\n",
    "\n",
    "**5. Extra Credit: Consider a data matrix\n",
    "$X \\in \\mathbb{R}^{n \\times m}$, target vector $y \\in \\mathbb{R}^n$, and\n",
    "the resulting least squares solution $\\hat{w} \\in \\mathbb{R}^m$. Now let\n",
    "$y'$ be the vector that results from squaring every value in the target\n",
    "vector $y$, and let $\\hat{w}'$ be the vector that results from squaring\n",
    "every value in $\\hat{w}$.**\n",
    "\n",
    "**$y' = [y_1^2, \\dots, y_n^2]$**\n",
    "\n",
    "**$\\hat{w}' = [\\hat{w}_1^2, \\dots, \\hat{w}_m^2]$**\n",
    "\n",
    "**If we leave the data matrix $X$ unchanged and we use $y'$ as our new\n",
    "target vector, the resulting least squares solution will be\n",
    "$\\hat{w}'$.** \\* (a) False \\* (b) True\n",
    "\n",
    "**6. Reducing the regularization of a model would typically . . .** \\*\n",
    "(a) Decrease its bias and increase its variance \\* (b) Decrease its bias\n",
    "and decrease its variance \\* (c) Increase its bias and decrease its\n",
    "variance \\* (d) Increase its bias and increase its variance\n",
    "\n",
    "**7. How many models must be trained when using $k$-fold\n",
    "cross-validation to determine which of three possible $\\lambda$ values\n",
    "($\\lambda_1, \\lambda_2, \\lambda_3$) is best for ridge regression on\n",
    "training set with $n$ samples (assume $n$ is a multiple of $k$)?** \\*\n",
    "(a) $3n/k$ \\* (b) $k$ \\* (c) $n$ \\* (d) $3k$\n",
    "\n",
    "**8. $k$-fold cross-validation is equivalent to leave-one-out (LOO)\n",
    "cross-validation on a training set of $n$ samples when $k$ is equal to**\n",
    "\\* (a) $k$ is not computable \\* (b) $n-1$ \\* (c) $n$ \\* (d) $1$\n",
    "\n",
    "**9. Let $X \\in \\mathbb{R}^{m \\times n}$, and $Y \\in \\mathbb{R}^m$. We\n",
    "want to fit a linear regression model. We call a matrix a “short wide”\n",
    "matrix if there are more columns than rows. Which of the following is\n",
    "NOT always true when $X$ is a “short wide” matrix (i.e., $n > m$):** \\*\n",
    "(a) $X^T X$ is symmetric and positive semidefinite. \\* (b) $X^T X$ is\n",
    "not invertible. \\* (c) The columns of $X$ are linearly independent. \\*\n",
    "(d) The null space of $X$ is non-empty.\n",
    "\n",
    "**10. Assume you (1) standardized a training set and (2) trained a\n",
    "machine learning model on this standardized training set. Before you use\n",
    "your model to make predictions on a test set, you should do which of the\n",
    "following (choose exactly one answer)** \\* (a) not standardize the test\n",
    "set. \\* (b) use the mean and standard deviation from train set to\n",
    "standardize the test set. \\* (c) use the mean and standard deviation\n",
    "from test set to standardize the test set. \\* (d) collect new data and\n",
    "use the new data’s mean and standard deviation to standardize the test\n",
    "set.\n",
    "\n",
    "**11. Let $x_1, x_2,..., x_n \\sim N(\\mu, \\sigma^2)$, where\n",
    "$\\mu \\in \\mathbb{R}$ is an unknown variable. The PDF of\n",
    "$N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "for any $x \\in \\mathbb{R}$. Using the log-likelihood, find the maximum\n",
    "likelihood estimation of $\\mu$ in terms of $x_i$.** \\* (a)\n",
    "$\\sum_{i=1}^n x_i$ \\* (b) $\\frac{1}{n}\\sum_{i=1}^n x_i$ \\* (c)\n",
    "$\\sum_{i=1}^n \\frac{x_i}{\\sigma^2}$ \\* (d) $\\sigma\\sum_{i=1}^n x_i$\n",
    "\n",
    "**12. True/False: We can make the irreducible error smaller by using a\n",
    "larger number of training samples.** \\* (a) True \\* (b) False\n",
    "\n",
    "**13. Let $f(x_1, x_2, x_3) = x_1x_2 - x_2^3 + x_1x_3$. What is\n",
    "$\\nabla_{x_1,x_2,x_3} f$?** \\* (a) $x_2 - 3x_2^2 + x_1$ \\* (b)\n",
    "$[x_2+x_3, x_1 - 3x_2^2, x_1]$ \\* (c) $x_2+x_3$ \\* (d)\n",
    "$[x_2, -3x_2^2, x_1]$\n",
    "\n",
    "**14. True/False: Convex optimization problems are attractive because\n",
    "they always have exactly one global minimum.** \\* (a) True \\* (b) False\n",
    "\n",
    "**15. Ridge regression** \\* (a) reduces variance at the expense of bias\n",
    "\\* (b) adds an $l_1$ penalty norm to the cost function \\* (c) often sets\n",
    "many of the weights to 0 when the regularization parameter $\\lambda$ is\n",
    "very large \\* (d) is more sensitive to outliers than least squares\n",
    "\n",
    "**16. For a linear regression model, start with random values for each\n",
    "coefficient. The sum of the squared errors is calculated for each pair\n",
    "of input and output values. A learning rate is used as a scale factor,\n",
    "and the coefficients are updated in the direction towards minimizing the\n",
    "error. The process is repeated until a minimum sum squared error is\n",
    "achieved or no further improvement is possible. What is this process\n",
    "called?** \\* (a) LASSO \\* (b) Gradient Descent \\* (c) Least squares \\*\n",
    "(d) Regularization\n",
    "\n",
    "**17. Let $X \\in \\mathbb{R}^{m \\times n}$, $w \\in \\mathbb{R}^n$, and\n",
    "$Y \\in \\mathbb{R}^m$. Consider mean squared error $L(w) = ||Xw-Y||_2^2$.\n",
    "What is $\\nabla_w L(w)$?** \\* (a) $2Y^T (X^T Xw - Y)$ \\* (b)\n",
    "$2X^T(X^T Xw - Y)$ \\* (c) $2Y^T (Xw - Y)$ \\* (d) $2X^T (Xw - Y)$\n",
    "\n",
    "**18. Write down a closed-form solution for the optimal parameters $w$\n",
    "that minimize the loss function**\n",
    "$$L(w) = \\sum_{i=1}^{n}(y_i - x_i^T w)^2 + \\lambda||w||_2^2$$ **in terms\n",
    "of (1) the $n \\times d$ matrix $X$ whose $i$-th row is a $1 \\times n$\n",
    "vector $x_i^T$ (a sample), (2) the $n \\times 1$ vector $y$ whose $i$-th\n",
    "entry is $y_i$, and (3) the scalar $\\lambda$. (You may assume that any\n",
    "relevant matrix is invertible.)** \\* (a)\n",
    "$\\hat{w} = (X^T X)^{-1} X y + \\lambda I$ \\* (b)\n",
    "$\\hat{w} = 2(X^T X + \\lambda I)^{-1} X^T y$ \\* (c)\n",
    "$\\hat{w} = \\lambda(X^T X)^{-1} X^T y$ \\* (d)\n",
    "$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "**19. How can overfitting be reduced in polynomial regression?** \\* (a)\n",
    "By decreasing the size of the validation set during hyperparameter\n",
    "tuning. \\* (b) By increasing the degree of the polynomial. \\* (c) By\n",
    "using regularization techniques such as $l_1$ or $l_2$ regularization.\n",
    "\\* (d) By reducing the size of your training set.\n",
    "\n",
    "**20. True/False: Linear least squares has a nonconvex loss function.**\n",
    "\\* (a) True \\* (b) False\n",
    "\n",
    "**21. True/False: It is possible to apply gradient descent method on\n",
    "linear least squares loss.** \\* (a) True \\* (b) False\n",
    "\n",
    "**22. Let $x_1, x_2 \\in \\mathbb{R}_+$ be sampled from the distribution\n",
    "$\\text{Exp}(\\lambda)$, where $\\lambda \\in \\mathbb{R}_+$ is an unknown\n",
    "variable. Remember that the PDF of the exponential distribution is\n",
    "$f(x) = \\lambda e^{-\\lambda x}$ for any $x > 0$ and $f(x) = 0$\n",
    "otherwise. Using the log-likelihood, find the maximum likelihood\n",
    "estimation of $\\lambda$ in terms of $x_1, x_2$. Hint:\n",
    "$\\frac{d}{dx} e^x = e^x$.** \\* (a) $\\frac{\\log(x_1)+\\log(x_2)}{2}$ \\*\n",
    "(b) $\\log\\left(\\frac{e^{x_1}+e^{x_2}}{2}\\right)$ \\* (c)\n",
    "$\\frac{x_1+x_2}{2}$ \\* (d) $\\frac{2}{x_1+x_2}$\n",
    "\n",
    "**23. Extra Credit: You are taking a multiple-choice exam that has 4\n",
    "answers for each question. You are a smart student, so in answering a\n",
    "question on this exam, the probability that you know the correct answer\n",
    "is $p$, and you always choose the correct answer when you know it. If\n",
    "you don’t know the answer, you choose one (uniformly) at random. What is\n",
    "the probability that you knew the correct answer to a question, given\n",
    "that you answered it correctly?** \\* (a) $p+\\frac{1-p}{4}$ \\* (b)\n",
    "$\\frac{p}{1+p}$ \\* (c) $\\frac{p}{p+\\frac{1-p}{4}}$ \\* (d)\n",
    "$\\frac{p}{\\frac{p}{4}+1}$\n",
    "\n",
    "**24. Select all of the following statements that are False. When\n",
    "training a machine learning model you should** \\* (a) manually select\n",
    "samples from your data to form a test set. \\* (b) use a test set to help\n",
    "choose hyperparameter values. \\* (c) never use the test set to make\n",
    "changes to the model. \\* (d) split your data into training and test\n",
    "sets.\n",
    "\n",
    "**25. Let $X \\sim \\text{Uniform}[0, 3]$ and $Y \\sim N(2, 2)$ be\n",
    "independent random variables. Then compute $E[XY^2] - E[X]E[Y]^2$.** \\*\n",
    "(a) 4 \\* (b) 6 \\* (c) 0 \\* (d) 3\n",
    "\n",
    "**26. (True/False: ) Stochastic Gradient Descent (SGD) will always be at\n",
    "least as computationally expensive as Gradient Descent (GD) and\n",
    "(True/False: ) the number of update steps in SGD is greater than or\n",
    "equal to the number of update steps in GD.** \\* (a) True, True \\* (b)\n",
    "True, False \\* (c) False, True \\* (d) False, False\n",
    "\n",
    "**27. Which technique is most likely to reduce the variance of a model,\n",
    "holding all else fixed?** \\* (a) Reducing the complexity of the model \\*\n",
    "(b) Using a smaller number of training samples \\* (c) Increasing the\n",
    "number of features"
   ],
   "id": "fa6caaa9-59d2-44ca-a199-f6abe43c1ec3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
