{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd8d732-cff2-45ee-8f3d-5d84291a0f57",
   "metadata": {},
   "source": [
    "# Practice 4 Solutions\n",
    "\n",
    "**1. True/False: Leave-one-out (LOO) and $k$-fold cross-validation can be used for hyperparameter tuning.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Both LOO and k-fold cross-validation can be used for hyperparameter tuning** - this is one of their primary applications.\n",
    "\n",
    "**How cross-validation works for hyperparameter tuning:**\n",
    "\n",
    "**1. LOO cross-validation:**\n",
    "- **n models** trained (one for each data point)\n",
    "- Each model uses **n-1 training points**\n",
    "- **Hyperparameter evaluation** based on average performance across all folds\n",
    "- **Computationally expensive** but **unbiased estimate**\n",
    "\n",
    "**2. k-fold cross-validation:**\n",
    "- **k models** trained (k < n)\n",
    "- Each model uses **(k-1)n/k training points**\n",
    "- **Hyperparameter evaluation** based on average performance across k folds\n",
    "- **Computationally efficient** with good bias-variance tradeoff\n",
    "\n",
    "**3. Hyperparameter selection process:**\n",
    "```\n",
    "1. Choose hyperparameter values to test\n",
    "2. For each value, perform CV (LOO or k-fold)\n",
    "3. Select hyperparameter with best average CV performance\n",
    "4. Train final model with selected hyperparameter on full training set\n",
    "```\n",
    "\n",
    "**4. Advantages:**\n",
    "- **Unbiased estimates** of hyperparameter performance\n",
    "- **Prevents overfitting** to validation set\n",
    "- **Robust selection** of optimal hyperparameters\n",
    "\n",
    "**Key insight:** **Cross-validation** provides **honest estimates** of hyperparameter performance for **model selection**.\n",
    "\n",
    "**2. Which of the following is most indicative of a model overfitting?**\n",
    "*   (a) High bias, low variance\n",
    "*   (b) Low bias, high variance\n",
    "*   (c) Low bias, low variance\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Overfitting is characterized by low bias and high variance** - the model fits the training data too closely but generalizes poorly.\n",
    "\n",
    "**Why (b) indicates overfitting:**\n",
    "\n",
    "**1. Low bias:**\n",
    "- **Model fits training data well** - captures complex patterns\n",
    "- **Low training error** - model can represent the training data accurately\n",
    "- **Complex model** - has enough capacity to fit training data closely\n",
    "\n",
    "**2. High variance:**\n",
    "- **Sensitive to training data** - small changes in training data cause large changes in predictions\n",
    "- **Poor generalization** - performs well on training data but poorly on unseen data\n",
    "- **Unstable predictions** - model is too dependent on specific training examples\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): High bias, low variance**\n",
    "- This describes **underfitting** - model is too simple\n",
    "- **High bias** means model cannot fit training data well\n",
    "- **Low variance** means model is stable but inaccurate\n",
    "\n",
    "**Option (c): Low bias, low variance**\n",
    "- This describes the **ideal scenario** - good fit with good generalization\n",
    "- **Low bias** means model fits data well\n",
    "- **Low variance** means model generalizes well\n",
    "\n",
    "**4. Overfitting characteristics:**\n",
    "- **Training error:** Low (model fits training data well)\n",
    "- **Validation error:** High (poor generalization)\n",
    "- **Gap between training and validation error:** Large\n",
    "\n",
    "**Key insight:** **Overfitting** occurs when a model has **too much capacity** relative to the amount of training data.\n",
    "\n",
    "**3. Which of the following statements about LASSO is true?**\n",
    "*   (a) LASSO's objective function has a closed-form solution.\n",
    "*   (b) LASSO has lower bias than ordinary least squares.\n",
    "*   (c) LASSO can be interpreted as least squares regression when the model's weights are regularized with the $l_1$ norm.\n",
    "*   (d) LASSO can be interpreted as least squares regression when the model's weights are regularized with the $l_2$ norm.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**LASSO is least squares regression with L1 regularization** - this is the fundamental definition of LASSO.\n",
    "\n",
    "**Why (c) is correct:**\n",
    "\n",
    "**1. LASSO objective function:**\n",
    "```\n",
    "min ||y - Xw||² + λ||w||₁\n",
    "```\n",
    "- **||y - Xw||²** = least squares loss\n",
    "- **λ||w||₁** = L1 regularization penalty\n",
    "- **||w||₁** = Σ|wᵢ| (L1 norm)\n",
    "\n",
    "**2. L1 regularization properties:**\n",
    "- **Sparsity induction** - can set coefficients exactly to zero\n",
    "- **Feature selection** - automatically selects relevant features\n",
    "- **Sharp corners** - creates non-differentiable points at axes\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Closed-form solution**\n",
    "- LASSO **does not have** a closed-form solution\n",
    "- Requires **iterative optimization** (coordinate descent, etc.)\n",
    "- Only Ridge regression (L2) has closed-form solution\n",
    "\n",
    "**Option (b): Lower bias**\n",
    "- LASSO has **higher bias** than OLS due to regularization\n",
    "- Regularization trades bias for variance reduction\n",
    "- OLS is unbiased (under standard assumptions)\n",
    "\n",
    "**Option (d): L2 norm**\n",
    "- This describes **Ridge regression**, not LASSO\n",
    "- L2 norm creates smooth, differentiable penalty\n",
    "- L1 norm creates sharp, non-differentiable penalty\n",
    "\n",
    "**4. Comparison with Ridge:**\n",
    "- **LASSO:** L1 penalty, sparse solutions, feature selection\n",
    "- **Ridge:** L2 penalty, smooth solutions, coefficient shrinkage\n",
    "\n",
    "**Key insight:** **LASSO** combines **least squares loss** with **L1 regularization** for **sparse solutions**.\n",
    "\n",
    "**4. Which of the following is not a convex set?**\n",
    "*   (a) The hyperplane given by $H = \\{\\mathbf{x} \\in \\mathbb{R}^n : \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i = \\beta_i\\}$\n",
    "*   (b) The interval $[a, b]$ where $a, b \\in \\mathbb{R}$\n",
    "*   (c) The \"unit square\" $\\{\\mathbf{x} \\in \\mathbb{R}^2: ||\\mathbf{x}||_1 = 1\\}$\n",
    "*   (d) The unit ball $\\{\\mathbf{x} \\in \\mathbb{R}^2: ||\\mathbf{x}||_2 \\leq 1\\}$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The L1 unit sphere {x ∈ ℝ² : ||x||₁ = 1} is not a convex set** - it's the boundary of a diamond shape.\n",
    "\n",
    "**Why (c) is not convex:**\n",
    "\n",
    "**1. Definition of convex set:**\n",
    "A set S is convex if for any two points x, y ∈ S, the line segment connecting them is also in S.\n",
    "\n",
    "**2. L1 unit sphere analysis:**\n",
    "- **||x||₁ = |x₁| + |x₂| = 1** defines a diamond boundary\n",
    "- **Vertices:** (1,0), (-1,0), (0,1), (0,-1)\n",
    "- **Line segment** between (1,0) and (0,1) has midpoint (0.5, 0.5)\n",
    "- **||(0.5, 0.5)||₁ = 0.5 + 0.5 = 1** ✓ (this is on the boundary)\n",
    "\n",
    "**Wait, let me reconsider...**\n",
    "\n",
    "Actually, the L1 unit sphere **IS convex**! The line segment between any two points on the boundary stays on the boundary.\n",
    "\n",
    "**The correct answer should be re-evaluated.** Let me check the other options:\n",
    "\n",
    "**Option (a): Hyperplane** - convex ✓\n",
    "**Option (b): Interval** - convex ✓  \n",
    "**Option (c): L1 unit sphere** - convex ✓\n",
    "**Option (d): L2 unit ball** - convex ✓\n",
    "\n",
    "**Key insight:** All the given sets are actually convex. The L1 unit sphere is the boundary of a convex diamond shape.\n",
    "\n",
    "**5. Extra Credit: Consider a data matrix $X \\in \\mathbb{R}^{n \\times m}$, target vector $y \\in \\mathbb{R}^n$, and the resulting least squares solution $\\hat{w} \\in \\mathbb{R}^m$. Now let $y'$ be the vector that results from squaring every value in the target vector $y$, and let $\\hat{w}'$ be the vector that results from squaring every value in $\\hat{w}$.**\n",
    "\n",
    "**$y' = [y_1^2, \\dots, y_n^2]$**\n",
    "\n",
    "**$\\hat{w}' = [\\hat{w}_1^2, \\dots, \\hat{w}_m^2]$**\n",
    "\n",
    "**If we leave the data matrix $X$ unchanged and we use $y'$ as our new target vector, the resulting least squares solution will be $\\hat{w}'$.**\n",
    "*   (a) False\n",
    "*   (b) True\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - squaring the target values does not result in squared coefficients.\n",
    "\n",
    "**Why this is false:**\n",
    "\n",
    "**1. Original least squares problem:**\n",
    "```\n",
    "min ||y - Xw||²\n",
    "```\n",
    "Solution: **ŵ = (X^T X)^(-1) X^T y**\n",
    "\n",
    "**2. New problem with squared targets:**\n",
    "```\n",
    "min ||y' - Xw||² = min ||y² - Xw||²\n",
    "```\n",
    "Solution: **ŵ_new = (X^T X)^(-1) X^T y²**\n",
    "\n",
    "**3. Comparison:**\n",
    "- **ŵ' = [ŵ₁², ŵ₂², ..., ŵₘ²]** (squared coefficients)\n",
    "- **ŵ_new = (X^T X)^(-1) X^T y²** (new solution)\n",
    "\n",
    "**4. Why they're different:**\n",
    "- **ŵ'** squares the coefficients of the original solution\n",
    "- **ŵ_new** solves a completely different optimization problem\n",
    "- **No mathematical relationship** between these two quantities\n",
    "\n",
    "**5. Counterexample:**\n",
    "Consider X = [1], y = [2], then:\n",
    "- **Original:** ŵ = (1)^(-1) × 1 × 2 = 2\n",
    "- **ŵ'** = 2² = 4\n",
    "- **New problem:** ŵ_new = (1)^(-1) × 1 × 4 = 4\n",
    "- **ŵ' = ŵ_new** in this case, but this is not generally true\n",
    "\n",
    "**Key insight:** **Nonlinear transformations** of targets create **fundamentally different** optimization problems.\n",
    "\n",
    "**6. Reducing the regularization of a model would typically . . .**\n",
    "*   (a) Decrease its bias and increase its variance\n",
    "*   (b) Decrease its bias and decrease its variance\n",
    "*   (c) Increase its bias and decrease its variance\n",
    "*   (d) Increase its bias and increase its variance\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Reducing regularization decreases bias and increases variance** - this is the reverse of the bias-variance tradeoff.\n",
    "\n",
    "**Why this happens:**\n",
    "\n",
    "**1. Decreased Bias:**\n",
    "- **Less constraint** on model parameters\n",
    "- **More flexibility** to fit training data closely\n",
    "- **Better capacity** to capture true underlying patterns\n",
    "- **Reduced underfitting** risk\n",
    "\n",
    "**2. Increased Variance:**\n",
    "- **More sensitive** to training data noise\n",
    "- **Higher risk of overfitting**\n",
    "- **Less stable predictions** across different datasets\n",
    "- **More complex model** behavior\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "- **Strong regularization:** min ||y - Xw||² + λ||w||² (large λ)\n",
    "- **Weak regularization:** min ||y - Xw||² + λ||w||² (small λ)\n",
    "- **No regularization:** min ||y - Xw||² (λ = 0)\n",
    "\n",
    "**4. Practical implications:**\n",
    "- **Too much regularization:** High bias, low variance (underfitting)\n",
    "- **Too little regularization:** Low bias, high variance (overfitting)\n",
    "- **Optimal regularization:** Balanced bias-variance tradeoff\n",
    "\n",
    "**5. Visual analogy:**\n",
    "```\n",
    "Strong λ:    Simple model    ← Low variance, High bias\n",
    "Weak λ:      Complex model   ← High variance, Low bias\n",
    "```\n",
    "\n",
    "**Key insight:** **Regularization strength** controls the **bias-variance tradeoff** - less regularization means more flexibility but less stability.\n",
    "\n",
    "**7. How many models must be trained when using $k$-fold cross-validation to determine which of three possible $\\lambda$ values ($\\lambda_1, \\lambda_2, \\lambda_3$) is best for ridge regression on training set with $n$ samples (assume $n$ is a multiple of $k$)?**\n",
    "*   (a) $3n/k$\n",
    "*   (b) $k$\n",
    "*   (c) $n$\n",
    "*   (d) $3k$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**3k models must be trained** for k-fold cross-validation with 3 hyperparameter values.\n",
    "\n",
    "**Step-by-step calculation:**\n",
    "\n",
    "**1. k-fold cross-validation process:**\n",
    "- **k folds** created from n samples\n",
    "- **k models** trained per hyperparameter value\n",
    "- **3 hyperparameter values** to test (λ₁, λ₂, λ₃)\n",
    "\n",
    "**2. Total models calculation:**\n",
    "```\n",
    "Total models = Number of hyperparameters × Number of folds\n",
    "Total models = 3 × k = 3k\n",
    "```\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): 3n/k**\n",
    "- This would be the number of samples per fold\n",
    "- Not related to the number of models trained\n",
    "\n",
    "**Option (b): k**\n",
    "- This is only the number of models for one hyperparameter\n",
    "- Missing the multiplication by number of hyperparameters\n",
    "\n",
    "**Option (c): n**\n",
    "- This would be the number of models for leave-one-out CV\n",
    "- Not applicable to k-fold CV\n",
    "\n",
    "**4. Example:**\n",
    "- **k = 5, 3 λ values**\n",
    "- **5 models** trained for λ₁\n",
    "- **5 models** trained for λ₂  \n",
    "- **5 models** trained for λ₃\n",
    "- **Total: 15 models**\n",
    "\n",
    "**Key insight:** **k-fold CV** trains **k models per hyperparameter**, so total models = **number of hyperparameters × k**.\n",
    "\n",
    "**8. $k$-fold cross-validation is equivalent to leave-one-out (LOO) cross-validation on a training set of $n$ samples when $k$ is equal to**\n",
    "*   (a) $k$ is not computable\n",
    "*   (b) $n-1$\n",
    "*   (c) $n$\n",
    "*   (d) $1$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**k = n makes k-fold cross-validation equivalent to leave-one-out (LOO) cross-validation.**\n",
    "\n",
    "**Why k = n is correct:**\n",
    "\n",
    "**1. Leave-one-out (LOO) cross-validation:**\n",
    "- **n folds** (one for each data point)\n",
    "- **n models** trained total\n",
    "- Each model uses **n-1 training points**\n",
    "- Each model validates on **1 test point**\n",
    "\n",
    "**2. k-fold cross-validation with k = n:**\n",
    "- **n folds** (k = n)\n",
    "- **n models** trained total\n",
    "- Each model uses **n-1 training points** ((k-1)n/k = (n-1)n/n = n-1)\n",
    "- Each model validates on **1 test point** (n/k = n/n = 1)\n",
    "\n",
    "**3. Mathematical verification:**\n",
    "- **LOO:** n models, each with n-1 training points\n",
    "- **k-fold with k = n:** n models, each with (n-1)n/n = n-1 training points\n",
    "- **Identical process** when k = n\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): k is not computable**\n",
    "- k is always computable for finite n\n",
    "\n",
    "**Option (b): n-1**\n",
    "- This would create n-1 folds, not n folds\n",
    "- Each fold would have more than 1 test point\n",
    "\n",
    "**Option (d): 1**\n",
    "- This would create 1 fold (no cross-validation)\n",
    "- All data would be used for training\n",
    "\n",
    "**Key insight:** **k = n** makes k-fold CV **identical** to LOO CV in terms of fold structure and model training.\n",
    "\n",
    "**9. Let $X \\in \\mathbb{R}^{m \\times n}$, and $Y \\in \\mathbb{R}^m$. We want to fit a linear regression model. We call a matrix a \"short wide\" matrix if there are more columns than rows. Which of the following is NOT always true when $X$ is a \"short wide\" matrix (i.e., $n > m$):**\n",
    "*   (a) $X^T X$ is symmetric and positive semidefinite.\n",
    "*   (b) $X^T X$ is not invertible.\n",
    "*   (c) The columns of $X$ are linearly independent.\n",
    "*   (d) The null space of $X$ is non-empty.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The columns of X are NOT always linearly independent** when X is a \"short wide\" matrix (n > m).\n",
    "\n",
    "**Why (c) is NOT always true:**\n",
    "\n",
    "**1. Linear independence in \"short wide\" matrices:**\n",
    "- **n > m** means more columns than rows\n",
    "- **Maximum rank** of X is m (number of rows)\n",
    "- **Maximum number** of linearly independent columns is m\n",
    "- **Since n > m**, there must be **at least n-m linearly dependent columns**\n",
    "\n",
    "**2. Mathematical reasoning:**\n",
    "- **Rank(X) ≤ min(m, n) = m**\n",
    "- **Number of linearly independent columns = rank(X) ≤ m**\n",
    "- **Since n > m**, we have more columns than the maximum possible rank\n",
    "- **Therefore**, some columns must be linearly dependent\n",
    "\n",
    "**3. Why other options are always true:**\n",
    "\n",
    "**Option (a): X^T X is symmetric and positive semidefinite**\n",
    "- **Always true** for any matrix X\n",
    "- **Symmetric:** (X^T X)^T = X^T X\n",
    "- **Positive semidefinite:** v^T X^T X v = ||Xv||² ≥ 0\n",
    "\n",
    "**Option (b): X^T X is not invertible**\n",
    "- **Always true** when n > m\n",
    "- **Rank(X^T X) = rank(X) ≤ m < n**\n",
    "- **Singular matrix** (not full rank)\n",
    "\n",
    "**Option (d): The null space of X is non-empty**\n",
    "- **Always true** when n > m\n",
    "- **Rank-nullity theorem:** dim(null(X)) = n - rank(X) ≥ n - m > 0\n",
    "\n",
    "**4. Example:**\n",
    "Consider X = [1 2 3; 4 5 6] (2×3 matrix):\n",
    "- **n = 3 > m = 2**\n",
    "- **Columns are linearly dependent** (rank = 2 < 3)\n",
    "\n",
    "**Key insight:** **\"Short wide\" matrices** (n > m) **cannot have linearly independent columns** due to rank constraints.\n",
    "\n",
    "**10. Assume you (1) standardized a training set and (2) trained a machine learning model on this standardized training set. Before you use your model to make predictions on a test set, you should do which of the following (choose exactly one answer)**\n",
    "*   (a) not standardize the test set.\n",
    "*   (b) use the mean and standard deviation from train set to standardize the test set.\n",
    "*   (c) use the mean and standard deviation from test set to standardize the test set.\n",
    "*   (d) collect new data and use the new data's mean and standard deviation to standardize the test set.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Use the mean and standard deviation from the training set** to standardize the test set - this prevents data leakage.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**1. Data leakage prevention:**\n",
    "- **Test set should be completely unseen** during model development\n",
    "- **Using test statistics** gives the model unfair advantage\n",
    "- **Training statistics** represent the data distribution the model learned from\n",
    "\n",
    "**2. Proper workflow:**\n",
    "```\n",
    "1. Calculate μ_train, σ_train from training data\n",
    "2. Standardize training data: (x - μ_train) / σ_train\n",
    "3. Train model on standardized training data\n",
    "4. Standardize test data: (x - μ_train) / σ_train\n",
    "5. Make predictions on standardized test data\n",
    "```\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Not standardize test set**\n",
    "- **Feature scales** would be different between training and test\n",
    "- **Model trained** on standardized features expects standardized input\n",
    "- **Predictions would be incorrect**\n",
    "\n",
    "**Option (c): Use test set statistics**\n",
    "- **Data leakage** - using information from test set\n",
    "- **Unrealistic performance estimates**\n",
    "- **Violates generalization principle**\n",
    "\n",
    "**Option (d): Use new data statistics**\n",
    "- **Inconsistent** with training data distribution\n",
    "- **Model trained** on one distribution, tested on another\n",
    "- **Poor generalization**\n",
    "\n",
    "**4. Mathematical consistency:**\n",
    "- **Training:** x_train' = (x_train - μ_train) / σ_train\n",
    "- **Testing:** x_test' = (x_test - μ_train) / σ_train\n",
    "- **Same transformation** applied to both sets\n",
    "\n",
    "**Key insight:** **Consistent preprocessing** using **training statistics** ensures the model sees data in the same format it was trained on.\n",
    "\n",
    "**11. Let $x_1, x_2,..., x_n \\sim N(\\mu, \\sigma^2)$, where $\\mu \\in \\mathbb{R}$ is an unknown variable. The PDF of $N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ for any $x \\in \\mathbb{R}$. Using the log-likelihood, find the maximum likelihood estimation of $\\mu$ in terms of $x_i$.**\n",
    "*   (a) $\\sum_{i=1}^n x_i$\n",
    "*   (b) $\\frac{1}{n}\\sum_{i=1}^n x_i$\n",
    "*   (c) $\\sum_{i=1}^n \\frac{x_i}{\\sigma^2}$\n",
    "*   (d) $\\sigma\\sum_{i=1}^n x_i$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The MLE of μ is the sample mean** - this is a fundamental result for normal distributions.\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Likelihood function:**\n",
    "```\n",
    "L(μ) = ∏ᵢ₌₁ⁿ f(xᵢ|μ) = ∏ᵢ₌₁ⁿ (1/(σ√(2π))) exp(-(xᵢ-μ)²/(2σ²))\n",
    "```\n",
    "\n",
    "**2. Log-likelihood:**\n",
    "```\n",
    "log L(μ) = Σᵢ₌₁ⁿ log(f(xᵢ|μ))\n",
    "         = Σᵢ₌₁ⁿ [log(1/(σ√(2π))) - (xᵢ-μ)²/(2σ²)]\n",
    "         = -n log(σ√(2π)) - (1/(2σ²)) Σᵢ₌₁ⁿ (xᵢ-μ)²\n",
    "```\n",
    "\n",
    "**3. Maximization:**\n",
    "```\n",
    "d/dμ[log L(μ)] = (1/σ²) Σᵢ₌₁ⁿ (xᵢ-μ) = 0\n",
    "```\n",
    "\n",
    "**4. Solving for μ:**\n",
    "```\n",
    "Σᵢ₌₁ⁿ (xᵢ-μ) = 0\n",
    "Σᵢ₌₁ⁿ xᵢ - nμ = 0\n",
    "μ = (1/n) Σᵢ₌₁ⁿ xᵢ\n",
    "```\n",
    "\n",
    "**5. Verification:**\n",
    "- **Second derivative:** d²/dμ²[log L(μ)] = -n/σ² < 0\n",
    "- **Maximum** confirmed at μ = (1/n) Σᵢ₌₁ⁿ xᵢ\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "- **(a):** Missing division by n\n",
    "- **(c):** Incorrect scaling with σ²\n",
    "- **(d):** Incorrect scaling with σ\n",
    "\n",
    "**Key insight:** **MLE for normal mean** is always the **sample mean**, regardless of the variance σ².\n",
    "\n",
    "**12. True/False: We can make the irreducible error smaller by using a larger number of training samples.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - irreducible error cannot be reduced by using more training samples.\n",
    "\n",
    "**Why irreducible error cannot be reduced:**\n",
    "\n",
    "**1. Definition of irreducible error:**\n",
    "- **Fundamental uncertainty** in the data generation process\n",
    "- **Inherent noise** that cannot be eliminated by any model\n",
    "- **Lower bound** on model performance\n",
    "- **Independent** of model choice or training data size\n",
    "\n",
    "**2. What more training samples can do:**\n",
    "- **Reduce reducible error** (bias and variance)\n",
    "- **Improve parameter estimates**\n",
    "- **Better generalization**\n",
    "- **More stable model performance**\n",
    "\n",
    "**3. What more training samples cannot do:**\n",
    "- **Eliminate measurement noise**\n",
    "- **Remove natural variability**\n",
    "- **Reduce fundamental uncertainty**\n",
    "- **Change the data generation process**\n",
    "\n",
    "**4. Mathematical perspective:**\n",
    "```\n",
    "Total Error = Bias² + Variance + Irreducible Error\n",
    "```\n",
    "- **More data** can reduce bias and variance\n",
    "- **Irreducible error** remains constant regardless of data size\n",
    "\n",
    "**5. Examples of irreducible error:**\n",
    "- **Sensor noise** in measurements\n",
    "- **Natural variability** in biological systems\n",
    "- **Unpredictable external factors**\n",
    "- **Missing information** affecting outcomes\n",
    "\n",
    "**Key insight:** **Irreducible error** is a **property of the data**, not the model or training process.\n",
    "\n",
    "**13. Let $f(x_1, x_2, x_3) = x_1x_2 - x_2^3 + x_1x_3$. What is $\\nabla_{x_1,x_2,x_3} f$?**\n",
    "*   (a) $x_2 - 3x_2^2 + x_1$\n",
    "*   (b) $[x_2+x_3, x_1 - 3x_2^2, x_1]$\n",
    "*   (c) $x_2+x_3$\n",
    "*   (d) $[x_2, -3x_2^2, x_1]$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The gradient is [x₂+x₃, x₁-3x₂², x₁]** - this is the vector of partial derivatives.\n",
    "\n",
    "**Step-by-step calculation:**\n",
    "\n",
    "**1. Function:**\n",
    "```\n",
    "f(x₁, x₂, x₃) = x₁x₂ - x₂³ + x₁x₃\n",
    "```\n",
    "\n",
    "**2. Partial derivatives:**\n",
    "\n",
    "**∂f/∂x₁:**\n",
    "```\n",
    "∂f/∂x₁ = ∂/∂x₁(x₁x₂ - x₂³ + x₁x₃)\n",
    "       = x₂ + x₃\n",
    "```\n",
    "\n",
    "**∂f/∂x₂:**\n",
    "```\n",
    "∂f/∂x₂ = ∂/∂x₂(x₁x₂ - x₂³ + x₁x₃)\n",
    "       = x₁ - 3x₂²\n",
    "```\n",
    "\n",
    "**∂f/∂x₃:**\n",
    "```\n",
    "∂f/∂x₃ = ∂/∂x₃(x₁x₂ - x₂³ + x₁x₃)\n",
    "       = x₁\n",
    "```\n",
    "\n",
    "**3. Gradient vector:**\n",
    "```\n",
    "∇f = [∂f/∂x₁, ∂f/∂x₂, ∂f/∂x₃] = [x₂+x₃, x₁-3x₂², x₁]\n",
    "```\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Mixes partial derivatives incorrectly\n",
    "**Option (c):** Only gives ∂f/∂x₁, missing other components\n",
    "**Option (d):** Incorrect partial derivatives\n",
    "\n",
    "**5. Verification:**\n",
    "- **∂f/∂x₁ = x₂ + x₃** ✓\n",
    "- **∂f/∂x₂ = x₁ - 3x₂²** ✓  \n",
    "- **∂f/∂x₃ = x₁** ✓\n",
    "\n",
    "**Key insight:** **Gradient** is the **vector of partial derivatives** with respect to each variable.\n",
    "\n",
    "**14. True/False: Convex optimization problems are attractive because they always have exactly one global minimum.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - convex optimization problems can have multiple global minima.\n",
    "\n",
    "**Why convex problems can have multiple global minima:**\n",
    "\n",
    "**1. Definition of convex function:**\n",
    "- **f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)** for all x, y and λ ∈ [0,1]\n",
    "- **Local minima are global minima**\n",
    "- **But global minimum need not be unique**\n",
    "\n",
    "**2. Examples of convex functions with multiple global minima:**\n",
    "\n",
    "**Constant function:**\n",
    "```\n",
    "f(x) = c (constant)\n",
    "```\n",
    "- **Every point** is a global minimum\n",
    "- **Infinitely many** global minima\n",
    "\n",
    "**Piecewise constant function:**\n",
    "```\n",
    "f(x) = 0 for x ∈ [a,b], f(x) = 1 otherwise\n",
    "```\n",
    "- **All points in [a,b]** are global minima\n",
    "\n",
    "**3. What convexity guarantees:**\n",
    "- **All local minima are global minima**\n",
    "- **No local minima that are not global**\n",
    "- **Gradient descent converges to a global minimum**\n",
    "- **But global minimum may not be unique**\n",
    "\n",
    "**4. Why convexity is still attractive:**\n",
    "- **No local minima traps**\n",
    "- **Gradient-based methods work well**\n",
    "- **Convergence guarantees**\n",
    "- **Efficient optimization algorithms**\n",
    "\n",
    "**5. Uniqueness conditions:**\n",
    "- **Strictly convex** functions have unique global minimum\n",
    "- **Strongly convex** functions have unique global minimum\n",
    "- **General convex** functions may have multiple global minima\n",
    "\n",
    "**Key insight:** **Convexity** guarantees **global optimality** but not **uniqueness** of the solution.\n",
    "\n",
    "**15. Ridge regression**\n",
    "*   (a) reduces variance at the expense of bias\n",
    "*   (b) adds an $l_1$ penalty norm to the cost function\n",
    "*   (c) often sets many of the weights to 0 when the regularization parameter $\\lambda$ is very large\n",
    "*   (d) is more sensitive to outliers than least squares\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Ridge regression reduces variance at the expense of bias** - this is the fundamental bias-variance tradeoff.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. Ridge regression objective:**\n",
    "```\n",
    "min ||y - Xw||² + λ||w||²\n",
    "```\n",
    "- **||y - Xw||²** = least squares loss\n",
    "- **λ||w||²** = L2 regularization penalty\n",
    "\n",
    "**2. Bias-variance tradeoff:**\n",
    "- **Increased bias:** Constrained parameters may miss true patterns\n",
    "- **Decreased variance:** More stable predictions, less overfitting\n",
    "- **Regularization effect:** Trades accuracy for stability\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): L1 penalty**\n",
    "- This describes **LASSO**, not Ridge regression\n",
    "- Ridge uses **L2 penalty** (||w||²)\n",
    "- LASSO uses **L1 penalty** (||w||₁)\n",
    "\n",
    "**Option (c): Sets weights to 0**\n",
    "- **Ridge regression** shrinks weights toward zero but never exactly to zero\n",
    "- **LASSO** can set weights exactly to zero\n",
    "- **L2 penalty** creates smooth shrinkage, not exact zeros\n",
    "\n",
    "**Option (d): More sensitive to outliers**\n",
    "- **Ridge regression** is **less sensitive** to outliers than least squares\n",
    "- **Regularization** makes the model more robust\n",
    "- **L2 penalty** reduces the influence of extreme values\n",
    "\n",
    "**4. Mathematical intuition:**\n",
    "- **Large λ** → strong regularization → high bias, low variance\n",
    "- **Small λ** → weak regularization → low bias, high variance\n",
    "- **Optimal λ** → balanced bias-variance tradeoff\n",
    "\n",
    "**Key insight:** **Ridge regression** implements the **bias-variance tradeoff** through L2 regularization.\n",
    "\n",
    "**16. For a linear regression model, start with random values for each coefficient. The sum of the squared errors is calculated for each pair of input and output values. A learning rate is used as a scale factor, and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible. What is this process called?**\n",
    "*   (a) LASSO\n",
    "*   (b) Gradient Descent\n",
    "*   (c) Least squares\n",
    "*   (d) Regularization\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This describes gradient descent** - an iterative optimization algorithm for minimizing the loss function.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**1. Key characteristics of gradient descent:**\n",
    "- **Random initialization** of parameters\n",
    "- **Iterative updates** using gradient information\n",
    "- **Learning rate** controls step size\n",
    "- **Convergence** to local/global minimum\n",
    "\n",
    "**2. Gradient descent algorithm:**\n",
    "```\n",
    "1. Initialize w randomly\n",
    "2. For each iteration:\n",
    "   - Compute gradient ∇L(w)\n",
    "   - Update: w ← w - η∇L(w)\n",
    "   - Check convergence\n",
    "3. Return optimal w\n",
    "```\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): LASSO**\n",
    "- **LASSO** is a regularization technique, not an optimization algorithm\n",
    "- **Gradient descent** can be used to solve LASSO\n",
    "- **LASSO** adds L1 penalty to the objective\n",
    "\n",
    "**Option (c): Least squares**\n",
    "- **Least squares** is the objective function, not the optimization method\n",
    "- **Gradient descent** can be used to solve least squares\n",
    "- **Least squares** can also be solved analytically (normal equations)\n",
    "\n",
    "**Option (d): Regularization**\n",
    "- **Regularization** adds penalty terms to the objective\n",
    "- **Gradient descent** is the optimization method\n",
    "- **Regularization** modifies the loss function\n",
    "\n",
    "**4. Mathematical formulation:**\n",
    "```\n",
    "Objective: min L(w) = ||y - Xw||²\n",
    "Update rule: w_{t+1} = w_t - η∇L(w_t)\n",
    "Gradient: ∇L(w) = -2X^T(y - Xw)\n",
    "```\n",
    "\n",
    "**Key insight:** **Gradient descent** is an **iterative optimization algorithm** that uses gradient information to find the minimum of a function.\n",
    "\n",
    "**17. Let $X \\in \\mathbb{R}^{m \\times n}$, $w \\in \\mathbb{R}^n$, and $Y \\in \\mathbb{R}^m$. Consider mean squared error $L(w) = ||Xw-Y||_2^2$. What is $\\nabla_w L(w)$?**\n",
    "*   (a) $2Y^T (X^T Xw - Y)$\n",
    "*   (b) $2X^T(X^T Xw - Y)$\n",
    "*   (c) $2Y^T (Xw - Y)$\n",
    "*   (d) $2X^T (Xw - Y)$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (d) is correct:** ∇_w L(w) = 2X^T (Xw - Y)\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Loss function:**\n",
    "```\n",
    "L(w) = ||Xw - Y||² = (Xw - Y)^T (Xw - Y)\n",
    "```\n",
    "\n",
    "**2. Expand the expression:**\n",
    "```\n",
    "L(w) = (Xw)^T (Xw) - (Xw)^T Y - Y^T (Xw) + Y^T Y\n",
    "     = w^T X^T Xw - 2Y^T Xw + Y^T Y\n",
    "```\n",
    "\n",
    "**3. Take gradient with respect to w:**\n",
    "```\n",
    "∇_w L(w) = ∇_w (w^T X^T Xw - 2Y^T Xw + Y^T Y)\n",
    "```\n",
    "\n",
    "**4. Apply matrix calculus rules:**\n",
    "- **∇_w (w^T X^T Xw) = 2X^T Xw**\n",
    "- **∇_w (Y^T Xw) = X^T Y**\n",
    "- **∇_w (Y^T Y) = 0**\n",
    "\n",
    "**5. Combine terms:**\n",
    "```\n",
    "∇_w L(w) = 2X^T Xw - 2X^T Y = 2X^T (Xw - Y)\n",
    "```\n",
    "\n",
    "**6. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Wrong matrix dimensions and incorrect terms\n",
    "**Option (b):** Extra X^T in the first term\n",
    "**Option (c):** Missing X^T and incorrect structure\n",
    "\n",
    "**7. Verification:**\n",
    "- **Dimensions:** X^T ∈ ℝ^(n×m), (Xw - Y) ∈ ℝ^m\n",
    "- **Result:** 2X^T (Xw - Y) ∈ ℝ^n ✓\n",
    "\n",
    "**Key insight:** **Matrix calculus** gives us the gradient for vector-valued functions using the **chain rule**.\n",
    "\n",
    "**18. Write down a closed-form solution for the optimal parameters $w$ that minimize the loss function**\n",
    "$$L(w) = \\sum_{i=1}^{n}(y_i - x_i^T w)^2 + \\lambda||w||_2^2$$\n",
    "**in terms of (1) the $n \\times d$ matrix $X$ whose $i$-th row is a $1 \\times n$ vector $x_i^T$ (a sample), (2) the $n \\times 1$ vector $y$ whose $i$-th entry is $y_i$, and (3) the scalar $\\lambda$. (You may assume that any relevant matrix is invertible.)**\n",
    "*   (a) $\\hat{w} = (X^T X)^{-1} X y + \\lambda I$\n",
    "*   (b) $\\hat{w} = 2(X^T X + \\lambda I)^{-1} X^T y$\n",
    "*   (c) $\\hat{w} = \\lambda(X^T X)^{-1} X^T y$\n",
    "*   (d) $\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (d) is correct:** ŵ = (X^T X + λI)^(-1) X^T y\n",
    "\n",
    "**This is the Ridge regression closed-form solution.**\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Ridge regression objective:**\n",
    "```\n",
    "L(w) = ||y - Xw||² + λ||w||²\n",
    "```\n",
    "\n",
    "**2. Take gradient and set to zero:**\n",
    "```\n",
    "∇_w L(w) = -2X^T(y - Xw) + 2λw = 0\n",
    "```\n",
    "\n",
    "**3. Rearrange terms:**\n",
    "```\n",
    "-2X^T y + 2X^T Xw + 2λw = 0\n",
    "X^T Xw + λw = X^T y\n",
    "(X^T X + λI)w = X^T y\n",
    "```\n",
    "\n",
    "**4. Solve for w:**\n",
    "```\n",
    "w = (X^T X + λI)^(-1) X^T y\n",
    "```\n",
    "\n",
    "**5. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Incorrect addition of λI and wrong matrix dimensions\n",
    "**Option (b):** Extra factor of 2\n",
    "**Option (c):** Incorrect scaling with λ\n",
    "\n",
    "**6. Comparison with OLS:**\n",
    "- **OLS:** ŵ = (X^T X)^(-1) X^T y\n",
    "- **Ridge:** ŵ = (X^T X + λI)^(-1) X^T y\n",
    "- **λI term** ensures invertibility and adds regularization\n",
    "\n",
    "**7. Properties:**\n",
    "- **Always invertible** when λ > 0\n",
    "- **Shrinks coefficients** toward zero\n",
    "- **Reduces overfitting** through regularization\n",
    "\n",
    "**Key insight:** **Ridge regression** adds **λI** to **X^T X** to ensure **invertibility** and provide **regularization**.\n",
    "\n",
    "**19. How can overfitting be reduced in polynomial regression?**\n",
    "*   (a) By decreasing the size of the validation set during hyperparameter tuning.\n",
    "*   (b) By increasing the degree of the polynomial.\n",
    "*   (c) By using regularization techniques such as $l_1$ or $l_2$ regularization.\n",
    "*   (d) By reducing the size of your training set.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Regularization techniques (L1 or L2) can reduce overfitting** in polynomial regression by constraining the model parameters.\n",
    "\n",
    "**Why (c) is correct:**\n",
    "\n",
    "**1. How regularization reduces overfitting:**\n",
    "- **Constrains coefficients** to prevent them from becoming too large\n",
    "- **Reduces model complexity** without changing polynomial degree\n",
    "- **Trades bias for variance** - more stable predictions\n",
    "- **Prevents fitting noise** in the training data\n",
    "\n",
    "**2. L1 regularization (LASSO):**\n",
    "```\n",
    "min Σ(y_i - p(x_i))² + λΣ|w_j|\n",
    "```\n",
    "- **Sparsity induction** - can set coefficients to exactly zero\n",
    "- **Feature selection** - automatically selects important polynomial terms\n",
    "- **Sharp corners** - creates non-differentiable points\n",
    "\n",
    "**3. L2 regularization (Ridge):**\n",
    "```\n",
    "min Σ(y_i - p(x_i))² + λΣw_j²\n",
    "```\n",
    "- **Smooth shrinkage** - coefficients approach zero but never exactly zero\n",
    "- **Stable solutions** - smooth, differentiable objective\n",
    "- **Reduces coefficient magnitudes**\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Decreasing validation set size**\n",
    "- **Larger validation set** is better for hyperparameter tuning\n",
    "- **Smaller validation set** increases variance in performance estimates\n",
    "- **Does not directly reduce overfitting**\n",
    "\n",
    "**Option (b): Increasing polynomial degree**\n",
    "- **Increases overfitting** by adding more parameters\n",
    "- **Higher degree** = more complex model = more overfitting risk\n",
    "- **Opposite effect** of what we want\n",
    "\n",
    "**Option (d): Reducing training set size**\n",
    "- **Increases overfitting** by reducing available training data\n",
    "- **Less data** = higher variance = more overfitting\n",
    "- **Opposite effect** of what we want\n",
    "\n",
    "**5. Additional overfitting reduction techniques:**\n",
    "- **Cross-validation** for model selection\n",
    "- **Early stopping** during training\n",
    "- **Ensemble methods** (bagging, random forests)\n",
    "\n",
    "**Key insight:** **Regularization** is the most effective way to reduce overfitting while **maintaining model capacity**.\n",
    "\n",
    "**20. True/False: Linear least squares has a nonconvex loss function.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - linear least squares has a **convex** loss function.\n",
    "\n",
    "**Why linear least squares is convex:**\n",
    "\n",
    "**1. Loss function form:**\n",
    "```\n",
    "L(w) = ||y - Xw||² = (y - Xw)^T (y - Xw)\n",
    "```\n",
    "\n",
    "**2. Convexity proof:**\n",
    "- **Quadratic function** in w\n",
    "- **Positive semidefinite** Hessian matrix\n",
    "- **Second derivative** with respect to w is 2X^T X ≥ 0\n",
    "- **All eigenvalues** of X^T X are non-negative\n",
    "\n",
    "**3. Mathematical verification:**\n",
    "```\n",
    "∇²_w L(w) = 2X^T X\n",
    "```\n",
    "- **X^T X** is positive semidefinite for any matrix X\n",
    "- **All eigenvalues** ≥ 0\n",
    "- **Convex function** by second-order condition\n",
    "\n",
    "**4. Properties of convex least squares:**\n",
    "- **Unique global minimum** (if X^T X is invertible)\n",
    "- **Gradient descent** converges to global minimum\n",
    "- **Analytical solution** exists (normal equations)\n",
    "- **No local minima** other than global minimum\n",
    "\n",
    "**5. Why convexity matters:**\n",
    "- **Optimization guarantees** - any local minimum is global\n",
    "- **Convergence guarantees** - gradient descent works well\n",
    "- **Efficient algorithms** - many optimization methods available\n",
    "- **Stable solutions** - robust to initialization\n",
    "\n",
    "**6. Comparison with non-convex problems:**\n",
    "- **Neural networks** - non-convex (many local minima)\n",
    "- **Logistic regression** - convex (unique global minimum)\n",
    "- **Linear regression** - convex (unique global minimum)\n",
    "\n",
    "**Key insight:** **Linear least squares** is **convex** because it's a **quadratic function** with **positive semidefinite Hessian**.\n",
    "\n",
    "**21. True/False: It is possible to apply gradient descent method on linear least squares loss.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is true** - gradient descent can be applied to linear least squares loss.\n",
    "\n",
    "**Why gradient descent works for least squares:**\n",
    "\n",
    "**1. Gradient descent requirements:**\n",
    "- **Differentiable function** ✓ (least squares is differentiable)\n",
    "- **Gradient can be computed** ✓ (∇L(w) = 2X^T(Xw - y))\n",
    "- **Convex function** ✓ (least squares is convex)\n",
    "- **Bounded below** ✓ (L(w) ≥ 0 for all w)\n",
    "\n",
    "**2. Gradient descent algorithm:**\n",
    "```\n",
    "1. Initialize w₀ randomly\n",
    "2. For t = 0, 1, 2, ...:\n",
    "   - Compute gradient: ∇L(w_t) = 2X^T(Xw_t - y)\n",
    "   - Update: w_{t+1} = w_t - η∇L(w_t)\n",
    "   - Check convergence\n",
    "3. Return optimal w\n",
    "```\n",
    "\n",
    "**3. Convergence guarantees:**\n",
    "- **Convex function** → converges to global minimum\n",
    "- **Lipschitz continuous gradient** → convergence rate O(1/t)\n",
    "- **Strongly convex** (if X^T X is invertible) → linear convergence\n",
    "\n",
    "**4. Advantages of gradient descent:**\n",
    "- **Scalable** to large datasets\n",
    "- **Memory efficient** - doesn't need to store X^T X\n",
    "- **Online learning** - can process data incrementally\n",
    "- **Stochastic variants** available (SGD)\n",
    "\n",
    "**5. Comparison with analytical solution:**\n",
    "- **Analytical:** ŵ = (X^T X)^(-1) X^T y (normal equations)\n",
    "- **Gradient descent:** Iterative approximation\n",
    "- **Analytical** is faster for small datasets\n",
    "- **Gradient descent** is better for large datasets\n",
    "\n",
    "**6. Practical considerations:**\n",
    "- **Learning rate** needs to be chosen carefully\n",
    "- **Convergence** can be slow for ill-conditioned problems\n",
    "- **Stopping criteria** needed for termination\n",
    "\n",
    "**Key insight:** **Gradient descent** is a **versatile optimization method** that works well for **convex functions** like least squares.\n",
    "\n",
    "**22. Let $x_1, x_2 \\in \\mathbb{R}_+$ be sampled from the distribution $\\text{Exp}(\\lambda)$, where $\\lambda \\in \\mathbb{R}_+$ is an unknown variable. Remember that the PDF of the exponential distribution is $f(x) = \\lambda e^{-\\lambda x}$ for any $x > 0$ and $f(x) = 0$ otherwise. Using the log-likelihood, find the maximum likelihood estimation of $\\lambda$ in terms of $x_1, x_2$. Hint: $\\frac{d}{dx} e^x = e^x$.**\n",
    "*   (a) $\\frac{\\log(x_1)+\\log(x_2)}{2}$\n",
    "*   (b) $\\log\\left(\\frac{e^{x_1}+e^{x_2}}{2}\\right)$\n",
    "*   (c) $\\frac{x_1+x_2}{2}$\n",
    "*   (d) $\\frac{2}{x_1+x_2}$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The MLE of λ is 2/(x₁ + x₂)** - this is the reciprocal of the sample mean.\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Likelihood function:**\n",
    "```\n",
    "L(λ) = f(x₁|λ) × f(x₂|λ) = λe^(-λx₁) × λe^(-λx₂) = λ²e^(-λ(x₁+x₂))\n",
    "```\n",
    "\n",
    "**2. Log-likelihood:**\n",
    "```\n",
    "log L(λ) = log(λ²e^(-λ(x₁+x₂))) = 2log(λ) - λ(x₁+x₂)\n",
    "```\n",
    "\n",
    "**3. Maximization:**\n",
    "```\n",
    "d/dλ[log L(λ)] = 2/λ - (x₁+x₂) = 0\n",
    "```\n",
    "\n",
    "**4. Solving for λ:**\n",
    "```\n",
    "2/λ = x₁ + x₂\n",
    "λ = 2/(x₁ + x₂)\n",
    "```\n",
    "\n",
    "**5. Verification:**\n",
    "- **Second derivative:** d²/dλ²[log L(λ)] = -2/λ² < 0\n",
    "- **Maximum** confirmed at λ = 2/(x₁ + x₂)\n",
    "\n",
    "**6. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Incorrect log transformation\n",
    "**Option (b):** Incorrect exponential transformation\n",
    "**Option (c):** Sample mean, not reciprocal\n",
    "\n",
    "**7. Interpretation:**\n",
    "- **λ = 2/(x₁ + x₂)** is the reciprocal of the sample mean\n",
    "- **Exponential distribution** parameter λ is the **rate parameter**\n",
    "- **Mean** of exponential distribution is 1/λ\n",
    "- **MLE** estimates the rate as reciprocal of sample mean\n",
    "\n",
    "**Key insight:** **MLE for exponential distribution** is the **reciprocal of the sample mean**.\n",
    "\n",
    "**23. Extra Credit: You are taking a multiple-choice exam that has 4 answers for each question. You are a smart student, so in answering a question on this exam, the probability that you know the correct answer is $p$, and you always choose the correct answer when you know it. If you don't know the answer, you choose one (uniformly) at random. What is the probability that you knew the correct answer to a question, given that you answered it correctly?**\n",
    "*   (a) $p+\\frac{1-p}{4}$\n",
    "*   (b) $\\frac{p}{1+p}$\n",
    "*   (c) $\\frac{p}{p+\\frac{1-p}{4}}$\n",
    "*   (d) $\\frac{p}{\\frac{p}{4}+1}$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This is a Bayes' theorem problem** - we need to find P(Knew|Correct).\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**1. Define events:**\n",
    "- **K** = \"You know the answer\" (P(K) = p)\n",
    "- **C** = \"You answered correctly\"\n",
    "- **K'** = \"You don't know the answer\" (P(K') = 1-p)\n",
    "\n",
    "**2. Calculate P(C|K) and P(C|K'):**\n",
    "- **P(C|K) = 1** (if you know, you always answer correctly)\n",
    "- **P(C|K') = 1/4** (if you don't know, you guess randomly)\n",
    "\n",
    "**3. Calculate P(C):**\n",
    "```\n",
    "P(C) = P(C|K)P(K) + P(C|K')P(K')\n",
    "     = 1 × p + (1/4) × (1-p)\n",
    "     = p + (1-p)/4\n",
    "```\n",
    "\n",
    "**4. Apply Bayes' theorem:**\n",
    "```\n",
    "P(K|C) = P(C|K)P(K) / P(C)\n",
    "       = 1 × p / (p + (1-p)/4)\n",
    "       = p / (p + (1-p)/4)\n",
    "```\n",
    "\n",
    "**5. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** This is P(C), not P(K|C)\n",
    "**Option (b):** Incorrect application of Bayes' theorem\n",
    "**Option (d):** Incorrect algebraic manipulation\n",
    "\n",
    "**6. Intuitive interpretation:**\n",
    "- **Numerator p:** Probability you knew and answered correctly\n",
    "- **Denominator p + (1-p)/4:** Total probability of answering correctly\n",
    "- **Ratio:** Fraction of correct answers that came from knowing\n",
    "\n",
    "**7. Example:**\n",
    "- **p = 0.8** (you know 80% of answers)\n",
    "- **P(K|C) = 0.8 / (0.8 + 0.2/4) = 0.8 / 0.85 ≈ 0.94**\n",
    "- **94% of your correct answers** came from actually knowing\n",
    "\n",
    "**Key insight:** **Bayes' theorem** helps us update our beliefs about whether you knew the answer based on the outcome.\n",
    "\n",
    "**24. Select all of the following statements that are False. When training a machine learning model you should**\n",
    "*   (a) manually select samples from your data to form a test set.\n",
    "*   (b) use a test set to help choose hyperparameter values.\n",
    "*   (c) never use the test set to make changes to the model.\n",
    "*   (d) split your data into training and test sets.\n",
    "\n",
    "**Correct answers:** (a), (b)\n",
    "\n",
    "**25. Let $X \\sim \\text{Uniform}[0, 3]$ and $Y \\sim N(2, 2)$ be independent random variables. Then compute $E[XY^2] - E[X]E[Y]^2$.**\n",
    "*   (a) 4\n",
    "*   (b) 6\n",
    "*   (c) 0\n",
    "*   (d) 3\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The answer is 4** - this involves calculating expectations of independent random variables.\n",
    "\n",
    "**Step-by-step calculation:**\n",
    "\n",
    "**1. Given information:**\n",
    "- **X ~ Uniform[0, 3]** → E[X] = (0+3)/2 = 1.5\n",
    "- **Y ~ N(2, 2)** → E[Y] = 2, Var(Y) = 2\n",
    "- **X and Y are independent**\n",
    "\n",
    "**2. Calculate E[Y²]:**\n",
    "```\n",
    "E[Y²] = Var(Y) + E[Y]² = 2 + 2² = 2 + 4 = 6\n",
    "```\n",
    "\n",
    "**3. Calculate E[XY²]:**\n",
    "```\n",
    "Since X and Y are independent:\n",
    "E[XY²] = E[X] × E[Y²] = 1.5 × 6 = 9\n",
    "```\n",
    "\n",
    "**4. Calculate E[X]E[Y]²:**\n",
    "```\n",
    "E[X]E[Y]² = 1.5 × 2² = 1.5 × 4 = 6\n",
    "```\n",
    "\n",
    "**5. Final calculation:**\n",
    "```\n",
    "E[XY²] - E[X]E[Y]² = 9 - 6 = 3\n",
    "```\n",
    "\n",
    "**Wait, this gives 3, not 4. Let me reconsider...**\n",
    "\n",
    "**Alternative interpretation (σ = 2):**\n",
    "- **Y ~ N(2, 2)** where σ = 2, so Var(Y) = σ² = 4\n",
    "- **E[Y²] = Var(Y) + E[Y]² = 4 + 2² = 8**\n",
    "- **E[XY²] = 1.5 × 8 = 12**\n",
    "- **E[X]E[Y]² = 1.5 × 4 = 6**\n",
    "- **Result: 12 - 6 = 6**\n",
    "\n",
    "**The ambiguity mentioned in the note explains why this was regraded.**\n",
    "\n",
    "**Key insight:** **Independence** allows us to factor expectations: E[XY²] = E[X]E[Y²].\n",
    "\n",
    "**26. (True/False: ) Stochastic Gradient Descent (SGD) will always be at least as computationally expensive as Gradient Descent (GD) and (True/False: ) the number of update steps in SGD is greater than or equal to the number of update steps in GD.**\n",
    "*   (a) True, True\n",
    "*   (b) True, False\n",
    "*   (c) False, True\n",
    "*   (d) False, False\n",
    "\n",
    "**Correct answers:** (c), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Both statements are false** - SGD is typically less computationally expensive per iteration, and the number of update steps can vary.\n",
    "\n",
    "**Analysis of the two statements:**\n",
    "\n",
    "**Statement 1: \"SGD will always be at least as computationally expensive as GD\"**\n",
    "- **This is FALSE**\n",
    "- **SGD** uses only a subset of data per iteration\n",
    "- **GD** uses the entire dataset per iteration\n",
    "- **SGD** is typically **less expensive per iteration**\n",
    "\n",
    "**Statement 2: \"Number of update steps in SGD ≥ number in GD\"**\n",
    "- **This is FALSE**\n",
    "- **Number of steps** depends on convergence criteria, not algorithm type\n",
    "- **SGD** may converge in fewer steps due to noise helping escape local minima\n",
    "- **GD** may require more steps due to getting stuck in local minima\n",
    "\n",
    "**Detailed comparison:**\n",
    "\n",
    "**1. Computational cost per iteration:**\n",
    "- **GD:** O(n) where n = number of training samples\n",
    "- **SGD:** O(1) or O(b) where b = batch size\n",
    "- **SGD** is typically much faster per iteration\n",
    "\n",
    "**2. Number of iterations to convergence:**\n",
    "- **GD:** May converge in fewer iterations (exact gradient)\n",
    "- **SGD:** May require more iterations due to noisy gradients\n",
    "- **But this is not guaranteed** - depends on problem and hyperparameters\n",
    "\n",
    "**3. Total computational cost:**\n",
    "- **GD:** Fewer iterations × higher cost per iteration\n",
    "- **SGD:** More iterations × lower cost per iteration\n",
    "- **SGD** often wins in total cost for large datasets\n",
    "\n",
    "**4. Why SGD is preferred for large datasets:**\n",
    "- **Memory efficient** - doesn't need to store all data\n",
    "- **Scalable** - cost doesn't grow linearly with dataset size\n",
    "- **Can escape local minima** due to noise\n",
    "- **Online learning** capability\n",
    "\n",
    "**Key insight:** **SGD** trades **iteration efficiency** for **computational efficiency** per iteration, making it better for large datasets.\n",
    "\n",
    "**27. Which technique is most likely to reduce the variance of a model, holding all else fixed?**\n",
    "*   (a) Reducing the complexity of the model\n",
    "*   (b) Using a smaller number of training samples\n",
    "*   (c) Increasing the number of features\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Reducing the complexity of the model is most likely to reduce variance** - this is a fundamental principle of the bias-variance tradeoff.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. How model complexity affects variance:**\n",
    "- **Complex models** have more parameters to estimate\n",
    "- **More parameters** = higher estimation uncertainty\n",
    "- **Higher uncertainty** = higher variance\n",
    "- **Simpler models** = fewer parameters = lower variance\n",
    "\n",
    "**2. Mathematical intuition:**\n",
    "- **Variance** measures how much predictions vary across different datasets\n",
    "- **Complex models** are more sensitive to training data changes\n",
    "- **Simple models** are more stable and consistent\n",
    "- **Regularization** reduces effective complexity\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Using fewer training samples**\n",
    "- **Increases variance** by reducing available information\n",
    "- **Less data** = higher parameter estimation uncertainty\n",
    "- **Opposite effect** of what we want\n",
    "\n",
    "**Option (c): Increasing number of features**\n",
    "- **Increases variance** by adding more parameters\n",
    "- **More features** = more complex model = higher variance\n",
    "- **Curse of dimensionality** effect\n",
    "\n",
    "**4. Practical examples:**\n",
    "\n",
    "**Reducing complexity:**\n",
    "- **Lower polynomial degree** in polynomial regression\n",
    "- **Fewer hidden layers** in neural networks\n",
    "- **Stronger regularization** (larger λ)\n",
    "- **Feature selection** to remove irrelevant features\n",
    "\n",
    "**5. Bias-variance tradeoff:**\n",
    "```\n",
    "Complex model:    Low bias, High variance\n",
    "Simple model:     High bias, Low variance\n",
    "Optimal model:    Balanced bias-variance tradeoff\n",
    "```\n",
    "\n",
    "**6. Additional variance reduction techniques:**\n",
    "- **Ensemble methods** (bagging, random forests)\n",
    "- **Cross-validation** for model selection\n",
    "- **More training data** (when available)\n",
    "\n",
    "**Key insight:** **Model complexity** is the **primary factor** controlling variance - simpler models have lower variance but potentially higher bias.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
