{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2\n",
    "\n",
    "**1. Which of the following is the definition of irreducible error in\n",
    "machine learning?** \\* (a) The error that cannot be eliminated by any\n",
    "model \\* (b) The error that is caused by overfitting to the training\n",
    "data \\* (c) The error that is caused by underfitting to the testing data\n",
    "\\* (d) All of the above\n",
    "\n",
    "**2. What is the general model for $P(Y = 1|X = x,\\theta)$ in logistic\n",
    "regression, where $X = (X_0, X_1,..., X_n)$ is the features, $Y$ is the\n",
    "predictions, and $\\theta$ is the parameters? Assume that a bias term has\n",
    "already been appended to $X$ (i.e., $X_0 = 1$).** \\* (a)\n",
    "$P(Y = 1|X = x, \\theta) = \\frac{1}{1+e^{-\\theta^T x}}$ \\* (b)\n",
    "$P(Y = 1|X = x, \\theta) = \\theta^T x$ \\* (c)\n",
    "$P(Y = 1|X = x, \\theta) = \\log(1 + e^{-\\theta^T x})$ \\* (d)\n",
    "$P(Y = 1|X = x, \\theta) = \\log(1 + e^{\\theta^T x})$\n",
    "\n",
    "**3. Two realtors are creating machine learning models to predict house\n",
    "costs based on house traits (i.e. house size, neighborhood, school\n",
    "district, etc.) trained on the same set of houses, using the same model\n",
    "hyperparameters. Realtor A includes 30 different housing traits in their\n",
    "model. Realtor B includes 5 traits in their model. Which of the\n",
    "following outcomes is most likely?** \\* (a) Realtor B’s model has higher\n",
    "variance and lower bias than Realtor A’s model \\* (b) Realtor A’s model\n",
    "has higher variance than Realtor B’s model and without additional\n",
    "information, we cannot know which model has a higher bias \\* (c) Realtor\n",
    "A’s model has higher variance and lower bias than Realtor B’s model \\*\n",
    "(d) Realtor A’s model has higher variance and higher bias than Realtor\n",
    "B’s model\n",
    "\n",
    "**4. When $L(w,b) = \\sum_{i=1}^{n}(y_i - (w^T x_i + b))^2$ is used as a\n",
    "loss function to train a model, which of the following is true?** \\* (a)\n",
    "It minimizes the sum of the absolute differences between observed and\n",
    "predicted values. \\* (b) It maximizes the correlation coefficient\n",
    "between the independent and dependent variables. \\* (c) It requires the\n",
    "use of gradient descent optimization to find the best-fit line. \\* (d)\n",
    "It minimizes the sum of the squared difference between observed and\n",
    "predicted values.\n",
    "\n",
    "**5. True/False: As the value of the regularization term coefficient in\n",
    "Ridge Regression increases, the sensitivity of predictions to inputs\n",
    "decreases.** \\* (a) True \\* (b) False\n",
    "\n",
    "**6. Which of the following statements about logistic regression is\n",
    "true?** \\* (a) The loss function of logistic regression without\n",
    "regularization is convex, and the loss function of logistic regression\n",
    "with L2 regularization is convex. \\* (b) Neither the loss function of\n",
    "logistic regression without regularization is convex nor the loss\n",
    "function of logistic regression with L2 regularization is convex. \\* (c)\n",
    "The loss function of logistic regression without regularization is\n",
    "convex, but the loss function of logistic regression with L2\n",
    "regularization is non-convex. \\* (d) The loss function of logistic\n",
    "regression without regularization is non-convex, but the loss function\n",
    "of logistic regression with L2 regularization is convex.\n",
    "\n",
    "**7. Which of the following is NOT an assumption of logistic\n",
    "regression?** \\* (a) The output target is binary. \\* (b) The input\n",
    "features can be continuous or categorical. \\* (c) The residual errors\n",
    "are normally distributed.\n",
    "\n",
    "**8. Suppose we’ve split a dataset into train, validation, and test\n",
    "sets; trained a regression model on the train set; and found the optimal\n",
    "value for a regularization constant $\\lambda$. Select all of the\n",
    "regression methods for which adding the validation set into the train\n",
    "set and retraining can change the optimal value for $\\lambda$.** \\* (a)\n",
    "LASSO regression \\* (b) Ridge regression\n",
    "\n",
    "**9. Suppose that we want to estimate the ideal parameter $\\theta^*$ for\n",
    "$p(x, y, \\theta)$ given a set of observations $\\{x_i, y_i\\}$. Which of\n",
    "the following is a key assumption made when using\n",
    "$\\hat{\\theta}_{MLE} = \\arg \\max_{\\theta} \\sum_i \\log(p(x_i, y_i|\\theta_i))$\n",
    "for Maximum Likelihood Estimation (MLE) to estimate the model\n",
    "parameter?** \\* (a) The data is normally distributed. \\* (b) The data is\n",
    "independent and identically distributed (i.i.d.). \\* (c) The data\n",
    "contains no outliers. \\* (d) The data is linearly separable.\n",
    "\n",
    "**10. Provide one advantage and one disadvantage of Stochastic Gradient\n",
    "Descent (SGD) over Gradient Descent (GD).**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**11. Assume a simple linear model $Y = \\beta_1 X$. For simplicity, no\n",
    "intercept is considered. Given the following dataset:**\n",
    "\n",
    "$X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$\n",
    "\n",
    "$Y = \\begin{pmatrix} 3 \\\\ 5 \\\\ 7 \\end{pmatrix}$\n",
    "\n",
    "**(a) (1 point) Compute the least squares estimate of $\\beta_1$ without\n",
    "any regularization. You may leave your answer as a fraction, if\n",
    "necessary.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**(b) (1 point) Using Lasso Regression (equation 11) with a penalty term\n",
    "$\\alpha = 2$, would $\\beta_1$ increase or decrease? Provide a short\n",
    "explanation.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**12. Suppose you’re given a scatter plot of a dataset, and the pattern\n",
    "appears to be a periodic wave-like curve that repeats itself at regular\n",
    "intervals.**\n",
    "\n",
    "<img src=\"./scatter_plot.png\" width=\"450px\">\n",
    "\n",
    "**Which of the following basis functions might be most appropriate to\n",
    "capture the relationship between $x$ and $y$ for this dataset?**\n",
    "\n",
    "- 1.  Polynomial basis functions: $\\phi(x) = \\{1, x, x^2, x^3, ...\\}$\n",
    "- 1.  Radial basis functions: $\\phi(x) = \\exp(-\\lambda||x - c||^2)$\n",
    "- 1.  Fourier basis functions:\n",
    "      $\\phi(x) = \\{1, \\sin(\\omega x), \\cos(\\omega x), \\sin(2\\omega x), \\cos(2\\omega x), ...\\}$\n",
    "- 1.  Logarithmic basis function: $\\phi(x) = \\log(x)$\n",
    "- 1.  Exponential basis function: $\\phi(x) = \\exp(\\lambda x)$\n",
    "\n",
    "**13. Which of the following statements about convexity is true?**\n",
    "\n",
    "- 1.  If $f(x)$ is convex, then $g(x) = \\frac{1}{3}f(x)$ is also convex\n",
    "- 1.  If $f(x)$ is convex, then gradient descent on minimizing $f(x)$\n",
    "      will always reach global minimum\n",
    "- 1.  If $f(x)$ is convex, then $f(x)$ is everywhere differentiable\n",
    "\n",
    "**14. What are the unbiased maximum likelihood estimates (MLE) for the\n",
    "parameters $(\\mu, \\sigma)$ of a univariate Gaussian distribution, given\n",
    "a dataset of $n$ independently sampled 1-dimensional data points\n",
    "$X = \\{x_1, ..., x_n\\}$ and the sample mean $\\bar{x}$?**\n",
    "\n",
    "- 1.  $\\hat{\\mu}_{MLE} = \\bar{x}$,\n",
    "      $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n} \\sum_{i=1}^n x_i$\n",
    "- 1.  $\\hat{\\mu}_{MLE} = \\bar{x}$,\n",
    "      $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu}_{MLE})^2$\n",
    "- 1.  $\\hat{\\mu}_{MLE} = \\bar{x}$,\n",
    "      $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\hat{\\mu}_{MLE})^2$\n",
    "- 1.  $\\hat{\\mu}_{MLE} = \\frac{1}{n} \\bar{x}$,\n",
    "      $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\hat{\\mu}_{MLE})^2$\n",
    "\n",
    "**15. True/False: When performing gradient descent, decreasing the\n",
    "learning rate enough will slow down convergence but will eventually\n",
    "guarantee you arrive at the global minimum.**\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**16. Which of the following functions is strictly convex over its\n",
    "entire domain?**\n",
    "\n",
    "- 1.  $f(x) = -x^2$\n",
    "- 1.  $f(x) = x^3$\n",
    "- 1.  $f(x) = \\ln(x)$\n",
    "- 1.  $f(x) = e^x$\n",
    "\n",
    "**17. Which of the following is true about a validation set and how it\n",
    "is used?**\n",
    "\n",
    "- 1.  The validation set allows us to estimate how a model would perform\n",
    "      on unseen data\n",
    "- 1.  When deciding to use a validation set, you do not need a separate\n",
    "      test set\n",
    "- 1.  After hyperparameter tuning, the validation set is always added\n",
    "      back into the training set before training the final model\n",
    "- 1.  The validation set allows us to train a model quicker by\n",
    "      decreasing the size of our training data set\n",
    "\n",
    "**18. (2 points) Suppose we have the function**\n",
    "\n",
    "$$f(x) = \\begin{cases} 1 - e^{-\\frac{1}{x^2}} & x \\neq 0 \\\\ 1 & x = 0 \\end{cases}$$\n",
    "\n",
    "<img src=\"./function.png\" width=\"450px\">\n",
    "\n",
    "**(a) (1 point) Suppose that we perform gradient descent starting at\n",
    "$x_0 = 0$ with step size $\\eta = 1$. What is the asymptotic behavior of\n",
    "gradient descent given by Equation 12?**\n",
    "\n",
    "$$x_{n+1} = x_n - \\eta f'(x_n) \\quad (12)$$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**(b) (1 point) Now suppose that $x_0 \\sim \\mathcal{N}(0, \\epsilon)$ for\n",
    "some small $\\epsilon$. What is the behavior then?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**19. A bag contains 4 red balls and 3 green balls. We draw 3 balls from\n",
    "the bag without replacement. What is the probability that all 3 balls\n",
    "are red? Express your result as a fraction, or as a percentage rounded\n",
    "to the integer percentage (e.g. 77%).**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**20. True/False: For a matrix $X \\in \\mathbb{R}^{n \\times d}$ of rank\n",
    "$d$, there exists an orthogonal matrix $V$ and diagonal matrix $D$ such\n",
    "that $X^T X = VDV^T$.**\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**21. You have built a spam detection classifier to help you clean up\n",
    "your email inbox. Your system has uncovered that 90% of all spam emails\n",
    "contain the word “discount”. If you assume that the overall probability\n",
    "of an email being spam is 5% and 15% of all incoming emails contain the\n",
    "word “discount”, what is the probability that an email containing\n",
    "“discount” is actually spam?**\n",
    "\n",
    "- 1.  0.9\n",
    "- 1.  0.135\n",
    "- 1.  0.3\n",
    "- 1.  0.045\n",
    "\n",
    "**22. Determine if the following statements about k-fold\n",
    "cross-validation are true or false:**\n",
    "\n",
    "**Statement (1):** For large datasets with $n$ samples, it is\n",
    "recommended to use k-fold cross-validation with a value of $k$ that is\n",
    "close to $n$.\n",
    "\n",
    "**Statement (2):** In k-fold cross-validation, a larger value of $k$\n",
    "results in a more computationally efficient process, as it requires\n",
    "fewer model training.\n",
    "\n",
    "- 1.  Both statements are True\n",
    "- 1.  Both statements are False\n",
    "- 1.  Statement (1) is True, Statement (2) is False\n",
    "- 1.  Statement (1) is False, Statement (2) is True\n",
    "\n",
    "**23. What is the primary encouragement of the L1 regularization term in\n",
    "Lasso regression?**\n",
    "\n",
    "- 1.  Encourages the model to fit the training data more closely.\n",
    "- 1.  Encourages the model to have large coefficients for all features.\n",
    "- 1.  Encourages the model to have small but non-zero coefficients for\n",
    "      all features.\n",
    "- 1.  Encourages sparsity by driving some feature coefficients to zero.\n",
    "\n",
    "**24. Consider fitting a linear model by minimizing the least squares\n",
    "objective $\\hat{w} = \\arg \\min_w \\sum_{i=1}^n (y_i - x_i^T w)^2$ for a\n",
    "training dataset of i.i.d. input-output pairs $\\{(x_i, y_i)\\}_{i=1}^n$.\n",
    "Which of the following statements about this objective is true?**\n",
    "\n",
    "- 1.  The least squares objective is equivalent to maximizing the\n",
    "      likelihood function of the observed data assuming Gaussian noise.\n",
    "- 1.  The least squares objective is equivalent to minimizing the\n",
    "      likelihood function of the observed data assuming Gaussian noise.\n",
    "- 1.  The least squares objective is equivalent to maximizing the\n",
    "      likelihood function of the observed data assuming Laplace noise.\n",
    "- 1.  The least squares objective is equivalent to minimizing the\n",
    "      likelihood function of the observed data assuming Laplace noise.\n",
    "\n",
    "**25. Consider a matrix $A \\in \\mathbb{R}^{n \\times n}$ that is\n",
    "symmetric and has orthonormal columns. Which of the following statements\n",
    "is true?**\n",
    "\n",
    "- 1.  All eigenvalues of $A$ are real.\n",
    "- 1.  At least one eigenvalue of $A$ is complex.\n",
    "- 1.  All eigenvalues of $A$ are either 0 or 1.\n",
    "- 1.  The eigenvalues of $A$ cannot be determined from the given\n",
    "      information.\n",
    "\n",
    "**26. Consider the closed form of the optimal weight for Ridge\n",
    "Regression, as derived in a previous homework (HW1):**\n",
    "\n",
    "$$\\hat{W} = (X^T X + \\lambda I)^{-1} X^T Y$$\n",
    "\n",
    "**where $X = [x_1 \\cdots x_n]^T \\in \\mathbb{R}^{n \\times d}$ and\n",
    "$Y = [y_1 \\cdots y_n]^T \\in \\mathbb{R}^{n \\times k}$.**\n",
    "\n",
    "**Show that when $\\lambda > 0$, the matrix $X^T X + \\lambda I$ is\n",
    "invertible.**"
   ],
   "id": "5d3d88d4-5054-4546-ab4d-e6387a4775b1"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
