{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe3ec87-6dff-40e4-b055-4d2935295c65",
   "metadata": {},
   "source": [
    "# Practice 5 Solutions\n",
    "\n",
    "**1. If $X$ and $Y$ are independent random variables, which of the following are true?**\n",
    "*   (a) $\\text{Cov}(X, Y) = 0$\n",
    "*   (b) $E[XY] = E[X]E[Y]$\n",
    "*   (c) $\\text{Var}(XY) = \\text{Var}(X)\\text{Var}(Y)$\n",
    "*   (d) $P(X,Y) = P(Y|X)P(X|Y)$\n",
    "\n",
    "**Correct answers:** (a), (b), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Options (a), (b), and (d) are true** for independent random variables.\n",
    "\n",
    "**Why (a) is true - Covariance is zero:**\n",
    "\n",
    "**1. Definition of independence:**\n",
    "- **X and Y are independent** if P(X,Y) = P(X)P(Y)\n",
    "- **No relationship** between X and Y\n",
    "\n",
    "**2. Covariance definition:**\n",
    "\n",
    "$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$\n",
    "\n",
    "**3. For independent variables:**\n",
    "- **$E[XY] = E[X]E[Y]$** (from option b)\n",
    "- **Therefore:** $\\text{Cov}(X,Y) = E[X]E[Y] - E[X]E[Y] = 0$\n",
    "\n",
    "**Why (b) is true - Expectation of product:**\n",
    "\n",
    "**1. Independence property:**\n",
    "- **E[XY] = E[X]E[Y]** is a fundamental property of independence\n",
    "- **No correlation** means no linear relationship\n",
    "\n",
    "**2. Intuitive explanation:**\n",
    "- **Independent variables** don't influence each other\n",
    "- **Product expectation** factors into individual expectations\n",
    "\n",
    "**Why (c) is false - Variance of product:**\n",
    "\n",
    "**1. Correct formula:**\n",
    "\n",
    "$\\text{Var}(XY) = E[X^2]E[Y^2] - (E[X]E[Y])^2$\n",
    "\n",
    "**2. This is NOT equal to:**\n",
    "\n",
    "$\\text{Var}(X)\\text{Var}(Y) = (E[X^2] - E[X]^2)(E[Y^2] - E[Y]^2)$\n",
    "\n",
    "**3. Example:**\n",
    "- **X, Y ~ N(0,1)** independent\n",
    "- **Var(XY) = 1** (not Var(X)Var(Y) = 1×1 = 1, but this is a special case)\n",
    "\n",
    "**Why (d) is true - Joint probability:**\n",
    "\n",
    "**1. For independent variables:**\n",
    "\n",
    "$P(X,Y) = P(X)P(Y)$\n",
    "\n",
    "**2. Also:**\n",
    "\n",
    "$P(Y|X) = P(Y)$ and $P(X|Y) = P(X)$\n",
    "\n",
    "**3. Therefore:**\n",
    "\n",
    "$P(X,Y) = P(Y|X)P(X|Y) = P(Y)P(X)$ ✓\n",
    "\n",
    "**Key insight:** **Independence** implies **no linear relationship** (zero covariance) and **factorization** of expectations and probabilities.\n",
    "\n",
    "**2. A certain disease affects 2% of the population. A diagnostic test for this disease has the following characteristics:**\n",
    "*   **Sensitivity (True Positive Rate):** If a person has the disease, the test returns a positive result with probability 0.90.\n",
    "*   **False Positive Rate:** If a person does not have the disease, the test returns a positive result with probability 0.10.\n",
    "\n",
    "**If a randomly selected person tests positive, what is the probability that they actually have the disease?**\n",
    "*   (a) $\\frac{11}{58}$\n",
    "*   (b) $\\frac{9}{58}$\n",
    "*   (c) $\\frac{9}{50}$\n",
    "*   (d) $\\frac{49}{58}$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This is a classic Bayes' theorem problem** - finding the probability of having the disease given a positive test result.\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**1. Define events:**\n",
    "- **D** = \"Person has the disease\" (P(D) = 0.02)\n",
    "- **T** = \"Test result is positive\"\n",
    "- **D^c** = \"Person does not have the disease\" (P(D^c) = 0.98)\n",
    "\n",
    "**2. Given information:**\n",
    "- **Sensitivity:** P(T|D) = 0.90 (90% of diseased people test positive)\n",
    "- **False Positive Rate:** P(T|D^c) = 0.10 (10% of healthy people test positive)\n",
    "\n",
    "**3. Apply Bayes' theorem:**\n",
    "\n",
    "$P(D|T) = \\frac{P(T|D)P(D)}{P(T)}$\n",
    "\n",
    "**4. Calculate P(T) using law of total probability:**\n",
    "\n",
    "$P(T) = P(T|D)P(D) + P(T|D^c)P(D^c)$\n",
    "\n",
    "$= (0.90)(0.02) + (0.10)(0.98)$\n",
    "\n",
    "$= 0.018 + 0.098$\n",
    "\n",
    "$= 0.116$\n",
    "\n",
    "**5. Substitute into Bayes' theorem:**\n",
    "\n",
    "$P(D|T) = \\frac{(0.90)(0.02)}{0.116}$\n",
    "\n",
    "$= \\frac{0.018}{0.116}$\n",
    "\n",
    "$= \\frac{18}{116}$\n",
    "\n",
    "$= \\frac{9}{58}$\n",
    "\n",
    "**6. Intuitive interpretation:**\n",
    "- **Only 9/58 ≈ 15.5%** of positive test results indicate actual disease\n",
    "- **Low prevalence** (2%) combined with **high false positive rate** (10%) leads to many false alarms\n",
    "- **Most positive tests** are false positives due to the large healthy population\n",
    "\n",
    "**7. Why other options are incorrect:**\n",
    "- **(a) 11/58:** Incorrect calculation\n",
    "- **(c) 9/50:** Wrong denominator\n",
    "- **(d) 49/58:** Much too high\n",
    "\n",
    "**Key insight:** **Low disease prevalence** combined with **imperfect test accuracy** means most positive results are **false positives**.\n",
    "\n",
    "**3.**\n",
    "\n",
    "**The probability mass function of a geometric distribution with unknown parameter $0 < p \\le 1$ is**\n",
    "$$P(X=k|p) = (1-p)^{k-1}p$$\n",
    "**where $k = 1,2,3,....$ The interpretation of $X$ is that it is the number of independent Bernoulli trials needed to get one success, if each trial has success probability $p$.**\n",
    "\n",
    "**Given a set of $n$ observations $\\{x_1, x_2,..., x_n\\}$ from a geometric distribution, derive the Maximum Likelihood Estimate (MLE) $\\hat{p}_{MLE}$ for the parameter $p$.**\n",
    "\n",
    "**Hint: don't forget about the chain rule: for $h(x) = f(g(x))$, $h'(x) = f'(g(x))g'(x)$.**\n",
    "\n",
    "**Answer:** $\\hat{p} = \\frac{n}{\\sum_{i=1}^{n} x_i}$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The MLE for the geometric distribution parameter p is the reciprocal of the sample mean.**\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Likelihood function:**\n",
    "\n",
    "$L_n(p) = \\prod_{i=1}^{n} P(X=x_i|p) = \\prod_{i=1}^{n} (1-p)^{x_i-1}p$\n",
    "\n",
    "**2. Log-likelihood function:**\n",
    "\n",
    "$\\log L_n(p) = \\sum_{i=1}^{n} \\log((1-p)^{x_i-1}p)$\n",
    "\n",
    "$= \\sum_{i=1}^{n} [(x_i-1)\\log(1-p) + \\log(p)]$\n",
    "\n",
    "$= \\sum_{i=1}^{n} (x_i-1)\\log(1-p) + \\sum_{i=1}^{n} \\log(p)$\n",
    "\n",
    "$= (\\sum_{i=1}^{n} x_i - n)\\log(1-p) + n \\log(p)$\n",
    "\n",
    "**3. Take derivative with respect to p:**\n",
    "\n",
    "$\\frac{d}{dp}[\\log L_n(p)] = -\\frac{\\sum_{i=1}^{n} x_i - n}{1-p} + \\frac{n}{p}$\n",
    "\n",
    "**4. Set derivative to zero:**\n",
    "\n",
    "$0 = -\\frac{\\sum_{i=1}^{n} x_i - n}{1-p} + \\frac{n}{p}$\n",
    "\n",
    "**5. Solve for p:**\n",
    "\n",
    "$\\frac{n}{p} = \\frac{\\sum_{i=1}^{n} x_i - n}{1-p}$\n",
    "\n",
    "$n(1-p) = p(\\sum_{i=1}^{n} x_i - n)$\n",
    "\n",
    "$n - np = p\\sum_{i=1}^{n} x_i - np$\n",
    "\n",
    "$n = p\\sum_{i=1}^{n} x_i$\n",
    "\n",
    "$p = \\frac{n}{\\sum_{i=1}^{n} x_i}$\n",
    "\n",
    "**6. Verification:**\n",
    "- **Second derivative:** $\\frac{d^2}{dp^2}[\\log L_n(p)] < 0$ for $0 < p < 1$\n",
    "- **Maximum** confirmed at $p = \\frac{n}{\\sum_{i=1}^{n} x_i}$\n",
    "\n",
    "**7. Interpretation:**\n",
    "- **p = n/Σᵢ₌₁ⁿ xᵢ** is the **reciprocal of the sample mean**\n",
    "- **Geometric distribution** models number of trials until first success\n",
    "- **MLE** estimates success probability as reciprocal of average trials needed\n",
    "\n",
    "**Key insight:** **MLE for geometric distribution** is the **reciprocal of the sample mean**, similar to the exponential distribution.\n",
    "\n",
    "**4. Select All That Apply**\n",
    "\n",
    "**Which of the following is true about maximum likelihood estimation, in general?**\n",
    "*   (a) It always produces unbiased parameter estimates.\n",
    "*   (b) It can be used for continuous probability distributions.\n",
    "*   (c) It can be used for discrete probability distributions.\n",
    "*   (d) It maximizes the likelihood of the data given the model parameters.\n",
    "*   (e) It maximizes the likelihood of the model parameters given the data.\n",
    "\n",
    "**Correct answers:** (b), (c), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Options (b), (c), and (d) are true** about maximum likelihood estimation.\n",
    "\n",
    "**Why (a) is false - MLE is not always unbiased:**\n",
    "\n",
    "**1. Counterexample - Sample variance:**\n",
    "- **MLE for variance:** $\\sigma^2_{\\text{MLE}} = \\frac{1}{n}\\sum(x_i - \\mu)^2$\n",
    "- **Unbiased estimator:** $\\sigma^2_{\\text{unbiased}} = \\frac{1}{n-1}\\sum(x_i - \\mu)^2$\n",
    "- **MLE is biased** - underestimates true variance\n",
    "\n",
    "**2. Other examples:**\n",
    "- **MLE for normal mean:** unbiased\n",
    "- **MLE for exponential rate:** unbiased\n",
    "- **MLE for variance:** biased (Bessel's correction needed)\n",
    "\n",
    "**Why (b) and (c) are true - Applicable to all distributions:**\n",
    "\n",
    "**1. Continuous distributions:**\n",
    "- **Normal distribution** (linear regression)\n",
    "- **Exponential distribution**\n",
    "- **Uniform distribution**\n",
    "- **Any continuous PDF**\n",
    "\n",
    "**2. Discrete distributions:**\n",
    "- **Bernoulli distribution** (logistic regression)\n",
    "- **Poisson distribution**\n",
    "- **Geometric distribution**\n",
    "- **Any discrete PMF**\n",
    "\n",
    "**Why (d) is true - Correct interpretation:**\n",
    "\n",
    "**1. MLE objective:**\n",
    "\n",
    "$\\max L(\\theta) = \\max P(\\text{data}|\\theta)$\n",
    "\n",
    "- **$\\theta$** = model parameters\n",
    "- **data** = observed data\n",
    "- **$L(\\theta)$** = likelihood function\n",
    "\n",
    "**2. Frequentist framework:**\n",
    "- **Parameters are fixed** (not random)\n",
    "- **Data is random**\n",
    "- **We maximize P(data|θ)**, not P(θ|data)\n",
    "\n",
    "**Why (e) is false - Wrong interpretation:**\n",
    "\n",
    "**1. P(θ|data) is Bayesian:**\n",
    "- **Requires prior P(θ)**\n",
    "- **Posterior P(θ|data) = P(data|θ)P(θ)/P(data)**\n",
    "- **MLE doesn't use priors**\n",
    "\n",
    "**2. MLE vs MAP:**\n",
    "- **MLE:** max P(data|θ) (frequentist)\n",
    "- **MAP:** max P(θ|data) (Bayesian)\n",
    "\n",
    "**Key insight:** **MLE** is a **frequentist method** that maximizes **data likelihood** and works for **any distribution type**.\n",
    "\n",
    "**5. Select All That Apply**\n",
    "\n",
    "**Suppose $A \\in \\mathbb{R}^{n \\times n}$ is a positive semi-definite (PSD) matrix. Which of the following is always true about $A$?**\n",
    "*   (a) All eigenvalues of $A$ are non-negative.\n",
    "*   (b) All elements of $A$ are non-negative.\n",
    "*   (c) $A$ is invertible.\n",
    "*   (d) $x^T A x \\leq 0$ for all $x$.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Only option (a) is always true** for positive semi-definite matrices.\n",
    "\n",
    "**Why (a) is true - Non-negative eigenvalues:**\n",
    "\n",
    "**1. Definition of PSD:**\n",
    "- **A is PSD** if $x^T A x \\geq 0$ for all $x \\in \\mathbb{R}^n$\n",
    "- **Equivalent condition:** All eigenvalues of A are $\\geq 0$\n",
    "\n",
    "**2. Spectral theorem:**\n",
    "- **Symmetric matrices** have real eigenvalues\n",
    "- **PSD matrices** have non-negative eigenvalues\n",
    "- **Eigendecomposition:** $A = Q\\Lambda Q^T$ where $\\Lambda \\geq 0$\n",
    "\n",
    "**3. Mathematical proof:**\n",
    "\n",
    "For eigenvector $v$ with eigenvalue $\\lambda$:\n",
    "\n",
    "$Av = \\lambda v$\n",
    "\n",
    "$v^T Av = \\lambda v^T v = \\lambda\\|v\\|^2 \\geq 0$\n",
    "\n",
    "Since $\\|v\\|^2 > 0$, we must have $\\lambda \\geq 0$\n",
    "\n",
    "**Why other options are false:**\n",
    "\n",
    "**Option (b): All elements non-negative**\n",
    "- **PSD** is about quadratic forms, not individual elements\n",
    "- **Counterexample:** A = [2 -1; -1 2] is PSD but has negative elements\n",
    "- **Individual elements** can be negative\n",
    "\n",
    "**Option (c): A is invertible**\n",
    "- **PSD** allows zero eigenvalues\n",
    "- **Zero eigenvalue** → singular matrix → not invertible\n",
    "- **Example:** A = [1 0; 0 0] is PSD but not invertible\n",
    "\n",
    "**Option (d): x^T A x ≤ 0**\n",
    "- **Wrong direction** - should be ≥ 0\n",
    "- **This would be** negative semi-definite\n",
    "- **PSD** means x^T A x ≥ 0 for all x\n",
    "\n",
    "**4. Examples of PSD matrices:**\n",
    "- **X^T X** (Gram matrix)\n",
    "- **Covariance matrices**\n",
    "- **Correlation matrices**\n",
    "- **Identity matrix**\n",
    "\n",
    "**Key insight:** **PSD matrices** have **non-negative eigenvalues** and satisfy **x^T A x ≥ 0** for all vectors x.\n",
    "\n",
    "**6.**\n",
    "\n",
    "**Assume we have $X \\in \\mathbb{R}^{n \\times p}$ representing $n$ data points with $p$ features each and $Y \\in \\mathbb{R}^n$ representing the corresponding outcomes. Using linear regression with no offset/intercept, provide an expression to predict the outcome for a new data point $x_{\\text{new}} \\in \\mathbb{R}^p$ in terms of $X$ and $Y$.**\n",
    "\n",
    "**Answer:** $\\hat{y}_{\\text{new}} = x_{\\text{new}}^T (X^T X)^{-1} X^T Y$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This is the prediction formula for linear regression without intercept** using the normal equations solution.\n",
    "\n",
    "**Step-by-step derivation:**\n",
    "\n",
    "**1. Linear regression model (no intercept):**\n",
    "\n",
    "$y = Xw + \\varepsilon$\n",
    "\n",
    "where $w \\in \\mathbb{R}^p$ is the weight vector\n",
    "\n",
    "**2. Normal equations solution:**\n",
    "\n",
    "$\\hat{w} = (X^T X)^{-1} X^T Y$\n",
    "\n",
    "**3. Prediction for new data point:**\n",
    "\n",
    "$\\hat{y}_{\\text{new}} = x_{\\text{new}}^T \\hat{w}$\n",
    "\n",
    "$= x_{\\text{new}}^T (X^T X)^{-1} X^T Y$\n",
    "\n",
    "**4. Why this works:**\n",
    "\n",
    "**Training phase:**\n",
    "- **Minimize:** $\\|Y - Xw\\|^2$\n",
    "- **Solution:** $\\hat{w} = (X^T X)^{-1} X^T Y$\n",
    "- **Assumes:** $X^T X$ is invertible (full rank)\n",
    "\n",
    "**Prediction phase:**\n",
    "- **New input:** $x_{\\text{new}} \\in \\mathbb{R}^p$\n",
    "- **Prediction:** $\\hat{y}_{\\text{new}} = x_{\\text{new}}^T \\hat{w}$\n",
    "- **Substitute:** $\\hat{w}$ from training\n",
    "\n",
    "**5. Comparison with intercept model:**\n",
    "- **With intercept:** $\\hat{y}_{\\text{new}} = [x_{\\text{new}}^T, 1] \\times [\\hat{w}^T, b]^T$\n",
    "- **Without intercept:** $\\hat{y}_{\\text{new}} = x_{\\text{new}}^T \\hat{w}$\n",
    "- **No bias term** $b$ in the model\n",
    "\n",
    "**6. Matrix dimensions:**\n",
    "- **X:** $n \\times p$\n",
    "- **Y:** $n \\times 1$\n",
    "- **$x_{\\text{new}}$:** $p \\times 1$\n",
    "- **$\\hat{w}$:** $p \\times 1$\n",
    "- **$\\hat{y}_{\\text{new}}$:** scalar\n",
    "\n",
    "**Key insight:** **Prediction** is the **dot product** of the new feature vector with the **learned weight vector**.\n",
    "\n",
    "**7.**\n",
    "\n",
    "**Suppose you want to use linear regression to fit a weight vector $w \\in \\mathbb{R}^d$ and an offset/intercept term $b \\in \\mathbb{R}$ using data points $x_i \\in \\mathbb{R}^d$. What is the minimum number of data points $n$ required in your training set such that there will be a single unique solution?**\n",
    "\n",
    "**Answer:** $n = d+1$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The minimum number of data points needed is n = d+1** to ensure a unique solution for linear regression with intercept.\n",
    "\n",
    "**Step-by-step reasoning:**\n",
    "\n",
    "**1. Linear regression model with intercept:**\n",
    "\n",
    "$y_i = x_i^T w + b + \\varepsilon_i$\n",
    "\n",
    "**2. Augmented data matrix:**\n",
    "\n",
    "$X_{\\text{aug}} = [X, 1] \\in \\mathbb{R}^{n \\times (d+1)}$\n",
    "\n",
    "where 1 is a column of ones\n",
    "\n",
    "**3. Normal equations:**\n",
    "\n",
    "$\\hat{w}_{\\text{aug}} = (X_{\\text{aug}}^T X_{\\text{aug}})^{-1} X_{\\text{aug}}^T Y$\n",
    "\n",
    "**4. Rank requirement for unique solution:**\n",
    "- **$X_{\\text{aug}}^T X_{\\text{aug}}$** must be **invertible**\n",
    "- **Invertible** requires **full rank**\n",
    "- **Full rank** requires **$n \\geq d+1$**\n",
    "\n",
    "**5. Why n = d+1 is minimum:**\n",
    "\n",
    "**If $n < d+1$:**\n",
    "- **$X_{\\text{aug}}$** has more columns than rows\n",
    "- **$\\text{rank}(X_{\\text{aug}}) \\leq n < d+1$**\n",
    "- **$X_{\\text{aug}}^T X_{\\text{aug}}$** is singular\n",
    "- **No unique solution**\n",
    "\n",
    "**If $n = d+1$:**\n",
    "- **$X_{\\text{aug}}$** is square $(d+1) \\times (d+1)$\n",
    "- **Full rank** possible (if data is well-conditioned)\n",
    "- **Unique solution** exists\n",
    "\n",
    "**6. Geometric interpretation:**\n",
    "- **d+1 points** in d-dimensional space\n",
    "- **Can fit** a unique hyperplane through these points\n",
    "- **Fewer points** → infinite solutions\n",
    "- **More points** → overdetermined system\n",
    "\n",
    "**7. Example:**\n",
    "- **d = 2 features**\n",
    "- **n = 3 data points** minimum\n",
    "- **Fits unique plane** through 3 points in 3D space\n",
    "\n",
    "**Key insight:** **n = d+1** ensures the **augmented data matrix** has **full rank** for a **unique solution**.\n",
    "\n",
    "**8. One Answer**\n",
    "\n",
    "**In a regression model, what is the primary purpose of using general basis functions?**\n",
    "*   (a) Transform nonlinear relationships between features and the target variable into a linear form.\n",
    "*   (b) Regularize the model to prevent overfitting.\n",
    "*   (c) Reduce the number of data samples needed for model training.\n",
    "*   (d) Simplify the model by reducing the number of features.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (a) is correct** - basis functions transform nonlinear relationships into linear form.\n",
    "\n",
    "**Why (a) is correct - Nonlinear to linear transformation:**\n",
    "\n",
    "**1. Primary purpose:**\n",
    "- **Capture nonlinear patterns** in data\n",
    "- **Transform features** into higher-dimensional space\n",
    "- **Enable linear modeling** of nonlinear relationships\n",
    "- **Maintain interpretability** of linear models\n",
    "\n",
    "**2. Mathematical approach:**\n",
    "\n",
    "Original: $y = f(x)$ (nonlinear)\n",
    "\n",
    "Basis expansion: $y = w_1\\phi_1(x) + w_2\\phi_2(x) + \\cdots + w_k\\phi_k(x)$\n",
    "\n",
    "where $\\phi_i(x)$ are basis functions\n",
    "\n",
    "**3. Examples of basis functions:**\n",
    "- **Polynomial:** φ(x) = [1, x, x², x³, ...]\n",
    "- **Radial:** φ(x) = exp(-||x-c||²/2σ²)\n",
    "- **Fourier:** φ(x) = [sin(x), cos(x), sin(2x), ...]\n",
    "- **Spline:** Piecewise polynomial functions\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Regularization**\n",
    "- **Basis functions** don't regularize by themselves\n",
    "- **Regularization** is separate (L1/L2 penalties)\n",
    "- **Basis functions** can actually increase overfitting risk\n",
    "\n",
    "**Option (c): Reduce data requirements**\n",
    "- **More complex models** typically need more data\n",
    "- **Higher-dimensional space** requires more samples\n",
    "- **Curse of dimensionality** effect\n",
    "\n",
    "**Option (d): Simplify model**\n",
    "- **Basis functions** increase feature count\n",
    "- **More parameters** to estimate\n",
    "- **Higher complexity**, not lower\n",
    "\n",
    "**5. Practical benefits:**\n",
    "- **Flexible modeling** of complex patterns\n",
    "- **Linear optimization** techniques still applicable\n",
    "- **Feature engineering** for domain knowledge\n",
    "- **Kernel methods** foundation\n",
    "\n",
    "**6. Trade-offs:**\n",
    "- **Increased complexity** (more features)\n",
    "- **Higher computational cost**\n",
    "- **Risk of overfitting**\n",
    "- **Need for regularization**\n",
    "\n",
    "**Key insight:** **Basis functions** enable **linear models** to capture **nonlinear patterns** through **feature transformation**.\n",
    "\n",
    "**9. One Answer**\n",
    "\n",
    "**In regression, when our prediction model is linear-Gaussian, i.e., $y_i \\sim N(x_i^T w, \\sigma^2)$ for target output $y_i \\in \\mathbb{R}$ and feature vectors $x_i \\in \\mathbb{R}^d$, finding the $w$ that maximizes the data likelihood is equivalent to minimizing the average absolute difference between the target output and predicted output.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - MLE for linear-Gaussian regression minimizes squared differences, not absolute differences.\n",
    "\n",
    "**Why this is false:**\n",
    "\n",
    "**1. Linear-Gaussian model:**\n",
    "\n",
    "$y_i \\sim N(x_i^T w, \\sigma^2)$\n",
    "\n",
    "**2. Likelihood function:**\n",
    "\n",
    "$L(w) = \\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T w)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "**3. Log-likelihood:**\n",
    "\n",
    "$\\log L(w) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_i (y_i - x_i^T w)^2$\n",
    "\n",
    "**4. MLE objective:**\n",
    "\n",
    "$\\max \\log L(w) = \\min \\sum_i (y_i - x_i^T w)^2$\n",
    "\n",
    "**5. Comparison of loss functions:**\n",
    "\n",
    "**Squared error (MLE for Gaussian):**\n",
    "\n",
    "$L_{\\text{squared}} = \\sum_i (y_i - x_i^T w)^2$\n",
    "\n",
    "**Absolute error (MLE for Laplace):**\n",
    "\n",
    "$L_{\\text{absolute}} = \\sum_i |y_i - x_i^T w|$\n",
    "\n",
    "**6. Why Gaussian noise leads to squared error:**\n",
    "- **Gaussian distribution** has exponential decay with squared distance\n",
    "- **Log-likelihood** contains squared terms\n",
    "- **MLE** naturally leads to squared error minimization\n",
    "\n",
    "**7. Alternative noise distributions:**\n",
    "- **Laplace noise** → absolute error loss\n",
    "- **Gaussian noise** → squared error loss\n",
    "- **Poisson noise** → different loss function\n",
    "\n",
    "**8. Mathematical intuition:**\n",
    "- **Squared error** penalizes large errors more heavily\n",
    "- **Absolute error** penalizes all errors equally\n",
    "- **Gaussian MLE** prefers squared error due to distribution shape\n",
    "\n",
    "**Key insight:** **Gaussian noise assumption** leads to **squared error loss**, not **absolute error loss**.\n",
    "\n",
    "**10. Select All That Apply**\n",
    "\n",
    "**In ridge regression, we obtain $\\hat{w}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$ for $\\lambda \\geq 0$. Which of the following is true?**\n",
    "*   (a) $X^T X$ is always invertible.\n",
    "*   (b) $X^T X + \\lambda I$ is always invertible.\n",
    "*   (c) Increasing $\\lambda$ typically adds bias to the model.\n",
    "*   (d) Increasing $\\lambda$ typically adds variance to the model.\n",
    "*   (e) When $\\lambda = 0$, ridge regression reduces to ordinary (unregularized) linear regression.\n",
    "*   (f) As $\\lambda \\rightarrow \\infty$, $\\hat{w}_{\\text{ridge}} \\rightarrow 0$.\n",
    "\n",
    "**Correct answers:** (c), (e), (f)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Options (c), (e), and (f) are true** about ridge regression.\n",
    "\n",
    "**Why (c) is true - Increasing λ adds bias:**\n",
    "\n",
    "**1. Ridge regression objective:**\n",
    "\n",
    "$\\min \\|y - Xw\\|^2 + \\lambda\\|w\\|^2$\n",
    "\n",
    "**2. Effect of increasing λ:**\n",
    "- **Stronger penalty** on large weights\n",
    "- **Weights shrink** toward zero\n",
    "- **Model becomes less flexible**\n",
    "- **Higher bias, lower variance**\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "- **Large λ** → strong regularization\n",
    "- **Constrained parameters** → less capacity to fit data\n",
    "- **Underfitting risk** → increased bias\n",
    "\n",
    "**Why (e) is true - λ = 0 reduces to OLS:**\n",
    "\n",
    "**1. When λ = 0:**\n",
    "```\n",
    "\\hat{w}_{\\text{ridge}} = (X^T X + 0I)^{-1} X^T y\n",
    "                      = (X^T X)^{-1} X^T y\n",
    "                      = \\hat{w}_{\\text{OLS}}\n",
    "```\n",
    "\n",
    "**2. No regularization:**\n",
    "- **$\\lambda = 0$** means no penalty term\n",
    "- **Objective:** $\\min \\|y - Xw\\|^2$\n",
    "- **Same as** ordinary least squares\n",
    "\n",
    "**Why (f) is true - λ → ∞ shrinks to zero:**\n",
    "\n",
    "**1. As $\\lambda \\to \\infty$:**\n",
    "- **Regularization term** dominates\n",
    "- **Objective:** approximately $\\min \\lambda\\|w\\|^2$\n",
    "- **Solution:** $w \\to 0$\n",
    "\n",
    "**2. Intuitive explanation:**\n",
    "- **Infinite penalty** on non-zero weights\n",
    "- **Optimal solution** is w = 0\n",
    "- **Simplest possible model**\n",
    "\n",
    "**Why other options are false:**\n",
    "\n",
    "**Option (a): X^T X always invertible**\n",
    "- **X^T X** is positive semi-definite\n",
    "- **Can be singular** if X has null space\n",
    "- **Not always invertible**\n",
    "\n",
    "**Option (b): X^T X + λI always invertible**\n",
    "- **True when λ > 0**\n",
    "- **False when λ = 0** (reduces to option a)\n",
    "\n",
    "**Option (d): Increasing λ adds variance**\n",
    "- **Opposite effect** - λ reduces variance\n",
    "- **More regularization** → less sensitivity to data\n",
    "- **Lower variance, higher bias**\n",
    "\n",
    "**Key insight:** **Ridge regression** trades **bias for variance** through **L2 regularization**.\n",
    "\n",
    "**11. One Answer**\n",
    "\n",
    "**You have a dataset with many features. You know a priori that only a small portion of those features are relevant to your prediction problem, but you don't know which are the relevant features. Is it better to use Ridge regression or Lasso regression?**\n",
    "*   (a) Ridge regression\n",
    "*   (b) Lasso regression\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Lasso regression is better** when you know only a small portion of features are relevant.\n",
    "\n",
    "**Why Lasso is preferred:**\n",
    "\n",
    "**1. Sparsity induction:**\n",
    "- **L1 regularization** can set coefficients exactly to zero\n",
    "- **L2 regularization** shrinks coefficients but rarely to exactly zero\n",
    "- **Feature selection** happens automatically with Lasso\n",
    "\n",
    "**2. Mathematical difference:**\n",
    "\n",
    "**Lasso (L1):**\n",
    "\n",
    "$\\min \\|y - Xw\\|^2 + \\lambda\\|w\\|_1$\n",
    "\n",
    "- **$\\|w\\|_1 = \\sum|w_i|$** (L1 norm)\n",
    "- **Sharp corners** at axes\n",
    "- **Can produce exact zeros**\n",
    "\n",
    "**Ridge (L2):**\n",
    "\n",
    "$\\min \\|y - Xw\\|^2 + \\lambda\\|w\\|_2^2$\n",
    "\n",
    "- **$\\|w\\|_2^2 = \\sum w_i^2$** (L2 norm)\n",
    "- **Smooth surface**\n",
    "- **Rarely produces exact zeros**\n",
    "\n",
    "**3. Geometric interpretation:**\n",
    "\n",
    "**L1 constraint region:**\n",
    "- **Diamond-shaped** in 2D\n",
    "- **Sharp corners** touch the axes\n",
    "- **Optimal solution** often at corners (zeros)\n",
    "\n",
    "**L2 constraint region:**\n",
    "- **Circular** in 2D\n",
    "- **Smooth surface**\n",
    "- **Optimal solution** rarely on axes\n",
    "\n",
    "**4. Feature selection capability:**\n",
    "\n",
    "**Lasso:**\n",
    "- **Automatic feature selection**\n",
    "- **Zero coefficients** = irrelevant features\n",
    "- **Non-zero coefficients** = relevant features\n",
    "- **Matches a priori knowledge**\n",
    "\n",
    "**Ridge:**\n",
    "- **No feature selection**\n",
    "- **All features** get non-zero weights\n",
    "- **Assigns meaning** to irrelevant features\n",
    "\n",
    "**5. Practical example:**\n",
    "- **100 features, 10 relevant**\n",
    "- **Lasso:** May select 10-15 features\n",
    "- **Ridge:** Uses all 100 features with small weights\n",
    "\n",
    "**6. When to use each:**\n",
    "- **Lasso:** When you expect sparsity\n",
    "- **Ridge:** When all features might be relevant\n",
    "- **Elastic Net:** When you want both properties\n",
    "\n",
    "**Key insight:** **Lasso's sparsity** makes it ideal for **feature selection** when you know most features are irrelevant.\n",
    "\n",
    "**12. One Answer**\n",
    "\n",
    "**Which of the following best explains the effect of Lasso regression on the bias-variance tradeoff?**\n",
    "*   (a) Lasso regression reduces both bias and variance simultaneously, leading to a more accurate model.\n",
    "*   (b) Lasso regression reduces bias by shrinking coefficients, often at the expense of increasing variance.\n",
    "*   (c) Lasso regression reduces variance by shrinking coefficients and can increase bias, especially when some features are dropped entirely from the learned predictor.\n",
    "*   (d) Lasso regression increases both bias and variance as it enforces sparsity in the learned predictor.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (c) is correct** - Lasso reduces variance and can increase bias through coefficient shrinkage and feature selection.\n",
    "\n",
    "**Why (c) is correct:**\n",
    "\n",
    "**1. Lasso's effect on model complexity:**\n",
    "- **Shrinks coefficients** toward zero\n",
    "- **Sets some coefficients** exactly to zero\n",
    "- **Reduces effective** number of features\n",
    "- **Simplifies the model**\n",
    "\n",
    "**2. Bias-variance tradeoff:**\n",
    "- **Reduced complexity** → **lower variance**\n",
    "- **Reduced complexity** → **higher bias**\n",
    "- **Feature selection** can increase bias if important features are dropped\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "\n",
    "**Variance reduction:**\n",
    "- **Fewer parameters** to estimate\n",
    "- **Less sensitive** to training data noise\n",
    "- **More stable** predictions across datasets\n",
    "- **Lower overfitting** risk\n",
    "\n",
    "**Bias increase:**\n",
    "- **Simpler model** may miss true patterns\n",
    "- **Dropped features** could be important\n",
    "- **Underfitting** risk if regularization is too strong\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Reduces both bias and variance**\n",
    "- **Impossible** - bias and variance typically trade off\n",
    "- **Cannot reduce** both simultaneously\n",
    "- **Violates** fundamental tradeoff principle\n",
    "\n",
    "**Option (b): Reduces bias, increases variance**\n",
    "- **Opposite effect** - Lasso reduces variance\n",
    "- **Simpler models** have lower variance\n",
    "- **Wrong direction** of tradeoff\n",
    "\n",
    "**Option (d): Increases both bias and variance**\n",
    "- **Incorrect** - Lasso reduces variance\n",
    "- **Simpler models** are more stable\n",
    "- **Lower variance** is a key benefit\n",
    "\n",
    "**5. Practical implications:**\n",
    "- **Strong L1 penalty** → high bias, low variance\n",
    "- **Weak L1 penalty** → low bias, high variance\n",
    "- **Optimal λ** balances the tradeoff\n",
    "\n",
    "**6. Comparison with Ridge:**\n",
    "- **Both** reduce variance and increase bias\n",
    "- **Lasso** can set coefficients to zero\n",
    "- **Ridge** only shrinks coefficients\n",
    "\n",
    "**Key insight:** **Lasso** implements the **bias-variance tradeoff** through **L1 regularization** and **feature selection**.\n",
    "\n",
    "**13. One Answer**\n",
    "\n",
    "**In prediction, the total expected prediction error can be decomposed into three components: bias squared, variance, and irreducible error. By optimizing the model complexity and increasing the size of the dataset, it is possible to reduce all three components.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - irreducible error cannot be reduced by any modeling technique.\n",
    "\n",
    "**Why this is false:**\n",
    "\n",
    "**1. Error decomposition:**\n",
    "\n",
    "$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$\n",
    "\n",
    "**2. What can be reduced:**\n",
    "\n",
    "**Bias²:**\n",
    "- **Model complexity** optimization\n",
    "- **Feature engineering**\n",
    "- **Algorithm selection**\n",
    "- **Hyperparameter tuning**\n",
    "\n",
    "**Variance:**\n",
    "- **More training data**\n",
    "- **Regularization**\n",
    "- **Ensemble methods**\n",
    "- **Cross-validation**\n",
    "\n",
    "**3. What cannot be reduced:**\n",
    "\n",
    "**Irreducible Error:**\n",
    "- **Inherent noise** in the data\n",
    "- **Measurement uncertainty**\n",
    "- **Missing information**\n",
    "- **Fundamental randomness**\n",
    "\n",
    "**4. Why irreducible error is irreducible:**\n",
    "\n",
    "**Definition:**\n",
    "- **Fundamental uncertainty** in the data generation process\n",
    "- **Lower bound** on model performance\n",
    "- **Independent** of model choice or data size\n",
    "- **Cannot be eliminated** by any algorithm\n",
    "\n",
    "**5. Examples of irreducible error:**\n",
    "- **Sensor noise** in measurements\n",
    "- **Natural variability** in biological systems\n",
    "- **Unpredictable external factors**\n",
    "- **Quantum uncertainty** in physical systems\n",
    "\n",
    "**6. What optimization can do:**\n",
    "- **Reduce bias** through better model selection\n",
    "- **Reduce variance** through more data/regularization\n",
    "- **Cannot touch** irreducible error\n",
    "\n",
    "**7. Practical implications:**\n",
    "- **Perfect models** still have irreducible error\n",
    "- **Performance limits** exist regardless of data size\n",
    "- **Realistic expectations** about model accuracy\n",
    "\n",
    "**Key insight:** **Irreducible error** represents the **fundamental limit** on prediction accuracy that **cannot be overcome**.\n",
    "\n",
    "**14. One Answer**\n",
    "\n",
    "**Which strategy is most effective for reducing variance in a high-variance, low-bias model?**\n",
    "*   (a) Increasing the number of training examples.\n",
    "*   (b) Increasing the model complexity.\n",
    "*   (c) Decreasing regularization.\n",
    "*   (d) Removing the features that exhibit high variance across training examples.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (a) is correct** - increasing the number of training examples is most effective for reducing variance.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. High-variance, low-bias model characteristics:**\n",
    "- **Complex model** that fits training data well\n",
    "- **Sensitive** to training data changes\n",
    "- **Overfitting** to training data\n",
    "- **Poor generalization** to unseen data\n",
    "\n",
    "**2. How more training data reduces variance:**\n",
    "- **Larger sample size** reduces parameter estimation uncertainty\n",
    "- **More stable** parameter estimates\n",
    "- **Better generalization** to unseen data\n",
    "- **Natural regularization** effect\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "- **Variance** $\\propto \\sigma^2/n$ (for many estimators)\n",
    "- **Larger $n$** → smaller variance\n",
    "- **More data** → more stable estimates\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Increasing model complexity**\n",
    "- **Increases variance** by adding more parameters\n",
    "- **More complex models** are more sensitive to data\n",
    "- **Opposite effect** of what we want\n",
    "\n",
    "**Option (c): Decreasing regularization**\n",
    "- **Increases variance** by allowing more overfitting\n",
    "- **Less constraint** on model parameters\n",
    "- **Higher sensitivity** to training data\n",
    "\n",
    "**Option (d): Removing high-variance features**\n",
    "- **Feature variance** ≠ model variance\n",
    "- **Removing features** could increase or decrease model variance\n",
    "- **No a priori guarantee** of improvement\n",
    "\n",
    "**5. Alternative variance reduction strategies:**\n",
    "- **Regularization** (L1/L2 penalties)\n",
    "- **Ensemble methods** (bagging, random forests)\n",
    "- **Cross-validation** for model selection\n",
    "- **Feature selection** (when appropriate)\n",
    "\n",
    "**6. Practical considerations:**\n",
    "- **Data collection** can be expensive\n",
    "- **Quality** of additional data matters\n",
    "- **Diminishing returns** as n increases\n",
    "- **Balance** with computational cost\n",
    "\n",
    "**7. When to use each strategy:**\n",
    "- **More data:** When available and affordable\n",
    "- **Regularization:** When data is limited\n",
    "- **Ensemble methods:** When individual models are unstable\n",
    "- **Feature selection:** When many irrelevant features exist\n",
    "\n",
    "**Key insight:** **More training data** is the **most effective** way to reduce variance in high-variance models.\n",
    "\n",
    "**15. One Answer**\n",
    "\n",
    "**If your model has high validation loss and high training loss, which action is most appropriate to improve the model?**\n",
    "*   (a) Increase the model complexity.\n",
    "*   (b) Increase $k$ in $k$-fold cross-validation.\n",
    "*   (c) Increase the number of training examples.\n",
    "*   (d) Apply regularization to reduce overfitting.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (a) is correct** - increasing model complexity is appropriate when both training and validation loss are high.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. High training and validation loss indicates underfitting:**\n",
    "- **Model is too simple** to capture data patterns\n",
    "- **High bias, low variance** situation\n",
    "- **Poor performance** on both training and test data\n",
    "- **Model capacity** is insufficient\n",
    "\n",
    "**2. Underfitting characteristics:**\n",
    "- **Training loss:** High (model can't fit training data)\n",
    "- **Validation loss:** High (model can't generalize)\n",
    "- **Gap between losses:** Small (low variance)\n",
    "- **Model complexity:** Too low\n",
    "\n",
    "**3. Why increasing complexity helps:**\n",
    "- **More parameters** to capture patterns\n",
    "- **Higher model capacity** to fit data\n",
    "- **Reduced bias** through better representation\n",
    "- **Better performance** on both training and validation\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Increase k in k-fold CV**\n",
    "- **Cross-validation** is for model evaluation, not improvement\n",
    "- **Larger k** doesn't change model performance\n",
    "- **Doesn't address** the fundamental underfitting issue\n",
    "\n",
    "**Option (c): Increase training examples**\n",
    "- **More data** helps with variance, not bias\n",
    "- **Underfitting** persists regardless of data size\n",
    "- **Model capacity** is the limiting factor\n",
    "\n",
    "**Option (d): Apply regularization**\n",
    "- **Regularization** reduces complexity\n",
    "- **Underfitting** means model is already too simple\n",
    "- **Would make the problem worse**\n",
    "\n",
    "**5. Specific ways to increase complexity:**\n",
    "- **Add more features** or basis functions\n",
    "- **Increase polynomial degree** in polynomial regression\n",
    "- **Add more layers** in neural networks\n",
    "- **Use more complex** algorithms\n",
    "\n",
    "**6. When to use other strategies:**\n",
    "- **More data:** When model is overfitting (high variance)\n",
    "- **Regularization:** When model is overfitting (high variance)\n",
    "- **Feature engineering:** When current features are insufficient\n",
    "- **Different algorithm:** When current algorithm is inappropriate\n",
    "\n",
    "**7. Monitoring improvement:**\n",
    "- **Training loss** should decrease\n",
    "- **Validation loss** should decrease\n",
    "- **Gap** between losses may increase (higher variance)\n",
    "- **Overall performance** should improve\n",
    "\n",
    "**Key insight:** **High training and validation loss** indicates **underfitting**, which requires **increasing model complexity**.\n",
    "\n",
    "**16.**\n",
    "\n",
    "**In a project using a customer purchase history dataset with a 60/20/20 train, validation, and test split, the validation accuracy remains consistently lower than the training accuracy. What could be a reason for this?**\n",
    "\n",
    "**Answer:** Overfitting - the model is learning training data patterns that don't generalize to validation data.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**The most likely reason is overfitting** - the model is learning patterns specific to the training data that don't generalize.\n",
    "\n",
    "**Why overfitting occurs:**\n",
    "\n",
    "**1. Model complexity vs. data size:**\n",
    "- **Complex model** relative to available training data\n",
    "- **Too many parameters** to estimate reliably\n",
    "- **Model memorizes** training data instead of learning generalizable patterns\n",
    "\n",
    "**2. Training vs. validation performance gap:**\n",
    "- **Training accuracy:** High (model fits training data well)\n",
    "- **Validation accuracy:** Lower (model doesn't generalize)\n",
    "- **Gap indicates** overfitting\n",
    "\n",
    "**3. Specific causes in this scenario:**\n",
    "\n",
    "**Data characteristics:**\n",
    "- **Customer purchase patterns** may be complex\n",
    "- **Seasonal effects** or temporal dependencies\n",
    "- **Individual customer idiosyncrasies** in training data\n",
    "\n",
    "**Model issues:**\n",
    "- **Too many features** relative to data size\n",
    "- **Insufficient regularization**\n",
    "- **Complex algorithm** for the problem\n",
    "\n",
    "**4. Other possible reasons:**\n",
    "\n",
    "**Data leakage:**\n",
    "- **Validation set** may have different characteristics\n",
    "- **Temporal split** issues (validation data from different time period)\n",
    "- **Sampling bias** between train and validation sets\n",
    "\n",
    "**Feature distribution shift:**\n",
    "- **Different customer segments** in validation set\n",
    "- **Different time periods** with different purchase patterns\n",
    "- **Different data collection** methods\n",
    "\n",
    "**5. Solutions to address overfitting:**\n",
    "\n",
    "**Reduce model complexity:**\n",
    "- **Feature selection** to remove irrelevant features\n",
    "- **Regularization** (L1/L2 penalties)\n",
    "- **Simpler algorithms** or fewer parameters\n",
    "\n",
    "**Increase effective data size:**\n",
    "- **Data augmentation** techniques\n",
    "- **Cross-validation** for better model selection\n",
    "- **Ensemble methods** to reduce variance\n",
    "\n",
    "**6. Monitoring and diagnosis:**\n",
    "- **Track training vs. validation** performance over time\n",
    "- **Use learning curves** to identify overfitting\n",
    "- **Cross-validation** to get more reliable estimates\n",
    "\n",
    "**Key insight:** **Consistent gap** between training and validation performance typically indicates **overfitting** due to **model complexity** exceeding **data capacity**.\n",
    "\n",
    "**17. One Answer**\n",
    "\n",
    "**A consortium of 10 hospitals have pooled together their Electronic Health Records data and want to build a machine learning model to predict patient prognosis based on patient records in their hospitals. They want to maximize the accuracy of their model across all 10 hospitals and do not plan to deploy their model in other hospitals. How should they split the data into train / validation / test sets?**\n",
    "*   (a) Leave out data from 1 hospital for the validation set, data from another hospital for the test set, and use the rest for train set.\n",
    "*   (b) Leave out data from 1 hospital for the validation set, data from another hospital for the test set, and use the rest for train set. After training, add the validation data to the train set and re-train the model on the combined data.\n",
    "*   (c) Use data from 8 hospitals with the most number of records for training, and use data from the other 2 hospitals for validation and test sets.\n",
    "*   (d) Mix data from all hospitals, randomly shuffle all the records, and then do the 80/10/10 train/validation/test split.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (d) is correct** - mixing data from all hospitals and random splitting avoids overfitting to specific hospital characteristics.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**1. Problem with hospital-specific splits:**\n",
    "- **Each hospital** has unique characteristics\n",
    "- **Different patient populations** and demographics\n",
    "- **Different medical practices** and protocols\n",
    "- **Different data collection** methods and quality\n",
    "\n",
    "**2. Why hospital-specific splits cause issues:**\n",
    "\n",
    "**Options (a) and (b):**\n",
    "- **Validation/test sets** from single hospitals\n",
    "- **Hyperparameter tuning** overfits to one hospital's characteristics\n",
    "- **Model selection** biased toward that hospital's patterns\n",
    "- **Poor generalization** to other hospitals\n",
    "\n",
    "**Option (c):**\n",
    "- **Validation/test** from hospitals with fewer records\n",
    "- **Small validation set** leads to unreliable hyperparameter selection\n",
    "- **Test set** may not represent overall hospital population\n",
    "- **Statistical power** issues with small samples\n",
    "\n",
    "**3. Benefits of mixed data splitting:**\n",
    "\n",
    "**Representative samples:**\n",
    "- **All hospitals** represented in each split\n",
    "- **Balanced distribution** of hospital characteristics\n",
    "- **More reliable** performance estimates\n",
    "- **Better generalization** across hospitals\n",
    "\n",
    "**Statistical advantages:**\n",
    "- **Larger validation/test sets** for reliable estimates\n",
    "- **IID assumption** more likely to hold\n",
    "- **Reduced variance** in performance estimates\n",
    "- **More robust** model selection\n",
    "\n",
    "**4. Practical considerations:**\n",
    "\n",
    "**Data heterogeneity:**\n",
    "- **Different hospitals** may have different patient mixes\n",
    "- **Varying data quality** across institutions\n",
    "- **Different coding** practices and standards\n",
    "- **Temporal differences** in data collection\n",
    "\n",
    "**Model deployment:**\n",
    "- **Model will be used** across all 10 hospitals\n",
    "- **Performance should be** representative of all hospitals\n",
    "- **Mixed splitting** ensures this representation\n",
    "\n",
    "**5. Alternative approaches (if needed):**\n",
    "- **Stratified sampling** to ensure hospital representation\n",
    "- **Cross-validation** with hospital-aware folds\n",
    "- **Ensemble methods** trained on different hospital subsets\n",
    "\n",
    "**6. When hospital-specific splits might be appropriate:**\n",
    "- **Testing generalization** to completely new hospitals\n",
    "- **Domain adaptation** scenarios\n",
    "- **Transfer learning** applications\n",
    "\n",
    "**Key insight:** **Mixed data splitting** ensures **representative performance estimates** and **avoids overfitting** to specific hospital characteristics.\n",
    "\n",
    "**18.**\n",
    "\n",
    "**Given the task of determining loan approval for applicants using a predictive model given applicant features such as race, salary, education, etc., is it always best practice to allow the model to use all of the given features? Why or why not?**\n",
    "\n",
    "**Answer:** No, we should not always use all features. Ethical considerations and potential bias require careful feature selection.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**No, we should not always use all features** - ethical considerations and potential bias require thoughtful feature selection.\n",
    "\n",
    "**Why not to use all features:**\n",
    "\n",
    "**1. Ethical and legal concerns:**\n",
    "\n",
    "**Protected attributes:**\n",
    "- **Race, gender, age** may be legally protected\n",
    "- **Direct discrimination** is illegal in many contexts\n",
    "- **Model may learn** discriminatory patterns\n",
    "- **Fair lending laws** prohibit certain features\n",
    "\n",
    "**2. Bias and discrimination:**\n",
    "\n",
    "**Historical bias:**\n",
    "- **Training data** may reflect historical discrimination\n",
    "- **Model learns** biased patterns from biased data\n",
    "- **Perpetuates** existing inequalities\n",
    "- **Proxy discrimination** through correlated features\n",
    "\n",
    "**3. Correlation vs. causation:**\n",
    "\n",
    "**Spurious correlations:**\n",
    "- **Race and income** may be correlated due to historical factors\n",
    "- **Model may use race** as a proxy for income\n",
    "- **Correlation doesn't imply** causation\n",
    "- **Reinforces** existing biases\n",
    "\n",
    "**4. Practical considerations:**\n",
    "\n",
    "**Model interpretability:**\n",
    "- **Fewer features** often more interpretable\n",
    "- **Regulatory compliance** requires transparency\n",
    "- **Stakeholder trust** depends on understanding\n",
    "- **Debugging** easier with simpler models\n",
    "\n",
    "**5. Alternative approaches:**\n",
    "\n",
    "**Feature engineering:**\n",
    "- **Remove protected attributes** entirely\n",
    "- **Create fair features** that don't encode bias\n",
    "- **Use domain knowledge** to select relevant features\n",
    "- **Feature selection** based on business logic\n",
    "\n",
    "**Fairness techniques:**\n",
    "- **Preprocessing** to remove bias\n",
    "- **In-processing** fairness constraints\n",
    "- **Post-processing** to ensure fairness\n",
    "- **Regularization** against unfair predictions\n",
    "\n",
    "**6. When to include features:**\n",
    "\n",
    "**Legitimate business need:**\n",
    "- **Features directly related** to creditworthiness\n",
    "- **Income, employment history, credit score**\n",
    "- **Features that predict** ability to repay\n",
    "- **Features that don't encode** protected attributes\n",
    "\n",
    "**7. Best practices:**\n",
    "\n",
    "**Documentation:**\n",
    "- **Justify** each feature inclusion\n",
    "- **Document** potential bias sources\n",
    "- **Monitor** model fairness over time\n",
    "- **Regular audits** of model decisions\n",
    "\n",
    "**Testing:**\n",
    "- **Fairness metrics** (disparate impact, equalized odds)\n",
    "- **Bias detection** in predictions\n",
    "- **A/B testing** with different feature sets\n",
    "- **Stakeholder review** of feature choices\n",
    "\n",
    "**Key insight:** **Feature selection** in sensitive applications requires **ethical consideration** beyond just **predictive accuracy**.\n",
    "\n",
    "**19. One Answer**\n",
    "\n",
    "**You are building a predictive model about users of a website. Suppose that after you train your model on historical user data, the distribution of users shifts dramatically. What can happen if you deploy your machine learning system without addressing this distribution shift?**\n",
    "*   (a) The model will automatically adapt to new data distributions.\n",
    "*   (b) The model will generate more diverse predictions, increasing its overall accuracy.\n",
    "*   (c) The model will maintain its original performance indefinitely regardless of data changes.\n",
    "*   (d) The model's predictions may become unreliable or biased.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Option (d) is correct** - distribution shift can make model predictions unreliable or biased.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**1. Distribution shift problem:**\n",
    "- **Training data** comes from one distribution\n",
    "- **Deployment data** comes from different distribution\n",
    "- **Model assumptions** no longer hold\n",
    "- **Performance degradation** inevitable\n",
    "\n",
    "**2. What happens during distribution shift:**\n",
    "\n",
    "**Covariate shift:**\n",
    "- **Feature distributions** change (e.g., user demographics)\n",
    "- **Label distribution** remains same\n",
    "- **Model trained** on old user types\n",
    "- **Poor performance** on new user types\n",
    "\n",
    "**Label shift:**\n",
    "- **Target variable distribution** changes\n",
    "- **Feature distributions** may remain same\n",
    "- **Model calibrated** for old label distribution\n",
    "- **Biased predictions** on new data\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Automatic adaptation**\n",
    "- **Models don't adapt** automatically\n",
    "- **Static parameters** learned during training\n",
    "- **No online learning** mechanism\n",
    "- **Requires retraining** to adapt\n",
    "\n",
    "**Option (b): More diverse predictions**\n",
    "- **Diversity doesn't imply** accuracy\n",
    "- **Random predictions** are diverse but useless\n",
    "- **Distribution shift** typically reduces accuracy\n",
    "- **Wrong direction** of effect\n",
    "\n",
    "**Option (c): Maintain performance indefinitely**\n",
    "- **Models are not** distribution-agnostic\n",
    "- **Performance depends** on data distribution\n",
    "- **Assumes** unrealistic model robustness\n",
    "- **Contradicts** fundamental ML principles\n",
    "\n",
    "**4. Specific consequences:**\n",
    "\n",
    "**Performance degradation:**\n",
    "- **Lower accuracy** on new data\n",
    "- **Increased error rates**\n",
    "- **Poor generalization** to new users\n",
    "- **Unreliable predictions**\n",
    "\n",
    "**Bias introduction:**\n",
    "- **Systematic errors** in predictions\n",
    "- **Unfair treatment** of new user groups\n",
    "- **Discriminatory outcomes**\n",
    "- **Loss of trust** in system\n",
    "\n",
    "**5. Examples of distribution shift:**\n",
    "\n",
    "**Website user changes:**\n",
    "- **New user demographics** (age, location)\n",
    "- **Different usage patterns** (mobile vs. desktop)\n",
    "- **Seasonal effects** (holiday shopping)\n",
    "- **Platform changes** (new features, redesign)\n",
    "\n",
    "**6. Solutions to address distribution shift:**\n",
    "\n",
    "**Monitoring:**\n",
    "- **Track performance** over time\n",
    "- **Detect distribution** changes\n",
    "- **Alert systems** for performance drops\n",
    "- **Regular model** evaluation\n",
    "\n",
    "**Adaptation strategies:**\n",
    "- **Online learning** for continuous adaptation\n",
    "- **Transfer learning** to adapt to new domains\n",
    "- **Domain adaptation** techniques\n",
    "- **Regular retraining** with new data\n",
    "\n",
    "**7. Prevention measures:**\n",
    "- **Robust feature engineering**\n",
    "- **Domain-invariant** representations\n",
    "- **Ensemble methods** for stability\n",
    "- **Conservative model** selection\n",
    "\n",
    "**Key insight:** **Distribution shift** is a **fundamental challenge** in ML that requires **proactive monitoring** and **adaptation strategies**.\n",
    "\n",
    "**20. One Answer**\n",
    "\n",
    "**For a possibly non-convex optimization problem, gradient descent on the full dataset always finds a better solution than stochastic gradient descent.**\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This statement is false** - SGD can sometimes find better solutions than full gradient descent for non-convex problems.\n",
    "\n",
    "**Why this is false:**\n",
    "\n",
    "**1. Non-convex optimization challenges:**\n",
    "- **Multiple local minima** exist\n",
    "- **Global minimum** is hard to find\n",
    "- **Optimization landscape** is complex\n",
    "- **Convergence** to local optima is common\n",
    "\n",
    "**2. How SGD can outperform GD:**\n",
    "\n",
    "**Escape local minima:**\n",
    "- **Stochastic noise** helps escape local minima\n",
    "- **Random perturbations** explore more of the landscape\n",
    "- **Less likely** to get stuck in poor local optima\n",
    "- **Better exploration** of solution space\n",
    "\n",
    "**3. Why GD can get stuck:**\n",
    "\n",
    "**Deterministic nature:**\n",
    "- **Always follows** exact gradient direction\n",
    "- **Can get trapped** in local minima\n",
    "- **No randomness** to escape poor solutions\n",
    "- **Sensitive** to initialization\n",
    "\n",
    "**4. Mathematical intuition:**\n",
    "\n",
    "**GD update rule:**\n",
    "```\n",
    "w_{t+1} = w_t - \\eta \\nabla L(w_t)\n",
    "```\n",
    "- **Exact gradient** from all data\n",
    "- **Deterministic** path\n",
    "- **Converges** to nearest local minimum\n",
    "\n",
    "**SGD update rule:**\n",
    "```\n",
    "w_{t+1} = w_t - \\eta \\nabla L_i(w_t)\n",
    "```\n",
    "- **Noisy gradient** from mini-batch\n",
    "- **Stochastic** path\n",
    "- **Can escape** local minima\n",
    "\n",
    "**5. Practical examples:**\n",
    "\n",
    "**Neural networks:**\n",
    "- **Highly non-convex** optimization\n",
    "- **SGD often finds** better solutions\n",
    "- **GD may converge** to poor local minima\n",
    "- **SGD's noise** is beneficial\n",
    "\n",
    "**6. Trade-offs:**\n",
    "\n",
    "**SGD advantages:**\n",
    "- **Better exploration** of solution space\n",
    "- **Escape local minima**\n",
    "- **Computational efficiency**\n",
    "- **Scalability** to large datasets\n",
    "\n",
    "**SGD disadvantages:**\n",
    "- **Noisy convergence**\n",
    "- **Requires careful** learning rate tuning\n",
    "- **May not converge** to exact minimum\n",
    "- **Higher variance** in final solution\n",
    "\n",
    "**7. When each is preferred:**\n",
    "\n",
    "**Use GD when:**\n",
    "- **Convex optimization** problems\n",
    "- **Small datasets** that fit in memory\n",
    "- **Need exact** convergence\n",
    "- **Computational cost** is not a concern\n",
    "\n",
    "**Use SGD when:**\n",
    "- **Non-convex problems** (like neural networks)\n",
    "- **Large datasets** that don't fit in memory\n",
    "- **Want to escape** local minima\n",
    "- **Computational efficiency** is important\n",
    "\n",
    "**Key insight:** **SGD's stochasticity** can be **advantageous** for **non-convex optimization** by helping escape **local minima**.\n",
    "\n",
    "**21. Select All That Apply**\n",
    "\n",
    "**Given the gradient descent algorithm, $w_{t+1} = w_t - \\eta \\frac{df(w)}{dw} \\Big|_{w=w_t}$, which of the following statement is correct regarding the hyperparameter $\\eta$?**\n",
    "*   (a) $\\eta$ controls the magnitude of each step.\n",
    "*   (b) $\\eta$ determines the initial value of $w$.\n",
    "*   (c) A larger $\\eta$ guarantees faster convergence to the global minimum.\n",
    "*   (d) A smaller $\\eta$ guarantees faster convergence to the global minimum.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Only option (a) is correct** - η controls the step size in gradient descent.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**1. Learning rate role:**\n",
    "- **η** is the **learning rate** or **step size**\n",
    "- **Controls how far** to move in gradient direction\n",
    "- **Scales the gradient** vector\n",
    "- **Determines convergence** behavior\n",
    "\n",
    "**2. Mathematical interpretation:**\n",
    "```\n",
    "w_{t+1} = w_t - η∇f(w_t)\n",
    "```\n",
    "- **η** multiplies the gradient\n",
    "- **Larger η** → larger steps\n",
    "- **Smaller η** → smaller steps\n",
    "- **Step magnitude** = η × ||∇f(w_t)||\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): η determines initial value**\n",
    "- **η and w₀** are set independently\n",
    "- **Initial value w₀** is chosen separately\n",
    "- **η only affects** step size, not starting point\n",
    "- **No relationship** between η and w₀\n",
    "\n",
    "**Option (c): Larger η guarantees faster convergence**\n",
    "- **Too large η** can cause overshooting\n",
    "- **May oscillate** around minimum\n",
    "- **May diverge** if η is too large\n",
    "- **No guarantee** of faster convergence\n",
    "\n",
    "**Option (d): Smaller η guarantees faster convergence**\n",
    "- **Too small η** leads to slow convergence\n",
    "- **May get stuck** in local minima\n",
    "- **Many iterations** needed to converge\n",
    "- **No guarantee** of faster convergence\n",
    "\n",
    "**4. Learning rate effects:**\n",
    "\n",
    "**Too large η:**\n",
    "- **Overshooting** the minimum\n",
    "- **Oscillations** around optimal point\n",
    "- **Divergence** in extreme cases\n",
    "- **Unstable** convergence\n",
    "\n",
    "**Too small η:**\n",
    "- **Slow convergence** to minimum\n",
    "- **Many iterations** required\n",
    "- **May get stuck** in local minima\n",
    "- **Computationally expensive**\n",
    "\n",
    "**Optimal η:**\n",
    "- **Fast convergence** without overshooting\n",
    "- **Stable** optimization path\n",
    "- **Reaches minimum** efficiently\n",
    "- **Problem-dependent** choice\n",
    "\n",
    "**5. Practical considerations:**\n",
    "\n",
    "**Adaptive learning rates:**\n",
    "- **Adam, RMSprop** adjust η automatically\n",
    "- **Learning rate scheduling** reduces η over time\n",
    "- **Line search** methods find optimal η\n",
    "- **Grid search** for hyperparameter tuning\n",
    "\n",
    "**6. Guidelines for choosing η:**\n",
    "- **Start small** (e.g., 0.01, 0.001)\n",
    "- **Monitor convergence** behavior\n",
    "- **Adjust based on** problem characteristics\n",
    "- **Use validation** to tune η\n",
    "\n",
    "**Key insight:** **Learning rate η** controls **step size** and **convergence behavior**, but **optimal value** depends on the **specific problem**.\n",
    "\n",
    "**22. Select All That Apply**\n",
    "\n",
    "**Which of the following functions are convex?**\n",
    "*   (a) $f(x) = x^3$\n",
    "*   (b) $f(x) = \\frac{3x(x-1)}{2}$\n",
    "*   (c) $f(x) = \\sin x$, for $x \\in [\\pi, 2\\pi]$\n",
    "*   (d) $f(x) = \\log_{10}(x)$\n",
    "\n",
    "**Correct answers:** (b), (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Options (b) and (c) are convex** on their specified domains.\n",
    "\n",
    "**Analysis of each function:**\n",
    "\n",
    "**Option (a): f(x) = x³**\n",
    "- **f'(x) = 3x²**\n",
    "- **f''(x) = 6x**\n",
    "- **f''(x) < 0** for x < 0 (concave)\n",
    "- **f''(x) > 0** for x > 0 (convex)\n",
    "- **Not convex** on $\\mathbb{R}$ (mixed convexity)\n",
    "\n",
    "**Option (b): f(x) = 3x(x-1)/2 = (3x² - 3x)/2**\n",
    "- **f'(x) = (6x - 3)/2 = 3x - 3/2**\n",
    "- **f''(x) = 3 > 0** for all x\n",
    "- **Convex** on $\\mathbb{R}$ (positive second derivative)\n",
    "\n",
    "**Option (c): f(x) = sin(x) on [π, 2π]**\n",
    "- **f'(x) = cos(x)**\n",
    "- **f''(x) = -sin(x)**\n",
    "- **On [π, 2π]:** sin(x) ≤ 0\n",
    "- **Therefore:** f''(x) = -sin(x) ≥ 0\n",
    "- **Convex** on [π, 2π]\n",
    "\n",
    "**Option (d): f(x) = log₁₀(x)**\n",
    "- **f'(x) = 1/(x ln(10))**\n",
    "- **f''(x) = -1/(x² ln(10)) < 0** for x > 0\n",
    "- **Concave** on (0, ∞)\n",
    "- **Not convex**\n",
    "\n",
    "**3. Visual interpretation:**\n",
    "\n",
    "**Option (b):**\n",
    "- **Quadratic function** with positive coefficient\n",
    "- **U-shaped curve** opening upward\n",
    "- **Always convex**\n",
    "\n",
    "**Option (c):**\n",
    "- **Sine function** on [π, 2π]\n",
    "- **Curve bends upward** in this interval\n",
    "- **Convex** in this domain\n",
    "\n",
    "**4. Why convexity matters:**\n",
    "\n",
    "**Optimization properties:**\n",
    "- **Local minima** are global minima\n",
    "- **Gradient descent** converges to global minimum\n",
    "- **No local minima traps**\n",
    "- **Efficient optimization** algorithms\n",
    "\n",
    "**5. Second derivative test:**\n",
    "- **f''(x) > 0** → convex\n",
    "- **f''(x) < 0** → concave\n",
    "- **f''(x) = 0** → inflection point\n",
    "\n",
    "**6. Practical implications:**\n",
    "- **Convex functions** are easier to optimize\n",
    "- **Guaranteed convergence** to global minimum\n",
    "- **No initialization** sensitivity\n",
    "- **Deterministic** optimization path\n",
    "\n",
    "**Key insight:** **Convexity** is determined by **second derivative sign** and provides **optimization guarantees**.\n",
    "\n",
    "**23. Which of the following are true about a convex function $f(x): \\mathbb{R}^d \\rightarrow \\mathbb{R}$?**\n",
    "*   (a) $f(x)$ must be differentiable across its entire domain.\n",
    "*   (b) $f(x)$ has a unique global minimum.\n",
    "*   (c) $g(x) = -f(x)$ is also convex.\n",
    "*   (d) If $f(x)$ is twice differentiable, then $z^T \\nabla^2 f(x)z \\geq 0$ for all $z \\in \\mathbb{R}^d$.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Only option (d) is correct** - the Hessian matrix must be positive semi-definite for twice differentiable convex functions.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**1. Second-order condition for convexity:**\n",
    "- **f is convex** if and only if ∇²f(x) is positive semi-definite\n",
    "- **z^T ∇²f(x) z ≥ 0** for all z ∈ ℝ^d\n",
    "- **All eigenvalues** of ∇²f(x) are non-negative\n",
    "- **This is the** fundamental characterization\n",
    "\n",
    "**2. Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Must be differentiable**\n",
    "- **Counterexample:** f(x) = |x| is convex but not differentiable at x = 0\n",
    "- **Convex functions** can have non-differentiable points\n",
    "- **Subdifferential** exists at all points\n",
    "- **Differentiability** is not required\n",
    "\n",
    "**Option (b): Unique global minimum**\n",
    "- **Counterexample:** f(x) = 0 (constant function) has infinitely many global minima\n",
    "- **Counterexample:** f(x) = x² on [-1,1] has minimum at x = 0, but f(x) = 0 on [0,1] has multiple minima\n",
    "- **Convex functions** can have multiple connected global minima\n",
    "- **Uniqueness** requires strict convexity\n",
    "\n",
    "**Option (c): Negative is also convex**\n",
    "- **Counterexample:** f(x) = x² is convex, but g(x) = -x² is concave\n",
    "- **Only true** for linear/affine functions\n",
    "- **Convexity** is not preserved under negation\n",
    "- **Negation** flips convexity to concavity\n",
    "\n",
    "**3. Mathematical details:**\n",
    "\n",
    "**Hessian condition:**\n",
    "```\n",
    "For twice differentiable f: ℝ^d → ℝ\n",
    "f is convex ⇔ ∇²f(x) is positive semi-definite ∀x\n",
    "```\n",
    "\n",
    "**Positive semi-definite matrix:**\n",
    "- **z^T A z ≥ 0** for all z ∈ ℝ^d\n",
    "- **All eigenvalues** ≥ 0\n",
    "- **Symmetric** matrix\n",
    "\n",
    "**4. Examples:**\n",
    "\n",
    "**Convex functions:**\n",
    "- **f(x) = x²** → f''(x) = 2 > 0\n",
    "- **f(x) = e^x** → f''(x) = e^x > 0\n",
    "- **f(x) = ||x||²** → ∇²f(x) = 2I (positive definite)\n",
    "\n",
    "**Non-differentiable convex:**\n",
    "- **f(x) = |x|** (convex but not differentiable at 0)\n",
    "\n",
    "**5. Practical implications:**\n",
    "\n",
    "**Optimization:**\n",
    "- **Local minima** are global minima\n",
    "- **Gradient descent** converges to global minimum\n",
    "- **No local minima traps**\n",
    "- **Efficient algorithms** available\n",
    "\n",
    "**Key insight:** **Convexity** is characterized by **positive semi-definite Hessian** for twice differentiable functions, but **differentiability** is not required.\n",
    "\n",
    "**24. Which of the following have convex objective functions?**\n",
    "*   (a) Linear regression\n",
    "*   (b) Linear regression with arbitrary nonlinear basis functions\n",
    "*   (c) Ridge regression\n",
    "*   (d) Lasso regression\n",
    "*   (e) Logistic regression\n",
    "\n",
    "**Correct answers:** (a), (b), (c), (d), (e)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**All options have convex objective functions** - this is a key property of these models.\n",
    "\n",
    "**Why all are convex:**\n",
    "\n",
    "**1. Linear regression:**\n",
    "```\n",
    "\\text{Objective: } \\min \\|y - Xw\\|^2\n",
    "```\n",
    "- **Quadratic function** in w\n",
    "- **Positive semi-definite** Hessian (2X^T X)\n",
    "- **Convex** for any X\n",
    "\n",
    "**2. Linear regression with basis functions:**\n",
    "```\n",
    "\\text{Objective: } \\min \\|y - \\Phi w\\|^2\n",
    "```\n",
    "where $\\Phi = \\phi(X)$ are basis functions\n",
    "- **Still quadratic** in w\n",
    "- **Same convexity** properties as linear regression\n",
    "- **Basis transformation** preserves convexity\n",
    "\n",
    "**3. Ridge regression:**\n",
    "```\n",
    "\\text{Objective: } \\min \\|y - Xw\\|^2 + \\lambda\\|w\\|^2\n",
    "```\n",
    "- **Sum of convex functions** is convex\n",
    "- **$\\|y - Xw\\|^2$** is convex (quadratic)\n",
    "- **$\\lambda\\|w\\|^2$** is convex (quadratic)\n",
    "- **Convex combination** remains convex\n",
    "\n",
    "**4. Lasso regression:**\n",
    "```\n",
    "\\text{Objective: } \\min \\|y - Xw\\|^2 + \\lambda\\|w\\|_1\n",
    "```\n",
    "- **$\\|y - Xw\\|^2$** is convex (quadratic)\n",
    "- **$\\|w\\|_1$** is convex (L1 norm)\n",
    "- **Sum of convex functions** is convex\n",
    "- **L1 penalty** preserves convexity\n",
    "\n",
    "**5. Logistic regression:**\n",
    "```\n",
    "\\text{Objective: } \\min \\sum[-y_i \\log(\\sigma(x_i^T w)) - (1-y_i)\\log(1-\\sigma(x_i^T w))]\n",
    "```\n",
    "- **Log-likelihood** of Bernoulli distribution\n",
    "- **Sigmoid function** $\\sigma(z) = 1/(1+e^{-z})$\n",
    "- **Concave log-likelihood** → convex negative log-likelihood\n",
    "- **Well-known** to be convex\n",
    "\n",
    "**6. Mathematical properties:**\n",
    "\n",
    "**Convexity preservation:**\n",
    "- **Linear combinations** of convex functions are convex\n",
    "- **Composition** with linear functions preserves convexity\n",
    "- **Sum** of convex functions is convex\n",
    "- **Regularization** terms are typically convex\n",
    "\n",
    "**7. Practical implications:**\n",
    "\n",
    "**Optimization guarantees:**\n",
    "- **Global minimum** exists and is unique (if strictly convex)\n",
    "- **Gradient descent** converges to global minimum\n",
    "- **No local minima** traps\n",
    "- **Efficient algorithms** available\n",
    "\n",
    "**8. Why convexity matters:**\n",
    "\n",
    "**Reliable optimization:**\n",
    "- **Predictable** convergence behavior\n",
    "- **No initialization** sensitivity\n",
    "- **Deterministic** optimization path\n",
    "- **Well-understood** algorithms\n",
    "\n",
    "**Key insight:** **All these models** have **convex objectives** because they use **linear functions** with **convex loss functions** and **convex regularization**.\n",
    "\n",
    "**25. Which of the following scenarios are better suited for a logistic regression model over a linear regression model?**\n",
    "*   (a) Forecasting the price of stocks for the next year, given the price of stocks for the past year.\n",
    "*   (b) Diagnosing the presence or absence of a rare disease, given a medical x-ray.\n",
    "*   (c) Predicting what the average global temperature will be in the next year, given historical climate data.\n",
    "*   (d) Predicting how likely a student is to successfully complete a 4-year college degree, given their high school grades.\n",
    "*   (e) Predicting the hardness of a material on a scale of 1-10 given the molecular structure of the material.\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Options (b) and (d) are classification problems** that are better suited for logistic regression.\n",
    "\n",
    "**Why (b) and (d) are classification problems:**\n",
    "\n",
    "**Option (b): Disease diagnosis**\n",
    "- **Binary outcome:** Disease present (1) or absent (0)\n",
    "- **Classification task:** Predict discrete categories\n",
    "- **Logistic regression** outputs probabilities P(disease = 1)\n",
    "- **Medical diagnosis** is inherently categorical\n",
    "\n",
    "**Option (d): College completion prediction**\n",
    "- **Binary outcome:** Complete degree (1) or not (0)\n",
    "- **Classification task:** Predict success/failure\n",
    "- **Logistic regression** outputs P(completion = 1)\n",
    "- **Educational outcomes** are often binary\n",
    "\n",
    "**Why other options are regression problems:**\n",
    "\n",
    "**Option (a): Stock price forecasting**\n",
    "- **Continuous outcome:** Price can be any real number\n",
    "- **Regression task:** Predict continuous values\n",
    "- **Linear regression** appropriate for continuous targets\n",
    "- **Time series** prediction problem\n",
    "\n",
    "**Option (c): Temperature prediction**\n",
    "- **Continuous outcome:** Temperature can be any real number\n",
    "- **Regression task:** Predict continuous values\n",
    "- **Linear regression** appropriate for continuous targets\n",
    "- **Climate modeling** problem\n",
    "\n",
    "**Option (e): Material hardness prediction**\n",
    "- **Continuous outcome:** Hardness scale 1-10 (continuous)\n",
    "- **Regression task:** Predict continuous values\n",
    "- **Linear regression** appropriate for continuous targets\n",
    "- **Material science** prediction problem\n",
    "\n",
    "**3. Key differences:**\n",
    "\n",
    "**Logistic regression:**\n",
    "- **Binary classification** (0 or 1)\n",
    "- **Outputs probabilities** P(y = 1)\n",
    "- **Sigmoid activation** function\n",
    "- **Log-likelihood** loss function\n",
    "\n",
    "**Linear regression:**\n",
    "- **Continuous prediction** (any real number)\n",
    "- **Outputs continuous** values\n",
    "- **Linear activation** function\n",
    "- **Squared error** loss function\n",
    "\n",
    "**4. When to use each:**\n",
    "\n",
    "**Use logistic regression for:**\n",
    "- **Binary classification** problems\n",
    "- **Probability estimation** tasks\n",
    "- **Categorical outcomes**\n",
    "- **Risk assessment** problems\n",
    "\n",
    "**Use linear regression for:**\n",
    "- **Continuous prediction** problems\n",
    "- **Quantity estimation** tasks\n",
    "- **Numerical outcomes**\n",
    "- **Forecasting** problems\n",
    "\n",
    "**5. Model outputs:**\n",
    "\n",
    "**Logistic regression:**\n",
    "- **P(y = 1)** ∈ [0, 1]\n",
    "- **Interpretable** as probability\n",
    "- **Decision threshold** needed for classification\n",
    "- **ROC curves** and AUC metrics\n",
    "\n",
    "**Linear regression:**\n",
    "- **ŷ** ∈ ℝ (any real number)\n",
    "- **Direct prediction** of target\n",
    "- **No threshold** needed\n",
    "- **R²** and RMSE metrics\n",
    "\n",
    "**Key insight:** **Logistic regression** is for **binary classification**, while **linear regression** is for **continuous prediction**.\n",
    "\n",
    "**26. Which of the following statements about classification are true?**\n",
    "\n",
    "**Recall that the softmax function $\\sigma: \\mathbb{R}^k \\rightarrow (0,1)^k$ takes a vector $z \\in \\mathbb{R}^k$ and returns a vector $\\sigma(z) \\in (0,1)^k$ with**\n",
    "$$\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^k \\exp(z_j)}$$\n",
    "\n",
    "*   (a) Consider a binary classification setting. If the data is linearly separable, we can use a logistic regression model with quadratic features to avoid overfitting.\n",
    "*   (b) Because binary logistic regression is a convex optimization problem, it has a closed-form solution.\n",
    "*   (c) The softmax function is used when we are classifying $k > 2$ classes. When we are classifying only $k = 2$ classes, softmax regression will overfit, so we use binary logistic regression instead.\n",
    "*   (d) We can use linear regression to solve classification problems, though the model we learn might not be as accurate compared to using logistic/softmax regression.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Only option (d) is correct** - linear regression can be used for classification, though it's not optimal.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**1. Linear regression for classification:**\n",
    "- **Can predict** binary outcomes (0 or 1)\n",
    "- **Outputs continuous** values that can be thresholded\n",
    "- **Threshold** at 0.5 for binary classification\n",
    "- **Works** but not optimal for classification\n",
    "\n",
    "**2. Why linear regression works:**\n",
    "- **Can learn** linear decision boundaries\n",
    "- **Outputs** can be interpreted as scores\n",
    "- **Thresholding** converts to binary predictions\n",
    "- **Simple** and interpretable\n",
    "\n",
    "**3. Why it's not optimal:**\n",
    "- **No probability** interpretation\n",
    "- **Predictions** outside [0,1] range\n",
    "- **Squared error** not appropriate for classification\n",
    "- **Poor calibration** for probabilities\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Quadratic features avoid overfitting**\n",
    "- **Quadratic features** increase model complexity\n",
    "- **More parameters** to estimate\n",
    "- **Higher risk** of overfitting\n",
    "- **Opposite effect** of what's claimed\n",
    "\n",
    "**Option (b): Closed-form solution for logistic regression**\n",
    "- **Logistic regression** is convex but has no closed-form solution\n",
    "- **Requires iterative** optimization (gradient descent, Newton's method)\n",
    "- **Only linear regression** has closed-form solution among these models\n",
    "- **Nonlinear** objective function\n",
    "\n",
    "**Option (c): Softmax overfits for k=2**\n",
    "- **Softmax** works fine for k=2 (equivalent to logistic regression)\n",
    "- **No overfitting** issue specific to k=2\n",
    "- **Logistic regression** is special case of softmax\n",
    "- **Choice between** them is implementation preference\n",
    "\n",
    "**4. Comparison of approaches:**\n",
    "\n",
    "**Linear regression for classification:**\n",
    "- **Pros:** Simple, interpretable, convex\n",
    "- **Cons:** No probability output, poor calibration\n",
    "- **Use when:** Quick prototype, interpretability important\n",
    "\n",
    "**Logistic regression for classification:**\n",
    "- **Pros:** Probability output, appropriate loss function\n",
    "- **Cons:** Requires iterative optimization\n",
    "- **Use when:** Probability estimates needed\n",
    "\n",
    "**5. Practical considerations:**\n",
    "\n",
    "**When to use linear regression:**\n",
    "- **Quick prototyping**\n",
    "- **Interpretability** is crucial\n",
    "- **Computational** constraints\n",
    "- **Simple** decision boundaries\n",
    "\n",
    "**When to use logistic regression:**\n",
    "- **Probability estimates** needed\n",
    "- **Proper classification** metrics important\n",
    "- **Model calibration** matters\n",
    "- **Standard** classification practice\n",
    "\n",
    "**Key insight:** **Linear regression** can perform **basic classification** but **logistic regression** is **more appropriate** for classification tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
