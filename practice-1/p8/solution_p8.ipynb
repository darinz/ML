{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1aab881-25fc-4854-b15d-0c963000f383",
   "metadata": {},
   "source": [
    "# Practice 8 Solutions\n",
    "\n",
    "**Problem 1. One Answer**\n",
    "\n",
    "Saket is building a classification model from a dataset with two classes of student reactions to the jokes he makes during section: Funny jokes (F) and Dull Jokes (D). The probability of a randomly selected joke being funny is 0.3. The probability the model makes the correct classification given that the joke is dull is 0.7, and the probability the model makes the correct classification given that a joke is funny is 0.2. What is the probability that a randomly selected joke is dull given that it has been classified as dull?\n",
    "\n",
    "*   (a) $\\frac{42}{73}$\n",
    "*   (b) $\\frac{49}{73}$\n",
    "*   (c) $\\frac{21}{73}$\n",
    "*   (d) 1\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** The answer is (b), which is $\\frac{49}{73}$. \n",
    "\n",
    "Let A be the event that the sample is dull.\n",
    "\n",
    "Let B be the event that the sample is classified as dull. \n",
    "\n",
    "We have $P(A^c) = 0.3$ and $P(A) = 1 - P(A^c) = 1 - 0.3 = 0.7$. We have $P(B|A) = 0.7$ and $P(B^c|A^c) = 0.2$. (Note that $P(B|A^c) = 1 - P(B^c|A^c) = 1 - 0.2 = 0.8$) \n",
    "\n",
    "We want to calculate $P(A|B)$ here using Bayes Rule and the Law of Total Probability. \n",
    "\n",
    "$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c) = 0.7 \\cdot 0.7 + 0.8 \\cdot 0.3 = 0.73$ $P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{0.7 \\cdot 0.7}{0.73} = \\frac{0.49}{0.73} = \\frac{49}{73}$\n",
    "\n",
    "Here, our answer is $P(A|B) = \\frac{49}{73}$, which is (b).\n",
    "\n",
    "**Problem 2. One Answer**\n",
    "\n",
    "Suppose we train a model $f(x) = x^T \\hat{w}$ on dataset $\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^n$, by optimizing the following objective:\n",
    "\n",
    "$$\\hat{w} = \\arg \\min_w \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\log k ||w||_2^2$$\n",
    "\n",
    "for $k > 1$. As $k$ increases, what likely happens to the variance of our model?\n",
    "\n",
    "*   (a) The variance of our model increases.\n",
    "*   (b) The variance of our model decreases.\n",
    "*   (c) The variance of our model remains unaffected.\n",
    "*   (d) The variance of our model does not change in a predictable way.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of regularization** and its effect on model variance.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**As $k$ increases, the penalty term $\\log k ||w||_2^2$ approaches zero:**\n",
    "\n",
    "**1. Mathematical analysis:**\n",
    "- **Penalty term:** $\\log k ||w||_2^2$\n",
    "- **As $k \\to \\infty$:** $\\log k \\to \\infty$, but $\\log k$ grows very slowly\n",
    "- **Effectively:** Penalty becomes negligible\n",
    "- **Model approaches** unregularized linear regression\n",
    "\n",
    "**2. Variance implications:**\n",
    "- **Less regularization** = less constraint on weights\n",
    "- **Weights can take** larger values\n",
    "- **Model becomes** more flexible/complex\n",
    "- **Higher variance** due to increased model capacity\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Variance decreases**\n",
    "- **Contradicts** the effect of reducing regularization\n",
    "- **Less regularization** typically increases variance\n",
    "- **Wrong direction**\n",
    "\n",
    "**Option (c): Variance unaffected**\n",
    "- **Regularization** directly affects model variance\n",
    "- **Changing penalty** must affect variance\n",
    "- **Not realistic**\n",
    "\n",
    "**Option (d): Unpredictable change**\n",
    "- **Regularization** has predictable effects on variance\n",
    "- **Well-understood** relationship\n",
    "- **Not unpredictable**\n",
    "\n",
    "**Key insight:** **Reducing regularization** (increasing $k$) **increases model variance** by allowing weights to take larger values.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 3. One Answer**\n",
    "\n",
    "You are building a classification model for your favorite soccer team to determine whether a penalty kick will result in a goal or not, and your data set contains 300 positive examples (resulted in a goal) and 200 negative examples (did not result in a goal). After training, you find that your model has an accuracy of 70% and misclassifies 15% of negative examples as positive. What is the probability that your model will misclassify a positive example as negative?\n",
    "\n",
    "*   (a) 15%\n",
    "*   (b) 24%\n",
    "*   (c) 30%\n",
    "*   (d) 40%\n",
    "*   (e) Cannot be determined.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** We have 500 examples in total, and $500 * 70\\% = 350$ were correctly classified. The rest (150) were incorrectly classified. We also know that $200 * 15\\% = 30$ negative examples were misclassified. This leaves us with $150 - 30 = 120$ misclassified examples, and we know these must all be positive examples that were misclassified. This means that $\\frac{120}{300} = \\frac{2}{5} = 40\\%$ of positive examples were misclassified as negative.\n",
    "\n",
    "**Problem 4. One Answer**\n",
    "\n",
    "When using the least squares method for linear regression, outliers would have a minimal impact since the least squares method averages out their effects.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** Outliers would cause the regression line to skew, thus leading to a representation that may not be an accurate representation of the data.\n",
    "\n",
    "**Problem 5. Select All That Apply**\n",
    "\n",
    "In the standard MLE derivation for a parameter in a parameter in a probability distribution (like the ones we saw in class), why do we apply the logarithm to our equation?\n",
    "\n",
    "*   (a) The function is monotonically increasing and therefore does not change the result of our optimization objective.\n",
    "*   (b) The function is concave and therefore does not change the result of our optimization objective.\n",
    "*   (c) The function is monotonically decreasing and therefore allows us to change our \"argmax\" to an \"argmin.\"\n",
    "*   (d) Applying the logarithm allows us to convert products ($\\Pi$) into sums ($\\Sigma$), letting us calculate derivatives easier.\n",
    "\n",
    "**Correct answers:** (a), (d)\n",
    "\n",
    "**Explanation:** The logarithm function is monotonically increasing, which makes (A) true and (C) false. (B) is false. the logarithm function is concave, but that is not why it doesn't change the result of our optimization. (D) is true due to properties of logarithms and derivatives.\n",
    "\n",
    "**Problem 6. One Answer**\n",
    "\n",
    "After training a model, your model has a low train error and a high test error. Which of the following can be inferred?\n",
    "\n",
    "*   (a) The model is underfitting.\n",
    "*   (b) The model will generalize well because it has low bias.\n",
    "*   (c) Training on more data will likely increase the model's performance on unseen data.\n",
    "*   (d) Training on more highly-informative features will decrease the test error.\n",
    "*   (e) Reducing model complexity will reduce the irreducible error.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** A is incorrect since the model has a low, not high, bias. B is also incorrect since a low bias alone doesn't guarantee that the model will generalize to unseen data. C is correct since more training data generally decreases the variance. D is incorrect since increasing model complexity by increasing the number of features in the dataset will likely increase the variance. E is incorrect since irreducible error comes from underlying noise in the dataset and changing any factor of the model will not change this error.\n",
    "\n",
    "**Problem 7. One Answer**\n",
    "\n",
    "Given the following Hessian Matrix, which of the following could be the original $f(x,y)$ function?\n",
    "\n",
    "$$H_f(x, y) = \\begin{bmatrix} 2\\ln(y) + y^2e^{xy} & \\frac{2x}{y} + e^{xy}(1+xy) \\\\ \\frac{2x}{y} + e^{xy}(1+xy) & -\\frac{x^2}{y^2} + x^2e^{xy} - 6y \\end{bmatrix}$$\n",
    "\n",
    "*   (a) $f(x,y) = x^2 \\ln(y) + e^{xy} - y^3$\n",
    "*   (b) $f(x,y) = x^2 \\ln(y) + e^{x+y} - y^3$\n",
    "*   (c) $f(x,y) = x^3 \\ln(y) + e^{xy} - y^3$\n",
    "*   (d) $f(x,y) = x^2 \\ln(y) + e^{xy} - y^4$\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** Option A. Take the second derivative for all 4 $\\partial^2x$, $\\partial^2y$, $\\partial yx$, $\\partial xy$ and you will get option a matches.\n",
    "\n",
    "**Problem 8. Select All That Apply**\n",
    "\n",
    "Saket did not pay attention during lecture and did not split the data into a training set and testing set and instead used all the data to train and test a given model. What is the consequence of not splitting the data into a training set and testing set?\n",
    "\n",
    "*   (a) Nothing, Saket knows what he is doing\n",
    "*   (b) The model may overfit and perform poorly on unseen data\n",
    "*   (c) The model may underfit and perform poorly on unseen data\n",
    "*   (d) Saket will overestimate the performance of his model on unseen data\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:** Choice B and D. If we do not split the data into a training set and testing set, then the model will test on seen data. thus resulting in overfitting and a lower error rate than intended. So the performance is not going to be as expected.\n",
    "\n",
    "**Problem 9. One Answer**\n",
    "\n",
    "Suppose you are designing a model that predicts whether or not a patient will be readmitted into a hospital within a month. The hospital provides a dataset with 25 clinical features per patient (like age, gender, and blood pressure), but not all of them might be relevant to readmission. The data is such that it's possible to draw a straight line (or a higher-dimensional hyperplane) that perfectly divides the patients who were readmitted from those who were not. Which is the most appropriate choice of procedure to train a model in this scenario?\n",
    "\n",
    "*   (a) train an L1 regularized Logistic Regression, then retrain with unregularized Logistic Regression\n",
    "*   (b) train an L2 regularized Logistic Regression, then retrain with unregularized Logistic Regression\n",
    "*   (c) train an L1 regularized Logistic Regression, then retrain with L2 regularized Logistic Regression\n",
    "*   (d) train an L1 regularized Logistic Regression\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Train with L1 first, for feature selection. And then retrain with L2 regularization. L2 regularization is required an unregularized model will overfit linearly separable data.\n",
    "\n",
    "**Problem 10.**\n",
    "\n",
    "Describe a problem that might occur if you're training a Logistic Regression model and the data is linearly separable. Around 1-3 sentences.\n",
    "\n",
    "**Explanation:** The model will overfit extremely as the magnitudes of the weights increase towards infinity.\n",
    "\n",
    "**Problem 11. One Answer**\n",
    "\n",
    "Suppose we standardize a given dataset. The optimal bias term will be 0 in least-squares linear regression.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** The offset is the average y value.\n",
    "\n",
    "**Problem 12. One Answer**\n",
    "\n",
    "Let $f,g: \\mathbb{R} \\to \\mathbb{R}$ be convex. Which of the following functions is always convex?\n",
    "\n",
    "*   (a) $h(x) = f(x) \\cdot g(x)$\n",
    "*   (b) $h(x) = f \\circ g(x)$\n",
    "*   (c) $h(x) = \\min(f(x), g(x))$\n",
    "*   (d) $h(x) = \\max(f(x), g(x))$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** Pointwise maximum preserves convexity (see section 5)\n",
    "\n",
    "**Problem 13. One Answer**\n",
    "\n",
    "Given a small enough learning rate, gradient descent will converge to the global minima.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** This is false because non-convex functions can have multiple local minima / saddle points and GD may converge to one of those.\n",
    "\n",
    "**Problem 14.**\n",
    "\n",
    "This is the equation for the bias-variance tradeoff. $\\eta$ is the \"squared-error-optimal\" predictor. $D$ is a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ sampled from $P_{XY}$. $\\hat{f}_D \\in F$ is the learned least-squares predictor for some function class $F$.\n",
    "\n",
    "Which terms correspond with which concepts? Write the number of the term next to the concept you think it corresponds with.\n",
    "\n",
    "$$E_{Y|X}[E_D[(Y - \\hat{f}_D(x))^2]|X = x] = E_{Y|X}[(Y - \\eta(x))^2|X = x]$$\n",
    "\n",
    "**Term 1:** $E_{Y|X}[(Y - \\eta(x))^2|X = x]$\n",
    "\n",
    "**Term 2:** $+ (\\eta(x) - E_D[\\hat{f}_D(x)])^2$\n",
    "\n",
    "**Term 3:** $+ E_D[(E_D[\\hat{f}_D(x)] - \\hat{f}_D(x))^2]$\n",
    "\n",
    "**Variance:** _____\n",
    "\n",
    "**Bias:** _____\n",
    "\n",
    "**Irreducible error:** _____\n",
    "\n",
    "**Explanation:** Variance: term 3. Bias: term 2 Irreducible error: term 1.\n",
    "\n",
    "**Problem 15. One Answer**\n",
    "\n",
    "Which of the following is an advantage of using ridge regression over unregularized linear regression?\n",
    "\n",
    "*   (a) The ridge objective is concave\n",
    "*   (b) The ridge objective is convex\n",
    "*   (c) The ridge objective always has a unique solution\n",
    "*   (d) The ridge objective has a closed-form solution\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** (a) Ridge objective is not concave (b) Unregularized linear regression objective is convex as well. (c) Unregularized linear regression does not always have a unique solution, L2 penalty fixes this. (d) Unregularized linear regression has a closed form solution as well\n",
    "\n",
    "**Problem 16. One Answer**\n",
    "\n",
    "True/False: Lasso Regression uses the square of the L2 norm while Ridge Regression uses the L1 Norm.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of Lasso vs Ridge regression** and their regularization penalties.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**The statement is FALSE - it has the penalties backwards:**\n",
    "\n",
    "**1. Lasso Regression:**\n",
    "- **Uses L1 norm:** $||w||_1 = \\sum_{i=1}^{d} |w_i|$\n",
    "- **Creates sparsity** by setting coefficients to exactly zero\n",
    "- **Feature selection** capability\n",
    "\n",
    "**2. Ridge Regression:**\n",
    "- **Uses L2 norm squared:** $||w||_2^2 = \\sum_{i=1}^{d} w_i^2$\n",
    "- **Shrinks coefficients** toward zero but rarely to exactly zero\n",
    "- **Prevents overfitting** without feature selection\n",
    "\n",
    "**3. Mathematical comparison:**\n",
    "\n",
    "**Lasso objective:**\n",
    "$\\min_w \\sum_{i=1}^{n} (y_i - x_i^T w)^2 + \\lambda ||w||_1$\n",
    "\n",
    "**Ridge objective:**\n",
    "$\\min_w \\sum_{i=1}^{n} (y_i - x_i^T w)^2 + \\lambda ||w||_2^2$\n",
    "\n",
    "**4. Key differences:**\n",
    "- **Lasso:** L1 penalty, sparse solutions\n",
    "- **Ridge:** L2 penalty, dense solutions\n",
    "- **Different optimization** properties\n",
    "- **Different use cases**\n",
    "\n",
    "**Key insight:** **Lasso uses L1 norm, Ridge uses L2 norm squared** - the statement has them reversed.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 17. One Answer**\n",
    "\n",
    "You have independent random variables $X, Y$ such that $X \\sim N(1,2)$ and $Y \\sim N(3,4)$. What is $\\text{Var}(5X+6Y+7)$?\n",
    "\n",
    "*   (a) 35\n",
    "*   (b) 34\n",
    "*   (c) 195\n",
    "*   (d) 194\n",
    "*   (e) Cannot be determined\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** Because the variables are independent, $\\text{Var}(5X+6Y+7)= \\text{Var}(5X) + \\text{Var}(6Y) + \\text{Var}(7)$. The variance of a constant is 0, so we remove that term to get $\\text{Var}(5X) + \\text{Var}(6Y)$. Moving a coefficient outside of the variance function squares it. So we have $25\\text{Var}(X) + 36\\text{Var}(Y)$. We plug in the given variances for $X$ and $Y$ to get $25 \\cdot 2 + 36 \\cdot 4 = 194$\n",
    "\n",
    "**Problem 18. One Answer**\n",
    "\n",
    "The objective function is $L(w) = ||Xw-Y||_2^2$. What is the gradient of $L(w)$ with respect to $w$?\n",
    "\n",
    "*   (a) $2Y^T (Xw - Y)$\n",
    "*   (b) $2X^T(X^T Xw - Y)$\n",
    "*   (c) $2X^T (Xw - Y)$\n",
    "*   (d) $2Y^T (X^T Xw - Y)$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** $\\nabla_w ||Xw - Y||_2^2 = \\nabla_w ((Xw - Y)^T (Xw - Y))$ $= \\nabla_w ((w^T X^T - Y^T)(Xw - Y))$ $= \\nabla_w (w^T X^T Xw - Y^T Xw - w^T X^T Y + Y^T Y)$ $= 2X^T Xw - X^T Y - X^T Y + 0$ $= 2X^T (Xw - Y)$\n",
    "\n",
    "**Problem 19. One Answer**\n",
    "\n",
    "Which of the following is true, when choosing to use Maximum Likelihood Estimation (MLE)?\n",
    "\n",
    "*   (a) MLE cannot be used if we do not know the exact distribution of our data.\n",
    "*   (b) MLE works well for any data distribution, so we do need knowledge of the true distribution.\n",
    "*   (c) MLE will produce unbiased estimates regardless of the data distribution or the likelihood function that we choose\n",
    "*   (d) MLE works even if the true distribution of our data isn't known. We can make an educated guess for the distribution of our data for our likelihood function.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** We often use MLE in situations where the true distribution of the data is not known. In MLE, we construct a likelihood function based on this chosen distribution and find the parameter values that maximize the probability of observing our data. While ideal to know the true distribution, MLE enables estimation through an educated guess, though the accuracy of the estimates depends on the appropriateness of our chosen distribution.\n",
    "\n",
    "**Problem 20. One Answer**\n",
    "\n",
    "Consider the function $f(a) = 5a^2 - 3a + 2$. You want to use gradient descent to find the unique minimum, which you know is at $a_* = 0.3$. If at time $t$ you arrive at the point $a_t = 3$, what value for the step size would bring you to $a_*$ at time $t+1$?\n",
    "\n",
    "*   (a) 0.001\n",
    "*   (b) 0.01\n",
    "*   (c) 0.1\n",
    "*   (d) 1\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Following the standard gradient descent update formula, we get $0.3 = 3 - \\eta \\cdot \\nabla f(a)$. $\\nabla f(a) = 10a - 3$, so $\\nabla f(3) = 27$. Plugging this in, we get $0.3 = 3 - \\eta \\cdot 27$. Solving this equation, we get $\\eta = 0.1$.\n",
    "\n",
    "**Problem 21. One Answer**\n",
    "\n",
    "Donovan is training some machine learning model, and is telling you about it. He needed to standardize the data, so he computed the mean and standard deviation of each feature in the entire dataset $X$ and applied the transformation correctly. He then created non-overlapping subsets of $X$ called $X_{train}$, $X_{validation}$, and $X_{test}$. To train, validate, and test their model respectively. In this setup, was there train/test leakage?\n",
    "\n",
    "*   (a) Yes\n",
    "*   (b) No\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** They standardized the whole dataset using information from the test set (as it is a subset of $X$), and this is a form of train/test leakage.\n",
    "\n",
    "**Problem 22.**\n",
    "\n",
    "The following plots show 3 data points and 3 models. The data is the same for all 3 models. Match the learned model to the equation used for linear regression.\n",
    "\n",
    "<img src=\"./plots.png\" width=\"550px\">\n",
    "\n",
    "$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$. Plot number: 3 (for $\\lambda > 0$)\n",
    "\n",
    "$\\hat{w} = (X^T X)^{-1} X^T y$. Plot number: 1\n",
    "\n",
    "$\\hat{w} = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T y$, where $\\tilde{X} = [X \\quad \\vec{1}]$. Plot number: 2\n",
    "\n",
    "**Explanation:** $\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$: plot 3, because it is overregularized.\n",
    "\n",
    "**Problem 23. One Answer**\n",
    "\n",
    "True/False: The training error is a better estimate of the true error than the cross-validation error.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of cross-validation** and its advantages over training error.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**Cross-validation error is a better estimate of true error than training error:**\n",
    "\n",
    "**1. Training error issues:**\n",
    "- **Optimistic bias** - model has seen all training data\n",
    "- **Doesn't reflect** generalization performance\n",
    "- **Can be misleading** for model selection\n",
    "- **Overfitting** not detected\n",
    "\n",
    "**2. Cross-validation advantages:**\n",
    "- **Uses unseen data** for evaluation\n",
    "- **Better estimate** of generalization performance\n",
    "- **More realistic** performance assessment\n",
    "- **Detects overfitting** patterns\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "- **Training error:** $E_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, \\hat{f}(x_i))$\n",
    "- **CV error:** $E_{\\text{CV}} = \\frac{1}{k} \\sum_{j=1}^{k} E_{\\text{val}}^{(j)}$\n",
    "- **CV error** closer to true generalization error\n",
    "- **Training error** underestimates true error\n",
    "\n",
    "**4. Why (a) is incorrect:**\n",
    "- **Training error** is overly optimistic\n",
    "- **Doesn't account** for overfitting\n",
    "- **Poor estimate** of true performance\n",
    "- **CV error** is more reliable\n",
    "\n",
    "**Key insight:** **Cross-validation error** provides **better estimates** of true generalization performance than **training error**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 24. Select All That Apply**\n",
    "\n",
    "Let $f: \\mathbb{R} \\to \\mathbb{R}$ be a continuous, smooth function whose derivative $f'(x)$ is also continuous. Suppose $f$ has a unique global minimum $x^* \\in (-\\infty, \\infty)$, and you are using gradient descent to find $x^*$. You fix some $x^{(0)} \\in \\mathbb{R}$ and step size $\\eta > 0$, and run $x^{(t)} = x^{(t-1)} - \\eta f'(x^{(t-1)})$ repeatedly. Which of the following statements are true?\n",
    "\n",
    "*   (a) Gradient descent is sure to converge, to some value, for any step size $\\eta > 0$.\n",
    "*   (b) If $f$ has a local minimum $x'$ different from the global one, i.e., $x' \\neq x^*$, and $x^{(t)} = x'$ for some $t$, gradient descent will not converge to $x^*$.\n",
    "*   (c) Assuming gradient descent converges, it converges to $x^*$ if and only if $f$ is convex.\n",
    "*   (d) If, additionally, $f$ is the objective function of logistic regression, and gradient descent converges, then it converges to $x^*$.\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:** A is false because for a large enough step size, gradient descent may not converge. B is correct because $f'(x') = 0$, so gradient descent will never move from a local minimum. C is false because you could \"accidentally\" initialize GD at $x^*$ even if $f$ is non-convex. D is correct because the objective of logistic regression is convex.\n",
    "\n",
    "**Problem 25.**\n",
    "\n",
    "What is the tradeoff between the size of the validation set and the size of the training set? Around 1-3 sentences.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of validation set sizing** and the trade-off with training data.\n",
    "\n",
    "**Why this trade-off exists:**\n",
    "\n",
    "**1. Larger validation set benefits:**\n",
    "- **Better estimate** of model performance\n",
    "- **More reliable** performance assessment\n",
    "- **Reduced variance** in error estimates\n",
    "- **More confidence** in model selection\n",
    "\n",
    "**2. Larger validation set costs:**\n",
    "- **Less training data** available\n",
    "- **Reduced model capacity** to learn\n",
    "- **Potential underfitting** due to insufficient training data\n",
    "- **Worse model performance** due to data scarcity\n",
    "\n",
    "**3. Mathematical intuition:**\n",
    "- **Total data:** $N = N_{\\text{train}} + N_{\\text{val}}$\n",
    "- **Larger $N_{\\text{val}}$** = smaller $N_{\\text{train}}$\n",
    "- **Validation error variance:** $\\propto \\frac{1}{N_{\\text{val}}}$\n",
    "- **Training error variance:** $\\propto \\frac{1}{N_{\\text{train}}}$\n",
    "\n",
    "**4. Optimal balance:**\n",
    "- **Typical split:** 70-80% training, 20-30% validation\n",
    "- **Depends on** dataset size and model complexity\n",
    "- **Cross-validation** can help with small datasets\n",
    "- **Domain knowledge** guides optimal split\n",
    "\n",
    "**Key insight:** **Validation set size** trades **estimation accuracy** against **training data availability** - larger validation sets provide better performance estimates but reduce training data.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 26.**\n",
    "\n",
    "Consider $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$. Suppose $\\hat{w} = \\arg \\min_w \\|Xw - y\\|_2$ has a unique solution. Fill in the blank for the following vector spaces. Write NA if the there is not enough information to determine the answer.\n",
    "\n",
    "Col(X) = NA\n",
    "\n",
    "Row(X) = $\\mathbb{R}^d$\n",
    "\n",
    "Null(X) = $\\{0\\}$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of linear algebra** and matrix properties in linear regression.\n",
    "\n",
    "**Why these answers are correct:**\n",
    "\n",
    "**1. Unique solution implies full rank:**\n",
    "- **$\\hat{w} = \\arg \\min_w \\|Xw - y\\|_2$** has unique solution\n",
    "- **$X$ must be full rank** (rank = $d$)\n",
    "- **$n \\geq d$** (more data points than features)\n",
    "- **No linear dependencies** in columns\n",
    "\n",
    "**2. Row space analysis:**\n",
    "- **Row(X) = $\\mathbb{R}^d$** because $X$ is full rank\n",
    "- **All $d$ rows** are linearly independent\n",
    "- **Row space spans** entire $\\mathbb{R}^d$ space\n",
    "- **No missing dimensions**\n",
    "\n",
    "**3. Null space analysis:**\n",
    "- **Null(X) = $\\{0\\}$** because $X$ is full rank\n",
    "- **Only zero vector** maps to zero\n",
    "- **No non-trivial** solutions to $Xw = 0$\n",
    "- **Unique solution** guaranteed\n",
    "\n",
    "**4. Column space analysis:**\n",
    "- **Col(X) = NA** because $n \\geq d$ but not necessarily $n = d$\n",
    "- **Column space** is a subspace of $\\mathbb{R}^n$\n",
    "- **Cannot guarantee** it spans entire $\\mathbb{R}^n$\n",
    "- **Depends on** specific values in $X$\n",
    "\n",
    "**Key insight:** **Full rank matrix** with **unique solution** determines **row space** and **null space**, but **column space** depends on **data dimensions**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 27.**\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ where $f(x, y, z) = xy + x^2 \\ln(z) + e^{yz}$. Calculate the gradient of $f$.\n",
    "\n",
    "**Gradient =**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of gradient computation** for multivariate functions.\n",
    "\n",
    "**Step-by-step gradient calculation:**\n",
    "\n",
    "**1. Function breakdown:**\n",
    "$f(x, y, z) = xy + x^2 \\ln(z) + e^{yz}$\n",
    "\n",
    "**2. Partial derivatives:**\n",
    "\n",
    "**$\\frac{\\partial f}{\\partial x}$:**\n",
    "- **$xy$ term:** $\\frac{\\partial}{\\partial x}(xy) = y$\n",
    "- **$x^2 \\ln(z)$ term:** $\\frac{\\partial}{\\partial x}(x^2 \\ln(z)) = 2x \\ln(z)$\n",
    "- **$e^{yz}$ term:** $\\frac{\\partial}{\\partial x}(e^{yz}) = 0$\n",
    "- **Total:** $y + 2x \\ln(z)$\n",
    "\n",
    "**$\\frac{\\partial f}{\\partial y}$:**\n",
    "- **$xy$ term:** $\\frac{\\partial}{\\partial y}(xy) = x$\n",
    "- **$x^2 \\ln(z)$ term:** $\\frac{\\partial}{\\partial y}(x^2 \\ln(z)) = 0$\n",
    "- **$e^{yz}$ term:** $\\frac{\\partial}{\\partial y}(e^{yz}) = z e^{yz}$\n",
    "- **Total:** $x + z e^{yz}$\n",
    "\n",
    "**$\\frac{\\partial f}{\\partial z}$:**\n",
    "- **$xy$ term:** $\\frac{\\partial}{\\partial z}(xy) = 0$\n",
    "- **$x^2 \\ln(z)$ term:** $\\frac{\\partial}{\\partial z}(x^2 \\ln(z)) = \\frac{x^2}{z}$\n",
    "- **$e^{yz}$ term:** $\\frac{\\partial}{\\partial z}(e^{yz}) = y e^{yz}$\n",
    "- **Total:** $\\frac{x^2}{z} + y e^{yz}$\n",
    "\n",
    "**3. Gradient vector:**\n",
    "$$\\nabla f(x, y, z) = \\begin{pmatrix} y + 2x \\ln(z) \\\\ x + z e^{yz} \\\\ \\frac{x^2}{z} + y e^{yz} \\end{pmatrix}$$\n",
    "\n",
    "**Key insight:** **Gradient computation** requires **partial derivatives** with respect to each variable, treating other variables as constants.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 28.**\n",
    "\n",
    "Describe a scenario where one would choose to use Ridge regression over Lasso regression. Around 1-4 sentences.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of Ridge vs Lasso regression** and when to choose each.\n",
    "\n",
    "**Why Ridge regression is preferred in certain scenarios:**\n",
    "\n",
    "**1. When all features are important:**\n",
    "- **Ridge keeps all features** with reduced coefficients\n",
    "- **Lasso removes features** by setting coefficients to zero\n",
    "- **Feature preservation** is crucial in some domains\n",
    "- **Domain knowledge** indicates all features matter\n",
    "\n",
    "**2. High-dimensional data with correlations:**\n",
    "- **Gene expression data** - many related genes\n",
    "- **Financial data** - correlated market indicators\n",
    "- **Image features** - correlated pixel values\n",
    "- **Ridge handles correlations** better than Lasso\n",
    "\n",
    "**3. More features than data points ($d > n$):**\n",
    "- **Ridge provides unique solutions** even when $d > n$\n",
    "- **Lasso may not work** well in this regime\n",
    "- **Regularization** prevents overfitting\n",
    "- **Stable solutions** guaranteed\n",
    "\n",
    "**4. When interpretability is less important:**\n",
    "- **Ridge provides dense solutions** (all coefficients non-zero)\n",
    "- **Lasso provides sparse solutions** (some coefficients zero)\n",
    "- **Feature selection** not the primary goal\n",
    "- **Prediction accuracy** is priority\n",
    "\n",
    "**Key insight:** **Ridge regression** is preferred when **feature preservation** and **handling correlations** are more important than **feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 29.**\n",
    "\n",
    "Answer the following questions about the Softmax function.\n",
    "\n",
    "**(a) Explain how the Softmax function transforms an input vector (logits) and why it is suitable for multi-class classification.**\n",
    "\n",
    "**Explanation:** The Softmax function transforms an input vector into a probability distribution over $K$ classes by exponentiating each term and dividing it by the sum of all the exponentiated values in the vector.\n",
    "\n",
    "$$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "This is helpful for multiclass classification tasks because it shows the model's uncertainty over multiple classes in a normalized way.\n",
    "\n",
    "**(b) Suppose a model outputs the following values/logits for a 3-class classification problem.**\n",
    "\n",
    "$z = [2, 1, 5]$\n",
    "\n",
    "**Compute the softmax probabilities. You can leave the values in terms of exponentiated numbers.**\n",
    "\n",
    "**Softmax(z) = [ , , ]**\n",
    "\n",
    "**Explanation:** $\\sum_{j=1}^K e^{z_j} = e^2 + e^1 + e^5$\n",
    "\n",
    "**Softmax(z) = $\\left[ \\frac{e^2}{e^2+e^1+e^5}, \\frac{e^1}{e^2+e^1+e^5}, \\frac{e^5}{e^2+e^1+e^5} \\right]$**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
