{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1aab881-25fc-4854-b15d-0c963000f383",
   "metadata": {},
   "source": [
    "# Practice 8 Solutions\n",
    "\n",
    "**1. One Answer**\n",
    "\n",
    "Saket is building a classification model from a dataset with two classes of student reactions to the jokes he makes during section: Funny jokes (F) and Dull Jokes (D). The probability of a randomly selected joke being funny is 0.3. The probability the model makes the correct classification given that the joke is dull is 0.7, and the probability the model makes the correct classification given that a joke is funny is 0.2. What is the probability that a randomly selected joke is dull given that it has been classified as dull?\n",
    "\n",
    "*   (a) $\\frac{42}{73}$\n",
    "*   (b) $\\frac{49}{73}$\n",
    "*   (c) $\\frac{21}{73}$\n",
    "*   (d) 1\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** The answer is (b), which is $\\frac{49}{73}$. Let A be the event that the sample is dull. Let B be the event that the sample is classified as dull. We have $P(A^c) = 0.3$ and $P(A) = 1 - P(A^c) = 1 - 0.3 = 0.7$. We have $P(B|A) = 0.7$ and $P(B^c|A^c) = 0.2$. (Note that $P(B|A^c) = 1 - P(B^c|A^c) = 1 - 0.2 = 0.8$) We want to calculate $P(A|B)$ here using Bayes Rule and the Law of Total Probability. $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c) = 0.7 \\cdot 0.7 + 0.8 \\cdot 0.3 = 0.73$ $P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{0.7 \\cdot 0.7}{0.73} = \\frac{0.49}{0.73} = \\frac{49}{73}$ Here, our answer is $P(A|B) = \\frac{49}{73}$, which is (b).\n",
    "\n",
    "**2. One Answer**\n",
    "\n",
    "Suppose we train a model $f(x) = x^T \\hat{w}$ on dataset $\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^n$, by optimizing the following objective:\n",
    "\n",
    "$$\\hat{w} = \\arg \\min_w \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\log k ||w||_2^2$$\n",
    "\n",
    "for $k > 1$. As $k$ increases, what likely happens to the variance of our model?\n",
    "\n",
    "*   (a) The variance of our model increases.\n",
    "*   (b) The variance of our model decreases.\n",
    "*   (c) The variance of our model remains unaffected.\n",
    "*   (d) The variance of our model does not change in a predictable way.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** As $k$ increases, the penalty term goes to zero, thus increasing the variance of our estimator.\n",
    "\n",
    "**3. One Answer**\n",
    "\n",
    "You are building a classification model for your favorite soccer team to determine whether a penalty kick will result in a goal or not, and your data set contains 300 positive examples (resulted in a goal) and 200 negative examples (did not result in a goal). After training, you find that your model has an accuracy of 70% and misclassifies 15% of negative examples as positive. What is the probability that your model will misclassify a positive example as negative?\n",
    "\n",
    "*   (a) 15%\n",
    "*   (b) 24%\n",
    "*   (c) 30%\n",
    "*   (d) 40%\n",
    "*   (e) Cannot be determined.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** We have 500 examples in total, and $500 * 70\\% = 350$ were correctly classified. The rest (150) were incorrectly classified. We also know that $200 * 15\\% = 30$ negative examples were misclassified. This leaves us with $150 - 30 = 120$ misclassified examples, and we know these must all be positive examples that were misclassified. This means that $\\frac{120}{300} = \\frac{2}{5} = 40\\%$ of positive examples were misclassified as negative.\n",
    "\n",
    "**4. One Answer**\n",
    "\n",
    "When using the least squares method for linear regression, outliers would have a minimal impact since the least squares method averages out their effects.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** Outliers would cause the regression line to skew, thus leading to a representation that may not be an accurate representation of the data.\n",
    "\n",
    "**5. Select All That Apply**\n",
    "\n",
    "In the standard MLE derivation for a parameter in a probability distribution (like the ones we saw in class), why do we apply the logarithm to our equation?\n",
    "\n",
    "*   (a) The function is monotonically increasing and therefore does not change the result of our optimization objective.\n",
    "*   (b) The function is concave and therefore does not change the result of our optimization objective.\n",
    "*   (c) The function is monotonically decreasing and therefore allows us to change our \"argmax\" to an \"argmin.\"\n",
    "*   (d) Applying the logarithm allows us to convert products ($\\Pi$) into sums ($\\Sigma$), letting us calculate derivatives easier.\n",
    "\n",
    "**Correct answers:** (a), (d)\n",
    "\n",
    "**Explanation:** The logarithm function is monotonically increasing, which makes (A) true and (C) false. (B) is false. the logarithm function is concave, but that is not why it doesn't change the result of our optimization. (D) is true due to properties of logarithms and derivatives.\n",
    "\n",
    "**6. One Answer**\n",
    "\n",
    "After training a model, your model has a low train error and a high test error. Which of the following can be inferred?\n",
    "\n",
    "*   (a) The model is underfitting.\n",
    "*   (b) The model will generalize well because it has low bias.\n",
    "*   (c) Training on more data will likely increase the model's performance on unseen data.\n",
    "*   (d) Training on more highly-informative features will decrease the test error.\n",
    "*   (e) Reducing model complexity will reduce the irreducible error.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** A is incorrect since the model has a low, not high, bias. B is also incorrect since a low bias alone doesn't guarantee that the model will generalize to unseen data. C is correct since more training data generally decreases the variance. D is incorrect since increasing model complexity by increasing the number of features in the dataset will likely increase the variance. E is incorrect since irreducible error comes from underlying noise in the dataset and changing any factor of the model will not change this error.\n",
    "\n",
    "**7. One Answer**\n",
    "\n",
    "Given the following Hessian Matrix, which of the following could be the original $f(x,y)$ function?\n",
    "\n",
    "$$H_f(x, y) = \\begin{bmatrix} 2\\ln(y) + y^2e^{xy} & \\frac{2x}{y} + e^{xy}(1+xy) \\\\ \\frac{2x}{y} + e^{xy}(1+xy) & -\\frac{x^2}{y^2} + x^2e^{xy} - 6y \\end{bmatrix}$$\n",
    "\n",
    "*   (a) $f(x,y) = x^2 \\ln(y) + e^{xy} - y^3$\n",
    "*   (b) $f(x,y) = x^2 \\ln(y) + e^{x+y} - y^3$\n",
    "*   (c) $f(x,y) = x^3 \\ln(y) + e^{xy} - y^3$\n",
    "*   (d) $f(x,y) = x^2 \\ln(y) + e^{xy} - y^4$\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** Option A. Take the second derivative for all 4 $\\partial^2x$, $\\partial^2y$, $\\partial yx$, $\\partial xy$ and you will get option a matches.\n",
    "\n",
    "**8. Select All That Apply**\n",
    "\n",
    "Saket did not pay attention during lecture and did not split the data into a training set and testing set and instead used all the data to train and test a given model. What is the consequence of not splitting the data into a training set and testing set?\n",
    "\n",
    "*   (a) Nothing, Saket knows what he is doing\n",
    "*   (b) The model may overfit and perform poorly on unseen data\n",
    "*   (c) The model may underfit and perform poorly on unseen data\n",
    "*   (d) Saket will overestimate the performance of his model on unseen data\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:** Choice B and D. If we do not split the data into a training set and testing set, then the model will test on seen data. thus resulting in overfitting and a lower error rate than intended. So the performance is not going to be as expected.\n",
    "\n",
    "**9. One Answer**\n",
    "\n",
    "Suppose you are designing a model that predicts whether or not a patient will be readmitted into a hospital within a month. The hospital provides a dataset with 25 clinical features per patient (like age, gender, and blood pressure), but not all of them might be relevant to readmission. The data is such that it's possible to draw a straight line (or a higher-dimensional hyperplane) that perfectly divides the patients who were readmitted from those who were not. Which is the most appropriate choice of procedure to train a model in this scenario?\n",
    "\n",
    "*   (a) train an L1 regularized Logistic Regression, then retrain with unregularized Logistic Regression\n",
    "*   (b) train an L2 regularized Logistic Regression, then retrain with unregularized Logistic Regression\n",
    "*   (c) train an L1 regularized Logistic Regression, then retrain with L2 regularized Logistic Regression\n",
    "*   (d) train an L1 regularized Logistic Regression\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Train with L1 first, for feature selection. And then retrain with L2 regularization. L2 regularization is required an unregularized model will overfit linearly separable data.\n",
    "\n",
    "**10.**\n",
    "\n",
    "Describe a problem that might occur if you're training a Logistic Regression model and the data is linearly separable. Around 1-3 sentences.\n",
    "\n",
    "**Explanation:** The model will overfit extremely as the magnitudes of the weights increase towards infinity.\n",
    "\n",
    "**11. One Answer**\n",
    "\n",
    "Suppose we standardize a given dataset. The optimal bias term will be 0 in least-squares linear regression.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** The offset is the average y value.\n",
    "\n",
    "**12. One Answer**\n",
    "\n",
    "Let $f,g: \\mathbb{R} \\to \\mathbb{R}$ be convex. Which of the following functions is always convex?\n",
    "\n",
    "*   (a) $h(x) = f(x) \\cdot g(x)$\n",
    "*   (b) $h(x) = f \\circ g(x)$\n",
    "*   (c) $h(x) = \\min(f(x), g(x))$\n",
    "*   (d) $h(x) = \\max(f(x), g(x))$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** Pointwise maximum preserves convexity (see section 5)\n",
    "\n",
    "**13. One Answer**\n",
    "\n",
    "Given a small enough learning rate, gradient descent will converge to the global minima.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** This is false because non-convex functions can have multiple local minima / saddle points and GD may converge to one of those.\n",
    "\n",
    "**14.**\n",
    "\n",
    "This is the equation for the bias-variance tradeoff. $\\eta$ is the \"squared-error-optimal\" predictor. $D$ is a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ sampled from $P_{XY}$. $\\hat{f}_D \\in F$ is the learned least-squares predictor for some function class $F$.\n",
    "\n",
    "Which terms correspond with which concepts? Write the number of the term next to the concept you think it corresponds with.\n",
    "\n",
    "$$E_{Y|X}[E_D[(Y - \\hat{f}_D(x))^2]|X = x] = E_{Y|X}[(Y - \\eta(x))^2|X = x]$$\n",
    "\n",
    "**Term 1:** $E_{Y|X}[(Y - \\eta(x))^2|X = x]$\n",
    "\n",
    "**Term 2:** $+ (\\eta(x) - E_D[\\hat{f}_D(x)])^2$\n",
    "\n",
    "**Term 3:** $+ E_D[(E_D[\\hat{f}_D(x)] - \\hat{f}_D(x))^2]$\n",
    "\n",
    "**Variance:** _____\n",
    "\n",
    "**Bias:** _____\n",
    "\n",
    "**Irreducible error:** _____\n",
    "\n",
    "**Explanation:** Variance: term 3. Bias: term 2 Irreducible error: term 1.\n",
    "\n",
    "**15. One Answer**\n",
    "\n",
    "Which of the following is an advantage of using ridge regression over unregularized linear regression?\n",
    "\n",
    "*   (a) The ridge objective is concave\n",
    "*   (b) The ridge objective is convex\n",
    "*   (c) The ridge objective always has a unique solution\n",
    "*   (d) The ridge objective has a closed-form solution\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** (a) Ridge objective is not concave (b) Unregularized linear regression objective is convex as well. (c) Unregularized linear regression does not always have a unique solution, L2 penalty fixes this. (d) Unregularized linear regression has a closed form solution as well\n",
    "\n",
    "**16. One Answer**\n",
    "\n",
    "True/False: Lasso Regression uses the square of the L2 norm while Ridge Regression uses the L1 Norm.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**17. One Answer**\n",
    "\n",
    "You have independent random variables $X, Y$ such that $X \\sim N(1,2)$ and $Y \\sim N(3,4)$. What is $\\text{Var}(5X+6Y+7)$?\n",
    "\n",
    "*   (a) 35\n",
    "*   (b) 34\n",
    "*   (c) 195\n",
    "*   (d) 194\n",
    "*   (e) Cannot be determined\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** Because the variables are independent, $\\text{Var}(5X+6Y+7)= \\text{Var}(5X) + \\text{Var}(6Y) + \\text{Var}(7)$. The variance of a constant is 0, so we remove that term to get $\\text{Var}(5X) + \\text{Var}(6Y)$. Moving a coefficient outside of the variance function squares it. So we have $25\\text{Var}(X) + 36\\text{Var}(Y)$. We plug in the given variances for $X$ and $Y$ to get $25 \\cdot 2 + 36 \\cdot 4 = 194$\n",
    "\n",
    "**18. One Answer**\n",
    "\n",
    "The objective function is $L(w) = ||Xw-Y||_2^2$. What is the gradient of $L(w)$ with respect to $w$?\n",
    "\n",
    "*   (a) $2Y^T (Xw - Y)$\n",
    "*   (b) $2X^T(X^T Xw - Y)$\n",
    "*   (c) $2X^T (Xw - Y)$\n",
    "*   (d) $2Y^T (X^T Xw - Y)$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** $\\nabla_w ||Xw - Y||_2^2 = \\nabla_w ((Xw - Y)^T (Xw - Y))$ $= \\nabla_w ((w^T X^T - Y^T)(Xw - Y))$ $= \\nabla_w (w^T X^T Xw - Y^T Xw - w^T X^T Y + Y^T Y)$ $= 2X^T Xw - X^T Y - X^T Y + 0$ $= 2X^T (Xw - Y)$\n",
    "\n",
    "**19. One Answer**\n",
    "\n",
    "Which of the following is true, when choosing to use Maximum Likelihood Estimation (MLE)?\n",
    "\n",
    "*   (a) MLE cannot be used if we do not know the exact distribution of our data.\n",
    "*   (b) MLE works well for any data distribution, so we do need knowledge of the true distribution.\n",
    "*   (c) MLE will produce unbiased estimates regardless of the data distribution or the likelihood function that we choose\n",
    "*   (d) MLE works even if the true distribution of our data isn't known. We can make an educated guess for the distribution of our data for our likelihood function.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** We often use MLE in situations where the true distribution of the data is not known. In MLE, we construct a likelihood function based on this chosen distribution and find the parameter values that maximize the probability of observing our data. While ideal to know the true distribution, MLE enables estimation through an educated guess, though the accuracy of the estimates depends on the appropriateness of our chosen distribution.\n",
    "\n",
    "**20. One Answer**\n",
    "\n",
    "Consider the function $f(a) = 5a^2 - 3a + 2$. You want to use gradient descent to find the unique minimum, which you know is at $a_* = 0.3$. If at time $t$ you arrive at the point $a_t = 3$, what value for the step size would bring you to $a_*$ at time $t+1$?\n",
    "\n",
    "*   (a) 0.001\n",
    "*   (b) 0.01\n",
    "*   (c) 0.1\n",
    "*   (d) 1\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Following the standard gradient descent update formula, we get $0.3 = 3 - \\eta \\cdot \\nabla f(a)$. $\\nabla f(a) = 10a - 3$, so $\\nabla f(3) = 27$. Plugging this in, we get $0.3 = 3 - \\eta \\cdot 27$. Solving this equation, we get $\\eta = 0.1$.\n",
    "\n",
    "**21. One Answer**\n",
    "\n",
    "Donovan is training some machine learning model, and is telling you about it. He needed to standardize the data, so he computed the mean and standard deviation of each feature in the entire dataset $X$ and applied the transformation correctly. He then created non-overlapping subsets of $X$ called $X_{train}$, $X_{validation}$, and $X_{test}$. To train, validate, and test their model respectively. In this setup, was there train/test leakage?\n",
    "\n",
    "*   (a) Yes\n",
    "*   (b) No\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** They standardized the whole dataset using information from the test set (as it is a subset of $X$), and this is a form of train/test leakage.\n",
    "\n",
    "**22.**\n",
    "\n",
    "The following plots show 3 data points and 3 models. The data is the same for all 3 models. Match the learned model to the equation used for linear regression.\n",
    "\n",
    "<img src=\"./plots.png\" width=\"550px\">\n",
    "\n",
    "$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$. Plot number: 3 (for $\\lambda > 0$)\n",
    "\n",
    "$\\hat{w} = (X^T X)^{-1} X^T y$. Plot number: 1\n",
    "\n",
    "$\\hat{w} = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T y$, where $\\tilde{X} = [X \\quad \\vec{1}]$. Plot number: 2\n",
    "\n",
    "**Explanation:** $\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$: plot 3, because it is overregularized.\n",
    "\n",
    "**23. One Answer**\n",
    "\n",
    "True/False: The training error is a better estimate of the true error than the cross-validation error.\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** Cross validation is better and closer to true error since it deals with somewhat unseen data.\n",
    "\n",
    "**24. Select All That Apply**\n",
    "\n",
    "Let $f: \\mathbb{R} \\to \\mathbb{R}$ be a continuous, smooth function whose derivative $f'(x)$ is also continuous. Suppose $f$ has a unique global minimum $x^* \\in (-\\infty, \\infty)$, and you are using gradient descent to find $x^*$. You fix some $x^{(0)} \\in \\mathbb{R}$ and step size $\\eta > 0$, and run $x^{(t)} = x^{(t-1)} - \\eta f'(x^{(t-1)})$ repeatedly. Which of the following statements are true?\n",
    "\n",
    "*   (a) Gradient descent is sure to converge, to some value, for any step size $\\eta > 0$.\n",
    "*   (b) If $f$ has a local minimum $x'$ different from the global one, i.e., $x' \\neq x^*$, and $x^{(t)} = x'$ for some $t$, gradient descent will not converge to $x^*$.\n",
    "*   (c) Assuming gradient descent converges, it converges to $x^*$ if and only if $f$ is convex.\n",
    "*   (d) If, additionally, $f$ is the objective function of logistic regression, and gradient descent converges, then it converges to $x^*$.\n",
    "\n",
    "**Correct answers:** (b), (d)\n",
    "\n",
    "**Explanation:** A is false because for a large enough step size, gradient descent may not converge. B is correct because $f'(x') = 0$, so gradient descent will never move from a local minimum. C is false because you could \"accidentally\" initialize GD at $x^*$ even if $f$ is non-convex. D is correct because the objective of logistic regression is convex.\n",
    "\n",
    "**25.**\n",
    "\n",
    "What is the tradeoff between the size of the validation set and the size of the training set? Around 1-3 sentences.\n",
    "\n",
    "**Explanation:** Larger validation set means a better estimate of performance on unseen data. But at the cost of lost training data.\n",
    "\n",
    "**26.**\n",
    "\n",
    "Consider $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$. Suppose $\\hat{w} = \\arg \\min_w \\|Xw - y\\|_2$ has a unique solution. Fill in the blank for the following vector spaces. Write NA if the there is not enough information to determine the answer.\n",
    "\n",
    "Col(X) = NA\n",
    "\n",
    "Row(X) = $\\mathbb{R}^d$\n",
    "\n",
    "Null(X) = $\\{0\\}$\n",
    "\n",
    "**Explanation:** Since in this case the linear regression objective has a unique solution, X must be full rank as well as $n \\ge d$, so Row(X) = $\\mathbb{R}^d$ and Null(X) = $\\{0\\}$. Since $n \\ge d$, we are unable to guarantee anything about Col(X) other than the fact that it is a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "**27.**\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ where $f(x, y, z) = xy + x^2 \\ln(z) + e^{yz}$. Calculate the gradient of $f$.\n",
    "\n",
    "**Gradient =**\n",
    "\n",
    "**Explanation:**\n",
    "$$\\nabla f(x) = \\begin{pmatrix} y + 2x \\ln(z) \\\\ x + z e^{yz} \\\\ \\frac{x^2}{z} + y e^{yz} \\end{pmatrix}$$\n",
    "\n",
    "**28.**\n",
    "\n",
    "Describe a scenario where one would choose to use Ridge regression over Lasso regression. Around 1-4 sentences.\n",
    "\n",
    "**Explanation:**\n",
    "Ridge regression is better when all features are important and you don't want to remove any of them. For example, if you are predicting something using many related variables (like gene data), Ridge helps by keeping all the features and just shrinking their values, instead of setting some to zero like Lasso does. It's also useful when there are more features than data points or when features are highly correlated.\n",
    "\n",
    "**29.**\n",
    "\n",
    "Answer the following questions about the Softmax function.\n",
    "\n",
    "**(a) Explain how the Softmax function transforms an input vector (logits) and why it is suitable for multi-class classification.**\n",
    "\n",
    "**Explanation:** The Softmax function transforms an input vector into a probability distribution over $K$ classes by exponentiating each term and dividing it by the sum of all the exponentiated values in the vector.\n",
    "\n",
    "$$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "This is helpful for multiclass classification tasks because it shows the model's uncertainty over multiple classes in a normalized way.\n",
    "\n",
    "**(b) Suppose a model outputs the following values/logits for a 3-class classification problem.**\n",
    "\n",
    "$z = [2, 1, 5]$\n",
    "\n",
    "**Compute the softmax probabilities. You can leave the values in terms of exponentiated numbers.**\n",
    "\n",
    "**Softmax(z) = [ , , ]**\n",
    "\n",
    "**Explanation:** $\\sum_{j=1}^K e^{z_j} = e^2 + e^1 + e^5$\n",
    "\n",
    "**Softmax(z) = $\\left[ \\frac{e^2}{e^2+e^1+e^5}, \\frac{e^1}{e^2+e^1+e^5}, \\frac{e^5}{e^2+e^1+e^5} \\right]$**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
