{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 8\n",
    "\n",
    "**1. One Answer**\n",
    "\n",
    "Saket is building a classification model from a dataset with two classes\n",
    "of student reactions to the jokes he makes during section: Funny jokes\n",
    "(F) and Dull Jokes (D). The probability of a randomly selected joke\n",
    "being funny is 0.3. The probability the model makes the correct\n",
    "classification given that the joke is dull is 0.7, and the probability\n",
    "the model makes the correct classification given that a joke is funny is\n",
    "0.2. What is the probability that a randomly selected joke is dull given\n",
    "that it has been classified as dull?\n",
    "\n",
    "- 1.  $\\frac{42}{73}$\n",
    "- 1.  $\\frac{49}{73}$\n",
    "- 1.  $\\frac{21}{73}$\n",
    "- 1.  1\n",
    "\n",
    "**2. One Answer**\n",
    "\n",
    "Suppose we train a model $f(x) = x^T \\hat{w}$ on dataset\n",
    "$\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^n$, by optimizing the following\n",
    "objective:\n",
    "\n",
    "$$\\hat{w} = \\arg \\min_w \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\log k ||w||_2^2$$\n",
    "\n",
    "for $k > 1$. As $k$ increases, what likely happens to the variance of\n",
    "our model?\n",
    "\n",
    "- 1.  The variance of our model increases.\n",
    "- 1.  The variance of our model decreases.\n",
    "- 1.  The variance of our model remains unaffected.\n",
    "- 1.  The variance of our model does not change in a predictable way.\n",
    "\n",
    "**3. One Answer**\n",
    "\n",
    "You are building a classification model for your favorite soccer team to\n",
    "determine whether a penalty kick will result in a goal or not, and your\n",
    "data set contains 300 positive examples (resulted in a goal) and 200\n",
    "negative examples (did not result in a goal). After training, you find\n",
    "that your model has an accuracy of 70% and misclassifies 15% of negative\n",
    "examples as positive. What is the probability that your model will\n",
    "misclassify a positive example as negative?\n",
    "\n",
    "- 1.  15%\n",
    "- 1.  24%\n",
    "- 1.  30%\n",
    "- 1.  40%\n",
    "- 1.  Cannot be determined.\n",
    "\n",
    "**4. One Answer**\n",
    "\n",
    "When using the least squares method for linear regression, outliers\n",
    "would have a minimal impact since the least squares method averages out\n",
    "their effects.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**5. Select All That Apply**\n",
    "\n",
    "In the standard MLE derivation for a parameter in a probability\n",
    "distribution (like the ones we saw in class), why do we apply the\n",
    "logarithm to our equation?\n",
    "\n",
    "- 1.  The function is monotonically increasing and therefore does not\n",
    "      change the result of our optimization objective.\n",
    "- 1.  The function is concave and therefore does not change the result\n",
    "      of our optimization objective.\n",
    "- 1.  The function is monotonically decreasing and therefore allows us\n",
    "      to change our “argmax” to an “argmin.”\n",
    "- 1.  Applying the logarithm allows us to convert products ($\\Pi$) into\n",
    "      sums ($\\Sigma$), letting us calculate derivatives easier.\n",
    "\n",
    "**6. One Answer**\n",
    "\n",
    "After training a model, your model has a low train error and a high test\n",
    "error. Which of the following can be inferred?\n",
    "\n",
    "- 1.  The model is underfitting.\n",
    "- 1.  The model will generalize well because it has low bias.\n",
    "- 1.  Training on more data will likely increase the model’s performance\n",
    "      on unseen data.\n",
    "- 1.  Training on more highly-informative features will decrease the\n",
    "      test error.\n",
    "- 1.  Reducing model complexity will reduce the irreducible error.\n",
    "\n",
    "**7. One Answer**\n",
    "\n",
    "Given the following Hessian Matrix, which of the following could be the\n",
    "original $f(x,y)$ function?\n",
    "\n",
    "$$H_f(x, y) = \\begin{bmatrix} 2\\ln(y) + y^2e^{xy} & \\frac{2x}{y} + e^{xy}(1+xy) \\\\ \\frac{2x}{y} + e^{xy}(1+xy) & -\\frac{x^2}{y^2} + x^2e^{xy} - 6y \\end{bmatrix}$$\n",
    "\n",
    "- 1.  $f(x,y) = x^2 \\ln(y) + e^{xy} - y^3$\n",
    "- 1.  $f(x,y) = x^2 \\ln(y) + e^{x+y} - y^3$\n",
    "- 1.  $f(x,y) = x^3 \\ln(y) + e^{xy} - y^3$\n",
    "- 1.  $f(x,y) = x^2 \\ln(y) + e^{xy} - y^4$\n",
    "\n",
    "**8. Select All That Apply**\n",
    "\n",
    "Saket did not pay attention during lecture and did not split the data\n",
    "into a training set and testing set and instead used all the data to\n",
    "train and test a given model. What is the consequence of not splitting\n",
    "the data into a training set and testing set?\n",
    "\n",
    "- 1.  Nothing, Saket knows what he is doing\n",
    "- 1.  The model may overfit and perform poorly on unseen data\n",
    "- 1.  The model may underfit and perform poorly on unseen data\n",
    "- 1.  Saket will overestimate the performance of his model on unseen\n",
    "      data\n",
    "\n",
    "**9. One Answer**\n",
    "\n",
    "Suppose you are designing a model that predicts whether or not a patient\n",
    "will be readmitted into a hospital within a month. The hospital provides\n",
    "a dataset with 25 clinical features per patient (like age, gender, and\n",
    "blood pressure), but not all of them might be relevant to readmission.\n",
    "The data is such that it’s possible to draw a straight line (or a\n",
    "higher-dimensional hyperplane) that perfectly divides the patients who\n",
    "were readmitted from those who were not. Which is the most appropriate\n",
    "choice of procedure to train a model in this scenario?\n",
    "\n",
    "- 1.  train an L1 regularized Logistic Regression, then retrain with\n",
    "      unregularized Logistic Regression\n",
    "- 1.  train an L2 regularized Logistic Regression, then retrain with\n",
    "      unregularized Logistic Regression\n",
    "- 1.  train an L1 regularized Logistic Regression, then retrain with L2\n",
    "      regularized Logistic Regression\n",
    "- 1.  train an L1 regularized Logistic Regression\n",
    "\n",
    "**10.**\n",
    "\n",
    "Describe a problem that might occur if you’re training a Logistic\n",
    "Regression model and the data is linearly separable. Around 1-3\n",
    "sentences.\n",
    "\n",
    "**11. One Answer**\n",
    "\n",
    "Suppose we standardize a given dataset. The optimal bias term will be 0\n",
    "in least-squares linear regression.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**12. One Answer**\n",
    "\n",
    "Let $f,g: \\mathbb{R} \\to \\mathbb{R}$ be convex. Which of the following\n",
    "functions is always convex?\n",
    "\n",
    "- 1.  $h(x) = f(x) \\cdot g(x)$\n",
    "- 1.  $h(x) = f \\circ g(x)$\n",
    "- 1.  $h(x) = \\min(f(x), g(x))$\n",
    "- 1.  $h(x) = \\max(f(x), g(x))$\n",
    "\n",
    "**13. One Answer**\n",
    "\n",
    "Given a small enough learning rate, gradient descent will converge to\n",
    "the global minima.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**14.**\n",
    "\n",
    "This is the equation for the bias-variance tradeoff. $\\eta$ is the\n",
    "“squared-error-optimal” predictor. $D$ is a dataset\n",
    "$\\{(x_i, y_i)\\}_{i=1}^n$ sampled from $P_{XY}$. $\\hat{f}_D \\in F$ is the\n",
    "learned least-squares predictor for some function class $F$.\n",
    "\n",
    "Which terms correspond with which concepts? Write the number of the term\n",
    "next to the concept you think it corresponds with.\n",
    "\n",
    "$$E_{Y|X}[E_D[(Y - \\hat{f}_D(x))^2]|X = x] = E_{Y|X}[(Y - \\eta(x))^2|X = x]$$\n",
    "\n",
    "**Term 1:** $E_{Y|X}[(Y - \\eta(x))^2|X = x]$\n",
    "\n",
    "**Term 2:** $+ (\\eta(x) - E_D[\\hat{f}_D(x)])^2$\n",
    "\n",
    "**Term 3:** $+ E_D[(E_D[\\hat{f}_D(x)] - \\hat{f}_D(x))^2]$\n",
    "\n",
    "**Variance:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**Bias:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**Irreducible error:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**15. One Answer**\n",
    "\n",
    "Which of the following is an advantage of using ridge regression over\n",
    "unregularized linear regression?\n",
    "\n",
    "- 1.  The ridge objective is concave\n",
    "- 1.  The ridge objective is convex\n",
    "- 1.  The ridge objective always has a unique solution\n",
    "- 1.  The ridge objective has a closed-form solution\n",
    "\n",
    "**16. One Answer**\n",
    "\n",
    "True/False: Lasso Regression uses the square of the L2 norm while Ridge\n",
    "Regression uses the L1 Norm.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**17. One Answer**\n",
    "\n",
    "You have independent random variables $X, Y$ such that $X \\sim N(1,2)$\n",
    "and $Y \\sim N(3,4)$. What is $\\text{Var}(5X+6Y+7)$?\n",
    "\n",
    "- 1.  35\n",
    "- 1.  34\n",
    "- 1.  195\n",
    "- 1.  194\n",
    "- 1.  Cannot be determined\n",
    "\n",
    "**18. One Answer**\n",
    "\n",
    "The objective function is $L(w) = ||Xw-Y||_2^2$. What is the gradient of\n",
    "$L(w)$ with respect to $w$?\n",
    "\n",
    "- 1.  $2Y^T (Xw - Y)$\n",
    "- 1.  $2X^T(X^T Xw - Y)$\n",
    "- 1.  $2X^T (Xw - Y)$\n",
    "- 1.  $2Y^T (X^T Xw - Y)$\n",
    "\n",
    "**19. One Answer**\n",
    "\n",
    "Which of the following is true, when choosing to use Maximum Likelihood\n",
    "Estimation (MLE)?\n",
    "\n",
    "- 1.  MLE cannot be used if we do not know the exact distribution of our\n",
    "      data.\n",
    "- 1.  MLE works well for any data distribution, so we do need knowledge\n",
    "      of the true distribution.\n",
    "- 1.  MLE will produce unbiased estimates regardless of the data\n",
    "      distribution or the likelihood function that we choose\n",
    "- 1.  MLE works even if the true distribution of our data isn’t known.\n",
    "      We can make an educated guess for the distribution of our data for\n",
    "      our likelihood function.\n",
    "\n",
    "**20. One Answer**\n",
    "\n",
    "Consider the function $f(a) = 5a^2 - 3a + 2$. You want to use gradient\n",
    "descent to find the unique minimum, which you know is at $a_* = 0.3$. If\n",
    "at time $t$ you arrive at the point $a_t = 3$, what value for the step\n",
    "size would bring you to $a_*$ at time $t+1$?\n",
    "\n",
    "- 1.  0.001\n",
    "- 1.  0.01\n",
    "- 1.  0.1\n",
    "- 1.  1\n",
    "\n",
    "**21. One Answer**\n",
    "\n",
    "Donovan is training some machine learning model, and is telling you\n",
    "about it. He needed to standardize the data, so he computed the mean and\n",
    "standard deviation of each feature in the entire dataset $X$ and applied\n",
    "the transformation correctly. He then created non-overlapping subsets of\n",
    "$X$ called $X_{train}$, $X_{validation}$, and $X_{test}$. To train,\n",
    "validate, and test their model respectively. In this setup, was there\n",
    "train/test leakage?\n",
    "\n",
    "- 1.  Yes\n",
    "- 1.  No\n",
    "\n",
    "**22.**\n",
    "\n",
    "The following plots show 3 data points and 3 models. The data is the\n",
    "same for all 3 models. Match the learned model to the equation used for\n",
    "linear regression.\n",
    "\n",
    "<img src=\"./plots.png\" width=\"550px\">\n",
    "\n",
    "$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$. Plot number: \\_\\_\\_\\_\\_ (for\n",
    "$\\lambda > 0$)\n",
    "\n",
    "$\\hat{w} = (X^T X)^{-1} X^T y$. Plot number: \\_\\_\\_\\_\\_\n",
    "\n",
    "$\\hat{w} = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T y$, where\n",
    "$\\tilde{X} = [X \\quad \\vec{1}]$. Plot number: \\_\\_\\_\\_\\_\n",
    "\n",
    "**23. One Answer**\n",
    "\n",
    "True/False: The training error is a better estimate of the true error\n",
    "than the cross-validation error.\n",
    "\n",
    "- 1.  True\n",
    "- 1.  False\n",
    "\n",
    "**24. Select All That Apply**\n",
    "\n",
    "Let $f: \\mathbb{R} \\to \\mathbb{R}$ be a continuous, smooth function\n",
    "whose derivative $f'(x)$ is also continuous. Suppose $f$ has a unique\n",
    "global minimum $x^* \\in (-\\infty, \\infty)$, and you are using gradient\n",
    "descent to find $x^*$. You fix some $x^{(0)} \\in \\mathbb{R}$ and step\n",
    "size $\\eta > 0$, and run $x^{(t)} = x^{(t-1)} - \\eta f'(x^{(t-1)})$\n",
    "repeatedly. Which of the following statements are true?\n",
    "\n",
    "- 1.  Gradient descent is sure to converge, to some value, for any step\n",
    "      size $\\eta > 0$.\n",
    "- 1.  If $f$ has a local minimum $x'$ different from the global one,\n",
    "      i.e., $x' \\neq x^*$, and $x^{(t)} = x'$ for some $t$, gradient\n",
    "      descent will not converge to $x^*$.\n",
    "- 1.  Assuming gradient descent converges, it converges to $x^*$ if and\n",
    "      only if $f$ is convex.\n",
    "- 1.  If, additionally, $f$ is the objective function of logistic\n",
    "      regression, and gradient descent converges, then it converges to\n",
    "      $x^*$.\n",
    "\n",
    "**25.**\n",
    "\n",
    "What is the tradeoff between the size of the validation set and the size\n",
    "of the training set? Around 1-3 sentences.\n",
    "\n",
    "**26.**\n",
    "\n",
    "Consider $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$.\n",
    "Suppose $\\hat{w} = \\arg \\min_w \\|Xw - y\\|_2$ has a unique solution. Fill\n",
    "in the blank for the following vector spaces. Write NA if the there is\n",
    "not enough information to determine the answer.\n",
    "\n",
    "Col(X) = \\_\\_\\_\\_\\_\n",
    "\n",
    "Row(X) = \\_\\_\\_\\_\\_\n",
    "\n",
    "Null(X) = \\_\\_\\_\\_\\_\n",
    "\n",
    "**27.**\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ where\n",
    "$f(x, y, z) = xy + x^2 \\ln(z) + e^{yz}$. Calculate the gradient of $f$.\n",
    "\n",
    "**Gradient =**\n",
    "\n",
    "**28.**\n",
    "\n",
    "Describe a scenario where one would choose to use Ridge regression over\n",
    "Lasso regression. Around 1-4 sentences.\n",
    "\n",
    "**29.**\n",
    "\n",
    "Answer the following questions about the Softmax function.\n",
    "\n",
    "**(a) Explain how the Softmax function transforms an input vector\n",
    "(logits) and why it is suitable for multi-class classification.**\n",
    "\n",
    "**(b) Suppose a model outputs the following values/logits for a 3-class\n",
    "classification problem.**\n",
    "\n",
    "$z = [2, 1, 5]$\n",
    "\n",
    "**Compute the softmax probabilities. You can leave the values in terms\n",
    "of exponentiated numbers.**\n",
    "\n",
    "**Softmax(z) = \\[ , , \\]**"
   ],
   "id": "70af1ce2-fb05-4a43-abcd-98d520501d11"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
