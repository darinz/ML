{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbb690e-b668-4993-bce1-03b60dd57980",
   "metadata": {},
   "source": [
    "# Practice 7 Solutions\n",
    "\n",
    "**Problem 1. In a machine learning classification problem, you have a dataset with two classes: Positive (P) and Negative (N). The probability of a randomly selected sample being Positive is $3/5$. The probability of a correct classification given that the sample is Positive is $4/5$, and the probability of a correct classification given that the sample is Negative is $7/10$. What is the probability that a randomly selected sample is Positive given that it has been classified as Positive? One Answer**\n",
    "\n",
    "*   (a) $\\frac{4}{5}$\n",
    "*   (b) $\\frac{12}{25}$\n",
    "*   (c) $\\frac{3}{5}$\n",
    "*   (d) $\\frac{12}{19}$\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This is a Bayes' theorem problem** - finding the probability of being Positive given a Positive classification.\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**1. Define events:**\n",
    "- **P** = \"Sample is Positive\" \n",
    "- **N** = \"Sample is Negative\"\n",
    "- **CP** = \"Classified as Positive\"\n",
    "- **CN** = \"Classified as Negative\"\n",
    "\n",
    "**2. Given information:**\n",
    "- **$P(P) = 3/5$** (probability of being Positive)\n",
    "- **$P(N) = 1 - P(P) = 2/5$** (probability of being Negative)\n",
    "- **$P(CP|P) = 4/5$** (correct classification given Positive)\n",
    "- **$P(CN|N) = 7/10$** (correct classification given Negative)\n",
    "\n",
    "**3. Calculate additional probabilities:**\n",
    "- **$P(CN|P) = 1 - P(CP|P) = 1/5$** (incorrect classification given Positive)\n",
    "- **$P(CP|N) = 1 - P(CN|N) = 3/10$** (incorrect classification given Negative)\n",
    "\n",
    "**4. Apply Bayes' theorem:**\n",
    "$P(P|CP) = \\frac{P(CP|P)P(P)}{P(CP)}$\n",
    "\n",
    "**5. Calculate $P(CP)$ using law of total probability:**\n",
    "$P(CP) = P(CP|P)P(P) + P(CP|N)P(N)$\n",
    "\n",
    "$P(CP) = (4/5)(3/5) + (3/10)(2/5)$\n",
    "\n",
    "$P(CP) = 12/25 + 6/50 = 12/25 + 3/25 = 15/25 = 3/5$\n",
    "\n",
    "**6. Substitute into Bayes' theorem:**\n",
    "$P(P|CP) = \\frac{(4/5)(3/5)}{3/5} = \\frac{12/25}{3/5} = \\frac{12/25}{15/25} = \\frac{12}{15} = \\frac{4}{5}$\n",
    "\n",
    "**7. Verification:**\n",
    "- **Numerator:** $(4/5)(3/5) = 12/25$ (true positives)\n",
    "- **Denominator:** $3/5$ (all positive classifications)\n",
    "- **Ratio:** $4/5 = 0.8$ (80% of positive classifications are actually positive)\n",
    "\n",
    "**Key insight:** **High base rate** (60% positive) combined with **good classification accuracy** leads to high precision.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 2. Which of the following statements must be true for a square matrix A to have an inverse matrix $A^{-1}$?**\n",
    "\n",
    "*   (a) A must be symmetric.\n",
    "*   (b) The rank of A is less than its number of columns.\n",
    "*   (c) A must have at least one column of 0s.\n",
    "*   (d) The determinant of A is not equal to 0.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** A square matrix is invertible if and only if its determinant is non-zero, which is a fundamental theorem in linear algebra. Thus, choice (d) is correct. However, even if we forgot this fundamental theorems, we can use process-of-elimination. The symmetry of A has nothing to do with its inverse: imagine if A was all 0s; it's of course symmetric, but certainly non-invertible. Choices (b) and (c) being true would mean A has linearly dependent rows or columns, which cannot result in an invertible matrix.\n",
    "\n",
    "**Problem 3. Consider the following system of linear equations:**\n",
    "\n",
    "$$2x + 3y = 16$$\n",
    "$$4x + 6y = 32$$\n",
    "\n",
    "**Which of the following statements is true?**\n",
    "\n",
    "*   (a) The system has an infinite number of solutions because the two equations are linearly dependent.\n",
    "*   (b) The system has a unique solution because there are two equations for two unknowns.\n",
    "*   (c) The system has no solution because the determinant of the coefficient matrix is zero.\n",
    "*   (d) The system has no solution because the equations represent parallel lines that never intersect.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** The second equation is a multiple of the first, meaning they are linearly dependent and represent the same line. Since they are the same line, they intersect at every point, leading to an infinite number of solutions.\n",
    "\n",
    "**Problem 4. For any function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the gradient is defined as:**\n",
    "\n",
    "$$\\nabla_w f(w) = \\left[ \\frac{\\partial f(w)}{\\partial w_1} \\quad \\dots \\quad \\frac{\\partial f(w)}{\\partial w_n} \\right]^T$$\n",
    "\n",
    "**What is the value of $\\nabla_w (w^T Aw + u^T Bw + w^T Bv)$, given that $A, B \\in \\mathbb{R}^{n \\times n}$, $A$ is symmetric, and $u, v \\in \\mathbb{R}^n$?**\n",
    "\n",
    "*   (a) $Aw + Bu + B^T v$\n",
    "*   (b) $2Aw + B^T u + Bv$\n",
    "*   (c) $Aw + B^T u + Bv$\n",
    "*   (d) $2Aw + Bu + B^T v$\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This is a matrix calculus problem** - computing the gradient of a quadratic form with linear terms.\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**1. Break down the function:**\n",
    "$f(w) = w^T Aw + u^T Bw + w^T Bv$\n",
    "\n",
    "**2. Apply gradient rules:**\n",
    "\n",
    "**For $w^T Aw$ (where A is symmetric):**\n",
    "$\\nabla_w(w^T Aw) = 2Aw$\n",
    "\n",
    "**For $u^T Bw$:**\n",
    "$\\nabla_w(u^T Bw) = B^T u$\n",
    "\n",
    "**For $w^T Bv$:**\n",
    "$\\nabla_w(w^T Bv) = Bv$\n",
    "\n",
    "**3. Combine all terms:**\n",
    "$\\nabla_w f(w) = 2Aw + B^T u + Bv$\n",
    "\n",
    "**4. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** Missing factor of 2 for quadratic term\n",
    "**Option (c):** Wrong transpose for $B^T u$ term\n",
    "**Option (d):** Wrong transpose for $Bu$ term\n",
    "\n",
    "**5. Key matrix calculus rules:**\n",
    "- **$\\nabla_w(w^T Aw) = 2Aw$** (when A is symmetric)\n",
    "- **$\\nabla_w(u^T Bw) = B^T u$**\n",
    "- **$\\nabla_w(w^T Bv) = Bv$**\n",
    "\n",
    "**Key insight:** **Matrix calculus** requires careful attention to **transpose operations** and **symmetry assumptions**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 5. Which of the following statements is most accurate regarding the principle of Maximum Likelihood Estimation (MLE) in statistical modeling?**\n",
    "\n",
    "*   (a) MLE identifies model parameters that maximize the probability of the observed data under the model.\n",
    "*   (b) MLE directly computes the probability of parameters being correct, independent of observed data.\n",
    "*   (c) MLE is primarily concerned with minimizing the variance of parameter estimates for model stability.\n",
    "*   (d) MLE identifies model parameters that minimize the squared prediction error over the training data.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** MLE aims to maximize the probability of observing the given data under different model parameter values.\n",
    "\n",
    "**Problem 6. A machine learning engineer models the number of website requests per hour using a Poisson distribution. Over 6 hours, the observed requests are 4, 5, 6, 7, 8, and 9. Recall that the probability mass function for a Poisson distribution with parameter $\\lambda$ is:**\n",
    "\n",
    "$$P(x|\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!}$$\n",
    "\n",
    "**What is the maximum likelihood estimation of the rate parameter $\\lambda$ of this Poisson distribution?**\n",
    "\n",
    "*   (a) $e^{\\frac{39}{6}}$\n",
    "*   (b) $\\sqrt{\\frac{39}{6}}$\n",
    "*   (b) $\\sqrt{\\frac{39}{6}}$\n",
    "*   (c) 6\n",
    "*   (d) $\\frac{39}{6}$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** The MLE estimate of $\\lambda$ for a Poisson distribution is simply the mean of the observed data. The mean is $\\frac{39}{6} = 6.5$.\n",
    "\n",
    "**Problem 7. Assume a simple linear model $Y = Xw$. For simplicity, no intercept is considered. Given the following dataset:**\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "**(a) Compute the least squares estimate of $w$ without any regularization. You may leave your answer as a fraction, if necessary.**\n",
    "\n",
    "**Hint:** if $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, then $A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$\n",
    "\n",
    "**Answer:** $\\hat{w} = \\begin{bmatrix} 2 \\\\ -0.5 \\end{bmatrix}$\n",
    "\n",
    "**(b) Predict $\\hat{Y}$ for $X = \\begin{bmatrix} 6 \\\\ 7 \\end{bmatrix}$.**\n",
    "\n",
    "**Answer:** $\\hat{Y} = 8.5$\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "**This is a linear regression problem** - computing least squares estimates and making predictions.\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "**Part (a): Computing $\\hat{w}$**\n",
    "\n",
    "**1. Set up the normal equations:**\n",
    "$\\hat{w} = (X^T X)^{-1} X^T Y$\n",
    "\n",
    "**2. Compute $X^T X$:**\n",
    "$X^T X = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix} = \\begin{bmatrix} 5 & 4 \\\\ 4 & 4 \\end{bmatrix}$\n",
    "\n",
    "**3. Compute $(X^T X)^{-1}$:**\n",
    "Using the hint: $ad-bc = 5(4) - 4(4) = 20 - 16 = 4$\n",
    "\n",
    "$(X^T X)^{-1} = \\frac{1}{4} \\begin{bmatrix} 4 & -4 \\\\ -4 & 5 \\end{bmatrix} = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1.25 \\end{bmatrix}$\n",
    "\n",
    "**4. Compute $X^T Y$:**\n",
    "$X^T Y = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 6 \\end{bmatrix}$\n",
    "\n",
    "**5. Compute $\\hat{w}$:**\n",
    "$\\hat{w} = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1.25 \\end{bmatrix} \\begin{bmatrix} 8 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 8-6 \\\\ -8+7.5 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -0.5 \\end{bmatrix}$\n",
    "\n",
    "**Part (b): Making prediction**\n",
    "\n",
    "**6. For new input $x = \\begin{bmatrix} 6 \\\\ 7 \\end{bmatrix}$:**\n",
    "$\\hat{Y} = \\hat{w}^T x = \\begin{bmatrix} 2 & -0.5 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 7 \\end{bmatrix} = 2(6) + (-0.5)(7) = 12 - 3.5 = 8.5$\n",
    "\n",
    "**Key insight:** **Normal equations** provide the **closed-form solution** for linear regression without regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 8. You have access to data points $\\{(x_i, y_i)\\}_{i=1}^n$, where $x_i$ are $d$-dimensional vectors ($x_i \\in \\mathbb{R}^d$) and $y_i$ are scalars ($y_i \\in \\mathbb{R}$). Additionally, you have weights $\\{w_i\\}_{i=1}^n$, where $w_i \\in \\mathbb{R}$ and $w_i > 0$, representing the \"importance\" of each data point. You want to solve the weighted least squares regression problem:**\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta \\in \\mathbb{R}^d} \\sum_{i=1}^n w_i (x_i^T \\theta - y_i)^2$$\n",
    "\n",
    "**Let us define the matrices:**\n",
    "\n",
    "$$X = \\begin{bmatrix} x_1^T \\\\ \\vdots \\\\ x_n^T \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}$$\n",
    "$$Y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\in \\mathbb{R}^n$$\n",
    "$$W = \\begin{bmatrix} w_1 & 0 & \\dots & 0 \\\\ 0 & w_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & w_n \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "**What is $\\hat{\\theta}$ in terms of $X$, $Y$, and $W$?**\n",
    "\n",
    "*   (a) $(X^T X)^{-1} X^T W^{-1} Y$\n",
    "*   (b) $W(X^T X)^{-1} X^T Y$\n",
    "*   (c) $(X^T X)^{-1} X^T W Y$\n",
    "*   (d) $(X^T W X)^{-1} X^T Y$\n",
    "*   (e) $(X^T W X)^{-1} X^T W Y$\n",
    "\n",
    "**Correct answers:** (e)\n",
    "\n",
    "**Explanation:** The solution can be found by taking the gradient of the optimization algorithm, setting it to 0, and solving for $\\theta$. We can convert the objective function to matrix notation: $\\arg \\min_{\\theta} (X\\theta - Y)^T W (X\\theta - Y)$.\n",
    "\n",
    "**Problem 9. In the context of least squares regression, how does the presence of high noise levels in the data impact the reliability of the model's parameter estimates?**\n",
    "\n",
    "*   (a) High noise levels predominantly affect the intercept term of the regression model, but leave the slope estimates relatively unaffected.\n",
    "*   (b) High noise levels can increase the variability of the parameter estimates, potentially leading to a model that captures random noise rather than the true underlying relationship.\n",
    "*   (c) High noise levels decrease the variance of the estimated parameters, making the model more robust.\n",
    "*   (d) Noise in the data generally has minimal impact on the least squares estimates since the method inherently separates signal from noise in most scenarios.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** In least squares regression, high noise levels can lead to overfitting, where the model erroneously adjusts its parameters to account for these random fluctuations, resulting in a model that performs well on the training data but poorly on unseen data. This reduces the model's ability to generalize and accurately predict outcomes on new, unseen data.\n",
    "\n",
    "**Problem 10. In linear regression analysis using the least squares method, how might outliers in the dataset impact the resulting regression line?**\n",
    "\n",
    "*   (a) Outliers affect only the precision of the prediction intervals, not the regression line itself.\n",
    "*   (b) Outliers enhance the model's accuracy by providing a wider range of data points.\n",
    "*   (c) Outliers can significantly skew the regression line, potentially leading to an inaccurate representation of the overall data trend.\n",
    "*   (d) Outliers have a minimal impact, as the least squares method averages out their effects.\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:** Least squares aims to minimize the sum of the squared differences between observed and predicted values. Outliers, which are very distant from other data points, can cause the squared differences to become substantially larger, and consequently \"pull\" the regression line to themselves. This can lead to a skewed line that does not accurately represent the underlying trend of the majority of the data, affecting the model's predictive accuracy.\n",
    "\n",
    "**Problem 11. How does increasing the complexity of a model typically affect the properties of that model? Select all that apply.**\n",
    "\n",
    "*   (a) It tends to decrease bias but increase variance, potentially leading to overfitting.\n",
    "*   (b) It can increase training accuracy.\n",
    "*   (c) It tends to increase both bias and variance.\n",
    "*   (d) It tends to decrease variance but increase bias, potentially leading to underfitting.\n",
    "\n",
    "**Correct answers:** (a), (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of the bias-variance tradeoff** and how model complexity affects model properties.\n",
    "\n",
    "**Why (a) and (b) are correct:**\n",
    "\n",
    "**Option (a): Decreases bias, increases variance**\n",
    "- **Increasing complexity** allows model to capture more patterns\n",
    "- **Lower bias** = model can fit more complex relationships\n",
    "- **Higher variance** = model becomes more sensitive to training data\n",
    "- **Overfitting risk** increases with complexity\n",
    "\n",
    "**Option (b): Can increase training accuracy**\n",
    "- **More complex models** can fit training data better\n",
    "- **Higher training accuracy** is often achieved\n",
    "- **But this doesn't guarantee** better generalization\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (c): Increases both bias and variance**\n",
    "- **Contradicts** the bias-variance tradeoff\n",
    "- **Complexity typically** decreases bias, increases variance\n",
    "- **Not a typical** relationship\n",
    "\n",
    "**Option (d): Decreases variance, increases bias**\n",
    "- **This describes** underfitting scenario\n",
    "- **Simple models** have high bias, low variance\n",
    "- **Not what happens** when increasing complexity\n",
    "\n",
    "**Key insight:** **Model complexity** follows a **bias-variance tradeoff** - decreasing bias while increasing variance.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 12. True/False: A model with high variance tends to perform well on both the training and test data.**\n",
    "\n",
    "*   (a) False\n",
    "*   (b) True\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of high variance models** and their performance characteristics.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**High variance models typically:**\n",
    "- **Perform well on training data** (they fit it closely)\n",
    "- **Perform poorly on test data** (they don't generalize well)\n",
    "- **Are overfitted** to training data\n",
    "- **Have low bias but high variance**\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **High variance** = model predictions vary significantly with different training sets\n",
    "- **Good training performance** = model fits training data well\n",
    "- **Poor test performance** = model doesn't generalize to unseen data\n",
    "- **This is the classic** overfitting scenario\n",
    "\n",
    "**Why (b) is incorrect:**\n",
    "- **High variance models** do NOT perform well on test data\n",
    "- **They overfit** to training data\n",
    "- **Poor generalization** is the hallmark of high variance\n",
    "\n",
    "**Key insight:** **High variance** models **memorize training data** but **fail to generalize** to new data.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 13. The plots below show fits (in black) to the data points (\"x\" symbols in grey), using several different basis functions:**\n",
    "\n",
    "<img src=\"./plots.png\" width=\"550px\">\n",
    "\n",
    "**For each plot, please identify the basis function used:**\n",
    "\n",
    "**Plot (a): _____, Plot (b): _____, Plot (c): _____, Plot (d): _____**\n",
    "\n",
    "**Basis functions used:**\n",
    "1. $h_1(x) = [1,x]$\n",
    "2. $h_2(x) = [1,x,x^2]$\n",
    "3. $h_3(x) = [1,x,x^2,x^3]$\n",
    "4. $h_4(x) = [1, \\sin(\\frac{4\\pi}{25}x)]$\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "**This question tests understanding of basis functions** and their role in capturing non-linear relationships.\n",
    "\n",
    "**Step-by-step analysis:**\n",
    "\n",
    "**Plot (a):** Shows a **sinusoidal pattern** - matches $h_4(x) = [1, \\sin(\\frac{4\\pi}{25}x)]$\n",
    "\n",
    "**Plot (b):** Shows a **quadratic curve** - matches $h_2(x) = [1,x,x^2]$\n",
    "\n",
    "**Plot (c):** Shows a **linear relationship** - matches $h_1(x) = [1,x]$\n",
    "\n",
    "**Plot (d):** Shows a **cubic curve** - matches $h_3(x) = [1,x,x^2,x^3]$\n",
    "\n",
    "**Key insights:**\n",
    "- **Basis functions** transform input space to capture non-linear patterns\n",
    "- **Higher degree polynomials** can fit more complex curves\n",
    "- **Sinusoidal functions** capture periodic patterns\n",
    "- **Linear basis** captures only linear relationships\n",
    "\n",
    "**Answer:** Plot (a): $h_4$, Plot (b): $h_2$, Plot (c): $h_1$, Plot (d): $h_3$\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 14. What is the purpose of general basis functions in linear regression?**\n",
    "\n",
    "*   (a) To increase convergence speed in gradient descent.\n",
    "*   (b) To encourage sparsity in learned weights.\n",
    "*   (c) To minimize computational complexity.\n",
    "*   (d) To transform input data into a higher-dimensional space to capture non-linear relationships.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of basis functions** and their role in linear regression.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**Basis functions serve to:**\n",
    "- **Transform input data** into higher-dimensional space\n",
    "- **Capture non-linear relationships** in data\n",
    "- **Enable linear models** to fit non-linear patterns\n",
    "- **Expand feature space** without changing the linear nature of the model\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **Original model:** $y = w^T x$ (linear in $x$)\n",
    "- **With basis functions:** $y = w^T \\phi(x)$ (linear in $\\phi(x)$, non-linear in $x$)\n",
    "- **$\\phi(x)$** transforms input to capture non-linear patterns\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Convergence speed**\n",
    "- **Basis functions** don't directly affect convergence\n",
    "- **Convergence** depends on optimization algorithm\n",
    "- **Not the primary purpose**\n",
    "\n",
    "**Option (b): Sparsity**\n",
    "- **Basis functions** don't encourage sparsity\n",
    "- **Regularization** (L1/L2) encourages sparsity\n",
    "- **Different concept**\n",
    "\n",
    "**Option (c): Computational complexity**\n",
    "- **Basis functions** often **increase** complexity\n",
    "- **More features** = more computation\n",
    "- **Not a benefit**\n",
    "\n",
    "**Key insight:** **Basis functions** enable **linear models** to capture **non-linear relationships** through **feature transformation**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 15. What is the best description of 'irreducible error' in a machine learning predictor?**\n",
    "\n",
    "*   (a) It's due to inherent noise that cannot be eliminated by any model.\n",
    "*   (b) It's minimized by cross-validation.\n",
    "*   (c) It can be minimized by increasing training data size.\n",
    "*   (d) It arises from feature engineering or irrelevant features.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of irreducible error** - the fundamental limit on model performance.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**Irreducible error is:**\n",
    "- **Inherent noise** in the data generation process\n",
    "- **Cannot be eliminated** by any model, no matter how complex\n",
    "- **Lower bound** on prediction error\n",
    "- **Independent** of model choice or training data size\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **True function:** $g(x) = 7x^2 + \\epsilon$\n",
    "- **Noise term:** $\\epsilon \\sim \\mathcal{N}(0, 4)$\n",
    "- **Irreducible error:** $E[\\epsilon^2] = \\text{Var}(\\epsilon) = 4$\n",
    "- **No model** can predict $\\epsilon$ perfectly\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Minimized by cross-validation**\n",
    "- **Cross-validation** estimates model performance\n",
    "- **Doesn't reduce** irreducible error\n",
    "- **Different concept**\n",
    "\n",
    "**Option (c): Minimized by more data**\n",
    "- **More data** reduces variance, not irreducible error\n",
    "- **Irreducible error** is independent of data size\n",
    "- **Fundamental limit**\n",
    "\n",
    "**Option (d): Arises from features**\n",
    "- **Feature engineering** affects model bias/variance\n",
    "- **Not the source** of irreducible error\n",
    "- **Different error component**\n",
    "\n",
    "**Key insight:** **Irreducible error** is the **fundamental noise** in the data that **no model can eliminate**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 16. A polynomial regression model of degree $d=3$ approximates a quadratic function $g(x) = 7x^2 + \\epsilon$, where $\\epsilon$ is a Gaussian random variable with mean $\\mu=0$ and variance $\\sigma^2=4$. What is the irreducible error?**\n",
    "\n",
    "*   (a) 2\n",
    "*   (b) 0\n",
    "*   (c) 4\n",
    "*   (d) $x^3$\n",
    "\n",
    "**Correct answers:** (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of irreducible error** in the context of a specific model.\n",
    "\n",
    "**Why (c) is correct:**\n",
    "\n",
    "**Given the setup:**\n",
    "- **True function:** $g(x) = 7x^2 + \\epsilon$\n",
    "- **Noise:** $\\epsilon \\sim \\mathcal{N}(0, 4)$\n",
    "- **Irreducible error:** $E[\\epsilon^2] = \\text{Var}(\\epsilon) = 4$\n",
    "\n",
    "**Mathematical reasoning:**\n",
    "- **No matter how well** the model fits $7x^2$\n",
    "- **The noise term** $\\epsilon$ cannot be predicted\n",
    "- **Expected squared error** from noise is $E[\\epsilon^2] = 4$\n",
    "- **This is the irreducible error**\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): 2**\n",
    "- **This would be** $\\sqrt{\\text{Var}(\\epsilon)}$\n",
    "- **Standard deviation** is 2, not irreducible error\n",
    "- **Irreducible error** is variance, not standard deviation\n",
    "\n",
    "**Option (b): 0**\n",
    "- **Would mean** no noise in the system\n",
    "- **Contradicts** the given $\\epsilon \\sim \\mathcal{N}(0, 4)$\n",
    "- **Not realistic**\n",
    "\n",
    "**Option (d): $x^3$**\n",
    "- **This is a function** of $x$, not a constant\n",
    "- **Irreducible error** is independent of $x$\n",
    "- **Nonsensical** answer\n",
    "\n",
    "**Key insight:** **Irreducible error** equals the **variance of the noise** in the data generation process.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 17. True/False: Increasing the proportion of your dataset allocated to training (as opposed to testing) will guarantee better performance on unseen data.**\n",
    "\n",
    "*   (a) False\n",
    "*   (b) True\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of training-test split** and its impact on model evaluation.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**Increasing training data does NOT guarantee better performance because:**\n",
    "\n",
    "**1. Quality vs Quantity:**\n",
    "- **More data** doesn't guarantee **better data**\n",
    "- **Poor quality data** can hurt performance\n",
    "- **Data distribution** matters more than size\n",
    "\n",
    "**2. Overfitting risk:**\n",
    "- **More training data** can lead to overfitting\n",
    "- **Model complexity** should match data size\n",
    "- **Validation** is still needed\n",
    "\n",
    "**3. Data leakage:**\n",
    "- **Improper splits** can cause data leakage\n",
    "- **Test data contamination** leads to optimistic estimates\n",
    "- **Proper separation** is crucial\n",
    "\n",
    "**4. Model capacity:**\n",
    "- **Simple models** may not benefit from more data\n",
    "- **Complex models** need sufficient data\n",
    "- **Match model complexity** to data size\n",
    "\n",
    "**Why (b) is incorrect:**\n",
    "- **More training data** typically helps, but doesn't guarantee improvement\n",
    "- **Depends on** data quality, model choice, and proper evaluation\n",
    "- **Not an absolute** guarantee\n",
    "\n",
    "**Key insight:** **Data quality** and **proper evaluation** are more important than **data quantity** alone.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 18. Which of the following statements best describes a potential issue that can arise if the test dataset is not properly separated from the training dataset?**\n",
    "\n",
    "*   (a) The model will always underfit, regardless of the algorithm used.\n",
    "*   (b) The model will always overfit, regardless of the algorithm used.\n",
    "*   (c) The evaluation metrics will tend to overestimate the prediction error on unseen data.\n",
    "*   (d) The test data will influence the training process, leading to an overly optimistic estimate of the model's performance on new, unseen data.\n",
    "*   (e) The model's computational complexity will significantly increase, resulting in longer training times.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of data leakage** and proper test set separation.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**Data leakage occurs when:**\n",
    "- **Test data influences** training process\n",
    "- **Information from test set** leaks into model development\n",
    "- **Overly optimistic** performance estimates\n",
    "- **Poor generalization** to truly unseen data\n",
    "\n",
    "**Common causes of data leakage:**\n",
    "- **Preprocessing entire dataset** before splitting\n",
    "- **Feature selection** using all data\n",
    "- **Hyperparameter tuning** on test set\n",
    "- **Model selection** using test set\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Always underfit**\n",
    "- **Data leakage** doesn't guarantee underfitting\n",
    "- **Can cause** overfitting to leaked information\n",
    "- **Depends on** the specific leakage\n",
    "\n",
    "**Option (b): Always overfit**\n",
    "- **Not always** the case\n",
    "- **Depends on** nature of leakage\n",
    "- **Can cause** various issues\n",
    "\n",
    "**Option (c): Overestimate error**\n",
    "- **Data leakage** typically **underestimates** error\n",
    "- **Leads to optimistic** performance estimates\n",
    "- **Opposite** of what happens\n",
    "\n",
    "**Option (e): Increase complexity**\n",
    "- **Data leakage** doesn't affect computational complexity\n",
    "- **Affects** performance estimates, not training time\n",
    "- **Unrelated** to complexity\n",
    "\n",
    "**Key insight:** **Data leakage** leads to **overly optimistic** performance estimates by **contaminating** the training process.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 19. How should data preprocessing be applied when using k-fold cross-validation? Select the most accurate answer.**\n",
    "\n",
    "*   (a) Preprocess the entire dataset before splitting into folds to maintain consistency.\n",
    "*   (b) Avoid preprocessing as it can bias the cross-validation results.\n",
    "*   (c) Only preprocess the test folds and train our model on raw (unprocessed) data.\n",
    "*   (d) Apply preprocessing separately on each iteration of k-fold validation to avoid data leakage.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of proper preprocessing** in cross-validation to avoid data leakage.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**Proper preprocessing in k-fold CV:**\n",
    "- **Apply preprocessing separately** on each fold\n",
    "- **Use only training fold** to compute statistics (mean, std, etc.)\n",
    "- **Apply same transformation** to validation fold\n",
    "- **Prevents data leakage** from validation set\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Preprocess entire dataset**\n",
    "- **Causes data leakage** - validation data influences preprocessing\n",
    "- **Overly optimistic** performance estimates\n",
    "- **Violates** independence principle\n",
    "\n",
    "**Option (b): Avoid preprocessing**\n",
    "- **Preprocessing is often necessary** (scaling, normalization)\n",
    "- **Avoiding it** can hurt model performance\n",
    "- **Not a solution** to data leakage\n",
    "\n",
    "**Option (c): Only preprocess test folds**\n",
    "- **Incorrect approach** - should preprocess training data\n",
    "- **Test folds** should use training-derived transformations\n",
    "- **Backwards** logic\n",
    "\n",
    "**Key insight:** **Preprocessing must be applied separately** on each fold to **prevent data leakage** and maintain **proper evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 20. What is the main advantage of using k-fold cross-validation? One Answer**\n",
    "\n",
    "*   (a) It guarantees improvement in model accuracy on unseen data.\n",
    "*   (b) It provides an estimate of model performance for given hyperparameters.\n",
    "*   (c) It significantly reduces the training time of the model by dividing the dataset into smaller parts.\n",
    "*   (d) It eliminates the need for a separate test dataset.\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of k-fold cross-validation** and its primary benefits.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**k-fold cross-validation provides:**\n",
    "- **Robust performance estimates** for given hyperparameters\n",
    "- **Multiple evaluations** on different data splits\n",
    "- **Better generalization** estimates than single train-test split\n",
    "- **Statistical confidence** in model performance\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Guarantees improvement**\n",
    "- **CV doesn't guarantee** better performance\n",
    "- **It estimates** performance more accurately\n",
    "- **Model choice** still matters\n",
    "\n",
    "**Option (c): Reduces training time**\n",
    "- **CV actually increases** total training time\n",
    "- **Multiple models** must be trained\n",
    "- **Computational overhead** is higher\n",
    "\n",
    "**Option (d): Eliminates test set**\n",
    "- **CV doesn't eliminate** need for test set\n",
    "- **CV is for** hyperparameter tuning\n",
    "- **Final evaluation** still needs held-out test set\n",
    "\n",
    "**Key insight:** **k-fold CV** provides **reliable performance estimates** for **hyperparameter selection** and **model comparison**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 21. In Lasso regression, how does the regularization parameter $\\lambda$ influence the risk of overfitting? Select all that apply.**\n",
    "\n",
    "*   (a) Increasing $\\lambda$ always increases the risk of overfitting as it leads to higher model complexity.\n",
    "*   (b) Decreasing $\\lambda$ to zero may increase the risk of overfitting.\n",
    "*   (c) Increasing $\\lambda$ typically reduces the risk of overfitting by increasing sparsity.\n",
    "*   (d) The choice of $\\lambda$ in Ridge regression has no impact on the risk of overfitting.\n",
    "\n",
    "**Correct answers:** (b), (c)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of Lasso regularization** and its effect on overfitting.\n",
    "\n",
    "**Why (b) and (c) are correct:**\n",
    "\n",
    "**Option (b): Decreasing $\\lambda$ to zero increases overfitting risk**\n",
    "- **$\\lambda = 0$** reduces to ordinary least squares\n",
    "- **No regularization** = higher model complexity\n",
    "- **Increased risk** of overfitting to training data\n",
    "- **Poor generalization** to unseen data\n",
    "\n",
    "**Option (c): Increasing $\\lambda$ reduces overfitting risk**\n",
    "- **Higher $\\lambda$** = stronger regularization\n",
    "- **More coefficients** set to exactly zero\n",
    "- **Reduced model complexity**\n",
    "- **Better generalization** (up to a point)\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Increasing $\\lambda$ always increases overfitting**\n",
    "- **Contradicts** the purpose of regularization\n",
    "- **Higher $\\lambda$** reduces model complexity\n",
    "- **Should reduce** overfitting risk\n",
    "\n",
    "**Option (d): $\\lambda$ in Ridge has no impact**\n",
    "- **Ridge regression** also uses $\\lambda$ for regularization\n",
    "- **$\\lambda$ affects** both Lasso and Ridge\n",
    "- **Different penalty** functions but same concept\n",
    "\n",
    "**Key insight:** **Lasso regularization** controls overfitting through **sparsity induction** - higher $\\lambda$ = less overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 22. When comparing Lasso regression to Ridge regression, which of the following properties are true about Lasso regression? Select all that apply.**\n",
    "\n",
    "*   (a) Lasso regression can be used to select the most important features of a dataset.\n",
    "*   (b) Lasso regression tends to retain all features but with smaller coefficients.\n",
    "*   (c) Lasso regression is always better suited for handling high-dimensional data with a large number of features.\n",
    "*   (d) Lasso regression has fewer hyperparameters to tune.\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of Lasso vs Ridge** regression properties.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**Lasso regression can perform feature selection because:**\n",
    "- **L1 penalty** can set coefficients exactly to zero\n",
    "- **Sparsity induction** automatically selects relevant features\n",
    "- **Irrelevant features** get zero weights\n",
    "- **Automatic feature selection** is built-in\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (b): Retains all features with smaller coefficients**\n",
    "- **This describes Ridge regression** (L2 penalty)\n",
    "- **Ridge shrinks** coefficients but rarely to zero\n",
    "- **Lasso** sets coefficients exactly to zero\n",
    "\n",
    "**Option (c): Always better for high-dimensional data**\n",
    "- **Not always** - depends on data characteristics\n",
    "- **Ridge** can be better in some cases\n",
    "- **No universal** superiority\n",
    "\n",
    "**Option (d): Fewer hyperparameters**\n",
    "- **Both Lasso and Ridge** have same number of hyperparameters\n",
    "- **Both use** $\\lambda$ regularization parameter\n",
    "- **No difference** in hyperparameter count\n",
    "\n",
    "**Key insight:** **Lasso's L1 penalty** enables **automatic feature selection** by setting coefficients exactly to zero.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 23. A student is using ridge regression for housing price prediction. They notice that increasing the regularization strength improves validation set performance but worsens training set performance. What does this suggest about the model before adjusting regularization?**\n",
    "\n",
    "*   (a) The choice of features was inappropriate.\n",
    "*   (b) The model was underfitting the training data.\n",
    "*   (c) The regularization strength was too high.\n",
    "*   (d) The model was overfitting the training data.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:** The model was likely overfitting, capturing noise. Increasing regularization helps mitigate overfitting by penalizing large coefficients, leading to better generalization on unseen data (validation set).\n",
    "\n",
    "**Problem 24. For a twice-differentiable convex function $f: \\mathbb{R}^d \\to \\mathbb{R}$, what are the properties of the Hessian matrix, $\\nabla^2 f(x) \\in \\mathbb{R}^{d \\times d}$?**\n",
    "\n",
    "**Hint:** Consider the $d=1$ case (second derivative and shape).\n",
    "\n",
    "*   (a) $\\nabla^2 f(x)$ is negative semi-definite.\n",
    "*   (b) $\\nabla^2 f(x)$ is negative definite.\n",
    "*   (c) $\\nabla^2 f(x)$ is positive definite.\n",
    "*   (d) $\\nabla^2 f(x)$ is positive semi-definite.\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of convex functions** and their Hessian properties.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**For a convex function $f(x)$:**\n",
    "- **Second derivative** $f''(x) \\geq 0$ (in 1D)\n",
    "- **Hessian matrix** $\\nabla^2 f(x) \\succeq 0$ (positive semi-definite)\n",
    "- **All eigenvalues** are non-negative\n",
    "- **Curvature** is non-negative everywhere\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **Convex function** curves upward or is flat\n",
    "- **Second derivative** measures curvature\n",
    "- **Positive semi-definite** = non-negative curvature\n",
    "- **Can be flat** (eigenvalue = 0) but never curves down\n",
    "\n",
    "**Why other options are incorrect:**\n",
    "\n",
    "**Option (a): Negative semi-definite**\n",
    "- **This would be** concave function\n",
    "- **Opposite** of convex\n",
    "- **Curves downward**\n",
    "\n",
    "**Option (b): Negative definite**\n",
    "- **Strictly concave** function\n",
    "- **All eigenvalues** negative\n",
    "- **Not convex**\n",
    "\n",
    "**Option (c): Positive definite**\n",
    "- **Strictly convex** function\n",
    "- **All eigenvalues** positive\n",
    "- **More restrictive** than convex\n",
    "\n",
    "**Key insight:** **Convex functions** have **non-negative curvature** everywhere, making their Hessian **positive semi-definite**.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 25. True/False: A solution to a convex optimization problem is guaranteed to be a global minimum.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (a)\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "**This question tests understanding of convex optimization** and global optimality.\n",
    "\n",
    "**Why (a) is correct:**\n",
    "\n",
    "**For convex optimization problems:**\n",
    "- **Any local minimum** is also a global minimum\n",
    "- **No local minima** that aren't global\n",
    "- **Gradient-based methods** converge to global optimum\n",
    "- **Convexity guarantees** global optimality\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- **Convex function** has no \"valleys\" or local minima\n",
    "- **Any point** where gradient is zero is global minimum\n",
    "- **No risk** of getting stuck in local minimum\n",
    "- **Convexity** eliminates local vs global distinction\n",
    "\n",
    "**Why (b) is incorrect:**\n",
    "- **Convexity** doesn't guarantee uniqueness\n",
    "- **Multiple solutions** can exist (e.g., flat regions)\n",
    "- **Global minimum** can be attained at multiple points\n",
    "- **Uniqueness** requires additional conditions\n",
    "\n",
    "**Key insight:** **Convexity guarantees global optimality** but **not uniqueness** of the solution.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 26. True/False: A solution to a convex optimization problem is guaranteed to be unique.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:** The solution, while having minimal value, will not necessarily be unique. Consider the case of Least Squares with fewer data points than dimensions. There are an infinite number of solutions with the minimum value.\n",
    "\n",
    "**Problem 27. True/False: A convex optimization problem is guaranteed to have a closed-form solution.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of convex optimization** and solution methods.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**Convex optimization does NOT guarantee closed-form solutions because:**\n",
    "\n",
    "**1. Many convex problems require iterative methods:**\n",
    "- **Large-scale problems** (e.g., neural networks)\n",
    "- **Complex constraints** that can't be solved analytically\n",
    "- **Non-differentiable** convex functions\n",
    "- **High-dimensional** optimization problems\n",
    "\n",
    "**2. Examples requiring iterative methods:**\n",
    "- **Support Vector Machines** with kernel functions\n",
    "- **Logistic regression** with large datasets\n",
    "- **Neural network training**\n",
    "- **Lasso/Ridge regression** with many features\n",
    "\n",
    "**3. When closed-form solutions exist:**\n",
    "- **Linear regression** (normal equations)\n",
    "- **Simple quadratic** optimization problems\n",
    "- **Small-scale** problems with simple constraints\n",
    "- **Special cases** with analytical solutions\n",
    "\n",
    "**Why (a) is incorrect:**\n",
    "- **Many convex problems** require iterative optimization\n",
    "- **Closed-form solutions** are the exception, not the rule\n",
    "- **Computational complexity** often makes iterative methods necessary\n",
    "\n",
    "**Key insight:** **Convexity guarantees global optimality** but **not closed-form solutions** - many convex problems require iterative optimization methods.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 28. Briefly explain the main difference between Mini Batch Gradient Descent and Stochastic Gradient Descent. Then, describe one main advantage of using Mini Batch Gradient Descent over SGD.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Main Difference:** The main difference is that SGD uses a single training point to estimate the gradient, while Mini Batch chooses a set of $B$ training points (for some chosen constant $B$).\n",
    "\n",
    "**Main Advantage:** The main advantage of Mini Batch GD is that by using more points in the gradient estimation, we get a less noisy estimate which improves convergence.\n",
    "\n",
    "**Problem 29. True/False: Stochastic gradient descent provides biased estimates of the true gradient at each step.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**Correct answers:** (b)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of SGD gradient estimates** and their bias properties.\n",
    "\n",
    "**Why (b) is correct:**\n",
    "\n",
    "**SGD provides UNBIASED estimates of the true gradient:**\n",
    "\n",
    "**1. Unbiased gradient estimates:**\n",
    "- **$E[\\nabla L_i(\\theta)] = \\nabla L(\\theta)$** for random sample $i$\n",
    "- **Expected value** of SGD gradient equals true gradient\n",
    "- **No systematic bias** in gradient estimates\n",
    "- **Random sampling** preserves unbiasedness\n",
    "\n",
    "**2. Mathematical intuition:**\n",
    "- **True gradient:** $\\nabla L(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla L_i(\\theta)$\n",
    "- **SGD gradient:** $\\nabla L_i(\\theta)$ for random $i$\n",
    "- **$E[\\nabla L_i(\\theta)] = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla L_i(\\theta) = \\nabla L(\\theta)$**\n",
    "- **Unbiased** but high variance\n",
    "\n",
    "**3. Why SGD works:**\n",
    "- **Unbiased estimates** ensure convergence to true optimum\n",
    "- **High variance** is the trade-off for computational efficiency\n",
    "- **Law of large numbers** ensures convergence over many steps\n",
    "\n",
    "**Why (a) is incorrect:**\n",
    "- **SGD gradients** are unbiased, not biased\n",
    "- **Bias would prevent** convergence to true optimum\n",
    "- **Unbiasedness** is crucial for SGD convergence\n",
    "\n",
    "**Key insight:** **SGD provides unbiased gradient estimates** with **high variance** - the trade-off for computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 30. Consider some function $f(x): \\mathbb{R}^d \\to \\mathbb{R}$, and assume that we want to run an iterative algorithm to find the maximizer of $f$. Which update rule should we use to do this (for some $\\eta > 0$)?**\n",
    "\n",
    "*   (a) $x_{t+1} \\leftarrow -x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (b) $x_{t+1} \\leftarrow x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (c) $x_{t+1} \\leftarrow -x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (d) $x_{t+1} \\leftarrow x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "\n",
    "**Correct answers:** (d)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**This question tests understanding of optimization** and gradient ascent for maximization.\n",
    "\n",
    "**Why (d) is correct:**\n",
    "\n",
    "**For maximizing a function $f(x)$:**\n",
    "\n",
    "**1. Gradient ascent rule:**\n",
    "$x_{t+1} = x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "\n",
    "**2. Mathematical intuition:**\n",
    "- **Gradient points** in direction of steepest ascent\n",
    "- **Adding gradient** moves toward maximum\n",
    "- **Learning rate $\\eta$** controls step size\n",
    "- **Positive sign** for maximization\n",
    "\n",
    "**3. Why other options are incorrect:**\n",
    "\n",
    "**Option (a):** $x_{t+1} = -x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "- **Negative $x_t$** doesn't make sense for optimization\n",
    "- **Wrong direction** for maximization\n",
    "\n",
    "**Option (b):** $x_{t+1} = x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "- **This is gradient descent** (for minimization)\n",
    "- **Wrong direction** for maximization\n",
    "\n",
    "**Option (c):** $x_{t+1} = -x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "- **Both negative signs** are incorrect\n",
    "- **Neither direction** nor update makes sense\n",
    "\n",
    "**4. Key principle:**\n",
    "- **Gradient ascent:** $x_{t+1} = x_t + \\eta \\cdot \\nabla_x f(x_t)$ (maximization)\n",
    "- **Gradient descent:** $x_{t+1} = x_t - \\eta \\cdot \\nabla_x f(x_t)$ (minimization)\n",
    "\n",
    "**Key insight:** **Gradient ascent** moves **in the direction** of the gradient to **maximize** the function.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 31. You run a social media platform and are planning to implement a system to combat the spread of misinformation by detecting fake news articles. To keep things simple, the system only needs to identify articles as one of two classes: (1) being fake news, or (2) not being fake news. Of the model types we have learned in class so far, which would be the best choice to implement this system?**\n",
    "\n",
    "**Answer:** Logistic Regression\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "**This question tests understanding of model selection** for binary classification tasks.\n",
    "\n",
    "**Why Logistic Regression is the best choice:**\n",
    "\n",
    "**1. Binary classification task:**\n",
    "- **Two classes:** fake news vs not fake news\n",
    "- **Logistic regression** is designed for binary classification\n",
    "- **Outputs probabilities** $P(y=1|x)$\n",
    "- **Natural fit** for this problem\n",
    "\n",
    "**2. Advantages of Logistic Regression:**\n",
    "- **Interpretable** - coefficients show feature importance\n",
    "- **Probabilistic outputs** - confidence scores for predictions\n",
    "- **Efficient training** - convex optimization problem\n",
    "- **Good baseline** - often performs well on binary classification\n",
    "\n",
    "**3. Why other models are less suitable:**\n",
    "- **Linear regression:** Designed for continuous outputs, not classification\n",
    "- **Neural networks:** Overkill for simple binary classification\n",
    "- **Support Vector Machines:** More complex, less interpretable\n",
    "- **Decision trees:** Can be less stable than logistic regression\n",
    "\n",
    "**4. Practical considerations:**\n",
    "- **Text features** can be easily incorporated\n",
    "- **Feature engineering** (TF-IDF, word embeddings) works well\n",
    "- **Regularization** (L1/L2) can prevent overfitting\n",
    "- **Scalable** to large datasets\n",
    "\n",
    "**Key insight:** **Logistic Regression** is the **most appropriate** model for **binary classification** tasks like fake news detection.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
