{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed71a85-37d4-44d0-a3c6-9985de70a0b7",
   "metadata": {},
   "source": [
    "# Practice 7\n",
    "\n",
    "**1. In a machine learning classification problem, you have a dataset with two classes: Positive (P) and Negative (N). The probability of a randomly selected sample being Positive is $3/5$. The probability of a correct classification given that the sample is Positive is $4/5$, and the probability of a correct classification given that the sample is Negative is $7/10$. What is the probability that a randomly selected sample is Positive given that it has been classified as Positive? One Answer**\n",
    "\n",
    "*   (a) $\\frac{4}{5}$\n",
    "*   (b) $\\frac{12}{25}$\n",
    "*   (c) $\\frac{3}{5}$\n",
    "*   (d) $\\frac{12}{19}$\n",
    "\n",
    "**2. Which of the following statements must be true for a square matrix A to have an inverse matrix $A^{-1}$?**\n",
    "\n",
    "*   (a) A must be symmetric.\n",
    "*   (b) The rank of A is less than its number of columns.\n",
    "*   (c) A must have at least one column of 0s.\n",
    "*   (d) The determinant of A is not equal to 0.\n",
    "\n",
    "**3. Consider the following system of linear equations:**\n",
    "\n",
    "$$2x + 3y = 16$$\n",
    "$$4x + 6y = 32$$\n",
    "\n",
    "**Which of the following statements is true?**\n",
    "\n",
    "*   (a) The system has an infinite number of solutions because the two equations are linearly dependent.\n",
    "*   (b) The system has a unique solution because there are two equations for two unknowns.\n",
    "*   (c) The system has no solution because the determinant of the coefficient matrix is zero.\n",
    "*   (d) The system has no solution because the equations represent parallel lines that never intersect.\n",
    "\n",
    "**4. For any function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the gradient is defined as:**\n",
    "\n",
    "$$\\nabla_w f(w) = \\left[ \\frac{\\partial f(w)}{\\partial w_1} \\quad \\dots \\quad \\frac{\\partial f(w)}{\\partial w_n} \\right]^T$$\n",
    "\n",
    "**What is the value of $\\nabla_w (w^T Aw + u^T Bw + w^T Bv)$, given that $A, B \\in \\mathbb{R}^{n \\times n}$, $A$ is symmetric, and $u, v \\in \\mathbb{R}^n$?**\n",
    "\n",
    "*   (a) $Aw + Bu + B^T v$\n",
    "*   (b) $2Aw + B^T u + Bv$\n",
    "*   (c) $Aw + B^T u + Bv$\n",
    "*   (d) $2Aw + Bu + B^T v$\n",
    "\n",
    "**5. Which of the following statements is most accurate regarding the principle of Maximum Likelihood Estimation (MLE) in statistical modeling?**\n",
    "\n",
    "*   (a) MLE identifies model parameters that maximize the probability of the observed data under the model.\n",
    "*   (b) MLE directly computes the probability of parameters being correct, independent of observed data.\n",
    "*   (c) MLE is primarily concerned with minimizing the variance of parameter estimates for model stability.\n",
    "*   (d) MLE identifies model parameters that minimize the squared prediction error over the training data.\n",
    "\n",
    "**6. A machine learning engineer models the number of website requests per hour using a Poisson distribution. Over 6 hours, the observed requests are 4, 5, 6, 7, 8, and 9. Recall that the probability mass function for a Poisson distribution with parameter $\\lambda$ is:**\n",
    "\n",
    "$$P(x|\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!}$$\n",
    "\n",
    "**What is the maximum likelihood estimation of the rate parameter $\\lambda$ of this Poisson distribution?**\n",
    "\n",
    "*   (a) $e^{\\frac{39}{6}}$\n",
    "*   (b) $\\sqrt{\\frac{39}{6}}$\n",
    "*   (c) 6\n",
    "*   (d) $\\frac{39}{6}$\n",
    "\n",
    "**7. Assume a simple linear model $Y = Xw$. For simplicity, no intercept is considered. Given the following dataset:**\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "**(a) Compute the least squares estimate of $w$ without any regularization. You may leave your answer as a fraction, if necessary.**\n",
    "\n",
    "**Hint:** if $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, then $A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$\n",
    "\n",
    "**(b) Predict $\\hat{Y}$ for $X = \\begin{bmatrix} 6 \\\\ 7 \\end{bmatrix}$.**\n",
    "\n",
    "**8. You have access to data points $\\{(x_i, y_i)\\}_{i=1}^n$, where $x_i$ are $d$-dimensional vectors ($x_i \\in \\mathbb{R}^d$) and $y_i$ are scalars ($y_i \\in \\mathbb{R}$). Additionally, you have weights $\\{w_i\\}_{i=1}^n$, where $w_i \\in \\mathbb{R}$ and $w_i > 0$, representing the \"importance\" of each data point. You want to solve the weighted least squares regression problem:**\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta \\in \\mathbb{R}^d} \\sum_{i=1}^n w_i (x_i^T \\theta - y_i)^2$$\n",
    "\n",
    "**Let us define the matrices:**\n",
    "\n",
    "$$X = \\begin{bmatrix} x_1^T \\\\ \\vdots \\\\ x_n^T \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}$$\n",
    "$$Y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\in \\mathbb{R}^n$$\n",
    "$$W = \\begin{bmatrix} w_1 & 0 & \\dots & 0 \\\\ 0 & w_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & w_n \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "**What is $\\hat{\\theta}$ in terms of $X$, $Y$, and $W$?**\n",
    "\n",
    "*   (a) $(X^T X)^{-1} X^T W^{-1} Y$\n",
    "*   (b) $W(X^T X)^{-1} X^T Y$\n",
    "*   (c) $(X^T X)^{-1} X^T W Y$\n",
    "*   (d) $(X^T W X)^{-1} X^T Y$\n",
    "*   (e) $(X^T W X)^{-1} X^T W Y$\n",
    "\n",
    "**9. In the context of least squares regression, how does the presence of high noise levels in the data impact the reliability of the model's parameter estimates?**\n",
    "\n",
    "*   (a) High noise levels predominantly affect the intercept term of the regression model, but leave the slope estimates relatively unaffected.\n",
    "*   (b) High noise levels can increase the variability of the parameter estimates, potentially leading to a model that captures random noise rather than the true underlying relationship.\n",
    "*   (c) High noise levels decrease the variance of the estimated parameters, making the model more robust.\n",
    "*   (d) Noise in the data generally has minimal impact on the least squares estimates since the method inherently separates signal from noise in most scenarios.\n",
    "\n",
    "**10. In linear regression analysis using the least squares method, how might outliers in the dataset impact the resulting regression line?**\n",
    "\n",
    "*   (a) Outliers affect only the precision of the prediction intervals, not the regression line itself.\n",
    "*   (b) Outliers enhance the model's accuracy by providing a wider range of data points.\n",
    "*   (c) Outliers can significantly skew the regression line, potentially leading to an inaccurate representation of the overall data trend.\n",
    "*   (d) Outliers have a minimal impact, as the least squares method averages out their effects.\n",
    "\n",
    "**11. How does increasing the complexity of a model typically affect the properties of that model? Select all that apply.**\n",
    "\n",
    "*   (a) It tends to decrease bias but increase variance, potentially leading to overfitting.\n",
    "*   (b) It can increase training accuracy.\n",
    "*   (c) It tends to increase both bias and variance.\n",
    "*   (d) It tends to decrease variance but increase bias, potentially leading to underfitting.\n",
    "\n",
    "**12. True/False: A model with high variance tends to perform well on both the training and test data.**\n",
    "\n",
    "*   (a) False\n",
    "*   (b) True\n",
    "\n",
    "**13. The plots below show fits (in black) to the data points (\"x\" symbols in grey), using several different basis functions:**\n",
    "\n",
    "<img src=\"./plots.png\" width=\"550px\">\n",
    "\n",
    "**For each plot, please identify the basis function used:**\n",
    "\n",
    "**Plot (a): _____, Plot (b): _____, Plot (c): _____, Plot (d): _____**\n",
    "\n",
    "**Basis functions used:**\n",
    "1. $h_1(x) = [1,x]$\n",
    "2. $h_2(x) = [1,x,x^2]$\n",
    "3. $h_3(x) = [1,x,x^2,x^3]$\n",
    "4. $h_4(x) = [1, \\sin(\\frac{4\\pi}{25}x)]$\n",
    "\n",
    "**14. What is the purpose of general basis functions in linear regression?**\n",
    "\n",
    "*   (a) To increase convergence speed in gradient descent.\n",
    "*   (b) To encourage sparsity in learned weights.\n",
    "*   (c) To minimize computational complexity.\n",
    "*   (d) To transform input data into a higher-dimensional space to capture non-linear relationships.\n",
    "\n",
    "**15. What is the best description of 'irreducible error' in a machine learning predictor?**\n",
    "\n",
    "*   (a) It's due to inherent noise that cannot be eliminated by any model.\n",
    "*   (b) It's minimized by cross-validation.\n",
    "*   (c) It can be minimized by increasing training data size.\n",
    "*   (d) It arises from feature engineering or irrelevant features.\n",
    "\n",
    "**16. A polynomial regression model of degree $d=3$ approximates a quadratic function $g(x) = 7x^2 + \\epsilon$, where $\\epsilon$ is a Gaussian random variable with mean $\\mu=0$ and variance $\\sigma^2=4$. What is the irreducible error?**\n",
    "\n",
    "*   (a) 2\n",
    "*   (b) 0\n",
    "*   (c) 4\n",
    "*   (d) $x^3$\n",
    "\n",
    "**17. True/False: Increasing the proportion of your dataset allocated to training (as opposed to testing) will guarantee better performance on unseen data.**\n",
    "\n",
    "*   (a) False\n",
    "*   (b) True\n",
    "\n",
    "**18. Which of the following statements best describes a potential issue that can arise if the test dataset is not properly separated from the training dataset?**\n",
    "\n",
    "*   (a) The model will always underfit, regardless of the algorithm used.\n",
    "*   (b) The model will always overfit, regardless of the algorithm used.\n",
    "*   (c) The evaluation metrics will tend to overestimate the prediction error on unseen data.\n",
    "*   (d) The test data will influence the training process, leading to an overly optimistic estimate of the model's performance on new, unseen data.\n",
    "*   (e) The model's computational complexity will significantly increase, resulting in longer training times.\n",
    "\n",
    "**19. How should data preprocessing be applied when using k-fold cross-validation? Select the most accurate answer.**\n",
    "\n",
    "*   (a) Preprocess the entire dataset before splitting into folds to maintain consistency.\n",
    "*   (b) Avoid preprocessing as it can bias the cross-validation results.\n",
    "*   (c) Only preprocess the test folds and train our model on raw (unprocessed) data.\n",
    "*   (d) Apply preprocessing separately on each iteration of k-fold validation to avoid data leakage.\n",
    "\n",
    "**20. What is the main advantage of using k-fold cross-validation? One Answer**\n",
    "\n",
    "*   (a) It guarantees improvement in model accuracy on unseen data.\n",
    "*   (b) It provides an estimate of model performance for given hyperparameters.\n",
    "*   (c) It significantly reduces the training time of the model by dividing the dataset into smaller parts.\n",
    "*   (d) It eliminates the need for a separate test dataset.\n",
    "\n",
    "**21. In Lasso regression, how does the regularization parameter $\\lambda$ influence the risk of overfitting? Select all that apply.**\n",
    "\n",
    "*   (a) Increasing $\\lambda$ always increases the risk of overfitting as it leads to higher model complexity.\n",
    "*   (b) Decreasing $\\lambda$ to zero may increase the risk of overfitting.\n",
    "*   (c) Increasing $\\lambda$ typically reduces the risk of overfitting by increasing sparsity.\n",
    "*   (d) The choice of $\\lambda$ in Ridge regression has no impact on the risk of overfitting.\n",
    "\n",
    "**22. When comparing Lasso regression to Ridge regression, which of the following properties are true about Lasso regression? Select all that apply.**\n",
    "\n",
    "*   (a) Lasso regression can be used to select the most important features of a dataset.\n",
    "*   (b) Lasso regression tends to retain all features but with smaller coefficients.\n",
    "*   (c) Lasso regression is always better suited for handling high-dimensional data with a large number of features.\n",
    "*   (d) Lasso regression has fewer hyperparameters to tune.\n",
    "\n",
    "**23. A student is using ridge regression for housing price prediction. They notice that increasing the regularization strength improves validation set performance but worsens training set performance. What does this suggest about the model before adjusting regularization?**\n",
    "\n",
    "*   (a) The choice of features was inappropriate.\n",
    "*   (b) The model was underfitting the training data.\n",
    "*   (c) The regularization strength was too high.\n",
    "*   (d) The model was overfitting the training data.\n",
    "\n",
    "**24. For a twice-differentiable convex function $f: \\mathbb{R}^d \\to \\mathbb{R}$, what are the properties of the Hessian matrix, $\\nabla^2 f(x) \\in \\mathbb{R}^{d \\times d}$?**\n",
    "\n",
    "**Hint:** Consider the $d=1$ case (second derivative and shape).\n",
    "\n",
    "*   (a) $\\nabla^2 f(x)$ is negative semi-definite.\n",
    "*   (b) $\\nabla^2 f(x)$ is negative definite.\n",
    "*   (c) $\\nabla^2 f(x)$ is positive definite.\n",
    "*   (d) $\\nabla^2 f(x)$ is positive semi-definite.\n",
    "\n",
    "**25. True/False: A solution to a convex optimization problem is guaranteed to be a global minimum.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**26. True/False: A solution to a convex optimization problem is guaranteed to be unique.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**27. True/False: A convex optimization problem is guaranteed to have a closed-form solution.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**28. Briefly explain the main difference between Mini Batch Gradient Descent and Stochastic Gradient Descent. Then, describe one main advantage of using Mini Batch Gradient Descent over SGD.**\n",
    "\n",
    "**29. True/False: Stochastic gradient descent provides biased estimates of the true gradient at each step.**\n",
    "\n",
    "*   (a) True\n",
    "*   (b) False\n",
    "\n",
    "**30. Consider some function $f(x): \\mathbb{R}^d \\to \\mathbb{R}$, and assume that we want to run an iterative algorithm to find the maximizer of $f$. Which update rule should we use to do this (for some $\\eta > 0$)?**\n",
    "\n",
    "*   (a) $x_{t+1} \\leftarrow -x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (b) $x_{t+1} \\leftarrow x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (c) $x_{t+1} \\leftarrow -x_t - \\eta \\cdot \\nabla_x f(x_t)$\n",
    "*   (d) $x_{t+1} \\leftarrow x_t + \\eta \\cdot \\nabla_x f(x_t)$\n",
    "\n",
    "**31. You run a social media platform and are planning to implement a system to combat the spread of misinformation by detecting fake news articles. To keep things simple, the system only needs to identify articles as one of two classes: (1) being fake news, or (2) not being fake news. Of the model types we have learned in class so far, which would be the best choice to implement this system?**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
