{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454767e-7657-4769-8e31-d79a7bdd17f5",
   "metadata": {},
   "source": [
    "# Neural Network Modules: Building Blocks of Modern Deep Learning\n",
    "\n",
    "## Introduction to Modular Design: The LEGO Blocks of Deep Learning\n",
    "\n",
    "Modern neural networks are built using a modular approach, where complex architectures are constructed from simpler, reusable building blocks called modules. This modular design philosophy has several advantages:\n",
    "\n",
    "1. **Reusability**: Modules can be combined in different ways to create various architectures\n",
    "2. **Maintainability**: Each module has a well-defined interface and functionality\n",
    "3. **Composability**: Complex networks can be built by composing simple modules\n",
    "4. **Interpretability**: Each module can be understood and analyzed independently\n",
    "\n",
    "**Real-World Analogy: The LEGO Building Problem**\n",
    "Think of neural network modules like LEGO blocks:\n",
    "- **Individual blocks**: Each module has a specific function (linear layer, activation, etc.)\n",
    "- **Combination**: Blocks can be snapped together in different ways\n",
    "- **Complexity**: Simple blocks can build complex structures\n",
    "- **Reusability**: Same blocks can be used in different models\n",
    "- **Understanding**: Each block's function is clear and well-defined\n",
    "\n",
    "**Visual Analogy: The Kitchen Appliance Problem**\n",
    "Think of modules like kitchen appliances:\n",
    "- **Blender**: Processes ingredients (activation function)\n",
    "- **Oven**: Transforms food (linear transformation)\n",
    "- **Mixer**: Combines ingredients (concatenation)\n",
    "- **Recipe**: Combines appliances in sequence (module composition)\n",
    "- **Result**: Complex dishes from simple tools\n",
    "\n",
    "**Mathematical Intuition: Function Composition**\n",
    "Modules are mathematical functions that can be composed:\n",
    "```math\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "```\n",
    "Where each $f_i$ is a module that transforms its input into a more useful representation.\n",
    "\n",
    "### What Are Neural Network Modules? - The Atomic Units of Computation\n",
    "\n",
    "A neural network module is a mathematical function that:\n",
    "- Takes inputs and produces outputs\n",
    "- May have learnable parameters\n",
    "- Can be composed with other modules\n",
    "- Has a well-defined computational graph\n",
    "\n",
    "**Real-World Analogy: The Factory Machine Problem**\n",
    "Think of modules like factory machines:\n",
    "- **Input**: Raw materials (data)\n",
    "- **Processing**: Machine transforms materials (computation)\n",
    "- **Output**: Processed product (transformed data)\n",
    "- **Parameters**: Machine settings (learnable weights)\n",
    "- **Assembly Line**: Machines connected in sequence (module composition)\n",
    "\n",
    "**Visual Analogy: The Pipeline Problem**\n",
    "Think of modules like pipeline stages:\n",
    "- **Stage 1**: Filter water (remove noise)\n",
    "- **Stage 2**: Add chemicals (transform features)\n",
    "- **Stage 3**: Test quality (evaluate output)\n",
    "- **Connection**: Output of one stage feeds into next\n",
    "- **Result**: Clean water from dirty input\n",
    "\n",
    "### Mathematical Framework: The Formal Foundation\n",
    "\n",
    "A module can be viewed as a parameterized function:\n",
    "\n",
    "```math\n",
    "f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{X}$ is the input space\n",
    "- $\\mathcal{Y}$ is the output space\n",
    "- $\\theta$ are the learnable parameters\n",
    "\n",
    "**Real-World Analogy: The Translation Problem**\n",
    "Think of modules like language translators:\n",
    "- **Input**: Text in one language (input space)\n",
    "- **Output**: Text in another language (output space)\n",
    "- **Parameters**: Translation rules and vocabulary (learnable weights)\n",
    "- **Composition**: English → French → Spanish (multiple modules)\n",
    "\n",
    "**Practical Example - Module Composition:**\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def demonstrate_modular_design():\n",
    "    \"\"\"Demonstrate the power of modular design\"\"\"\n",
    "    \n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    X, y = make_circles(n_samples=1000, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define simple modules\n",
    "    def linear_module(x, W, b):\n",
    "        \"\"\"Linear transformation module\"\"\"\n",
    "        return np.dot(x, W.T) + b\n",
    "    \n",
    "    def relu_module(x):\n",
    "        \"\"\"ReLU activation module\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid_module(x):\n",
    "        \"\"\"Sigmoid activation module\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Compose modules to create different architectures\n",
    "    def simple_mlp(x, W1, b1, W2, b2):\n",
    "        \"\"\"Simple MLP: Linear → ReLU → Linear → Sigmoid\"\"\"\n",
    "        h1 = linear_module(x, W1, b1)  # Module 1: Linear\n",
    "        h2 = relu_module(h1)           # Module 2: ReLU\n",
    "        h3 = linear_module(h2, W2, b2) # Module 3: Linear\n",
    "        output = sigmoid_module(h3)    # Module 4: Sigmoid\n",
    "        return output\n",
    "    \n",
    "    def deep_mlp(x, weights, biases):\n",
    "        \"\"\"Deep MLP with multiple layers\"\"\"\n",
    "        h = x\n",
    "        for i in range(len(weights) - 1):\n",
    "            h = linear_module(h, weights[i], biases[i])  # Linear module\n",
    "            h = relu_module(h)                          # ReLU module\n",
    "        # Final layer\n",
    "        output = linear_module(h, weights[-1], biases[-1])  # Linear module\n",
    "        output = sigmoid_module(output)                     # Sigmoid module\n",
    "        return output\n",
    "    \n",
    "    # Train simple model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    simple_model = LogisticRegression(random_state=42)\n",
    "    simple_model.fit(X_train, y_train)\n",
    "    simple_score = simple_model.score(X_test, y_test)\n",
    "    \n",
    "    # Train neural network (modular approach)\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn_model = MLPClassifier(hidden_layer_sizes=(10, 5), random_state=42, max_iter=1000)\n",
    "    nn_model.fit(X_train, y_train)\n",
    "    nn_score = nn_model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Modular Design Results:\")\n",
    "    print(f\"Simple Linear Model: {simple_score:.3f}\")\n",
    "    print(f\"Modular Neural Network: {nn_score:.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original data\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, s=20)\n",
    "    plt.title('Original Data')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Simple model decision boundary\n",
    "    plt.subplot(1, 3, 2)\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    Z_simple = simple_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z_simple = Z_simple.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z_simple, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, s=20)\n",
    "    plt.title(f'Simple Model\\nAccuracy: {simple_score:.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Modular model decision boundary\n",
    "    plt.subplot(1, 3, 3)\n",
    "    Z_nn = nn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z_nn = Z_nn.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z_nn, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, s=20)\n",
    "    plt.title(f'Modular Neural Network\\nAccuracy: {nn_score:.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return simple_score, nn_score\n",
    "\n",
    "modular_demo = demonstrate_modular_design()\n",
    "```\n",
    "\n",
    "## From Neural Network Fundamentals to Modular Design: The Bridge to Scalability\n",
    "\n",
    "We've now explored the fundamental building blocks of neural networks - from individual neurons with their activation functions to multi-layer architectures that can learn complex patterns. We've seen how the mathematical principles of non-linear transformations and function composition translate into concrete neural network designs.\n",
    "\n",
    "However, as neural networks become more complex and are applied to increasingly sophisticated problems, we need to move beyond basic architectures to **modular design principles**. Modern deep learning systems are built using reusable components that can be composed to create complex architectures efficiently.\n",
    "\n",
    "This motivates our exploration of **neural network modules** - the building blocks that enable us to construct sophisticated architectures systematically. We'll see how common patterns (like fully connected layers, convolutional layers, and attention mechanisms) can be implemented as reusable modules that can be combined in various ways.\n",
    "\n",
    "The transition from neural network fundamentals to modular design represents the bridge from understanding basic architectures to building practical, scalable systems - taking our knowledge of how neural networks work and turning it into a systematic approach for constructing complex models.\n",
    "\n",
    "In this section, we'll explore how to design and implement neural network modules, how to compose them into larger architectures, and how this modular approach enables both flexibility and efficiency in deep learning systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Basic Building Blocks: The Foundation Modules\n",
    "\n",
    "### Matrix Multiplication Module: The Workhorse of Neural Networks\n",
    "\n",
    "The most fundamental module in neural networks is the matrix multiplication module, which performs linear transformations.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "```math\n",
    "\\mathrm{MM}_{W, b}(z) = Wz + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{n \\times m}$ is the weight matrix\n",
    "- $b \\in \\mathbb{R}^n$ is the bias vector\n",
    "- $z \\in \\mathbb{R}^m$ is the input vector\n",
    "- The output has dimension $n$\n",
    "\n",
    "**Real-World Analogy: The Recipe Scaling Problem**\n",
    "Think of matrix multiplication like scaling a recipe:\n",
    "- **Input**: Ingredients for 2 people (input vector)\n",
    "- **Weight Matrix**: Scaling factors for different ingredients\n",
    "- **Bias**: Base amounts that don't scale (salt, spices)\n",
    "- **Output**: Ingredients for 10 people (output vector)\n",
    "\n",
    "**Visual Analogy: The Factory Assembly Line**\n",
    "Think of matrix multiplication like a factory assembly line:\n",
    "- **Input**: Raw materials (input vector)\n",
    "- **Weight Matrix**: Processing instructions for each material\n",
    "- **Bias**: Fixed overhead costs\n",
    "- **Output**: Finished products (output vector)\n",
    "\n",
    "#### Properties\n",
    "\n",
    "1. **Linearity**: $\\mathrm{MM}_{W, b}(az_1 + bz_2) = a\\mathrm{MM}_{W, b}(z_1) + b\\mathrm{MM}_{W, b}(z_2)$\n",
    "2. **Parameter Count**: $nm + n$ parameters (weights + biases)\n",
    "3. **Computational Complexity**: $O(nm)$ operations\n",
    "\n",
    "**Real-World Analogy: The Tax System Problem**\n",
    "Think of linearity like a tax system:\n",
    "- **Linearity**: If you double your income, you double your tax\n",
    "- **Additivity**: Tax on income A + Tax on income B = Tax on (A + B)\n",
    "- **Result**: Predictable, fair, but limited in complexity\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The matrix multiplication module learns to:\n",
    "- **Project**: Transform data from one space to another\n",
    "- **Combine**: Linearly combine input features\n",
    "- **Scale**: Apply different weights to different features\n",
    "\n",
    "**Practical Example - Matrix Multiplication:**\n",
    "```python\n",
    "def demonstrate_matrix_multiplication():\n",
    "    \"\"\"Demonstrate matrix multiplication module\"\"\"\n",
    "    \n",
    "    # Example: House price prediction\n",
    "    # Features: [square_feet, bedrooms, age, location_score]\n",
    "    houses = np.array([\n",
    "        [2000, 3, 10, 8.5],  # House 1\n",
    "        [1500, 2, 5, 7.0],   # House 2\n",
    "        [3000, 4, 15, 9.0],  # House 3\n",
    "        [1200, 1, 3, 6.5]    # House 4\n",
    "    ])\n",
    "    \n",
    "    # Weight matrix: [price_per_sqft, price_per_bedroom, age_penalty, location_premium]\n",
    "    W = np.array([\n",
    "        [100, 5000, -1000, 2000],  # Model 1: Price-focused\n",
    "        [75, 3000, -500, 1500],    # Model 2: Balanced\n",
    "        [50, 2000, -200, 1000]     # Model 3: Budget-focused\n",
    "    ])\n",
    "    \n",
    "    # Bias: Base price for each model\n",
    "    b = np.array([50000, 75000, 100000])\n",
    "    \n",
    "    # Apply matrix multiplication module\n",
    "    predictions = np.dot(houses, W.T) + b\n",
    "    \n",
    "    print(\"Matrix Multiplication Module: House Price Prediction\")\n",
    "    print(\"Input Features: [square_feet, bedrooms, age, location_score]\")\n",
    "    print()\n",
    "    print(\"Houses:\")\n",
    "    for i, house in enumerate(houses):\n",
    "        print(f\"  House {i+1}: {house}\")\n",
    "    print()\n",
    "    print(\"Weight Matrix (price per unit):\")\n",
    "    print(f\"  Model 1 (Price-focused): {W[0]}\")\n",
    "    print(f\"  Model 2 (Balanced): {W[1]}\")\n",
    "    print(f\"  Model 3 (Budget-focused): {W[2]}\")\n",
    "    print()\n",
    "    print(\"Bias (base price):\", b)\n",
    "    print()\n",
    "    print(\"Predictions:\")\n",
    "    for i, house in enumerate(houses):\n",
    "        print(f\"  House {i+1}:\")\n",
    "        for j, model_name in enumerate(['Price-focused', 'Balanced', 'Budget-focused']):\n",
    "            print(f\"    {model_name}: ${predictions[i, j]:,.0f}\")\n",
    "        print()\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Show how different features contribute\n",
    "    plt.subplot(1, 3, 1)\n",
    "    feature_names = ['Square Feet', 'Bedrooms', 'Age', 'Location']\n",
    "    model_names = ['Price-focused', 'Balanced', 'Budget-focused']\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        plt.bar(feature_names, W[i], alpha=0.7, label=model_name)\n",
    "    \n",
    "    plt.title('Feature Weights by Model')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show predictions for each house\n",
    "    plt.subplot(1, 3, 2)\n",
    "    x_pos = np.arange(len(houses))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        plt.bar(x_pos + i*width, predictions[:, i], width, alpha=0.7, label=model_name)\n",
    "    \n",
    "    plt.title('Price Predictions by Model')\n",
    "    plt.xlabel('House')\n",
    "    plt.ylabel('Predicted Price ($)')\n",
    "    plt.xticks(x_pos + width, [f'House {i+1}' for i in range(len(houses))])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show linearity property\n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Test linearity: double the features, double the output\n",
    "    doubled_houses = houses * 2\n",
    "    doubled_predictions = np.dot(doubled_houses, W.T) + b\n",
    "    original_predictions = np.dot(houses, W.T) + b\n",
    "    \n",
    "    plt.scatter(original_predictions.flatten(), doubled_predictions.flatten(), alpha=0.7)\n",
    "    plt.plot([0, max(original_predictions.flatten())], [0, max(doubled_predictions.flatten())], 'r--', label='Perfect Linearity')\n",
    "    plt.xlabel('Original Predictions')\n",
    "    plt.ylabel('Doubled Input Predictions')\n",
    "    plt.title('Linearity Test')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return houses, W, b, predictions\n",
    "\n",
    "matrix_demo = demonstrate_matrix_multiplication()\n",
    "```\n",
    "\n",
    "### Activation Module: The Source of Non-Linearity\n",
    "\n",
    "Activation modules introduce non-linearity into neural networks, enabling them to learn complex patterns.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "```math\n",
    "\\sigma(z) = [\\sigma(z_1), \\sigma(z_2), \\ldots, \\sigma(z_n)]^T\n",
    "```\n",
    "\n",
    "Where $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a non-linear function applied element-wise.\n",
    "\n",
    "**Real-World Analogy: The Water Filter Problem**\n",
    "Think of activation functions like water filters:\n",
    "- **Input**: Dirty water (raw features)\n",
    "- **Filter**: Removes impurities (non-linear transformation)\n",
    "- **Output**: Clean water (processed features)\n",
    "- **Different filters**: Different types of cleaning (ReLU, sigmoid, tanh)\n",
    "\n",
    "**Visual Analogy: The Light Bulb Problem**\n",
    "Think of activation functions like light bulbs:\n",
    "- **Input**: Voltage (raw signal)\n",
    "- **Bulb Type**: Different response curves (activation function)\n",
    "- **Output**: Light intensity (processed signal)\n",
    "- **Non-linearity**: Brightness doesn't scale linearly with voltage\n",
    "\n",
    "#### Common Activation Functions\n",
    "\n",
    "1. **ReLU**: $\\sigma(z) = \\max(0, z)$\n",
    "2. **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "3. **Tanh**: $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "4. **GELU**: $\\sigma(z) = z \\cdot \\Phi(z)$\n",
    "\n",
    "**Real-World Analogy: The Decision Making Problem**\n",
    "Think of activation functions like decision-making processes:\n",
    "- **ReLU**: \"If positive, keep it; if negative, ignore it\"\n",
    "- **Sigmoid**: \"Convert to probability between 0 and 1\"\n",
    "- **Tanh**: \"Scale to range between -1 and 1\"\n",
    "- **GELU**: \"Smooth version of ReLU with better properties\"\n",
    "\n",
    "#### Properties\n",
    "\n",
    "1. **Non-linearity**: Essential for modeling complex relationships\n",
    "2. **Differentiability**: Required for gradient-based optimization\n",
    "3. **Element-wise**: Applied independently to each component\n",
    "\n",
    "**Practical Example - Activation Functions:**\n",
    "```python\n",
    "def demonstrate_activation_functions():\n",
    "    \"\"\"Demonstrate different activation functions\"\"\"\n",
    "    \n",
    "    # Generate input data\n",
    "    z = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Define activation functions\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def gelu(x):\n",
    "        return x * 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    # Calculate activations\n",
    "    relu_output = relu(z)\n",
    "    sigmoid_output = sigmoid(z)\n",
    "    tanh_output = tanh(z)\n",
    "    gelu_output = gelu(z)\n",
    "    \n",
    "    # Calculate derivatives\n",
    "    relu_deriv = np.where(z > 0, 1, 0)\n",
    "    sigmoid_deriv = sigmoid_output * (1 - sigmoid_output)\n",
    "    tanh_deriv = 1 - tanh_output**2\n",
    "    gelu_deriv = 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3))) + \\\n",
    "                 0.5 * z * (1 - np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3))**2) * \\\n",
    "                 np.sqrt(2/np.pi) * (1 + 3 * 0.044715 * z**2)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Activation functions\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.plot(z, relu_output, 'b-', linewidth=2)\n",
    "    plt.title('ReLU: max(0, z)')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.plot(z, sigmoid_output, 'r-', linewidth=2)\n",
    "    plt.title('Sigmoid: 1/(1 + e^(-z))')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.plot(z, tanh_output, 'g-', linewidth=2)\n",
    "    plt.title('Tanh: (e^z - e^(-z))/(e^z + e^(-z))')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.plot(z, gelu_output, 'm-', linewidth=2)\n",
    "    plt.title('GELU: z * Φ(z)')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Derivatives\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(z, relu_deriv, 'b-', linewidth=2)\n",
    "    plt.title('ReLU Derivative')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ\\'(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.plot(z, sigmoid_deriv, 'r-', linewidth=2)\n",
    "    plt.title('Sigmoid Derivative')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ\\'(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.plot(z, tanh_deriv, 'g-', linewidth=2)\n",
    "    plt.title('Tanh Derivative')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ\\'(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.plot(z, gelu_deriv, 'm-', linewidth=2)\n",
    "    plt.title('GELU Derivative')\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('σ\\'(z)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show practical example\n",
    "    print(\"Activation Function Properties:\")\n",
    "    print(\"ReLU:\")\n",
    "    print(\"  - Range: [0, ∞)\")\n",
    "    print(\"  - Pros: Simple, efficient, no vanishing gradient\")\n",
    "    print(\"  - Cons: Can 'die' (get stuck at 0)\")\n",
    "    print(\"  - Use case: Hidden layers in most networks\")\n",
    "    print()\n",
    "    print(\"Sigmoid:\")\n",
    "    print(\"  - Range: (0, 1)\")\n",
    "    print(\"  - Pros: Smooth, interpretable as probability\")\n",
    "    print(\"  - Cons: Vanishing gradient problem\")\n",
    "    print(\"  - Use case: Output layer for binary classification\")\n",
    "    print()\n",
    "    print(\"Tanh:\")\n",
    "    print(\"  - Range: (-1, 1)\")\n",
    "    print(\"  - Pros: Zero-centered, bounded\")\n",
    "    print(\"  - Cons: Still has vanishing gradient\")\n",
    "    print(\"  - Use case: Hidden layers when zero-centered output is desired\")\n",
    "    print()\n",
    "    print(\"GELU:\")\n",
    "    print(\"  - Range: (-∞, ∞)\")\n",
    "    print(\"  - Pros: Smooth, often performs better than ReLU\")\n",
    "    print(\"  - Cons: More computationally expensive\")\n",
    "    print(\"  - Use case: Transformer architectures\")\n",
    "    \n",
    "    return z, relu_output, sigmoid_output, tanh_output, gelu_output\n",
    "\n",
    "activation_demo = demonstrate_activation_functions()\n",
    "```\n",
    "\n",
    "### Composing Modules: The Power of Combination\n",
    "\n",
    "Modules can be composed to create more complex functions:\n",
    "\n",
    "```math\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "```\n",
    "\n",
    "Where each $f_i$ is a module.\n",
    "\n",
    "**Real-World Analogy: The Recipe Problem**\n",
    "Think of module composition like following a recipe:\n",
    "- **Step 1**: Mix ingredients (linear transformation)\n",
    "- **Step 2**: Apply heat (activation function)\n",
    "- **Step 3**: Add seasoning (another linear transformation)\n",
    "- **Step 4**: Final cooking (final activation)\n",
    "- **Result**: Complex dish from simple steps\n",
    "\n",
    "**Visual Analogy: The Assembly Line Problem**\n",
    "Think of module composition like an assembly line:\n",
    "- **Station 1**: Cut metal (linear transformation)\n",
    "- **Station 2**: Bend metal (activation function)\n",
    "- **Station 3**: Weld parts (linear transformation)\n",
    "- **Station 4**: Paint (activation function)\n",
    "- **Result**: Complex product from simple operations\n",
    "\n",
    "#### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "An MLP is a composition of matrix multiplication and activation modules:\n",
    "\n",
    "```math\n",
    "\\mathrm{MLP}(x) = \\mathrm{MM}_{W^{[L]}, b^{[L]}}(\\sigma(\\mathrm{MM}_{W^{[L-1]}, b^{[L-1]}}(\\cdots \\mathrm{MM}_{W^{[1]}, b^{[1]}}(x))\\cdots))\n",
    "```\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "```math\n",
    "\\mathrm{MLP}(x) = \\mathrm{MM}(\\sigma(\\mathrm{MM}(\\cdots \\mathrm{MM}(x))))\n",
    "```\n",
    "\n",
    "**Real-World Analogy: The Translation Chain Problem**\n",
    "Think of MLP like a translation chain:\n",
    "- **Input**: English text\n",
    "- **Layer 1**: English → French (linear + activation)\n",
    "- **Layer 2**: French → German (linear + activation)\n",
    "- **Layer 3**: German → Spanish (linear + activation)\n",
    "- **Output**: Spanish text\n",
    "- **Result**: Complex translation through simple steps\n",
    "\n",
    "#### Computational Graph\n",
    "\n",
    "The computational graph shows the flow of data through the modules:\n",
    "\n",
    "```math\n",
    "Input → MM₁ → σ₁ → MM₂ → σ₂ → ... → MMₗ → Output\n",
    "```\n",
    "\n",
    "**Practical Example - Module Composition:**\n",
    "```python\n",
    "def demonstrate_module_composition():\n",
    "    \"\"\"Demonstrate how modules can be composed\"\"\"\n",
    "    \n",
    "    # Define simple modules\n",
    "    def linear_module(x, W, b):\n",
    "        \"\"\"Linear transformation module\"\"\"\n",
    "        return np.dot(x, W.T) + b\n",
    "    \n",
    "    def relu_module(x):\n",
    "        \"\"\"ReLU activation module\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid_module(x):\n",
    "        \"\"\"Sigmoid activation module\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)  # 100 samples, 2 features\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Simple classification task\n",
    "    \n",
    "    # Define different compositions\n",
    "    def composition_1(x, W1, b1, W2, b2):\n",
    "        \"\"\"Simple composition: Linear → ReLU → Linear → Sigmoid\"\"\"\n",
    "        h1 = linear_module(x, W1, b1)      # Module 1: Linear\n",
    "        h2 = relu_module(h1)               # Module 2: ReLU\n",
    "        h3 = linear_module(h2, W2, b2)     # Module 3: Linear\n",
    "        output = sigmoid_module(h3)        # Module 4: Sigmoid\n",
    "        return output, [h1, h2, h3, output]\n",
    "    \n",
    "    def composition_2(x, W1, b1, W2, b2, W3, b3):\n",
    "        \"\"\"Deep composition: Linear → ReLU → Linear → ReLU → Linear → Sigmoid\"\"\"\n",
    "        h1 = linear_module(x, W1, b1)      # Module 1: Linear\n",
    "        h2 = relu_module(h1)               # Module 2: ReLU\n",
    "        h3 = linear_module(h2, W2, b2)     # Module 3: Linear\n",
    "        h4 = relu_module(h3)               # Module 4: ReLU\n",
    "        h5 = linear_module(h4, W3, b3)     # Module 5: Linear\n",
    "        output = sigmoid_module(h5)        # Module 6: Sigmoid\n",
    "        return output, [h1, h2, h3, h4, h5, output]\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    W1 = np.random.randn(5, 2) * 0.1\n",
    "    b1 = np.zeros(5)\n",
    "    W2 = np.random.randn(3, 5) * 0.1\n",
    "    b2 = np.zeros(3)\n",
    "    W3 = np.random.randn(1, 3) * 0.1\n",
    "    b3 = np.zeros(1)\n",
    "    \n",
    "    # Test compositions\n",
    "    output1, activations1 = composition_1(X[:5], W1, b1, W3, b3)\n",
    "    output2, activations2 = composition_2(X[:5], W1, b1, W2, b2, W3, b3)\n",
    "    \n",
    "    print(\"Module Composition Example:\")\n",
    "    print(\"Input shape:\", X[:5].shape)\n",
    "    print()\n",
    "    print(\"Composition 1 (Linear → ReLU → Linear → Sigmoid):\")\n",
    "    for i, (name, activation) in enumerate(zip(['Linear1', 'ReLU1', 'Linear2', 'Sigmoid'], activations1)):\n",
    "        print(f\"  {name}: shape {activation.shape}, range [{activation.min():.3f}, {activation.max():.3f}]\")\n",
    "    print()\n",
    "    print(\"Composition 2 (Linear → ReLU → Linear → ReLU → Linear → Sigmoid):\")\n",
    "    for i, (name, activation) in enumerate(zip(['Linear1', 'ReLU1', 'Linear2', 'ReLU2', 'Linear3', 'Sigmoid'], activations2)):\n",
    "        print(f\"  {name}: shape {activation.shape}, range [{activation.min():.3f}, {activation.max():.3f}]\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Show computational graphs\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.text(0.5, 0.5, 'Composition 1:\\nInput → Linear → ReLU → Linear → Sigmoid', \n",
    "             ha='center', va='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.title('Computational Graph 1')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.text(0.5, 0.5, 'Composition 2:\\nInput → Linear → ReLU → Linear → ReLU → Linear → Sigmoid', \n",
    "             ha='center', va='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.title('Computational Graph 2')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show activation ranges\n",
    "    plt.subplot(1, 3, 3)\n",
    "    names1 = ['Linear1', 'ReLU1', 'Linear2', 'Sigmoid']\n",
    "    ranges1 = [activations1[i].max() - activations1[i].min() for i in range(len(activations1))]\n",
    "    \n",
    "    names2 = ['Linear1', 'ReLU1', 'Linear2', 'ReLU2', 'Linear3', 'Sigmoid']\n",
    "    ranges2 = [activations2[i].max() - activations2[i].min() for i in range(len(activations2))]\n",
    "    \n",
    "    x1 = np.arange(len(names1))\n",
    "    x2 = np.arange(len(names2))\n",
    "    \n",
    "    plt.bar(x1 - 0.2, ranges1, 0.4, alpha=0.7, label='Composition 1')\n",
    "    plt.bar(x2 + 0.2, ranges2, 0.4, alpha=0.7, label='Composition 2')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Activation Range')\n",
    "    plt.title('Activation Ranges by Layer')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output1, output2, activations1, activations2\n",
    "\n",
    "composition_demo = demonstrate_module_composition()\n",
    "```\n",
    "\n",
    "**Key Insights from Basic Building Blocks:**\n",
    "1. **Matrix multiplication is fundamental**: All linear transformations are matrix multiplications\n",
    "2. **Activation functions add non-linearity**: Without them, we're just doing linear algebra\n",
    "3. **Composition is powerful**: Simple modules can build complex functions\n",
    "4. **Computational graphs show flow**: Visual representation of data transformation\n",
    "5. **Modular design enables flexibility**: Easy to swap, add, or remove modules\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Modules\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "Residual connections, introduced in ResNet, help with training very deep networks by providing direct paths for gradient flow.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "$$\n",
    "\\mathrm{Res}(z) = z + \\sigma(\\mathrm{MM}_1(\\sigma(\\mathrm{MM}_2(z))))\n",
    "$$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The residual connection allows the network to:\n",
    "1. **Learn Increments**: Focus on learning the difference from the identity mapping\n",
    "2. **Ease Training**: Provide direct paths for gradients to flow\n",
    "3. **Prevent Degradation**: Avoid performance degradation in very deep networks\n",
    "\n",
    "#### Why Residual Connections Work\n",
    "\n",
    "**Gradient Flow**: The derivative of the residual connection is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{Res}(z)}{\\partial z} = I + \\frac{\\partial}{\\partial z}[\\sigma(\\mathrm{MM}_1(\\sigma(\\mathrm{MM}_2(z))))]\n",
    "$$\n",
    "\n",
    "The identity term ensures that gradients can flow directly, preventing vanishing gradients.\n",
    "\n",
    "#### ResNet Architecture\n",
    "\n",
    "A simplified ResNet is a composition of residual blocks:\n",
    "\n",
    "$$\n",
    "\\mathrm{ResNet}\\text{-}\\mathcal{S}(x) = \\mathrm{MM}(\\mathrm{Res}(\\mathrm{Res}(\\cdots \\mathrm{Res}(x))))\n",
    "$$\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Layer normalization stabilizes training by normalizing activations within each layer.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "**Sub-module (LN-S)**:\n",
    "$$\n",
    "\\mathrm{LN\\text{-}S}(z) = \\begin{bmatrix}\n",
    "\\frac{z_1 - \\hat{\\mu}}{\\hat{\\sigma}} \\\\\n",
    "\\frac{z_2 - \\hat{\\mu}}{\\hat{\\sigma}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{z_m - \\hat{\\mu}}{\\hat{\\sigma}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\mu} = \\frac{1}{m}\\sum_{i=1}^m z_i$ is the empirical mean\n",
    "- $\\hat{\\sigma} = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m (z_i - \\hat{\\mu})^2}$ is the empirical standard deviation\n",
    "\n",
    "**Full Layer Normalization**:\n",
    "$$\n",
    "\\mathrm{LN}(z) = \\beta + \\gamma \\cdot \\mathrm{LN\\text{-}S}(z)\n",
    "$$\n",
    "\n",
    "Where $\\beta$ and $\\gamma$ are learnable parameters.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "1. **Scale Invariance**: $\\mathrm{LN}(\\alpha z) = \\mathrm{LN}(z)$ for any $\\alpha \\neq 0$\n",
    "2. **Translation Invariance**: $\\mathrm{LN}(z + c) = \\mathrm{LN}(z) + c$ for any constant $c$\n",
    "3. **Stabilization**: Helps prevent exploding or vanishing gradients\n",
    "\n",
    "#### Scale-Invariant Property\n",
    "\n",
    "Layer normalization has an important scale-invariant property:\n",
    "\n",
    "$$\n",
    "\\mathrm{LN}(\\mathrm{MM}_{aW, ab}(z)) = \\mathrm{LN}(\\mathrm{MM}_{W, b}(z)), \\forall a \\neq 0\n",
    "$$\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "1. **LN-S is scale-invariant**:\n",
    "$$\n",
    "\\mathrm{LN\\text{-}S}(\\alpha z) = \\begin{bmatrix}\n",
    "\\frac{\\alpha z_1 - \\alpha \\hat{\\mu}}{\\alpha \\hat{\\sigma}} \\\\\n",
    "\\frac{\\alpha z_2 - \\alpha \\hat{\\mu}}{\\alpha \\hat{\\sigma}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\alpha z_m - \\alpha \\hat{\\mu}}{\\alpha \\hat{\\sigma}}\n",
    "\\end{bmatrix} = \\mathrm{LN\\text{-}S}(z)\n",
    "$$\n",
    "\n",
    "2. **Full LN inherits scale-invariance**:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{LN}(\\mathrm{MM}_{aW, ab}(z)) &= \\beta + \\gamma \\mathrm{LN\\text{-}S}(\\mathrm{MM}_{aW, ab}(z)) \\\\\n",
    "&= \\beta + \\gamma \\mathrm{LN\\text{-}S}(a\\mathrm{MM}_{W, b}(z)) \\\\\n",
    "&= \\beta + \\gamma \\mathrm{LN\\text{-}S}(\\mathrm{MM}_{W, b}(z)) \\\\\n",
    "&= \\mathrm{LN}(\\mathrm{MM}_{W, b}(z))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Practical Implications\n",
    "\n",
    "This scale-invariant property means that:\n",
    "- The network is robust to weight scaling\n",
    "- Training is more stable\n",
    "- Learning rates can be chosen more freely\n",
    "- The network can adapt to different parameter scales automatically\n",
    "\n",
    "### Other Normalization Techniques\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes across the batch dimension:\n",
    "\n",
    "$$\n",
    "\\mathrm{BN}(x) = \\gamma \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "Where $\\mu_B$ and $\\sigma_B^2$ are computed across the batch dimension.\n",
    "\n",
    "#### Group Normalization\n",
    "\n",
    "Group normalization normalizes within groups of channels:\n",
    "\n",
    "$$\n",
    "\\mathrm{GN}(x) = \\gamma \\frac{x - \\mu_G}{\\sqrt{\\sigma_G^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "Where $\\mu_G$ and $\\sigma_G^2$ are computed within groups of channels.\n",
    "\n",
    "#### Comparison\n",
    "\n",
    "| Method | Normalization Dimension | Use Case |\n",
    "|--------|------------------------|----------|\n",
    "| Batch Norm | Batch | Large batch sizes |\n",
    "| Layer Norm | Features | Language models |\n",
    "| Group Norm | Groups of features | Small batch sizes |\n",
    "\n",
    "---\n",
    "\n",
    "## Convolutional Modules\n",
    "\n",
    "### 1D Convolution\n",
    "\n",
    "1D convolution is a specialized module that applies the same filter at different positions, enabling parameter sharing and local feature detection.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "**Simplified 1D Convolution (Conv1D-S)**:\n",
    "$$\n",
    "\\mathrm{Conv1D\\text{-}S}(z)_i = \\sum_{j=1}^{2\\ell+1} w_j z_{i-\\ell+(j-1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w \\in \\mathbb{R}^k$ is the filter (kernel) with $k = 2\\ell + 1$\n",
    "- $z$ is the input vector with zero padding\n",
    "- The output has the same dimension as the input\n",
    "\n",
    "#### Matrix Representation\n",
    "\n",
    "Conv1D-S can be represented as a matrix multiplication with a special structure:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "w_1 & \\cdots & w_{2\\ell+1} & 0 & \\cdots & 0 & 0 \\\\\n",
    "0 & w_1 & \\cdots & w_{2\\ell+1} & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & & & & & & \\vdots \\\\\n",
    "0 & \\cdots & 0 & w_1 & \\cdots & w_{2\\ell+1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\mathrm{Conv1D\\text{-}S}(z) = Qz\n",
    "$$\n",
    "\n",
    "#### Properties\n",
    "\n",
    "1. **Parameter Sharing**: The same filter is applied at all positions\n",
    "2. **Local Connectivity**: Each output depends only on a local window of inputs\n",
    "3. **Translation Invariance**: The same pattern is detected regardless of position\n",
    "4. **Efficiency**: $O(km)$ operations vs $O(m^2)$ for full matrix multiplication\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "1D convolution is particularly useful for:\n",
    "- **Signal Processing**: Audio, time series data\n",
    "- **Natural Language Processing**: Text sequences\n",
    "- **Feature Detection**: Finding patterns at different positions\n",
    "\n",
    "### Multi-Channel 1D Convolution\n",
    "\n",
    "Real-world applications often require multiple input and output channels.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "$$\n",
    "\\forall i \\in [C'], \\ \\mathrm{Conv1D}(z)_i = \\sum_{j=1}^C \\mathrm{Conv1D\\text{-}S}_{i,j}(z_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z_1, \\ldots, z_C$ are input channels\n",
    "- $\\mathrm{Conv1D\\text{-}S}_{i,j}$ is a separate filter for input channel $j$ and output channel $i$\n",
    "- The output has $C'$ channels\n",
    "\n",
    "#### Parameter Count\n",
    "\n",
    "- **Conv1D**: $k \\times C \\times C'$ parameters\n",
    "- **Full Matrix**: $m^2 \\times C \\times C'$ parameters\n",
    "\n",
    "The reduction in parameters comes from:\n",
    "1. **Parameter Sharing**: Same filter applied at all positions\n",
    "2. **Local Connectivity**: Each output depends only on a local window\n",
    "\n",
    "### 2D Convolution\n",
    "\n",
    "2D convolution extends the concept to 2D inputs like images.\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "**Simplified 2D Convolution (Conv2D-S)**:\n",
    "$$\n",
    "\\mathrm{Conv2D\\text{-}S}(z)_{i,j} = \\sum_{p=1}^k \\sum_{q=1}^k w_{p,q} z_{i+p-\\ell, j+q-\\ell}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z \\in \\mathbb{R}^{m \\times m}$ is the 2D input\n",
    "- $w \\in \\mathbb{R}^{k \\times k}$ is the 2D filter\n",
    "- $\\ell = (k-1)/2$ for odd $k$\n",
    "\n",
    "#### Multi-Channel 2D Convolution\n",
    "\n",
    "$$\n",
    "\\forall i \\in [C'], \\ \\mathrm{Conv2D}(z)_i = \\sum_{j=1}^C \\mathrm{Conv2D\\text{-}S}_{i,j}(z_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z_1, \\ldots, z_C$ are 2D input channels\n",
    "- Each $\\mathrm{Conv2D\\text{-}S}_{i,j}$ has $k^2$ parameters\n",
    "- Total parameters: $C \\times C' \\times k^2$\n",
    "\n",
    "#### Applications\n",
    "\n",
    "2D convolution is essential for:\n",
    "- **Computer Vision**: Image processing, feature detection\n",
    "- **Medical Imaging**: MRI, CT scan analysis\n",
    "- **Remote Sensing**: Satellite image processing\n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are neural networks built primarily from convolutional layers:\n",
    "\n",
    "$$\n",
    "\\mathrm{CNN}(x) = \\mathrm{MM}(\\mathrm{Conv2D}(\\sigma(\\mathrm{Conv2D}(\\cdots \\mathrm{Conv2D}(x)))))\n",
    "$$\n",
    "\n",
    "#### Key Advantages\n",
    "\n",
    "1. **Parameter Efficiency**: Fewer parameters than fully connected networks\n",
    "2. **Translation Invariance**: Robust to input translations\n",
    "3. **Hierarchical Features**: Learn features at multiple scales\n",
    "4. **Spatial Structure**: Respects the spatial structure of data\n",
    "\n",
    "---\n",
    "\n",
    "## Modern Architecture Patterns\n",
    "\n",
    "### Transformer Modules\n",
    "\n",
    "Transformers use attention mechanisms and layer normalization extensively.\n",
    "\n",
    "#### Self-Attention Module\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q, K, V$ are query, key, and value matrices\n",
    "- $d_k$ is the dimension of keys\n",
    "- The softmax is applied row-wise\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "$$\n",
    "\\mathrm{MHA}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head}_1, \\ldots, \\mathrm{head}_h)W^O\n",
    "$$\n",
    "\n",
    "Where each head is:\n",
    "$$\n",
    "\\mathrm{head}_i = \\mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "#### Transformer Block\n",
    "\n",
    "A typical transformer block consists of:\n",
    "\n",
    "$$\n",
    "\\mathrm{TransformerBlock}(x) = \\mathrm{LN}_2(x + \\mathrm{FFN}(\\mathrm{LN}_1(x + \\mathrm{MHA}(x))))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{FFN}$ is a feed-forward network\n",
    "- $\\mathrm{LN}_1, \\mathrm{LN}_2$ are layer normalizations\n",
    "- The residual connections help with gradient flow\n",
    "\n",
    "### Modern CNN Architectures\n",
    "\n",
    "#### ResNet with Batch Normalization\n",
    "\n",
    "$$\n",
    "\\mathrm{ResBlock}(x) = \\mathrm{BN}(\\sigma(\\mathrm{Conv}(\\mathrm{BN}(\\sigma(\\mathrm{Conv}(x)))))) + x\n",
    "$$\n",
    "\n",
    "#### DenseNet\n",
    "\n",
    "DenseNet connects each layer to every other layer:\n",
    "\n",
    "$$\n",
    "x_l = \\sigma(\\mathrm{Conv}([x_0, x_1, \\ldots, x_{l-1}]))\n",
    "$$\n",
    "\n",
    "Where $[x_0, x_1, \\ldots, x_{l-1}]$ denotes concatenation.\n",
    "\n",
    "---\n",
    "\n",
    "## Module Composition Strategies\n",
    "\n",
    "### Sequential Composition\n",
    "\n",
    "Modules can be composed sequentially:\n",
    "\n",
    "$$\n",
    "f(x) = f_n \\circ f_{n-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "### Parallel Composition\n",
    "\n",
    "Modules can be applied in parallel and their outputs combined:\n",
    "\n",
    "$$\n",
    "f(x) = \\mathrm{Combine}(f_1(x), f_2(x), \\ldots, f_n(x))\n",
    "$$\n",
    "\n",
    "### Residual Composition\n",
    "\n",
    "Modules can be combined with residual connections:\n",
    "\n",
    "$$\n",
    "f(x) = x + g(x)\n",
    "$$\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Long-range connections can bypass multiple layers:\n",
    "\n",
    "$$\n",
    "f(x) = f_n(f_{n-1}(\\cdots f_1(x))) + x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "### Module Design Principles\n",
    "\n",
    "1. **Simplicity**: Each module should have a clear, simple purpose\n",
    "2. **Composability**: Modules should be easy to combine\n",
    "3. **Efficiency**: Modules should be computationally efficient\n",
    "4. **Differentiability**: Modules should support gradient-based optimization\n",
    "\n",
    "### Hyperparameter Selection\n",
    "\n",
    "#### Convolutional Layers\n",
    "\n",
    "- **Filter Size**: Usually 3×3 or 5×5 for 2D, 3 or 5 for 1D\n",
    "- **Number of Channels**: Start with 32-64, increase with depth\n",
    "- **Stride**: Controls output size, usually 1 or 2\n",
    "- **Padding**: Maintains spatial dimensions\n",
    "\n",
    "#### Normalization Layers\n",
    "\n",
    "- **Layer Norm**: Usually applied after attention or MLP\n",
    "- **Batch Norm**: Applied after convolutions\n",
    "- **Group Norm**: Alternative when batch size is small\n",
    "\n",
    "### Training Considerations\n",
    "\n",
    "1. **Initialization**: Proper initialization is crucial for training\n",
    "2. **Learning Rate**: Different modules may require different learning rates\n",
    "3. **Regularization**: Apply regularization appropriately to each module\n",
    "4. **Gradient Flow**: Ensure gradients can flow through all modules\n",
    "\n",
    "---\n",
    "\n",
    "*This concludes our exploration of neural network modules. These building blocks form the foundation of modern deep learning architectures, enabling the creation of powerful and flexible models for a wide range of applications.*\n",
    "\n",
    "## From Modular Design to Training Algorithms\n",
    "\n",
    "We've now explored how to design and implement neural network modules - the building blocks that enable us to construct sophisticated architectures systematically. We've seen how common patterns can be implemented as reusable modules and how these modules can be composed to create complex neural networks.\n",
    "\n",
    "However, having well-designed modules is only part of the story. To make these modules learn from data, we need **training algorithms** that can efficiently compute gradients and update parameters. The modular design we've established provides the foundation, but we need algorithms that can work with these complex architectures.\n",
    "\n",
    "This motivates our exploration of **backpropagation** - the fundamental algorithm that enables neural networks to learn by efficiently computing gradients through the computational graph. We'll see how the modular structure we've designed enables efficient gradient computation and how this algorithm scales to deep architectures.\n",
    "\n",
    "The transition from modular design to training algorithms represents the bridge from architecture to learning - taking our systematic approach to building neural networks and turning it into a practical system that can learn from data.\n",
    "\n",
    "In the next section, we'll explore how backpropagation works, how it leverages the modular structure of neural networks, and how it enables efficient training of deep architectures.\n",
    "\n",
    "---\n",
    "\n",
    "**Previous: [Neural Networks](02_neural_networks.md)** - Learn how to build neural networks from individual neurons to deep architectures.\n",
    "\n",
    "**Next: [Backpropagation](04_backpropagation.md)** - Understand how neural networks learn through efficient gradient computation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
