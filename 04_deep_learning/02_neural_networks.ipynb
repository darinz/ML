{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454767e-7657-4769-8e31-d79a7bdd17f5",
   "metadata": {},
   "source": [
    "# Neural Networks: From Single Neurons to Deep Architectures\n",
    "\n",
    "## Introduction to Neural Networks: The Building Blocks of Deep Learning\n",
    "\n",
    "Neural networks represent one of the most powerful and flexible approaches to machine learning, capable of learning complex patterns and relationships from data. At their core, neural networks are computational models inspired by biological neural systems, consisting of interconnected processing units (neurons) organized in layers.\n",
    "\n",
    "### What Are Neural Networks? - The Computational Revolution\n",
    "\n",
    "Neural networks are mathematical models that can approximate any continuous function given sufficient capacity. They consist of:\n",
    "\n",
    "1. **Input Layer**: Receives the raw data\n",
    "2. **Hidden Layers**: Process and transform the data through non-linear operations\n",
    "3. **Output Layer**: Produces the final prediction or classification\n",
    "\n",
    "**Real-World Analogy: The Factory Assembly Line**\n",
    "Think of neural networks like a sophisticated factory assembly line:\n",
    "- **Input Layer**: Raw materials arrive (data)\n",
    "- **Hidden Layers**: Each station processes and transforms the materials (feature extraction)\n",
    "- **Output Layer**: Final product is produced (prediction)\n",
    "- **Quality Control**: Each station adds value and checks quality\n",
    "\n",
    "**Visual Analogy: The Recipe Problem**\n",
    "Think of neural networks like a complex cooking recipe:\n",
    "- **Input**: Raw ingredients (data features)\n",
    "- **Hidden Steps**: Each cooking step transforms ingredients (layer processing)\n",
    "- **Output**: Final dish (prediction)\n",
    "- **Learning**: The chef improves the recipe based on taste tests (training)\n",
    "\n",
    "**Mathematical Intuition: Function Composition**\n",
    "A neural network is essentially a composition of simple functions:\n",
    "\n",
    "$$\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "Where each $f_i$ is a layer that transforms its input into a more useful representation.\n",
    "\n",
    "### Key Characteristics: What Makes Neural Networks Special\n",
    "\n",
    "- **Non-linear**: Can model complex, non-linear relationships\n",
    "- **Universal**: Can approximate any continuous function (Universal Approximation Theorem)\n",
    "- **Hierarchical**: Learn features at multiple levels of abstraction\n",
    "- **Adaptive**: Parameters are learned from data through optimization\n",
    "\n",
    "**Real-World Analogy: The Language Learning Problem**\n",
    "Think of neural networks like learning a new language:\n",
    "- **Non-linear**: Language has complex rules that aren't just simple patterns\n",
    "- **Universal**: Can learn any language given enough examples\n",
    "- **Hierarchical**: Learn letters → words → sentences → meaning\n",
    "- **Adaptive**: Improve with practice and feedback\n",
    "\n",
    "**Visual Analogy: The Building Construction Problem**\n",
    "Think of neural networks like building construction:\n",
    "- **Non-linear**: Buildings aren't just straight lines - they have curves, angles, complex shapes\n",
    "- **Universal**: Can build any type of structure (house, skyscraper, bridge)\n",
    "- **Hierarchical**: Foundation → walls → roof → interior → finishing\n",
    "- **Adaptive**: Design improves based on experience and requirements\n",
    "\n",
    "### Mathematical Foundation: The Theoretical Backbone\n",
    "\n",
    "A neural network can be viewed as a composition of functions:\n",
    "\n",
    "$$\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "Where each $f_i$ represents a layer transformation, and $\\circ$ denotes function composition.\n",
    "\n",
    "**Intuition**: Each layer takes the output of the previous layer and transforms it into a new representation that's more useful for the final task.\n",
    "\n",
    "**Practical Example - Image Recognition:**\n",
    "\n",
    "See the complete implementation in [`code/neural_network_basics_demo.py`](code/neural_network_basics_demo.py) which demonstrates:\n",
    "\n",
    "- Comparison of different neural network architectures (single layer, one hidden layer, two hidden layers)\n",
    "- Training on non-linear data (moon-shaped dataset)\n",
    "- Visualization of decision boundaries for each architecture\n",
    "- Performance comparison showing how depth affects learning capacity\n",
    "\n",
    "The code shows that deeper networks can learn more complex decision boundaries and achieve higher accuracy on non-linear data.\n",
    "\n",
    "## From Mathematical Foundations to Neural Network Architectures: The Bridge to Practice\n",
    "\n",
    "We've now established the **mathematical foundations** of deep learning - understanding why non-linear models are necessary, how loss functions capture different types of learning objectives, and how optimization algorithms enable us to find the best parameters for our models. This theoretical framework provides the foundation for understanding how neural networks work.\n",
    "\n",
    "However, while we've discussed non-linear models in abstract terms, we need to move from mathematical concepts to concrete **neural network architectures**. The transition from understanding why non-linear models are powerful to actually building them requires us to explore how simple computational units (neurons) can be combined to create complex learning systems.\n",
    "\n",
    "This motivates our exploration of **neural networks** - the specific architectural framework that implements non-linear models through interconnected layers of neurons. We'll see how the mathematical principles we've established (non-linear transformations, function composition, optimization) translate into concrete neural network designs.\n",
    "\n",
    "The transition from non-linear models to neural networks represents the bridge from mathematical theory to practical architecture - taking our understanding of why non-linear models work and turning it into a systematic approach for building them.\n",
    "\n",
    "In this section, we'll explore how individual neurons work, how they can be combined into layers, and how these layers can be stacked to create deep architectures that can learn increasingly complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## From Linear to Non-Linear: The Single Neuron - The Atomic Unit of Intelligence\n",
    "\n",
    "### The Building Block: The Artificial Neuron - Understanding the Basic Unit\n",
    "\n",
    "The artificial neuron is the fundamental computational unit of neural networks. It performs three basic operations:\n",
    "\n",
    "1. **Linear Combination**: $z = w^T x + b$\n",
    "2. **Non-linear Activation**: $a = \\sigma(z)$\n",
    "3. **Output**: The activated value becomes the neuron's output\n",
    "\n",
    "**Real-World Analogy: The Decision Maker Problem**\n",
    "Think of a neuron like a decision maker in a company:\n",
    "- **Input**: Information from various sources (data features)\n",
    "- **Linear Combination**: Weigh the importance of each piece of information\n",
    "- **Non-linear Activation**: Make a decision based on the weighted information\n",
    "- **Output**: The decision (prediction)\n",
    "\n",
    "**Visual Analogy: The Recipe Ingredient Problem**\n",
    "Think of a neuron like combining ingredients in a recipe:\n",
    "- **Input**: Raw ingredients (data features)\n",
    "- **Linear Combination**: Mix ingredients in specific proportions (weights)\n",
    "- **Non-linear Activation**: Apply heat/cooking process (activation function)\n",
    "- **Output**: Final ingredient (processed feature)\n",
    "\n",
    "**Mathematical Intuition: The Weighted Sum Plus Transformation**\n",
    "A neuron takes multiple inputs, combines them linearly, adds a bias, and then applies a non-linear transformation.\n",
    "\n",
    "### Mathematical Formulation: The Neuron's Blueprint\n",
    "\n",
    "For a single neuron with input $x \\in \\mathbb{R}^d$:\n",
    "\n",
    "$$\n",
    "z = w^T x + b\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w \\in \\mathbb{R}^d$ is the weight vector\n",
    "- $b \\in \\mathbb{R}$ is the bias term\n",
    "- $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the activation function\n",
    "- $z$ is the pre-activation (or logit)\n",
    "- $a$ is the activation (or output)\n",
    "\n",
    "**Real-World Analogy: The Credit Score Problem**\n",
    "Think of a neuron like a credit scoring system:\n",
    "- **Input**: Income, debt, payment history, age (features)\n",
    "- **Weights**: How important each factor is (w)\n",
    "- **Bias**: Base credit score (b)\n",
    "- **Linear Combination**: Weighted sum of factors (z)\n",
    "- **Activation**: Final credit score (a)\n",
    "\n",
    "**Visual Example - Single Neuron Computation:**\n",
    "\n",
    "See the complete implementation in [`code/single_neuron_demo.py`](code/single_neuron_demo.py) which demonstrates:\n",
    "\n",
    "- Single neuron computation with different activation functions (ReLU, Sigmoid, Tanh)\n",
    "- House price prediction example with multiple weight configurations\n",
    "- Step-by-step computation showing linear combination and activation\n",
    "- Visualization of different activation functions and their properties\n",
    "\n",
    "The code shows how a single neuron processes inputs through linear combination and non-linear activation to produce meaningful outputs.\n",
    "\n",
    "### Why Non-Linear Activation Functions? - The Key to Power\n",
    "\n",
    "**The Problem with Linear Activations**: If we used $\\sigma(z) = z$ (linear activation), then:\n",
    "\n",
    "$$\n",
    "f(x) = w_2^T (W_1 x + b_1) + b_2 = (w_2^T W_1) x + (w_2^T b_1 + b_2) = W' x + b'\n",
    "$$\n",
    "\n",
    "This reduces to a linear function, losing the power of non-linearity.\n",
    "\n",
    "**Real-World Analogy: The Language Problem**\n",
    "Think of linear vs. non-linear like language complexity:\n",
    "- **Linear language**: \"I am happy\" (simple, direct)\n",
    "- **Non-linear language**: \"I'm feeling on top of the world\" (metaphorical, complex)\n",
    "- **Neural networks**: Need non-linearity to understand complex patterns\n",
    "\n",
    "**Visual Analogy: The Building Problem**\n",
    "Think of linear vs. non-linear like building construction:\n",
    "- **Linear building**: Only straight walls and flat roofs\n",
    "- **Non-linear building**: Curved walls, domes, arches, complex shapes\n",
    "- **Neural networks**: Need non-linearity to model complex relationships\n",
    "\n",
    "**The Solution**: Non-linear activation functions introduce the ability to model complex, non-linear relationships.\n",
    "\n",
    "**Practical Example - Linear vs. Non-linear:**\n",
    "\n",
    "See the complete implementation in [`code/linear_vs_nonlinear_activation_demo.py`](code/linear_vs_nonlinear_activation_demo.py) which demonstrates:\n",
    "\n",
    "- Comparison between linear and non-linear models on sinusoidal data\n",
    "- Training linear regression vs neural network with ReLU activation\n",
    "- Visualization of model fits and performance comparison\n",
    "- Demonstration of why non-linear activation functions are necessary\n",
    "\n",
    "The code shows that neural networks with non-linear activations can fit complex patterns that linear models cannot capture.\n",
    "\n",
    "### Common Activation Functions: The Tools in Our Toolkit\n",
    "\n",
    "#### 1. Rectified Linear Unit (ReLU) - The Workhorse\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "**Properties**:\n",
    "- **Range**: $[0, \\infty)$\n",
    "- **Derivative**: $\\sigma'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases}$\n",
    "- **Advantages**: Simple, computationally efficient, helps with vanishing gradient problem\n",
    "- **Disadvantages**: Can cause \"dying ReLU\" problem (neurons stuck at zero)\n",
    "\n",
    "**Real-World Analogy: The Water Valve Problem**\n",
    "Think of ReLU like a water valve:\n",
    "- **Input**: Water pressure (z)\n",
    "- **Output**: Water flow (a)\n",
    "- **Behavior**: No flow if pressure is negative, flow equals pressure if positive\n",
    "- **Result**: Simple, efficient, but can get stuck closed\n",
    "\n",
    "**Visual Analogy: The Light Switch Problem**\n",
    "Think of ReLU like a light switch:\n",
    "- **Input**: Voltage (z)\n",
    "- **Output**: Light intensity (a)\n",
    "- **Behavior**: Off if voltage ≤ 0, on with intensity = voltage if voltage > 0\n",
    "\n",
    "#### 2. Sigmoid Function - The Probability Converter\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Properties**:\n",
    "- **Range**: $(0, 1)$\n",
    "- **Derivative**: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "- **Advantages**: Smooth, bounded output, interpretable as probability\n",
    "- **Disadvantages**: Suffers from vanishing gradient problem\n",
    "\n",
    "**Real-World Analogy: The Thermostat Problem**\n",
    "Think of sigmoid like a thermostat:\n",
    "- **Input**: Temperature difference (z)\n",
    "- **Output**: Heating intensity (a)\n",
    "- **Behavior**: Smooth transition from 0 to 1 as temperature increases\n",
    "- **Result**: Smooth, bounded, but can saturate\n",
    "\n",
    "#### 3. Hyperbolic Tangent (tanh) - The Balanced Option\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Properties**:\n",
    "- **Range**: $(-1, 1)$\n",
    "- **Derivative**: $\\sigma'(z) = 1 - \\sigma(z)^2$\n",
    "- **Advantages**: Zero-centered, bounded\n",
    "- **Disadvantages**: Still suffers from vanishing gradient problem\n",
    "\n",
    "**Real-World Analogy: The Volume Control Problem**\n",
    "Think of tanh like a volume control:\n",
    "- **Input**: Volume setting (z)\n",
    "- **Output**: Actual volume (a)\n",
    "- **Behavior**: Smooth transition from -1 to 1\n",
    "- **Result**: Balanced around zero, bounded\n",
    "\n",
    "**Practical Example - Activation Function Comparison:**\n",
    "\n",
    "See the complete implementation in [`code/activation_functions_demo.py`](code/activation_functions_demo.py) which demonstrates:\n",
    "\n",
    "- Comparison of different activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Visualization of both activation functions and their derivatives\n",
    "- Detailed analysis of properties, advantages, and disadvantages\n",
    "- Interactive plots showing the behavior of each function\n",
    "\n",
    "The code provides a comprehensive comparison of activation functions, helping understand when to use each one based on their mathematical properties.\n",
    "\n",
    "### Single Neuron Example: Housing Price Prediction - Real-World Application\n",
    "\n",
    "Consider predicting house prices based on house size. A single neuron with ReLU activation can model the relationship:\n",
    "\n",
    "$$\n",
    "\\hat{h}_\\theta(x) = \\max(w \\cdot x + b, 0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the house size (square feet)\n",
    "- $w$ is the price per square foot\n",
    "- $b$ is the base price\n",
    "- The ReLU ensures non-negative predictions\n",
    "\n",
    "**Real-World Analogy: The Real Estate Appraisal Problem**\n",
    "Think of this like a real estate appraiser:\n",
    "- **Input**: House size (square footage)\n",
    "- **Weight**: Price per square foot (learned from market data)\n",
    "- **Bias**: Base price for a house (minimum value)\n",
    "- **ReLU**: Ensures price is never negative (makes sense for houses)\n",
    "\n",
    "**Intuition**: The neuron learns to predict a price that increases linearly with size, but never goes below zero (which makes sense for house prices).\n",
    "\n",
    "**Practical Example - House Price Prediction:**\n",
    "\n",
    "See the complete implementation in [`code/house_price_prediction_demo.py`](code/house_price_prediction_demo.py) which demonstrates:\n",
    "\n",
    "- Single neuron implementation for house price prediction\n",
    "- Data generation with realistic house pricing model\n",
    "- Training process using linear regression with ReLU activation\n",
    "- Comprehensive evaluation with MSE and MAE metrics\n",
    "- Visualization of predictions, errors, and error distribution\n",
    "- Step-by-step demonstration of neuron computation process\n",
    "\n",
    "The code shows how a single neuron can learn to predict house prices based on square footage, demonstrating the complete workflow from data to prediction.\n",
    "\n",
    "### Mathematical Analysis: Why This Works\n",
    "\n",
    "**Why ReLU Works Well**:\n",
    "1. **Non-linearity**: Introduces a \"kink\" at $x = -b/w$\n",
    "2. **Sparsity**: Can produce exact zeros, leading to sparse representations\n",
    "3. **Gradient Flow**: Simple derivative prevents vanishing gradients\n",
    "4. **Computational Efficiency**: Simple max operation\n",
    "\n",
    "**Real-World Analogy: The Tax System Problem**\n",
    "Think of ReLU like a progressive tax system:\n",
    "- **Input**: Income (x)\n",
    "- **Weight**: Tax rate (w)\n",
    "- **Bias**: Standard deduction (b)\n",
    "- **ReLU**: No negative taxes (makes sense)\n",
    "- **Result**: Simple, efficient, and realistic\n",
    "\n",
    "**Parameter Learning**:\n",
    "The parameters $w$ and $b$ are learned through gradient descent by minimizing a loss function (e.g., mean squared error):\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\hat{h}_\\theta(x^{(i)}))^2\n",
    "$$\n",
    "\n",
    "**Key Insights from Single Neurons**:\n",
    "1. **Non-linearity is crucial**: Without it, we're just doing linear regression\n",
    "2. **Activation functions matter**: Different functions have different properties\n",
    "3. **ReLU is often the best choice**: Simple, efficient, and effective\n",
    "4. **Single neurons are limited**: Can only model simple non-linear relationships\n",
    "5. **Foundation for complexity**: Multiple neurons can model much more complex functions\n",
    "\n",
    "---\n",
    "\n",
    "## Stacking Neurons: Multi-Layer Networks\n",
    "\n",
    "### The Power of Composition\n",
    "\n",
    "While a single neuron can model simple non-linear relationships, real-world problems often require more complex functions. By stacking multiple neurons in layers, we can create networks that learn hierarchical representations.\n",
    "\n",
    "### Mathematical Motivation\n",
    "\n",
    "**Universal Approximation Theorem**: A neural network with a single hidden layer containing a sufficient number of neurons can approximate any continuous function on a compact domain to arbitrary precision.\n",
    "\n",
    "**Intuition**: Just as any function can be approximated by a sum of basis functions, any function can be approximated by a combination of non-linear transformations.\n",
    "\n",
    "### Two-Layer Network Architecture\n",
    "\n",
    "A two-layer network consists of:\n",
    "1. **Input Layer**: $x \\in \\mathbb{R}^d$\n",
    "2. **Hidden Layer**: $h$ neurons with activations $a_1, a_2, \\ldots, a_h$\n",
    "3. **Output Layer**: Final prediction\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For a two-layer network with $h$ hidden neurons:\n",
    "\n",
    "**Hidden Layer**:\n",
    "$$\n",
    "z_j = w_j^T x + b_j, \\quad j = 1, 2, \\ldots, h\n",
    "a_j = \\sigma(z_j), \\quad j = 1, 2, \\ldots, h\n",
    "$$\n",
    "\n",
    "**Output Layer**:\n",
    "$$\n",
    "\\hat{y} = w_{out}^T a + b_{out}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w_j \\in \\mathbb{R}^d$ are the weights for the $j$-th hidden neuron\n",
    "- $b_j \\in \\mathbb{R}$ are the biases for the $j$-th hidden neuron\n",
    "- $a = [a_1, a_2, \\ldots, a_h]^T$ is the vector of hidden activations\n",
    "- $w_{out} \\in \\mathbb{R}^h$ and $b_{out} \\in \\mathbb{R}$ are the output layer parameters\n",
    "\n",
    "#### Vectorized Form\n",
    "\n",
    "We can write this more compactly using matrix notation:\n",
    "\n",
    "**Hidden Layer**:\n",
    "$$\n",
    "Z = W x + b\n",
    "A = \\sigma(Z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{h \\times d}$ is the weight matrix\n",
    "- $b \\in \\mathbb{R}^h$ is the bias vector\n",
    "- $Z, A \\in \\mathbb{R}^h$ are the pre-activations and activations\n",
    "\n",
    "**Output Layer**:\n",
    "$$\n",
    "\\hat{y} = w_{out}^T A + b_{out}\n",
    "$$\n",
    "\n",
    "### Feature Learning Interpretation\n",
    "\n",
    "Each hidden neuron learns to detect a specific feature or pattern in the input:\n",
    "\n",
    "1. **Feature Detectors**: Each neuron becomes specialized in recognizing certain input patterns\n",
    "2. **Feature Combination**: The output layer learns to combine these features for the final prediction\n",
    "3. **Hierarchical Learning**: Complex features are built from simpler ones\n",
    "\n",
    "### Example: Housing Price Prediction with Multiple Features\n",
    "\n",
    "Consider predicting house prices using multiple features: size, bedrooms, location, age.\n",
    "\n",
    "**Hidden Layer Features**:\n",
    "- Neuron 1: \"Family size indicator\" (combines size and bedrooms)\n",
    "- Neuron 2: \"Location premium\" (based on zip code)\n",
    "- Neuron 3: \"Maintenance cost\" (based on age and size)\n",
    "\n",
    "**Output Layer**: Combines these features to predict the final price.\n",
    "\n",
    "### Why Stacking Helps\n",
    "\n",
    "**Expressiveness**: Each additional layer increases the network's capacity to represent complex functions.\n",
    "\n",
    "**Mathematical Intuition**: \n",
    "- Single neuron: Can create one \"kink\" or threshold\n",
    "- Two neurons: Can create two kinks\n",
    "- $h$ neurons: Can create $h$ kinks, approximating any piecewise linear function\n",
    "- Multiple layers: Can create exponentially more complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Biological Inspiration and Analogies\n",
    "\n",
    "### Connection to Biological Neural Networks\n",
    "\n",
    "While artificial neural networks are inspired by biological systems, they are simplified mathematical models rather than accurate simulations.\n",
    "\n",
    "#### Biological Neuron Structure\n",
    "\n",
    "A biological neuron consists of:\n",
    "1. **Dendrites**: Receive signals from other neurons\n",
    "2. **Cell Body**: Processes the signals\n",
    "3. **Axon**: Transmits signals to other neurons\n",
    "4. **Synapses**: Connection points where signals are transmitted\n",
    "\n",
    "#### Artificial vs. Biological Neurons\n",
    "\n",
    "| Aspect | Biological Neuron | Artificial Neuron |\n",
    "|--------|-------------------|-------------------|\n",
    "| Input | Electrical/chemical signals | Numerical values |\n",
    "| Processing | Complex biochemical processes | Simple mathematical operations |\n",
    "| Output | Action potential (spike) | Continuous value |\n",
    "| Learning | Synaptic plasticity | Gradient descent |\n",
    "| Speed | Milliseconds | Nanoseconds |\n",
    "\n",
    "### Key Insights from Biology\n",
    "\n",
    "1. **Connectivity**: Neurons are highly interconnected\n",
    "2. **Plasticity**: Connections can strengthen or weaken based on activity\n",
    "3. **Hierarchy**: Information processing occurs in stages\n",
    "4. **Parallelism**: Many neurons operate simultaneously\n",
    "\n",
    "### Limitations of the Biological Analogy\n",
    "\n",
    "1. **Simplification**: Artificial neurons are much simpler than biological ones\n",
    "2. **Learning**: Biological learning is more complex than gradient descent\n",
    "3. **Architecture**: Biological networks have more complex connectivity patterns\n",
    "4. **Purpose**: Artificial networks are designed for mathematical convenience, not biological accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## Two-Layer Fully-Connected Neural Networks\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "A two-layer fully-connected network is the simplest form of a \"deep\" neural network. It consists of:\n",
    "\n",
    "1. **Input Layer**: $x \\in \\mathbb{R}^d$\n",
    "2. **Hidden Layer**: $m$ neurons with full connectivity\n",
    "3. **Output Layer**: Final prediction\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Layer-by-Layer Computation\n",
    "\n",
    "**Layer 1 (Hidden Layer)**:\n",
    "$$\n",
    "z_j^{[1]} = (w_j^{[1]})^T x + b_j^{[1]}, \\quad j = 1, 2, \\ldots, m\n",
    "a_j^{[1]} = \\sigma(z_j^{[1]}), \\quad j = 1, 2, \\ldots, m\n",
    "$$\n",
    "\n",
    "**Layer 2 (Output Layer)**:\n",
    "$$\n",
    "z^{[2]} = (w^{[2]})^T a^{[1]} + b^{[2]}\n",
    "\\hat{y} = z^{[2]} \\quad \\text{(for regression)}\n",
    "\\hat{y} = \\sigma(z^{[2]}) \\quad \\text{(for classification)}\n",
    "$$\n",
    "\n",
    "#### Matrix Notation\n",
    "\n",
    "**Forward Pass**:\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} x + b^{[1]}\n",
    "A^{[1]} = \\sigma(Z^{[1]})\n",
    "Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\n",
    "\\hat{y} = Z^{[2]}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W^{[1]} \\in \\mathbb{R}^{m \\times d}$: Weight matrix for layer 1\n",
    "- $b^{[1]} \\in \\mathbb{R}^m$: Bias vector for layer 1\n",
    "- $W^{[2]} \\in \\mathbb{R}^{1 \\times m}$: Weight matrix for layer 2\n",
    "- $b^{[2]} \\in \\mathbb{R}$: Bias for layer 2\n",
    "\n",
    "### Parameter Sharing and Efficiency\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "- **Forward Pass**: $O(md + m) = O(md)$ operations\n",
    "- **Memory**: $O(md + m + m + 1) = O(md)$ parameters\n",
    "- **Expressiveness**: Can represent any function that can be approximated by $m$ basis functions\n",
    "\n",
    "#### Comparison with Single Layer\n",
    "\n",
    "| Aspect | Single Neuron | Two-Layer Network |\n",
    "|--------|---------------|-------------------|\n",
    "| Parameters | $d + 1$ | $md + m + m + 1$ |\n",
    "| Expressiveness | Limited | High |\n",
    "| Training Time | Fast | Slower |\n",
    "| Overfitting Risk | Low | Higher |\n",
    "\n",
    "### Training Process\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "For regression:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2\n",
    "$$\n",
    "\n",
    "For classification:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "\n",
    "#### Gradient Computation\n",
    "\n",
    "The gradients are computed using backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[2]}} = \\frac{1}{n} \\sum_{i=1}^n (a^{[1](i)})^T (y^{(i)} - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[1]}} = \\frac{1}{n} \\sum_{i=1}^n x^{(i)} (\\sigma'(z^{[1](i)}) \\odot (W^{[2]})^T (y^{(i)} - \\hat{y}^{(i)}))^T\n",
    "$$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "**Weight Initialization**: Important for training success\n",
    "- **Xavier/Glorot Initialization**: $W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}})$\n",
    "- **He Initialization**: $W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})$ (for ReLU)\n",
    "\n",
    "**Bias Initialization**: Usually initialized to zero or small positive values\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "**L2 Regularization**:\n",
    "$$\n",
    "J_{reg}(\\theta) = J(\\theta) + \\frac{\\lambda}{2} (\\|W^{[1]}\\|_F^2 + \\|W^{[2]}\\|_F^2)\n",
    "$$\n",
    "\n",
    "**Dropout**: Randomly set some activations to zero during training\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "- **Number of hidden units**: Start with $m = \\sqrt{d}$ or $m = 2d$\n",
    "- **Learning rate**: Start with 0.01 and adjust based on convergence\n",
    "- **Batch size**: Balance between memory usage and training stability\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Layer Networks: Going Deeper\n",
    "\n",
    "### Why Go Deeper?\n",
    "\n",
    "#### Theoretical Motivation\n",
    "\n",
    "**Representation Learning**: Deep networks can learn hierarchical representations automatically.\n",
    "\n",
    "**Parameter Efficiency**: Deep networks can represent complex functions with fewer parameters than shallow networks.\n",
    "\n",
    "**Feature Hierarchy**: Early layers learn low-level features, later layers learn high-level abstractions.\n",
    "\n",
    "#### Mathematical Intuition\n",
    "\n",
    "A deep network with $L$ layers can be written as:\n",
    "\n",
    "$$\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "Each layer $f_i$ transforms the input into a new representation that becomes the input for the next layer.\n",
    "\n",
    "### Deep Network Architecture\n",
    "\n",
    "#### General Formulation\n",
    "\n",
    "For a network with $L$ layers:\n",
    "\n",
    "**Layer $l$**:\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\n",
    "A^{[l]} = \\sigma^{[l]}(Z^{[l]})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A^{[0]} = x$ (input)\n",
    "- $\\sigma^{[l]}$ is the activation function for layer $l$\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ is the weight matrix\n",
    "- $b^{[l]} \\in \\mathbb{R}^{n_l}$ is the bias vector\n",
    "\n",
    "#### Activation Functions by Layer\n",
    "\n",
    "- **Hidden Layers**: Usually ReLU or variants\n",
    "- **Output Layer**: \n",
    "  - Regression: Linear (no activation)\n",
    "  - Binary Classification: Sigmoid\n",
    "  - Multi-class Classification: Softmax\n",
    "\n",
    "### Training Deep Networks\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "1. **Vanishing/Exploding Gradients**: Gradients can become very small or very large\n",
    "2. **Overfitting**: More parameters increase risk of overfitting\n",
    "3. **Computational Cost**: Training time increases with depth\n",
    "4. **Hyperparameter Tuning**: More parameters to tune\n",
    "\n",
    "#### Solutions\n",
    "\n",
    "**Gradient Issues**:\n",
    "- **Batch Normalization**: Normalize activations within each batch\n",
    "- **Residual Connections**: Skip connections to help gradient flow\n",
    "- **Proper Initialization**: Use appropriate weight initialization schemes\n",
    "\n",
    "**Overfitting**:\n",
    "- **Regularization**: L2 regularization, dropout\n",
    "- **Early Stopping**: Stop training when validation loss increases\n",
    "- **Data Augmentation**: Increase effective dataset size\n",
    "\n",
    "**Computational Efficiency**:\n",
    "- **GPU Acceleration**: Use specialized hardware\n",
    "- **Mini-batch Training**: Process data in batches\n",
    "- **Optimized Libraries**: Use frameworks like PyTorch, TensorFlow\n",
    "\n",
    "### Modern Architectures\n",
    "\n",
    "#### Residual Networks (ResNets)\n",
    "\n",
    "Add skip connections to help with gradient flow:\n",
    "\n",
    "$$\n",
    "A^{[l+1]} = \\sigma(Z^{[l+1]} + A^{[l]})\n",
    "$$\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "Normalize activations to stabilize training:\n",
    "\n",
    "$$\n",
    "A_{norm}^{[l]} = \\frac{A^{[l]} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "A^{[l]} = \\gamma A_{norm}^{[l]} + \\beta\n",
    "$$\n",
    "\n",
    "#### Attention Mechanisms\n",
    "\n",
    "Allow the network to focus on relevant parts of the input:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Activation Functions Deep Dive\n",
    "\n",
    "### Why Activation Functions Matter\n",
    "\n",
    "Activation functions are crucial because they introduce non-linearity, enabling neural networks to learn complex patterns.\n",
    "\n",
    "### Properties of Good Activation Functions\n",
    "\n",
    "1. **Non-linearity**: Essential for modeling complex relationships\n",
    "2. **Differentiability**: Required for gradient-based optimization\n",
    "3. **Monotonicity**: Helps with optimization stability\n",
    "4. **Boundedness**: Can help prevent exploding gradients\n",
    "5. **Computational Efficiency**: Should be fast to compute\n",
    "\n",
    "### Detailed Analysis of Common Activations\n",
    "\n",
    "#### ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Definition**: $\\sigma(z) = \\max(0, z)$\n",
    "\n",
    "**Advantages**:\n",
    "- **Computational Efficiency**: Simple max operation\n",
    "- **Sparsity**: Can produce exact zeros\n",
    "- **Gradient Flow**: Simple derivative prevents vanishing gradients\n",
    "- **Biological Plausibility**: Similar to biological neuron firing\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Dying ReLU**: Neurons can get stuck at zero\n",
    "- **Not Zero-Centered**: Output is always non-negative\n",
    "- **Not Bounded**: Output can grow arbitrarily large\n",
    "\n",
    "**Variants**:\n",
    "- **Leaky ReLU**: $\\sigma(z) = \\max(\\alpha z, z)$ where $\\alpha < 1$\n",
    "- **Parametric ReLU**: $\\alpha$ is learned\n",
    "- **ELU**: $\\sigma(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha(e^z - 1) & \\text{if } z \\leq 0 \\end{cases}$\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "**Definition**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**Advantages**:\n",
    "- **Bounded**: Output always between 0 and 1\n",
    "- **Smooth**: Continuous and differentiable everywhere\n",
    "- **Interpretable**: Can be interpreted as probability\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Vanishing Gradient**: Derivative approaches zero for large inputs\n",
    "- **Not Zero-Centered**: Output is always positive\n",
    "- **Saturation**: Neurons can get stuck in saturation regions\n",
    "\n",
    "#### Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Definition**: $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "\n",
    "**Advantages**:\n",
    "- **Zero-Centered**: Output ranges from -1 to 1\n",
    "- **Bounded**: Output is always between -1 and 1\n",
    "- **Smooth**: Continuous and differentiable\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Vanishing Gradient**: Still suffers from gradient vanishing\n",
    "- **Saturation**: Can get stuck in saturation regions\n",
    "\n",
    "#### GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Definition**: $\\sigma(z) = z \\cdot \\Phi(z)$ where $\\Phi$ is the cumulative distribution function of the standard normal distribution\n",
    "\n",
    "**Advantages**:\n",
    "- **Smooth**: Continuous and differentiable\n",
    "- **Non-monotonic**: Can model more complex relationships\n",
    "- **Performance**: Often performs better than ReLU in practice\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Computational Cost**: More expensive to compute\n",
    "- **Complexity**: More complex than ReLU\n",
    "\n",
    "### Choosing Activation Functions\n",
    "\n",
    "#### Guidelines\n",
    "\n",
    "1. **Hidden Layers**: ReLU is usually a good default choice\n",
    "2. **Output Layer**: \n",
    "   - Regression: Linear (no activation)\n",
    "   - Binary Classification: Sigmoid\n",
    "   - Multi-class Classification: Softmax\n",
    "3. **Special Cases**: Consider alternatives based on specific requirements\n",
    "\n",
    "#### Empirical Considerations\n",
    "\n",
    "- **ReLU**: Good default for most cases\n",
    "- **Leaky ReLU**: If you observe dying ReLU problem\n",
    "- **Tanh**: If you need bounded outputs\n",
    "- **GELU**: For transformer-based architectures\n",
    "\n",
    "---\n",
    "\n",
    "## Connection to Kernel Methods\n",
    "\n",
    "### Theoretical Relationship\n",
    "\n",
    "Neural networks and kernel methods are both approaches to non-linear learning, but they work in fundamentally different ways.\n",
    "\n",
    "#### Kernel Methods\n",
    "\n",
    "Kernel methods rely on the \"kernel trick\" to implicitly map data to high-dimensional spaces:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i K(x, x_i)\n",
    "$$\n",
    "\n",
    "Where $K$ is a kernel function measuring similarity between points.\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "Neural networks learn explicit feature mappings:\n",
    "\n",
    "$$\n",
    "f(x) = W^{[L]} \\sigma(W^{[L-1]} \\sigma(\\cdots \\sigma(W^{[1]} x + b^{[1]}) \\cdots) + b^{[L-1]}) + b^{[L]}\n",
    "$$\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Kernel Methods | Neural Networks |\n",
    "|--------|----------------|-----------------|\n",
    "| Feature Learning | Fixed kernel functions | Learned feature mappings |\n",
    "| Scalability | Limited by number of training examples | Limited by model capacity |\n",
    "| Interpretability | Kernel functions are interpretable | Learned features may not be |\n",
    "| Flexibility | Limited by choice of kernel | Highly flexible architecture |\n",
    "\n",
    "### Mathematical Connection\n",
    "\n",
    "#### Neural Tangent Kernel (NTK)\n",
    "\n",
    "Recent research has shown that in the limit of infinite width, neural networks behave like kernel methods with a specific kernel called the Neural Tangent Kernel.\n",
    "\n",
    "**Intuition**: As the number of neurons approaches infinity, the network's behavior becomes more predictable and can be characterized by a kernel function.\n",
    "\n",
    "#### Practical Implications\n",
    "\n",
    "1. **Understanding**: NTK helps understand why neural networks work\n",
    "2. **Design**: Can guide architecture design\n",
    "3. **Training**: Provides insights into optimization behavior\n",
    "4. **Generalization**: Helps understand generalization properties\n",
    "\n",
    "---\n",
    "\n",
    "*This concludes our exploration of neural network fundamentals. In the next sections, we will dive deeper into specific architectures, training algorithms, and practical implementation details.*\n",
    "\n",
    "## From Neural Network Fundamentals to Modular Design\n",
    "\n",
    "We've now explored the fundamental building blocks of neural networks - from individual neurons with their activation functions to multi-layer architectures that can learn complex patterns. We've seen how the mathematical principles of non-linear transformations and function composition translate into concrete neural network designs.\n",
    "\n",
    "However, as neural networks become more complex and are applied to increasingly sophisticated problems, we need to move beyond basic architectures to **modular design principles**. Modern deep learning systems are built using reusable components that can be composed to create complex architectures efficiently.\n",
    "\n",
    "This motivates our exploration of **neural network modules** - the building blocks that enable us to construct sophisticated architectures systematically. We'll see how common patterns (like fully connected layers, convolutional layers, and attention mechanisms) can be implemented as reusable modules that can be combined in various ways.\n",
    "\n",
    "The transition from neural network fundamentals to modular design represents the bridge from understanding basic architectures to building practical, scalable systems - taking our knowledge of how neural networks work and turning it into a systematic approach for constructing complex models.\n",
    "\n",
    "In the next section, we'll explore how to design and implement neural network modules, how to compose them into larger architectures, and how this modular approach enables both flexibility and efficiency in deep learning systems.\n",
    "\n",
    "---\n",
    "\n",
    "**Previous: [Non-Linear Models](01_non-linear_models.md)** - Understand the mathematical foundations of deep learning and non-linear models.\n",
    "\n",
    "**Next: [Neural Network Modules](03_modules.md)** - Learn how to design and implement modular neural network components.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
